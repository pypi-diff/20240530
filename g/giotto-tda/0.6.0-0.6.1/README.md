# Comparing `tmp/giotto_tda-0.6.0-cp39-cp39-win_amd64.whl.zip` & `tmp/giotto_tda-0.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,145 +1,114 @@
-Zip file size: 1349790 bytes, number of entries: 143
--rw-rw-rw-  2.0 fat      387 b- defN 22-Aug-26 23:10 giotto_tda-0.6.0.dist-info/DELVEWHEEL
--rw-rw-rw-  2.0 fat    35868 b- defN 22-Aug-26 23:10 giotto_tda-0.6.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     8614 b- defN 22-Aug-26 23:10 giotto_tda-0.6.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat    13199 b- defN 22-Aug-26 23:10 giotto_tda-0.6.0.dist-info/RECORD
--rw-rw-rw-  2.0 fat        5 b- defN 22-Aug-26 23:10 giotto_tda-0.6.0.dist-info/top_level.txt
--rw-rw-rw-  2.0 fat      100 b- defN 22-Aug-26 23:10 giotto_tda-0.6.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       29 b- defN 22-Aug-26 23:10 giotto_tda.libs/.load-order-giotto_tda-0.6.0
--rw-rw-rw-  2.0 fat   316848 b- defN 22-Jan-05 15:34 giotto_tda.libs/concrt140.dll
--rw-rw-rw-  2.0 fat   620408 b- defN 21-Jul-21 16:09 giotto_tda.libs/msvcp140.dll
--rw-rw-rw-  2.0 fat     4370 b- defN 22-Aug-26 23:10 gtda/base.py
--rw-rw-rw-  2.0 fat    19824 b- defN 22-Aug-26 23:10 gtda/pipeline.py
--rw-rw-rw-  2.0 fat      611 b- defN 22-Aug-26 23:10 gtda/_version.py
--rw-rw-rw-  2.0 fat     1824 b- defN 22-Aug-26 23:10 gtda/__init__.py
--rw-rw-rw-  2.0 fat    10994 b- defN 22-Aug-26 23:10 gtda/curves/features.py
--rw-rw-rw-  2.0 fat     6993 b- defN 22-Aug-26 23:10 gtda/curves/preprocessing.py
--rw-rw-rw-  2.0 fat     1537 b- defN 22-Aug-26 23:10 gtda/curves/_functions.py
--rw-rw-rw-  2.0 fat      231 b- defN 22-Aug-26 23:10 gtda/curves/__init__.py
--rw-rw-rw-  2.0 fat     9177 b- defN 22-Aug-26 23:10 gtda/curves/tests/test_features.py
--rw-rw-rw-  2.0 fat     1814 b- defN 22-Aug-26 23:10 gtda/curves/tests/test_preprocessing.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/curves/tests/__init__.py
--rw-rw-rw-  2.0 fat    10883 b- defN 22-Aug-26 23:10 gtda/diagrams/distance.py
--rw-rw-rw-  2.0 fat    30266 b- defN 22-Aug-26 23:10 gtda/diagrams/features.py
--rw-rw-rw-  2.0 fat    19843 b- defN 22-Aug-26 23:10 gtda/diagrams/preprocessing.py
--rw-rw-rw-  2.0 fat    49932 b- defN 22-Aug-26 23:10 gtda/diagrams/representations.py
--rw-rw-rw-  2.0 fat      972 b- defN 22-Aug-26 23:10 gtda/diagrams/_features.py
--rw-rw-rw-  2.0 fat    17874 b- defN 22-Aug-26 23:10 gtda/diagrams/_metrics.py
--rw-rw-rw-  2.0 fat     8014 b- defN 22-Aug-26 23:10 gtda/diagrams/_utils.py
--rw-rw-rw-  2.0 fat      797 b- defN 22-Aug-26 23:10 gtda/diagrams/__init__.py
--rw-rw-rw-  2.0 fat    11280 b- defN 22-Aug-26 23:10 gtda/diagrams/tests/test_distance.py
--rw-rw-rw-  2.0 fat    13707 b- defN 22-Aug-26 23:10 gtda/diagrams/tests/test_features_representations.py
--rw-rw-rw-  2.0 fat    11549 b- defN 22-Aug-26 23:10 gtda/diagrams/tests/test_preprocessing.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/diagrams/tests/__init__.py
--rw-rw-rw-  2.0 fat      634 b- defN 22-Aug-26 23:10 gtda/externals/__init__.py
--rw-rw-rw-  2.0 fat   156160 b- defN 22-Aug-26 23:10 gtda/externals/modules/gtda_bottleneck.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   143360 b- defN 22-Aug-26 23:10 gtda/externals/modules/gtda_cech_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   148480 b- defN 22-Aug-26 23:10 gtda/externals/modules/gtda_cubical_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   178176 b- defN 22-Aug-26 23:10 gtda/externals/modules/gtda_periodic_cubical_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   187904 b- defN 22-Aug-26 23:10 gtda/externals/modules/gtda_persistent_cohomology.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   212992 b- defN 22-Aug-26 23:10 gtda/externals/modules/gtda_simplex_tree.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   137216 b- defN 22-Aug-26 23:10 gtda/externals/modules/gtda_sparse_rips_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   123392 b- defN 22-Aug-26 23:10 gtda/externals/modules/gtda_strong_witness_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   222208 b- defN 22-Aug-26 23:10 gtda/externals/modules/gtda_wasserstein.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   121856 b- defN 22-Aug-26 23:10 gtda/externals/modules/gtda_witness_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     1312 b- defN 22-Aug-26 23:10 gtda/externals/python/cech_complex_interface.py
--rw-rw-rw-  2.0 fat     6752 b- defN 22-Aug-26 23:10 gtda/externals/python/cubical_complex_interface.py
--rw-rw-rw-  2.0 fat     6797 b- defN 22-Aug-26 23:10 gtda/externals/python/periodic_cubical_complex_interface.py
--rw-rw-rw-  2.0 fat     4226 b- defN 22-Aug-26 23:10 gtda/externals/python/rips_complex_interface.py
--rw-rw-rw-  2.0 fat    18497 b- defN 22-Aug-26 23:10 gtda/externals/python/simplex_tree_interface.py
--rw-rw-rw-  2.0 fat     2011 b- defN 22-Aug-26 23:10 gtda/externals/python/strong_witness_complex_interface.py
--rw-rw-rw-  2.0 fat     1949 b- defN 22-Aug-26 23:10 gtda/externals/python/witness_complex_interface.py
--rw-rw-rw-  2.0 fat      416 b- defN 22-Aug-26 23:10 gtda/externals/python/__init__.py
--rw-rw-rw-  2.0 fat     4537 b- defN 22-Aug-26 23:10 gtda/externals/python/tests/test_bottleneck.py
--rw-rw-rw-  2.0 fat     2048 b- defN 22-Aug-26 23:10 gtda/externals/python/tests/test_cech_complex.py
--rw-rw-rw-  2.0 fat     1792 b- defN 22-Aug-26 23:10 gtda/externals/python/tests/test_periodic_cubical_complex.py
--rw-rw-rw-  2.0 fat     3032 b- defN 22-Aug-26 23:10 gtda/externals/python/tests/test_rips_complex.py
--rw-rw-rw-  2.0 fat     6412 b- defN 22-Aug-26 23:10 gtda/externals/python/tests/test_wasserstein.py
--rw-rw-rw-  2.0 fat     1953 b- defN 22-Aug-26 23:10 gtda/externals/python/tests/test_witness.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/externals/python/tests/__init__.py
--rw-rw-rw-  2.0 fat     8245 b- defN 22-Aug-26 23:10 gtda/graphs/geodesic_distance.py
--rw-rw-rw-  2.0 fat     6294 b- defN 22-Aug-26 23:10 gtda/graphs/kneighbors.py
--rw-rw-rw-  2.0 fat     7347 b- defN 22-Aug-26 23:10 gtda/graphs/transition.py
--rw-rw-rw-  2.0 fat      356 b- defN 22-Aug-26 23:10 gtda/graphs/__init__.py
--rw-rw-rw-  2.0 fat     4013 b- defN 22-Aug-26 23:10 gtda/graphs/tests/test_geodesic_distance.py
--rw-rw-rw-  2.0 fat     1841 b- defN 22-Aug-26 23:10 gtda/graphs/tests/test_kneighbors.py
--rw-rw-rw-  2.0 fat     1358 b- defN 22-Aug-26 23:10 gtda/graphs/tests/test_transition.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/graphs/tests/__init__.py
--rw-rw-rw-  2.0 fat    10467 b- defN 22-Aug-26 23:10 gtda/homology/cubical.py
--rw-rw-rw-  2.0 fat    79632 b- defN 22-Aug-26 23:10 gtda/homology/simplicial.py
--rw-rw-rw-  2.0 fat     3071 b- defN 22-Aug-26 23:10 gtda/homology/_utils.py
--rw-rw-rw-  2.0 fat      575 b- defN 22-Aug-26 23:10 gtda/homology/__init__.py
--rw-rw-rw-  2.0 fat     1707 b- defN 22-Aug-26 23:10 gtda/homology/tests/test_cubical.py
--rw-rw-rw-  2.0 fat    18876 b- defN 22-Aug-26 23:10 gtda/homology/tests/test_simplicial.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/homology/tests/__init__.py
--rw-rw-rw-  2.0 fat    50033 b- defN 22-Aug-26 23:10 gtda/images/filtrations.py
--rw-rw-rw-  2.0 fat    24087 b- defN 22-Aug-26 23:10 gtda/images/preprocessing.py
--rw-rw-rw-  2.0 fat      695 b- defN 22-Aug-26 23:10 gtda/images/_utils.py
--rw-rw-rw-  2.0 fat      641 b- defN 22-Aug-26 23:10 gtda/images/__init__.py
--rw-rw-rw-  2.0 fat    12452 b- defN 22-Aug-26 23:10 gtda/images/tests/test_filtrations.py
--rw-rw-rw-  2.0 fat     5918 b- defN 22-Aug-26 23:10 gtda/images/tests/test_preprocessing.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/images/tests/__init__.py
--rw-rw-rw-  2.0 fat    20295 b- defN 22-Aug-26 23:10 gtda/local_homology/simplicial.py
--rw-rw-rw-  2.0 fat      289 b- defN 22-Aug-26 23:10 gtda/local_homology/__init__.py
--rw-rw-rw-  2.0 fat     4162 b- defN 22-Aug-26 23:10 gtda/local_homology/tests/test_simplicial.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/local_homology/tests/__init__.py
--rw-rw-rw-  2.0 fat    24482 b- defN 22-Aug-26 23:10 gtda/mapper/cluster.py
--rw-rw-rw-  2.0 fat    22432 b- defN 22-Aug-26 23:10 gtda/mapper/cover.py
--rw-rw-rw-  2.0 fat     7693 b- defN 22-Aug-26 23:10 gtda/mapper/filter.py
--rw-rw-rw-  2.0 fat    10706 b- defN 22-Aug-26 23:10 gtda/mapper/nerve.py
--rw-rw-rw-  2.0 fat    18641 b- defN 22-Aug-26 23:10 gtda/mapper/pipeline.py
--rw-rw-rw-  2.0 fat    32615 b- defN 22-Aug-26 23:10 gtda/mapper/visualization.py
--rw-rw-rw-  2.0 fat     1023 b- defN 22-Aug-26 23:10 gtda/mapper/__init__.py
--rw-rw-rw-  2.0 fat     9978 b- defN 22-Aug-26 23:10 gtda/mapper/tests/test_cluster.py
--rw-rw-rw-  2.0 fat     8773 b- defN 22-Aug-26 23:10 gtda/mapper/tests/test_cover.py
--rw-rw-rw-  2.0 fat     6394 b- defN 22-Aug-26 23:10 gtda/mapper/tests/test_filter.py
--rw-rw-rw-  2.0 fat     5243 b- defN 22-Aug-26 23:10 gtda/mapper/tests/test_nerve.py
--rw-rw-rw-  2.0 fat    17338 b- defN 22-Aug-26 23:10 gtda/mapper/tests/test_visualization.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/mapper/tests/__init__.py
--rw-rw-rw-  2.0 fat     2608 b- defN 22-Aug-26 23:10 gtda/mapper/utils/decorators.py
--rw-rw-rw-  2.0 fat     2176 b- defN 22-Aug-26 23:10 gtda/mapper/utils/pipeline.py
--rw-rw-rw-  2.0 fat     2468 b- defN 22-Aug-26 23:10 gtda/mapper/utils/_cluster.py
--rw-rw-rw-  2.0 fat      575 b- defN 22-Aug-26 23:10 gtda/mapper/utils/_cover.py
--rw-rw-rw-  2.0 fat     1705 b- defN 22-Aug-26 23:10 gtda/mapper/utils/_list_feature_union.py
--rw-rw-rw-  2.0 fat     1057 b- defN 22-Aug-26 23:10 gtda/mapper/utils/_logging.py
--rw-rw-rw-  2.0 fat    21555 b- defN 22-Aug-26 23:10 gtda/mapper/utils/_visualization.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/mapper/utils/__init__.py
--rw-rw-rw-  2.0 fat     7091 b- defN 22-Aug-26 23:10 gtda/metaestimators/collection_transformer.py
--rw-rw-rw-  2.0 fat      245 b- defN 22-Aug-26 23:10 gtda/metaestimators/__init__.py
--rw-rw-rw-  2.0 fat     2059 b- defN 22-Aug-26 23:10 gtda/metaestimators/tests/test_collection_transformer.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/metaestimators/tests/__init__.py
--rw-rw-rw-  2.0 fat     7184 b- defN 22-Aug-26 23:10 gtda/plotting/diagram_representations.py
--rw-rw-rw-  2.0 fat     2438 b- defN 22-Aug-26 23:10 gtda/plotting/images.py
--rw-rw-rw-  2.0 fat     5317 b- defN 22-Aug-26 23:10 gtda/plotting/persistence_diagrams.py
--rw-rw-rw-  2.0 fat     4634 b- defN 22-Aug-26 23:10 gtda/plotting/point_clouds.py
--rw-rw-rw-  2.0 fat      455 b- defN 22-Aug-26 23:10 gtda/plotting/__init__.py
--rw-rw-rw-  2.0 fat     2745 b- defN 22-Aug-26 23:10 gtda/plotting/tests/test_diagram_representations.py
--rw-rw-rw-  2.0 fat      337 b- defN 22-Aug-26 23:10 gtda/plotting/tests/test_persistence_diagrams.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/plotting/tests/__init__.py
--rw-rw-rw-  2.0 fat    16778 b- defN 22-Aug-26 23:10 gtda/point_clouds/rescaling.py
--rw-rw-rw-  2.0 fat      284 b- defN 22-Aug-26 23:10 gtda/point_clouds/__init__.py
--rw-rw-rw-  2.0 fat     1428 b- defN 22-Aug-26 23:10 gtda/point_clouds/tests/test_rescaling.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/point_clouds/tests/__init__.py
--rw-rw-rw-  2.0 fat     2608 b- defN 22-Aug-26 23:10 gtda/tests/test_common.py
--rw-rw-rw-  2.0 fat     3940 b- defN 22-Aug-26 23:10 gtda/tests/test_pipeline.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/tests/__init__.py
--rw-rw-rw-  2.0 fat    28320 b- defN 22-Aug-26 23:10 gtda/time_series/embedding.py
--rw-rw-rw-  2.0 fat     3605 b- defN 22-Aug-26 23:10 gtda/time_series/features.py
--rw-rw-rw-  2.0 fat     3794 b- defN 22-Aug-26 23:10 gtda/time_series/multivariate.py
--rw-rw-rw-  2.0 fat     7032 b- defN 22-Aug-26 23:10 gtda/time_series/preprocessing.py
--rw-rw-rw-  2.0 fat     6115 b- defN 22-Aug-26 23:10 gtda/time_series/target.py
--rw-rw-rw-  2.0 fat     3579 b- defN 22-Aug-26 23:10 gtda/time_series/_utils.py
--rw-rw-rw-  2.0 fat      714 b- defN 22-Aug-26 23:10 gtda/time_series/__init__.py
--rw-rw-rw-  2.0 fat    10233 b- defN 22-Aug-26 23:10 gtda/time_series/tests/test_embedding.py
--rw-rw-rw-  2.0 fat     1078 b- defN 22-Aug-26 23:10 gtda/time_series/tests/test_features.py
--rw-rw-rw-  2.0 fat      508 b- defN 22-Aug-26 23:10 gtda/time_series/tests/test_multivariate.py
--rw-rw-rw-  2.0 fat     3549 b- defN 22-Aug-26 23:10 gtda/time_series/tests/test_preprocessing.py
--rw-rw-rw-  2.0 fat     2647 b- defN 22-Aug-26 23:10 gtda/time_series/tests/test_target.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/time_series/tests/__init__.py
--rw-rw-rw-  2.0 fat     6403 b- defN 22-Aug-26 23:10 gtda/utils/intervals.py
--rw-rw-rw-  2.0 fat    14676 b- defN 22-Aug-26 23:10 gtda/utils/validation.py
--rw-rw-rw-  2.0 fat     4058 b- defN 22-Aug-26 23:10 gtda/utils/_docs.py
--rw-rw-rw-  2.0 fat      288 b- defN 22-Aug-26 23:10 gtda/utils/__init__.py
--rw-rw-rw-  2.0 fat    11564 b- defN 22-Aug-26 23:10 gtda/utils/tests/test_validation.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-26 23:10 gtda/utils/tests/__init__.py
-143 files, 3547584 bytes uncompressed, 1328950 bytes compressed:  62.5%
+Zip file size: 1357802 bytes, number of entries: 112
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 giotto_tda-0.6.1.dist-info/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 giotto_tda.libs/
+-rw-r--r--  2.0 unx      148 b- defN 24-May-30 01:48 giotto_tda-0.6.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx    35192 b- defN 24-May-30 01:48 giotto_tda-0.6.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx        5 b- defN 24-May-30 01:48 giotto_tda-0.6.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     8482 b- defN 24-May-30 01:48 giotto_tda-0.6.1.dist-info/RECORD
+-rw-r--r--  2.0 unx     8697 b- defN 24-May-30 01:48 giotto_tda-0.6.1.dist-info/METADATA
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/images/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/plotting/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/curves/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/diagrams/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/local_homology/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/time_series/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/point_clouds/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/utils/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/graphs/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/mapper/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/externals/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/homology/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/metaestimators/
+-rw-r--r--  2.0 unx      288 b- defN 24-May-30 01:48 gtda/__init__.py
+-rw-r--r--  2.0 unx    19427 b- defN 24-May-30 01:48 gtda/pipeline.py
+-rw-r--r--  2.0 unx     4226 b- defN 24-May-30 01:48 gtda/base.py
+-rw-r--r--  2.0 unx      589 b- defN 24-May-30 01:48 gtda/_version.py
+-rw-r--r--  2.0 unx      672 b- defN 24-May-30 01:48 gtda/images/_utils.py
+-rw-r--r--  2.0 unx      620 b- defN 24-May-30 01:48 gtda/images/__init__.py
+-rw-r--r--  2.0 unx    48732 b- defN 24-May-30 01:48 gtda/images/filtrations.py
+-rw-r--r--  2.0 unx    23427 b- defN 24-May-30 01:48 gtda/images/preprocessing.py
+-rw-r--r--  2.0 unx     6997 b- defN 24-May-30 01:48 gtda/plotting/diagram_representations.py
+-rw-r--r--  2.0 unx      440 b- defN 24-May-30 01:48 gtda/plotting/__init__.py
+-rw-r--r--  2.0 unx     4499 b- defN 24-May-30 01:48 gtda/plotting/point_clouds.py
+-rw-r--r--  2.0 unx     2376 b- defN 24-May-30 01:48 gtda/plotting/images.py
+-rw-r--r--  2.0 unx     5172 b- defN 24-May-30 01:48 gtda/plotting/persistence_diagrams.py
+-rw-r--r--  2.0 unx      221 b- defN 24-May-30 01:48 gtda/curves/__init__.py
+-rw-r--r--  2.0 unx    10716 b- defN 24-May-30 01:48 gtda/curves/features.py
+-rw-r--r--  2.0 unx     1485 b- defN 24-May-30 01:48 gtda/curves/_functions.py
+-rw-r--r--  2.0 unx     6794 b- defN 24-May-30 01:48 gtda/curves/preprocessing.py
+-rw-r--r--  2.0 unx     7828 b- defN 24-May-30 01:48 gtda/diagrams/_utils.py
+-rw-r--r--  2.0 unx      930 b- defN 24-May-30 01:48 gtda/diagrams/_features.py
+-rw-r--r--  2.0 unx      771 b- defN 24-May-30 01:48 gtda/diagrams/__init__.py
+-rw-r--r--  2.0 unx    10641 b- defN 24-May-30 01:48 gtda/diagrams/distance.py
+-rw-r--r--  2.0 unx    29547 b- defN 24-May-30 01:48 gtda/diagrams/features.py
+-rw-r--r--  2.0 unx    17423 b- defN 24-May-30 01:48 gtda/diagrams/_metrics.py
+-rw-r--r--  2.0 unx    48712 b- defN 24-May-30 01:48 gtda/diagrams/representations.py
+-rw-r--r--  2.0 unx    19315 b- defN 24-May-30 01:48 gtda/diagrams/preprocessing.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/local_homology/tests/
+-rw-r--r--  2.0 unx      278 b- defN 24-May-30 01:48 gtda/local_homology/__init__.py
+-rw-r--r--  2.0 unx    19829 b- defN 24-May-30 01:48 gtda/local_homology/simplicial.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-30 01:48 gtda/local_homology/tests/__init__.py
+-rw-r--r--  2.0 unx     4053 b- defN 24-May-30 01:48 gtda/local_homology/tests/test_simplicial.py
+-rw-r--r--  2.0 unx     3490 b- defN 24-May-30 01:48 gtda/time_series/_utils.py
+-rw-r--r--  2.0 unx      692 b- defN 24-May-30 01:48 gtda/time_series/__init__.py
+-rw-r--r--  2.0 unx     3495 b- defN 24-May-30 01:48 gtda/time_series/features.py
+-rw-r--r--  2.0 unx    27566 b- defN 24-May-30 01:48 gtda/time_series/embedding.py
+-rw-r--r--  2.0 unx     3685 b- defN 24-May-30 01:48 gtda/time_series/multivariate.py
+-rw-r--r--  2.0 unx     5920 b- defN 24-May-30 01:48 gtda/time_series/target.py
+-rw-r--r--  2.0 unx     6787 b- defN 24-May-30 01:48 gtda/time_series/preprocessing.py
+-rw-r--r--  2.0 unx    16345 b- defN 24-May-30 01:48 gtda/point_clouds/rescaling.py
+-rw-r--r--  2.0 unx      274 b- defN 24-May-30 01:48 gtda/point_clouds/__init__.py
+-rw-r--r--  2.0 unx      277 b- defN 24-May-30 01:48 gtda/utils/__init__.py
+-rw-r--r--  2.0 unx     3944 b- defN 24-May-30 01:48 gtda/utils/_docs.py
+-rw-r--r--  2.0 unx    14321 b- defN 24-May-30 01:48 gtda/utils/validation.py
+-rw-r--r--  2.0 unx     6210 b- defN 24-May-30 01:48 gtda/utils/intervals.py
+-rw-r--r--  2.0 unx     8025 b- defN 24-May-30 01:48 gtda/graphs/geodesic_distance.py
+-rw-r--r--  2.0 unx      343 b- defN 24-May-30 01:48 gtda/graphs/__init__.py
+-rw-r--r--  2.0 unx     7143 b- defN 24-May-30 01:48 gtda/graphs/transition.py
+-rw-r--r--  2.0 unx     6132 b- defN 24-May-30 01:48 gtda/graphs/kneighbors.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/mapper/utils/
+-rw-r--r--  2.0 unx      993 b- defN 24-May-30 01:48 gtda/mapper/__init__.py
+-rw-r--r--  2.0 unx    18212 b- defN 24-May-30 01:48 gtda/mapper/pipeline.py
+-rw-r--r--  2.0 unx    10462 b- defN 24-May-30 01:48 gtda/mapper/nerve.py
+-rw-r--r--  2.0 unx    21871 b- defN 24-May-30 01:48 gtda/mapper/cover.py
+-rw-r--r--  2.0 unx    23879 b- defN 24-May-30 01:48 gtda/mapper/cluster.py
+-rw-r--r--  2.0 unx    31880 b- defN 24-May-30 01:48 gtda/mapper/visualization.py
+-rw-r--r--  2.0 unx     7445 b- defN 24-May-30 01:48 gtda/mapper/filter.py
+-rw-r--r--  2.0 unx     1655 b- defN 24-May-30 01:48 gtda/mapper/utils/_list_feature_union.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-30 01:48 gtda/mapper/utils/__init__.py
+-rw-r--r--  2.0 unx     2397 b- defN 24-May-30 01:48 gtda/mapper/utils/_cluster.py
+-rw-r--r--  2.0 unx     2107 b- defN 24-May-30 01:48 gtda/mapper/utils/pipeline.py
+-rw-r--r--  2.0 unx    21003 b- defN 24-May-30 01:48 gtda/mapper/utils/_visualization.py
+-rw-r--r--  2.0 unx     2536 b- defN 24-May-30 01:48 gtda/mapper/utils/decorators.py
+-rw-r--r--  2.0 unx      559 b- defN 24-May-30 01:48 gtda/mapper/utils/_cover.py
+-rw-r--r--  2.0 unx     1021 b- defN 24-May-30 01:48 gtda/mapper/utils/_logging.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/externals/modules/
+drwxr-xr-x  2.0 unx        0 b- stor 24-May-30 01:48 gtda/externals/python/
+-rw-r--r--  2.0 unx      628 b- defN 24-May-30 01:48 gtda/externals/__init__.py
+-rwxr-xr-x  2.0 unx   305257 b- defN 24-May-30 01:48 gtda/externals/modules/gtda_cubical_complex.cpython-39-x86_64-linux-gnu.so
+-rwxr-xr-x  2.0 unx   358913 b- defN 24-May-30 01:48 gtda/externals/modules/gtda_persistent_cohomology.cpython-39-x86_64-linux-gnu.so
+-rwxr-xr-x  2.0 unx   332177 b- defN 24-May-30 01:48 gtda/externals/modules/gtda_periodic_cubical_complex.cpython-39-x86_64-linux-gnu.so
+-rwxr-xr-x  2.0 unx   277905 b- defN 24-May-30 01:48 gtda/externals/modules/gtda_strong_witness_complex.cpython-39-x86_64-linux-gnu.so
+-rwxr-xr-x  2.0 unx   294225 b- defN 24-May-30 01:48 gtda/externals/modules/gtda_sparse_rips_complex.cpython-39-x86_64-linux-gnu.so
+-rwxr-xr-x  2.0 unx   249752 b- defN 24-May-30 01:48 gtda/externals/modules/gtda_bottleneck.cpython-39-x86_64-linux-gnu.so
+-rwxr-xr-x  2.0 unx   277849 b- defN 24-May-30 01:48 gtda/externals/modules/gtda_witness_complex.cpython-39-x86_64-linux-gnu.so
+-rwxr-xr-x  2.0 unx   344296 b- defN 24-May-30 01:48 gtda/externals/modules/gtda_wasserstein.cpython-39-x86_64-linux-gnu.so
+-rwxr-xr-x  2.0 unx   377281 b- defN 24-May-30 01:48 gtda/externals/modules/gtda_simplex_tree.cpython-39-x86_64-linux-gnu.so
+-rwxr-xr-x  2.0 unx   298561 b- defN 24-May-30 01:48 gtda/externals/modules/gtda_cech_complex.cpython-39-x86_64-linux-gnu.so
+-rw-r--r--  2.0 unx      409 b- defN 24-May-30 01:48 gtda/externals/python/__init__.py
+-rw-r--r--  2.0 unx     6595 b- defN 24-May-30 01:48 gtda/externals/python/cubical_complex_interface.py
+-rw-r--r--  2.0 unx     1958 b- defN 24-May-30 01:48 gtda/externals/python/strong_witness_complex_interface.py
+-rw-r--r--  2.0 unx    18087 b- defN 24-May-30 01:48 gtda/externals/python/simplex_tree_interface.py
+-rw-r--r--  2.0 unx     6641 b- defN 24-May-30 01:48 gtda/externals/python/periodic_cubical_complex_interface.py
+-rw-r--r--  2.0 unx     1278 b- defN 24-May-30 01:48 gtda/externals/python/cech_complex_interface.py
+-rw-r--r--  2.0 unx     4126 b- defN 24-May-30 01:48 gtda/externals/python/rips_complex_interface.py
+-rw-r--r--  2.0 unx     1896 b- defN 24-May-30 01:48 gtda/externals/python/witness_complex_interface.py
+-rw-r--r--  2.0 unx     3006 b- defN 24-May-30 01:48 gtda/homology/_utils.py
+-rw-r--r--  2.0 unx      557 b- defN 24-May-30 01:48 gtda/homology/__init__.py
+-rw-r--r--  2.0 unx    10208 b- defN 24-May-30 01:48 gtda/homology/cubical.py
+-rw-r--r--  2.0 unx    77843 b- defN 24-May-30 01:48 gtda/homology/simplicial.py
+-rw-r--r--  2.0 unx      237 b- defN 24-May-30 01:48 gtda/metaestimators/__init__.py
+-rw-r--r--  2.0 unx     7023 b- defN 24-May-30 01:48 gtda/metaestimators/collection_transformer.py
+-rwxr-xr-x  2.0 unx   168193 b- defN 24-May-30 01:48 giotto_tda.libs/libgomp-a34b3233.so.1.0.0
+112 files, 4034102 bytes uncompressed, 1342244 bytes compressed:  66.7%
```

## zipnote {}

```diff
@@ -1,430 +1,337 @@
-Filename: giotto_tda-0.6.0.dist-info/DELVEWHEEL
+Filename: giotto_tda-0.6.1.dist-info/
 Comment: 
 
-Filename: giotto_tda-0.6.0.dist-info/LICENSE
+Filename: gtda/
 Comment: 
 
-Filename: giotto_tda-0.6.0.dist-info/METADATA
+Filename: giotto_tda.libs/
 Comment: 
 
-Filename: giotto_tda-0.6.0.dist-info/RECORD
+Filename: giotto_tda-0.6.1.dist-info/WHEEL
 Comment: 
 
-Filename: giotto_tda-0.6.0.dist-info/top_level.txt
+Filename: giotto_tda-0.6.1.dist-info/LICENSE
 Comment: 
 
-Filename: giotto_tda-0.6.0.dist-info/WHEEL
+Filename: giotto_tda-0.6.1.dist-info/top_level.txt
 Comment: 
 
-Filename: giotto_tda.libs/.load-order-giotto_tda-0.6.0
+Filename: giotto_tda-0.6.1.dist-info/RECORD
 Comment: 
 
-Filename: giotto_tda.libs/concrt140.dll
+Filename: giotto_tda-0.6.1.dist-info/METADATA
 Comment: 
 
-Filename: giotto_tda.libs/msvcp140.dll
+Filename: gtda/images/
 Comment: 
 
-Filename: gtda/base.py
-Comment: 
-
-Filename: gtda/pipeline.py
-Comment: 
-
-Filename: gtda/_version.py
-Comment: 
-
-Filename: gtda/__init__.py
-Comment: 
-
-Filename: gtda/curves/features.py
+Filename: gtda/plotting/
 Comment: 
 
-Filename: gtda/curves/preprocessing.py
+Filename: gtda/curves/
 Comment: 
 
-Filename: gtda/curves/_functions.py
+Filename: gtda/diagrams/
 Comment: 
 
-Filename: gtda/curves/__init__.py
+Filename: gtda/local_homology/
 Comment: 
 
-Filename: gtda/curves/tests/test_features.py
+Filename: gtda/time_series/
 Comment: 
 
-Filename: gtda/curves/tests/test_preprocessing.py
+Filename: gtda/point_clouds/
 Comment: 
 
-Filename: gtda/curves/tests/__init__.py
+Filename: gtda/utils/
 Comment: 
 
-Filename: gtda/diagrams/distance.py
+Filename: gtda/graphs/
 Comment: 
 
-Filename: gtda/diagrams/features.py
+Filename: gtda/mapper/
 Comment: 
 
-Filename: gtda/diagrams/preprocessing.py
+Filename: gtda/externals/
 Comment: 
 
-Filename: gtda/diagrams/representations.py
+Filename: gtda/homology/
 Comment: 
 
-Filename: gtda/diagrams/_features.py
-Comment: 
-
-Filename: gtda/diagrams/_metrics.py
+Filename: gtda/metaestimators/
 Comment: 
 
-Filename: gtda/diagrams/_utils.py
-Comment: 
-
-Filename: gtda/diagrams/__init__.py
-Comment: 
-
-Filename: gtda/diagrams/tests/test_distance.py
-Comment: 
-
-Filename: gtda/diagrams/tests/test_features_representations.py
-Comment: 
-
-Filename: gtda/diagrams/tests/test_preprocessing.py
-Comment: 
-
-Filename: gtda/diagrams/tests/__init__.py
-Comment: 
-
-Filename: gtda/externals/__init__.py
-Comment: 
-
-Filename: gtda/externals/modules/gtda_bottleneck.cp39-win_amd64.pyd
-Comment: 
-
-Filename: gtda/externals/modules/gtda_cech_complex.cp39-win_amd64.pyd
+Filename: gtda/__init__.py
 Comment: 
 
-Filename: gtda/externals/modules/gtda_cubical_complex.cp39-win_amd64.pyd
+Filename: gtda/pipeline.py
 Comment: 
 
-Filename: gtda/externals/modules/gtda_periodic_cubical_complex.cp39-win_amd64.pyd
+Filename: gtda/base.py
 Comment: 
 
-Filename: gtda/externals/modules/gtda_persistent_cohomology.cp39-win_amd64.pyd
+Filename: gtda/_version.py
 Comment: 
 
-Filename: gtda/externals/modules/gtda_simplex_tree.cp39-win_amd64.pyd
+Filename: gtda/images/_utils.py
 Comment: 
 
-Filename: gtda/externals/modules/gtda_sparse_rips_complex.cp39-win_amd64.pyd
+Filename: gtda/images/__init__.py
 Comment: 
 
-Filename: gtda/externals/modules/gtda_strong_witness_complex.cp39-win_amd64.pyd
+Filename: gtda/images/filtrations.py
 Comment: 
 
-Filename: gtda/externals/modules/gtda_wasserstein.cp39-win_amd64.pyd
+Filename: gtda/images/preprocessing.py
 Comment: 
 
-Filename: gtda/externals/modules/gtda_witness_complex.cp39-win_amd64.pyd
+Filename: gtda/plotting/diagram_representations.py
 Comment: 
 
-Filename: gtda/externals/python/cech_complex_interface.py
+Filename: gtda/plotting/__init__.py
 Comment: 
 
-Filename: gtda/externals/python/cubical_complex_interface.py
+Filename: gtda/plotting/point_clouds.py
 Comment: 
 
-Filename: gtda/externals/python/periodic_cubical_complex_interface.py
+Filename: gtda/plotting/images.py
 Comment: 
 
-Filename: gtda/externals/python/rips_complex_interface.py
+Filename: gtda/plotting/persistence_diagrams.py
 Comment: 
 
-Filename: gtda/externals/python/simplex_tree_interface.py
+Filename: gtda/curves/__init__.py
 Comment: 
 
-Filename: gtda/externals/python/strong_witness_complex_interface.py
+Filename: gtda/curves/features.py
 Comment: 
 
-Filename: gtda/externals/python/witness_complex_interface.py
+Filename: gtda/curves/_functions.py
 Comment: 
 
-Filename: gtda/externals/python/__init__.py
+Filename: gtda/curves/preprocessing.py
 Comment: 
 
-Filename: gtda/externals/python/tests/test_bottleneck.py
+Filename: gtda/diagrams/_utils.py
 Comment: 
 
-Filename: gtda/externals/python/tests/test_cech_complex.py
+Filename: gtda/diagrams/_features.py
 Comment: 
 
-Filename: gtda/externals/python/tests/test_periodic_cubical_complex.py
+Filename: gtda/diagrams/__init__.py
 Comment: 
 
-Filename: gtda/externals/python/tests/test_rips_complex.py
+Filename: gtda/diagrams/distance.py
 Comment: 
 
-Filename: gtda/externals/python/tests/test_wasserstein.py
+Filename: gtda/diagrams/features.py
 Comment: 
 
-Filename: gtda/externals/python/tests/test_witness.py
+Filename: gtda/diagrams/_metrics.py
 Comment: 
 
-Filename: gtda/externals/python/tests/__init__.py
+Filename: gtda/diagrams/representations.py
 Comment: 
 
-Filename: gtda/graphs/geodesic_distance.py
+Filename: gtda/diagrams/preprocessing.py
 Comment: 
 
-Filename: gtda/graphs/kneighbors.py
+Filename: gtda/local_homology/tests/
 Comment: 
 
-Filename: gtda/graphs/transition.py
+Filename: gtda/local_homology/__init__.py
 Comment: 
 
-Filename: gtda/graphs/__init__.py
+Filename: gtda/local_homology/simplicial.py
 Comment: 
 
-Filename: gtda/graphs/tests/test_geodesic_distance.py
+Filename: gtda/local_homology/tests/__init__.py
 Comment: 
 
-Filename: gtda/graphs/tests/test_kneighbors.py
+Filename: gtda/local_homology/tests/test_simplicial.py
 Comment: 
 
-Filename: gtda/graphs/tests/test_transition.py
+Filename: gtda/time_series/_utils.py
 Comment: 
 
-Filename: gtda/graphs/tests/__init__.py
+Filename: gtda/time_series/__init__.py
 Comment: 
 
-Filename: gtda/homology/cubical.py
+Filename: gtda/time_series/features.py
 Comment: 
 
-Filename: gtda/homology/simplicial.py
+Filename: gtda/time_series/embedding.py
 Comment: 
 
-Filename: gtda/homology/_utils.py
+Filename: gtda/time_series/multivariate.py
 Comment: 
 
-Filename: gtda/homology/__init__.py
+Filename: gtda/time_series/target.py
 Comment: 
 
-Filename: gtda/homology/tests/test_cubical.py
+Filename: gtda/time_series/preprocessing.py
 Comment: 
 
-Filename: gtda/homology/tests/test_simplicial.py
+Filename: gtda/point_clouds/rescaling.py
 Comment: 
 
-Filename: gtda/homology/tests/__init__.py
+Filename: gtda/point_clouds/__init__.py
 Comment: 
 
-Filename: gtda/images/filtrations.py
+Filename: gtda/utils/__init__.py
 Comment: 
 
-Filename: gtda/images/preprocessing.py
+Filename: gtda/utils/_docs.py
 Comment: 
 
-Filename: gtda/images/_utils.py
+Filename: gtda/utils/validation.py
 Comment: 
 
-Filename: gtda/images/__init__.py
+Filename: gtda/utils/intervals.py
 Comment: 
 
-Filename: gtda/images/tests/test_filtrations.py
+Filename: gtda/graphs/geodesic_distance.py
 Comment: 
 
-Filename: gtda/images/tests/test_preprocessing.py
+Filename: gtda/graphs/__init__.py
 Comment: 
 
-Filename: gtda/images/tests/__init__.py
+Filename: gtda/graphs/transition.py
 Comment: 
 
-Filename: gtda/local_homology/simplicial.py
+Filename: gtda/graphs/kneighbors.py
 Comment: 
 
-Filename: gtda/local_homology/__init__.py
+Filename: gtda/mapper/utils/
 Comment: 
 
-Filename: gtda/local_homology/tests/test_simplicial.py
+Filename: gtda/mapper/__init__.py
 Comment: 
 
-Filename: gtda/local_homology/tests/__init__.py
+Filename: gtda/mapper/pipeline.py
 Comment: 
 
-Filename: gtda/mapper/cluster.py
+Filename: gtda/mapper/nerve.py
 Comment: 
 
 Filename: gtda/mapper/cover.py
 Comment: 
 
-Filename: gtda/mapper/filter.py
-Comment: 
-
-Filename: gtda/mapper/nerve.py
-Comment: 
-
-Filename: gtda/mapper/pipeline.py
+Filename: gtda/mapper/cluster.py
 Comment: 
 
 Filename: gtda/mapper/visualization.py
 Comment: 
 
-Filename: gtda/mapper/__init__.py
-Comment: 
-
-Filename: gtda/mapper/tests/test_cluster.py
+Filename: gtda/mapper/filter.py
 Comment: 
 
-Filename: gtda/mapper/tests/test_cover.py
+Filename: gtda/mapper/utils/_list_feature_union.py
 Comment: 
 
-Filename: gtda/mapper/tests/test_filter.py
+Filename: gtda/mapper/utils/__init__.py
 Comment: 
 
-Filename: gtda/mapper/tests/test_nerve.py
+Filename: gtda/mapper/utils/_cluster.py
 Comment: 
 
-Filename: gtda/mapper/tests/test_visualization.py
+Filename: gtda/mapper/utils/pipeline.py
 Comment: 
 
-Filename: gtda/mapper/tests/__init__.py
+Filename: gtda/mapper/utils/_visualization.py
 Comment: 
 
 Filename: gtda/mapper/utils/decorators.py
 Comment: 
 
-Filename: gtda/mapper/utils/pipeline.py
-Comment: 
-
-Filename: gtda/mapper/utils/_cluster.py
-Comment: 
-
 Filename: gtda/mapper/utils/_cover.py
 Comment: 
 
-Filename: gtda/mapper/utils/_list_feature_union.py
-Comment: 
-
 Filename: gtda/mapper/utils/_logging.py
 Comment: 
 
-Filename: gtda/mapper/utils/_visualization.py
-Comment: 
-
-Filename: gtda/mapper/utils/__init__.py
-Comment: 
-
-Filename: gtda/metaestimators/collection_transformer.py
-Comment: 
-
-Filename: gtda/metaestimators/__init__.py
-Comment: 
-
-Filename: gtda/metaestimators/tests/test_collection_transformer.py
-Comment: 
-
-Filename: gtda/metaestimators/tests/__init__.py
-Comment: 
-
-Filename: gtda/plotting/diagram_representations.py
-Comment: 
-
-Filename: gtda/plotting/images.py
-Comment: 
-
-Filename: gtda/plotting/persistence_diagrams.py
-Comment: 
-
-Filename: gtda/plotting/point_clouds.py
+Filename: gtda/externals/modules/
 Comment: 
 
-Filename: gtda/plotting/__init__.py
+Filename: gtda/externals/python/
 Comment: 
 
-Filename: gtda/plotting/tests/test_diagram_representations.py
+Filename: gtda/externals/__init__.py
 Comment: 
 
-Filename: gtda/plotting/tests/test_persistence_diagrams.py
+Filename: gtda/externals/modules/gtda_cubical_complex.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
-Filename: gtda/plotting/tests/__init__.py
+Filename: gtda/externals/modules/gtda_persistent_cohomology.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
-Filename: gtda/point_clouds/rescaling.py
+Filename: gtda/externals/modules/gtda_periodic_cubical_complex.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
-Filename: gtda/point_clouds/__init__.py
+Filename: gtda/externals/modules/gtda_strong_witness_complex.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
-Filename: gtda/point_clouds/tests/test_rescaling.py
+Filename: gtda/externals/modules/gtda_sparse_rips_complex.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
-Filename: gtda/point_clouds/tests/__init__.py
+Filename: gtda/externals/modules/gtda_bottleneck.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
-Filename: gtda/tests/test_common.py
+Filename: gtda/externals/modules/gtda_witness_complex.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
-Filename: gtda/tests/test_pipeline.py
+Filename: gtda/externals/modules/gtda_wasserstein.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
-Filename: gtda/tests/__init__.py
+Filename: gtda/externals/modules/gtda_simplex_tree.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
-Filename: gtda/time_series/embedding.py
-Comment: 
-
-Filename: gtda/time_series/features.py
+Filename: gtda/externals/modules/gtda_cech_complex.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
-Filename: gtda/time_series/multivariate.py
-Comment: 
-
-Filename: gtda/time_series/preprocessing.py
-Comment: 
-
-Filename: gtda/time_series/target.py
+Filename: gtda/externals/python/__init__.py
 Comment: 
 
-Filename: gtda/time_series/_utils.py
+Filename: gtda/externals/python/cubical_complex_interface.py
 Comment: 
 
-Filename: gtda/time_series/__init__.py
+Filename: gtda/externals/python/strong_witness_complex_interface.py
 Comment: 
 
-Filename: gtda/time_series/tests/test_embedding.py
+Filename: gtda/externals/python/simplex_tree_interface.py
 Comment: 
 
-Filename: gtda/time_series/tests/test_features.py
+Filename: gtda/externals/python/periodic_cubical_complex_interface.py
 Comment: 
 
-Filename: gtda/time_series/tests/test_multivariate.py
+Filename: gtda/externals/python/cech_complex_interface.py
 Comment: 
 
-Filename: gtda/time_series/tests/test_preprocessing.py
+Filename: gtda/externals/python/rips_complex_interface.py
 Comment: 
 
-Filename: gtda/time_series/tests/test_target.py
+Filename: gtda/externals/python/witness_complex_interface.py
 Comment: 
 
-Filename: gtda/time_series/tests/__init__.py
+Filename: gtda/homology/_utils.py
 Comment: 
 
-Filename: gtda/utils/intervals.py
+Filename: gtda/homology/__init__.py
 Comment: 
 
-Filename: gtda/utils/validation.py
+Filename: gtda/homology/cubical.py
 Comment: 
 
-Filename: gtda/utils/_docs.py
+Filename: gtda/homology/simplicial.py
 Comment: 
 
-Filename: gtda/utils/__init__.py
+Filename: gtda/metaestimators/__init__.py
 Comment: 
 
-Filename: gtda/utils/tests/test_validation.py
+Filename: gtda/metaestimators/collection_transformer.py
 Comment: 
 
-Filename: gtda/utils/tests/__init__.py
+Filename: giotto_tda.libs/libgomp-a34b3233.so.1.0.0
 Comment: 
 
 Zip file comment:
```

## filetype from file(1)

```diff
@@ -1 +1 @@
-Zip archive data, at least v2.0 to extract, compression method=deflate
+Zip archive data, at least v2.0 to extract, compression method=store
```

## gtda/base.py

 * *Ordering differences only*

```diff
@@ -1,144 +1,144 @@
-"""Implements a TransformerResamplerMixin for transformers that have a resample
-method and TransformerPlotterMixin for transformers that have a plot method."""
-# License: GNU AGPLv3
-
-
-class TransformerResamplerMixin:
-    """Mixin class for all transformers-resamplers in giotto-tda."""
-
-    _estimator_type = 'transformer_resampler'
-
-    def fit_transform(self, X, y=None, **fit_params):
-        """Fit to data, then transform it.
-
-        Fits transformer to `X` and `y` with optional parameters `fit_params`
-        and returns a transformed version of `X`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, ...)
-            Input data.
-
-        y : None
-            There is no need for a target, yet the pipeline API requires this
-            parameter.
-
-        Returns
-        -------
-        Xt : numpy array of shape (n_samples, ...)
-            Transformed input.
-
-        """
-        # non-optimized default implementation; override when a better
-        # method is possible for a given clustering algorithm
-        if y is None:
-            # fit method of arity 1 (unsupervised transformation)
-            return self.fit(X, **fit_params).transform(X)
-        else:
-            # fit method of arity 2 (supervised transformation)
-            return self.fit(X, y, **fit_params).transform(X, y)
-
-    def transform_resample(self, X, y):
-        """Fit to data, then transform it.
-
-        Fits transformer to `X` and `y` with optional parameters `fit_params`
-        and returns a transformed version of `X`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, ...)
-            Input data.
-
-        y : ndarray of shape (n_samples,)
-            Target data.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, ...)
-            Transformed input.
-
-        yr : ndarray of shape (n_samples, ...)
-            Resampled target.
-
-        """
-        return self.transform(X), self.resample(y, X)
-
-    def fit_transform_resample(self, X, y, **fit_params):
-        """Fit to data, then transform the input and resample the target.
-        Fits transformer to X and y with optional parameters fit_params
-        and returns a transformed version of X ans a resampled version of y.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, ...)
-            Input data.
-
-        y : ndarray of shape (n_samples,)
-            Target data.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, ...)
-            Transformed input.
-
-        yr : ndarray of shape (n_samples, ...)
-            Resampled target.
-
-        """
-        return self.fit(X, y, **fit_params).transform_resample(X, y)
-
-
-class PlotterMixin:
-    """Mixin class for all plotters in giotto-tda."""
-
-    def fit_transform_plot(self, X, y=None, sample=0, **plot_params):
-        """Fit to data, then apply :meth:`transform_plot`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, ...)
-            Input data.
-
-        y : ndarray of shape (n_samples,) or None
-            Target values for supervised problems.
-
-        sample : int
-            Sample to be plotted.
-
-        **plot_params
-            Optional plotting parameters.
-
-        Returns
-        -------
-        Xt : ndarray of shape (1, ...)
-            Transformed one-sample slice from the input.
-
-        """
-        self.fit(X, y)
-        Xt = self.transform_plot(X, sample=sample, **plot_params)
-        return Xt
-
-    def transform_plot(self, X, sample=0, **plot_params):
-        """Take a one-sample slice from the input collection and transform it.
-        Before returning the transformed object, plot the transformed sample.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, ...)
-            Input data.
-
-        sample : int
-            Sample to be plotted.
-
-        **plot_params
-            Optional plotting parameters.
-
-        Returns
-        -------
-        Xt : ndarray of shape (1, ...)
-            Transformed one-sample slice from the input.
-
-        """
-        Xt = self.transform(X[sample:sample+1])
-        self.plot({sample: Xt[0]}, sample=sample, **plot_params).show()
-        return Xt
+"""Implements a TransformerResamplerMixin for transformers that have a resample
+method and TransformerPlotterMixin for transformers that have a plot method."""
+# License: GNU AGPLv3
+
+
+class TransformerResamplerMixin:
+    """Mixin class for all transformers-resamplers in giotto-tda."""
+
+    _estimator_type = 'transformer_resampler'
+
+    def fit_transform(self, X, y=None, **fit_params):
+        """Fit to data, then transform it.
+
+        Fits transformer to `X` and `y` with optional parameters `fit_params`
+        and returns a transformed version of `X`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, ...)
+            Input data.
+
+        y : None
+            There is no need for a target, yet the pipeline API requires this
+            parameter.
+
+        Returns
+        -------
+        Xt : numpy array of shape (n_samples, ...)
+            Transformed input.
+
+        """
+        # non-optimized default implementation; override when a better
+        # method is possible for a given clustering algorithm
+        if y is None:
+            # fit method of arity 1 (unsupervised transformation)
+            return self.fit(X, **fit_params).transform(X)
+        else:
+            # fit method of arity 2 (supervised transformation)
+            return self.fit(X, y, **fit_params).transform(X, y)
+
+    def transform_resample(self, X, y):
+        """Fit to data, then transform it.
+
+        Fits transformer to `X` and `y` with optional parameters `fit_params`
+        and returns a transformed version of `X`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, ...)
+            Input data.
+
+        y : ndarray of shape (n_samples,)
+            Target data.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, ...)
+            Transformed input.
+
+        yr : ndarray of shape (n_samples, ...)
+            Resampled target.
+
+        """
+        return self.transform(X), self.resample(y, X)
+
+    def fit_transform_resample(self, X, y, **fit_params):
+        """Fit to data, then transform the input and resample the target.
+        Fits transformer to X and y with optional parameters fit_params
+        and returns a transformed version of X ans a resampled version of y.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, ...)
+            Input data.
+
+        y : ndarray of shape (n_samples,)
+            Target data.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, ...)
+            Transformed input.
+
+        yr : ndarray of shape (n_samples, ...)
+            Resampled target.
+
+        """
+        return self.fit(X, y, **fit_params).transform_resample(X, y)
+
+
+class PlotterMixin:
+    """Mixin class for all plotters in giotto-tda."""
+
+    def fit_transform_plot(self, X, y=None, sample=0, **plot_params):
+        """Fit to data, then apply :meth:`transform_plot`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, ...)
+            Input data.
+
+        y : ndarray of shape (n_samples,) or None
+            Target values for supervised problems.
+
+        sample : int
+            Sample to be plotted.
+
+        **plot_params
+            Optional plotting parameters.
+
+        Returns
+        -------
+        Xt : ndarray of shape (1, ...)
+            Transformed one-sample slice from the input.
+
+        """
+        self.fit(X, y)
+        Xt = self.transform_plot(X, sample=sample, **plot_params)
+        return Xt
+
+    def transform_plot(self, X, sample=0, **plot_params):
+        """Take a one-sample slice from the input collection and transform it.
+        Before returning the transformed object, plot the transformed sample.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, ...)
+            Input data.
+
+        sample : int
+            Sample to be plotted.
+
+        **plot_params
+            Optional plotting parameters.
+
+        Returns
+        -------
+        Xt : ndarray of shape (1, ...)
+            Transformed one-sample slice from the input.
+
+        """
+        Xt = self.transform(X[sample:sample+1])
+        self.plot({sample: Xt[0]}, sample=sample, **plot_params).show()
+        return Xt
```

## gtda/pipeline.py

```diff
@@ -1,517 +1,523 @@
-"""The module :mod:`gtda.pipeline` extends scikit-learn's module by defining
-Pipelines that include TransformerResamplers."""
-# License: GNU AGPLv3
-
-from sklearn import pipeline
-from sklearn.base import clone
-from sklearn.utils.metaestimators import if_delegate_has_method
-from sklearn.utils.validation import check_memory
-
-__all__ = ['Pipeline', 'make_pipeline']
-
-
-class Pipeline(pipeline.Pipeline):
-    """Pipeline of transforms and resamples with a final estimator.
-
-    Sequentially apply a list of transforms, sampling, and a final estimator.
-    Intermediate steps of the pipeline must be transformers or resamplers,
-    that is, they must implement fit, transform and sample methods.
-    The samplers are only applied during fit.
-    The final estimator only needs to implement fit.
-    The transformers and samplers in the pipeline can be cached using
-    ``memory`` argument.
-
-    The purpose of the pipeline is to assemble several steps that can be
-    cross-validated together while setting different parameters.
-    For this, it enables setting parameters of the various steps using their
-    names and the parameter name separated by a '__', as in the example below.
-    A step's estimator may be replaced entirely by setting the parameter
-    with its name to another estimator, or a transformer removed by setting
-    it to 'passthrough' or ``None``.
-
-    Parameters
-    ----------
-    steps : list
-        List of (name, transform) tuples (implementing
-        fit/transform) that are chained, in the order in which
-        they are chained, with the last object an estimator.
-
-    memory : Instance of joblib.Memory or string, optional (default: ``None``)
-        Used to cache the fitted transformers of the pipeline. By default,
-        no caching is performed. If a string is given, it is the path to
-        the caching directory. Enabling caching triggers a clone of
-        the transformers before fitting. Therefore, the transformer
-        instance given to the pipeline cannot be inspected
-        directly. Use the attribute ``named_steps`` or ``steps`` to
-        inspect estimators within the pipeline. Caching the
-        transformers is advantageous when fitting is time consuming.
-
-
-    Attributes
-    ----------
-    named_steps : dict
-        Read-only attribute to access any step parameter by user given name.
-        Keys are step names and values are steps parameters.
-
-    See also
-    --------
-    make_pipeline : helper function to make pipeline.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> import gtda.time_series as ts
-    >>> import gtda.homology as hl
-    >>> import gtda.diagrams as diag
-    >>> from gtda.pipeline import Pipeline
-    >>> import sklearn.preprocessing as skprep
-    >>>
-    >>> X = np.random.rand(600, 1)
-    >>> n_train, n_test = 400, 200
-    >>>
-    >>> labeller = ts.Labeller(size=6, percentiles=[80],
-    >>>                        n_steps_future=1)
-    >>> X_train = X[:n_train]
-    >>> y_train = X_train
-    >>> X_train, y_train = labeller.fit_transform_resample(X_train, y_train)
-    >>>
-    >>> print(X_train.shape, y_train.shape)
-    (395, 1) (395,)
-    >>> steps = [
-    >>>     ('embedding', ts.SingleTakensEmbedding()),
-    >>>     ('window', ts.SlidingWindow(size=6, stride=1)),
-    >>>     ('diagram', hl.VietorisRipsPersistence()),
-    >>>     ('rescaler', diag.Scaler()),
-    >>>     ('filter', diag.Filtering(epsilon=0.1)),
-    >>>     ('entropy', diag.PersistenceEntropy()),
-    >>>     ('scaling', skprep.MinMaxScaler(copy=True)),
-    >>> ]
-    >>> pipeline = Pipeline(steps)
-    >>>
-    >>> Xt_train, yr_train = pipeline.\\
-    >>>     fit_transform_resample(X_train, y_train)
-    >>>
-    >>> print(X_train_final.shape, y_train_final.shape)
-    (389, 2) (389,)
-    """
-
-    def _fit(self, X, y=None, **fit_params):
-        self.steps = list(self.steps)
-        self._validate_steps()
-        # Setup the memory
-        memory = check_memory(self.memory)
-
-        fit_transform_one_cached = memory.cache(_fit_transform_one)
-        fit_transform_resample_one_cached = memory.cache(
-            _fit_transform_resample_one)
-
-        fit_params_steps = {name: {} for name, step in self.steps
-                            if step is not None}
-        for pname, pval in fit_params.items():
-            step, param = pname.split('__', 1)
-            fit_params_steps[step][param] = pval
-        for step_idx, name, transformer in self._iter(with_final=False):
-            if hasattr(memory, 'location') and (memory.location is None):
-                # joblib >= 0.12. We do not clone when caching is disabled to
-                # preserve backward compatibility
-                cloned_transformer = transformer
-            else:
-                cloned_transformer = clone(transformer)
-            # Fit or load from cache the current transfomer
-            if hasattr(cloned_transformer, "resample") or \
-               hasattr(cloned_transformer, "fit_transform_resample"):
-                if y is None:
-                    X, fitted_transformer = fit_transform_one_cached(
-                        cloned_transformer, None, X, y,
-                        **fit_params_steps[name])
-                else:
-                    X, y, fitted_transformer = \
-                        fit_transform_resample_one_cached(
-                            cloned_transformer, None, X, y,
-                            **fit_params_steps[name])
-            else:
-                X, fitted_transformer = fit_transform_one_cached(
-                    cloned_transformer, None, X, y,
-                    **fit_params_steps[name])
-
-            # Replace the transformer of the step with the fitted
-            # transformer. This is necessary when loading the transformer
-            # from the cache.
-            self.steps[step_idx] = (name, fitted_transformer)
-        if self._final_estimator == 'passthrough':
-            return X, y, {}
-        return X, y, fit_params_steps[self.steps[-1][0]]
-
-    def fit(self, X, y=None, **fit_params):
-        """Fit the model.
-
-        Fit all the transforms/samplers one after the other and
-        transform/sample the data, then fit the transformed/sampled
-        data using the final estimator.
-
-        Parameters
-        ----------
-        X : iterable
-            Training data. Must fulfill input requirements of first step of the
-            pipeline.
-
-        y : iterable or None, default: ``None``
-            Training targets. Must fulfill label requirements for all steps of
-            the pipeline.
-
-        **fit_params : dict of string -> object
-            Parameters passed to the :meth:`fit` method of each step, where
-            each parameter name is prefixed such that parameter ``p`` for step
-            ``s`` has key ``s__p``.
-
-        Returns
-        -------
-        self : Pipeline
-            This estimator
-
-        """
-        Xt, yr, fit_params = self._fit(X, y, **fit_params)
-        if self._final_estimator != 'passthrough':
-            self._final_estimator.fit(Xt, yr, **fit_params)
-        return self
-
-    def fit_transform(self, X, y=None, **fit_params):
-        """Fit the model and transform with the final estimator.
-
-        Fits all the transformers/samplers one after the other and
-        transform/sample the data, then uses fit_transform on
-        transformed data with the final estimator.
-
-        Parameters
-        ----------
-        X : iterable
-            Training data. Must fulfill input requirements of first step of the
-            pipeline.
-
-        y : iterable, default: ``None``
-            Training targets. Must fulfill label requirements for all steps of
-            the pipeline.
-
-        **fit_params : dict of string -> object
-            Parameters passed to the :meth:`fit` method of each step, where
-            each parameter name is prefixed such that parameter ``p`` for step
-            ``s`` has key ``s__p``.
-
-        Returns
-        -------
-        Xt : array-like, shape (n_samples, n_transformed_features)
-            Transformed samples
-
-        """
-        last_step = self._final_estimator
-        Xt, yr, fit_params = self._fit(X, y, **fit_params)
-        if last_step == 'passthrough':
-            return Xt
-        elif hasattr(last_step, 'fit_transform'):
-            return last_step.fit_transform(Xt, yr, **fit_params)
-        else:
-            return last_step.fit(Xt, yr, **fit_params).transform(Xt)
-
-    def fit_transform_resample(self, X, y=None, **fit_params):
-        """Fit the model and sample with the final estimator.
-
-        Fits all the transformers/samplers one after the other and
-        transform/sample the data, then uses fit_resample on transformed
-        data with the final estimator.
-
-        Parameters
-        ----------
-        X : iterable
-            Training data. Must fulfill input requirements of first step of the
-            pipeline.
-
-        y : iterable, default: ``None``
-            Training targets. Must fulfill label requirements for all steps of
-            the pipeline.
-
-        **fit_params : dict of string -> object
-            Parameters passed to the :meth:`fit` method of each step, where
-            each parameter name is prefixed such that parameter ``p`` for step
-            ``s`` has key ``s__p``.
-
-        Returns
-        -------
-        Xt : array-like, shape (n_samples, n_transformed_features)
-            Transformed samples.
-
-        yr : array-like, shape (n_samples, n_transformed_features)
-            Transformed target.
-        """
-        last_step = self._final_estimator
-        Xt, yr, fit_params = self._fit(X, y, **fit_params)
-        if last_step == 'passthrough':
-            return Xt, yr
-        elif hasattr(last_step, 'fit_transform_resample'):
-            return last_step.fit_transform_resample(Xt, yr, **fit_params)
-        elif hasattr(last_step, 'fit_transform'):
-            return last_step.fit_transform(Xt, yr, **fit_params), yr
-
-    @if_delegate_has_method(delegate='_final_estimator')
-    def fit_predict(self, X, y=None, **fit_params):
-        """Applies fit_predict of last step in pipeline after transforms.
-
-        Applies fit_transforms of a pipeline to the data, followed by the
-        fit_predict method of the final estimator in the pipeline. Valid
-        only if the final estimator implements fit_predict.
-
-        Parameters
-        ----------
-        X : iterable
-            Training data. Must fulfill input requirements of first step of
-            the pipeline.
-
-        y : iterable or None, default: ``None``
-            Training targets. Must fulfill label requirements for all steps
-            of the pipeline.
-
-        **fit_params : dict of string -> object
-            Parameters passed to the :meth:`fit` method of each step, where
-            each parameter name is prefixed such that parameter ``p`` for step
-            ``s`` has key ``s__p``.
-
-        Returns
-        -------
-        y_pred : array-like
-        """
-        Xt, yr, fit_params = self._fit(X, y, **fit_params)
-        return self.steps[-1][-1].fit_predict(Xt, yr, **fit_params)
-
-    @property
-    def resample(self):
-        """Apply transformers/transformer_resamplers, and transform with the
-        final estimator.
-
-        This also works where final estimator is ``None``: all prior
-        transformations are applied.
-
-        Parameters
-        ----------
-        y : array-like, shape = (n_samples,)
-            Data to resample. Must fulfill input requirements of first step
-            of the pipeline.
-
-        Returns
-        -------
-        yr : array-like, shape = (n_samples_new,)
-        """
-        # _final_estimator is None or has transform, otherwise attribute error
-        if self._final_estimator != 'passthrough':
-            self._final_estimator.resample
-        return self._resample
-
-    def _resample(self, X, y=None):
-        yr = y
-        for _, _, transform in self._iter():
-            yr = transform.resample(yr)
-        return yr
-
-    @property
-    def transform_resample(self):
-        """Apply transformers/transformer_resamplers, and transform with the
-        final estimator.
-
-        This also works where final estimator is ``None``: all prior
-        transformations are applied.
-
-        Parameters
-        ----------
-        X : iterable
-            Data to transform. Must fulfill input requirements of first step
-            of the pipeline.
-
-        Returns
-        -------
-        Xt : array-like, shape = (n_samples_new, n_transformed_features)
-        yr : array-like, shape = (n_samples_new,)
-        """
-        # _final_estimator is None or has transform, otherwise attribute error
-        final_estimator = self._final_estimator
-        if final_estimator != 'passthrough':
-            if hasattr(final_estimator, 'transform_resample'):
-                final_estimator.transform_resample
-            else:
-                final_estimator.transform
-        return self._transform_resample
-
-    def _transform_resample(self, X, y):
-        Xt, yr = X, y
-        for _, _, transform in self._iter():
-            if hasattr(transform, 'transform_resample'):
-                Xt, yr = transform.transform_resample(Xt, yr)
-            else:
-                Xt = transform.transform(Xt)
-        return Xt, yr
-
-    @property
-    def transform(self):
-        """Apply transformers/transformer_resamplers, and transform with the
-        final estimator.
-
-        This also works where final estimator is ``None``: all prior
-        transformations are applied.
-
-        Parameters
-        ----------
-        X : iterable
-            Data to transform. Must fulfill input requirements of first step
-            of the pipeline.
-
-        Returns
-        -------
-        Xt : array-like, shape (n_samples, n_transformed_features)
-        """
-        # _final_estimator is None or has transform, otherwise attribute error
-        if self._final_estimator != 'passthrough':
-            self._final_estimator.transform
-        return self._transform
-
-    def _transform(self, X, y=None):
-        Xt = X
-        for _, _, transform in self._iter():
-            Xt = transform.transform(Xt)
-        return Xt
-
-    @property
-    def inverse_transform(self):
-        """Apply inverse transformations in reverse order
-
-        All estimators in the pipeline must support ``inverse_transform``.
-
-        Parameters
-        ----------
-
-        Xt : array-like, shape (n_samples, n_transformed_features)
-            Data samples, where ``n_samples`` is the number of samples and
-            ``n_features`` is the number of features. Must fulfill
-            input requirements of last step of pipeline's
-            ``inverse_transform`` method.
-
-        Returns
-        -------
-        Xt : array-like, shape (n_samples, n_features)
-        """
-        # raise AttributeError if necessary for hasattr behaviour
-        for _, _, transform in self._iter():
-            transform.inverse_transform
-        return self._inverse_transform
-
-    def _inverse_transform(self, X, y=None):
-        Xt, yr = X, y
-        reverse_iter = reversed(list(self._iter()))
-        for _, _, transform in reverse_iter:
-            Xt = transform.inverse_transform(Xt, yr)
-        return Xt
-
-    @if_delegate_has_method(delegate='_final_estimator')
-    def score(self, X, y=None, sample_weight=None):
-        """Apply transformers/samplers, and score with the final estimator
-
-        Parameters
-        ----------
-        X : iterable
-            Data to predict on. Must fulfill input requirements of first step
-            of the pipeline.
-
-        y : iterable or None, default: ``None``
-            Targets used for scoring. Must fulfill label requirements for all
-            steps of the pipeline.
-
-        sample_weight : array-like or None, default: ``None``
-            If not None, this argument is passed as ``sample_weight`` keyword
-            argument to the ``score`` method of the final estimator.
-
-        Returns
-        -------
-        score : float
-        """
-        Xt, yr = X, y
-        for _, _, transform in self._iter(with_final=False):
-            if (hasattr(transform, "transform_resample")):
-                Xt, yr = transform.transform_resample(Xt, yr)
-            else:
-                Xt = transform.transform(Xt)
-
-        score_params = {}
-        if sample_weight is not None:
-            score_params['sample_weight'] = sample_weight
-        return self.steps[-1][-1].score(Xt, yr, **score_params)
-
-
-def _fit_transform_one(transformer, weight, X, y, **fit_params):
-    if hasattr(transformer, 'fit_transform'):
-        X_res = transformer.fit_transform(X, y, **fit_params)
-    else:
-        X_res = transformer.fit(X, y, **fit_params).transform(X)
-    # if we have a weight for this transformer, multiply output
-    if weight is None:
-        return X_res, transformer
-    return X_res * weight, transformer
-
-
-def _fit_transform_resample_one(transformer_resampler, weight,
-                                X, y, **fit_params):
-    if hasattr(transformer_resampler, 'fit_transform_resample'):
-        X_res, y_res = transformer_resampler.fit_transform_resample(
-            X, y, **fit_params)
-    else:
-        X_res, y_res = transformer_resampler.fit(
-            X, y, **fit_params).transform_resample(
-            X, y)
-    if weight is None:
-        return X_res, y_res, transformer_resampler
-    return X_res * weight, y_res, transformer_resampler
-
-
-def make_pipeline(*steps, **kwargs):
-    """Construct a Pipeline from the given estimators.
-
-    This is a shorthand for the Pipeline constructor; it does not require, and
-    does not permit, naming the estimators. Instead, their names will be set
-    to the lowercase of their types automatically.
-
-    Parameters
-    ----------
-    *steps : list of estimators.
-
-    memory : None, str or object with the joblib.Memory interface, optional
-        Used to cache the fitted transformers of the pipeline. By default,
-        no caching is performed. If a string is given, it is the path to
-        the caching directory. Enabling caching triggers a clone of
-        the transformers before fitting. Therefore, the transformer
-        instance given to the pipeline cannot be inspected
-        directly. Use the attribute ``named_steps`` or ``steps`` to
-        inspect estimators within the pipeline. Caching the
-        transformers is advantageous when fitting is time consuming.
-
-    Returns
-    -------
-    p : Pipeline
-
-    See also
-    --------
-    imblearn.pipeline.Pipeline : Class for creating a pipeline of
-        transforms with a final estimator.
-
-    Examples
-    --------
-    >>> from sklearn.naive_bayes import GaussianNB
-    >>> from sklearn.preprocessing import StandardScaler
-    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))
-    ... # doctest: +NORMALIZE_WHITESPACE
-    Pipeline(memory=None,
-             steps=[('standardscaler',
-                     StandardScaler(copy=True, with_mean=True, with_std=True)),
-                    ('gaussiannb',
-                     GaussianNB(priors=None, var_smoothing=1e-09))],
-             verbose=False)
-    """
-    memory = kwargs.pop('memory', None)
-    if kwargs:
-        raise TypeError(
-            f'Unknown keyword arguments: "{list(kwargs.keys())[0]}"')
-    return Pipeline(pipeline._name_estimators(steps), memory=memory)
+"""The module :mod:`gtda.pipeline` extends scikit-learn's module by defining
+Pipelines that include TransformerResamplers."""
+# License: GNU AGPLv3
+
+from sklearn import pipeline
+from sklearn.base import clone
+from sklearn.utils.metaestimators import available_if
+from sklearn.utils.validation import check_memory
+
+__all__ = ['Pipeline', 'make_pipeline']
+
+
+class Pipeline(pipeline.Pipeline):
+    """Pipeline of transforms and resamples with a final estimator.
+
+    Sequentially apply a list of transforms, sampling, and a final estimator.
+    Intermediate steps of the pipeline must be transformers or resamplers,
+    that is, they must implement fit, transform and sample methods.
+    The samplers are only applied during fit.
+    The final estimator only needs to implement fit.
+    The transformers and samplers in the pipeline can be cached using
+    ``memory`` argument.
+
+    The purpose of the pipeline is to assemble several steps that can be
+    cross-validated together while setting different parameters.
+    For this, it enables setting parameters of the various steps using their
+    names and the parameter name separated by a '__', as in the example below.
+    A step's estimator may be replaced entirely by setting the parameter
+    with its name to another estimator, or a transformer removed by setting
+    it to 'passthrough' or ``None``.
+
+    Parameters
+    ----------
+    steps : list
+        List of (name, transform) tuples (implementing
+        fit/transform) that are chained, in the order in which
+        they are chained, with the last object an estimator.
+
+    memory : Instance of joblib.Memory or string, optional (default: ``None``)
+        Used to cache the fitted transformers of the pipeline. By default,
+        no caching is performed. If a string is given, it is the path to
+        the caching directory. Enabling caching triggers a clone of
+        the transformers before fitting. Therefore, the transformer
+        instance given to the pipeline cannot be inspected
+        directly. Use the attribute ``named_steps`` or ``steps`` to
+        inspect estimators within the pipeline. Caching the
+        transformers is advantageous when fitting is time consuming.
+
+
+    Attributes
+    ----------
+    named_steps : dict
+        Read-only attribute to access any step parameter by user given name.
+        Keys are step names and values are steps parameters.
+
+    See also
+    --------
+    make_pipeline : helper function to make pipeline.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> import gtda.time_series as ts
+    >>> import gtda.homology as hl
+    >>> import gtda.diagrams as diag
+    >>> from gtda.pipeline import Pipeline
+    >>> import sklearn.preprocessing as skprep
+    >>>
+    >>> X = np.random.rand(600, 1)
+    >>> n_train, n_test = 400, 200
+    >>>
+    >>> labeller = ts.Labeller(size=6, percentiles=[80],
+    >>>                        n_steps_future=1)
+    >>> X_train = X[:n_train]
+    >>> y_train = X_train
+    >>> X_train, y_train = labeller.fit_transform_resample(X_train, y_train)
+    >>>
+    >>> print(X_train.shape, y_train.shape)
+    (395, 1) (395,)
+    >>> steps = [
+    >>>     ('embedding', ts.SingleTakensEmbedding()),
+    >>>     ('window', ts.SlidingWindow(size=6, stride=1)),
+    >>>     ('diagram', hl.VietorisRipsPersistence()),
+    >>>     ('rescaler', diag.Scaler()),
+    >>>     ('filter', diag.Filtering(epsilon=0.1)),
+    >>>     ('entropy', diag.PersistenceEntropy()),
+    >>>     ('scaling', skprep.MinMaxScaler(copy=True)),
+    >>> ]
+    >>> pipeline = Pipeline(steps)
+    >>>
+    >>> Xt_train, yr_train = pipeline.\\
+    >>>     fit_transform_resample(X_train, y_train)
+    >>>
+    >>> print(X_train_final.shape, y_train_final.shape)
+    (389, 2) (389,)
+    """
+
+    def _final_estimator_has(attr):
+        def check(self):
+            return hasattr(self._final_estimator, attr)
+
+        return check
+
+    def _fit(self, X, y=None, **fit_params):
+        self.steps = list(self.steps)
+        self._validate_steps()
+        # Setup the memory
+        memory = check_memory(self.memory)
+
+        fit_transform_one_cached = memory.cache(_fit_transform_one)
+        fit_transform_resample_one_cached = memory.cache(
+            _fit_transform_resample_one)
+
+        fit_params_steps = {name: {} for name, step in self.steps
+                            if step is not None}
+        for pname, pval in fit_params.items():
+            step, param = pname.split('__', 1)
+            fit_params_steps[step][param] = pval
+        for step_idx, name, transformer in self._iter(with_final=False):
+            if hasattr(memory, 'location') and (memory.location is None):
+                # joblib >= 0.12. We do not clone when caching is disabled to
+                # preserve backward compatibility
+                cloned_transformer = transformer
+            else:
+                cloned_transformer = clone(transformer)
+            # Fit or load from cache the current transfomer
+            if hasattr(cloned_transformer, "resample") or \
+               hasattr(cloned_transformer, "fit_transform_resample"):
+                if y is None:
+                    X, fitted_transformer = fit_transform_one_cached(
+                        cloned_transformer, None, X, y,
+                        **fit_params_steps[name])
+                else:
+                    X, y, fitted_transformer = \
+                        fit_transform_resample_one_cached(
+                            cloned_transformer, None, X, y,
+                            **fit_params_steps[name])
+            else:
+                X, fitted_transformer = fit_transform_one_cached(
+                    cloned_transformer, None, X, y,
+                    **fit_params_steps[name])
+
+            # Replace the transformer of the step with the fitted
+            # transformer. This is necessary when loading the transformer
+            # from the cache.
+            self.steps[step_idx] = (name, fitted_transformer)
+        if self._final_estimator == 'passthrough':
+            return X, y, {}
+        return X, y, fit_params_steps[self.steps[-1][0]]
+
+    def fit(self, X, y=None, **fit_params):
+        """Fit the model.
+
+        Fit all the transforms/samplers one after the other and
+        transform/sample the data, then fit the transformed/sampled
+        data using the final estimator.
+
+        Parameters
+        ----------
+        X : iterable
+            Training data. Must fulfill input requirements of first step of the
+            pipeline.
+
+        y : iterable or None, default: ``None``
+            Training targets. Must fulfill label requirements for all steps of
+            the pipeline.
+
+        **fit_params : dict of string -> object
+            Parameters passed to the :meth:`fit` method of each step, where
+            each parameter name is prefixed such that parameter ``p`` for step
+            ``s`` has key ``s__p``.
+
+        Returns
+        -------
+        self : Pipeline
+            This estimator
+
+        """
+        Xt, yr, fit_params = self._fit(X, y, **fit_params)
+        if self._final_estimator != 'passthrough':
+            self._final_estimator.fit(Xt, yr, **fit_params)
+        return self
+
+    def fit_transform(self, X, y=None, **fit_params):
+        """Fit the model and transform with the final estimator.
+
+        Fits all the transformers/samplers one after the other and
+        transform/sample the data, then uses fit_transform on
+        transformed data with the final estimator.
+
+        Parameters
+        ----------
+        X : iterable
+            Training data. Must fulfill input requirements of first step of the
+            pipeline.
+
+        y : iterable, default: ``None``
+            Training targets. Must fulfill label requirements for all steps of
+            the pipeline.
+
+        **fit_params : dict of string -> object
+            Parameters passed to the :meth:`fit` method of each step, where
+            each parameter name is prefixed such that parameter ``p`` for step
+            ``s`` has key ``s__p``.
+
+        Returns
+        -------
+        Xt : array-like, shape (n_samples, n_transformed_features)
+            Transformed samples
+
+        """
+        last_step = self._final_estimator
+        Xt, yr, fit_params = self._fit(X, y, **fit_params)
+        if last_step == 'passthrough':
+            return Xt
+        elif hasattr(last_step, 'fit_transform'):
+            return last_step.fit_transform(Xt, yr, **fit_params)
+        else:
+            return last_step.fit(Xt, yr, **fit_params).transform(Xt)
+
+    def fit_transform_resample(self, X, y=None, **fit_params):
+        """Fit the model and sample with the final estimator.
+
+        Fits all the transformers/samplers one after the other and
+        transform/sample the data, then uses fit_resample on transformed
+        data with the final estimator.
+
+        Parameters
+        ----------
+        X : iterable
+            Training data. Must fulfill input requirements of first step of the
+            pipeline.
+
+        y : iterable, default: ``None``
+            Training targets. Must fulfill label requirements for all steps of
+            the pipeline.
+
+        **fit_params : dict of string -> object
+            Parameters passed to the :meth:`fit` method of each step, where
+            each parameter name is prefixed such that parameter ``p`` for step
+            ``s`` has key ``s__p``.
+
+        Returns
+        -------
+        Xt : array-like, shape (n_samples, n_transformed_features)
+            Transformed samples.
+
+        yr : array-like, shape (n_samples, n_transformed_features)
+            Transformed target.
+        """
+        last_step = self._final_estimator
+        Xt, yr, fit_params = self._fit(X, y, **fit_params)
+        if last_step == 'passthrough':
+            return Xt, yr
+        elif hasattr(last_step, 'fit_transform_resample'):
+            return last_step.fit_transform_resample(Xt, yr, **fit_params)
+        elif hasattr(last_step, 'fit_transform'):
+            return last_step.fit_transform(Xt, yr, **fit_params), yr
+
+    @available_if(_final_estimator_has('fit_predict'))
+    def fit_predict(self, X, y=None, **fit_params):
+        """Applies fit_predict of last step in pipeline after transforms.
+
+        Applies fit_transforms of a pipeline to the data, followed by the
+        fit_predict method of the final estimator in the pipeline. Valid
+        only if the final estimator implements fit_predict.
+
+        Parameters
+        ----------
+        X : iterable
+            Training data. Must fulfill input requirements of first step of
+            the pipeline.
+
+        y : iterable or None, default: ``None``
+            Training targets. Must fulfill label requirements for all steps
+            of the pipeline.
+
+        **fit_params : dict of string -> object
+            Parameters passed to the :meth:`fit` method of each step, where
+            each parameter name is prefixed such that parameter ``p`` for step
+            ``s`` has key ``s__p``.
+
+        Returns
+        -------
+        y_pred : array-like
+        """
+        Xt, yr, fit_params = self._fit(X, y, **fit_params)
+        return self.steps[-1][-1].fit_predict(Xt, yr, **fit_params)
+
+    @property
+    def resample(self):
+        """Apply transformers/transformer_resamplers, and transform with the
+        final estimator.
+
+        This also works where final estimator is ``None``: all prior
+        transformations are applied.
+
+        Parameters
+        ----------
+        y : array-like, shape = (n_samples,)
+            Data to resample. Must fulfill input requirements of first step
+            of the pipeline.
+
+        Returns
+        -------
+        yr : array-like, shape = (n_samples_new,)
+        """
+        # _final_estimator is None or has transform, otherwise attribute error
+        if self._final_estimator != 'passthrough':
+            self._final_estimator.resample
+        return self._resample
+
+    def _resample(self, X, y=None):
+        yr = y
+        for _, _, transform in self._iter():
+            yr = transform.resample(yr)
+        return yr
+
+    @property
+    def transform_resample(self):
+        """Apply transformers/transformer_resamplers, and transform with the
+        final estimator.
+
+        This also works where final estimator is ``None``: all prior
+        transformations are applied.
+
+        Parameters
+        ----------
+        X : iterable
+            Data to transform. Must fulfill input requirements of first step
+            of the pipeline.
+
+        Returns
+        -------
+        Xt : array-like, shape = (n_samples_new, n_transformed_features)
+        yr : array-like, shape = (n_samples_new,)
+        """
+        # _final_estimator is None or has transform, otherwise attribute error
+        final_estimator = self._final_estimator
+        if final_estimator != 'passthrough':
+            if hasattr(final_estimator, 'transform_resample'):
+                final_estimator.transform_resample
+            else:
+                final_estimator.transform
+        return self._transform_resample
+
+    def _transform_resample(self, X, y):
+        Xt, yr = X, y
+        for _, _, transform in self._iter():
+            if hasattr(transform, 'transform_resample'):
+                Xt, yr = transform.transform_resample(Xt, yr)
+            else:
+                Xt = transform.transform(Xt)
+        return Xt, yr
+
+    @property
+    def transform(self):
+        """Apply transformers/transformer_resamplers, and transform with the
+        final estimator.
+
+        This also works where final estimator is ``None``: all prior
+        transformations are applied.
+
+        Parameters
+        ----------
+        X : iterable
+            Data to transform. Must fulfill input requirements of first step
+            of the pipeline.
+
+        Returns
+        -------
+        Xt : array-like, shape (n_samples, n_transformed_features)
+        """
+        # _final_estimator is None or has transform, otherwise attribute error
+        if self._final_estimator != 'passthrough':
+            self._final_estimator.transform
+        return self._transform
+
+    def _transform(self, X, y=None):
+        Xt = X
+        for _, _, transform in self._iter():
+            Xt = transform.transform(Xt)
+        return Xt
+
+    @property
+    def inverse_transform(self):
+        """Apply inverse transformations in reverse order
+
+        All estimators in the pipeline must support ``inverse_transform``.
+
+        Parameters
+        ----------
+
+        Xt : array-like, shape (n_samples, n_transformed_features)
+            Data samples, where ``n_samples`` is the number of samples and
+            ``n_features`` is the number of features. Must fulfill
+            input requirements of last step of pipeline's
+            ``inverse_transform`` method.
+
+        Returns
+        -------
+        Xt : array-like, shape (n_samples, n_features)
+        """
+        # raise AttributeError if necessary for hasattr behaviour
+        for _, _, transform in self._iter():
+            transform.inverse_transform
+        return self._inverse_transform
+
+    def _inverse_transform(self, X, y=None):
+        Xt, yr = X, y
+        reverse_iter = reversed(list(self._iter()))
+        for _, _, transform in reverse_iter:
+            Xt = transform.inverse_transform(Xt, yr)
+        return Xt
+
+    @available_if(_final_estimator_has('score'))
+    def score(self, X, y=None, sample_weight=None):
+        """Apply transformers/samplers, and score with the final estimator
+
+        Parameters
+        ----------
+        X : iterable
+            Data to predict on. Must fulfill input requirements of first step
+            of the pipeline.
+
+        y : iterable or None, default: ``None``
+            Targets used for scoring. Must fulfill label requirements for all
+            steps of the pipeline.
+
+        sample_weight : array-like or None, default: ``None``
+            If not None, this argument is passed as ``sample_weight`` keyword
+            argument to the ``score`` method of the final estimator.
+
+        Returns
+        -------
+        score : float
+        """
+        Xt, yr = X, y
+        for _, _, transform in self._iter(with_final=False):
+            if (hasattr(transform, "transform_resample")):
+                Xt, yr = transform.transform_resample(Xt, yr)
+            else:
+                Xt = transform.transform(Xt)
+
+        score_params = {}
+        if sample_weight is not None:
+            score_params['sample_weight'] = sample_weight
+        return self.steps[-1][-1].score(Xt, yr, **score_params)
+
+
+def _fit_transform_one(transformer, weight, X, y, **fit_params):
+    if hasattr(transformer, 'fit_transform'):
+        X_res = transformer.fit_transform(X, y, **fit_params)
+    else:
+        X_res = transformer.fit(X, y, **fit_params).transform(X)
+    # if we have a weight for this transformer, multiply output
+    if weight is None:
+        return X_res, transformer
+    return X_res * weight, transformer
+
+
+def _fit_transform_resample_one(transformer_resampler, weight,
+                                X, y, **fit_params):
+    if hasattr(transformer_resampler, 'fit_transform_resample'):
+        X_res, y_res = transformer_resampler.fit_transform_resample(
+            X, y, **fit_params)
+    else:
+        X_res, y_res = transformer_resampler.fit(
+            X, y, **fit_params).transform_resample(
+            X, y)
+    if weight is None:
+        return X_res, y_res, transformer_resampler
+    return X_res * weight, y_res, transformer_resampler
+
+
+def make_pipeline(*steps, **kwargs):
+    """Construct a Pipeline from the given estimators.
+
+    This is a shorthand for the Pipeline constructor; it does not require, and
+    does not permit, naming the estimators. Instead, their names will be set
+    to the lowercase of their types automatically.
+
+    Parameters
+    ----------
+    *steps : list of estimators.
+
+    memory : None, str or object with the joblib.Memory interface, optional
+        Used to cache the fitted transformers of the pipeline. By default,
+        no caching is performed. If a string is given, it is the path to
+        the caching directory. Enabling caching triggers a clone of
+        the transformers before fitting. Therefore, the transformer
+        instance given to the pipeline cannot be inspected
+        directly. Use the attribute ``named_steps`` or ``steps`` to
+        inspect estimators within the pipeline. Caching the
+        transformers is advantageous when fitting is time consuming.
+
+    Returns
+    -------
+    p : Pipeline
+
+    See also
+    --------
+    imblearn.pipeline.Pipeline : Class for creating a pipeline of
+        transforms with a final estimator.
+
+    Examples
+    --------
+    >>> from sklearn.naive_bayes import GaussianNB
+    >>> from sklearn.preprocessing import StandardScaler
+    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))
+    ... # doctest: +NORMALIZE_WHITESPACE
+    Pipeline(memory=None,
+             steps=[('standardscaler',
+                     StandardScaler(copy=True, with_mean=True, with_std=True)),
+                    ('gaussiannb',
+                     GaussianNB(priors=None, var_smoothing=1e-09))],
+             verbose=False)
+    """
+    memory = kwargs.pop('memory', None)
+    if kwargs:
+        raise TypeError(
+            f'Unknown keyword arguments: "{list(kwargs.keys())[0]}"')
+    return Pipeline(pipeline._name_estimators(steps), memory=memory)
```

## gtda/_version.py

```diff
@@ -1,22 +1,22 @@
-"""``giotto-tda`` is a Python library implementing algorithms from
-Topological Data Analysis in a machine learning framework."""
-# License: GNU AGPLv3
-
-# PEP0440 compatible formatted version, see:
-# https://www.python.org/dev/peps/pep-0440/
-#
-# Generic release markers:
-# X.Y
-# X.Y.Z # For bugfix releases
-#
-# Admissible pre-release markers:
-# X.YaN # Alpha release
-# X.YbN # Beta release
-# X.YrcN # Release Candidate
-# X.Y # Final release
-#
-# Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
-# 'X.Y.dev0' is the canonical version of 'X.Y.dev'
-#
-
-__version__ = "0.6.0"
+"""``giotto-tda`` is a Python library implementing algorithms from
+Topological Data Analysis in a machine learning framework."""
+# License: GNU AGPLv3
+
+# PEP0440 compatible formatted version, see:
+# https://www.python.org/dev/peps/pep-0440/
+#
+# Generic release markers:
+# X.Y
+# X.Y.Z # For bugfix releases
+#
+# Admissible pre-release markers:
+# X.YaN # Alpha release
+# X.YbN # Beta release
+# X.YrcN # Release Candidate
+# X.Y # Final release
+#
+# Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
+# 'X.Y.dev0' is the canonical version of 'X.Y.dev'
+#
+
+__version__ = "0.6.1"
```

## gtda/__init__.py

```diff
@@ -1,114 +1,18 @@
-00000000: 0d0a 0d0a 2222 2222 2222 2320 7374 6172  ....""""""# star
-00000010: 7420 6465 6c76 6577 6865 656c 2070 6174  t delvewheel pat
-00000020: 6368 0d0a 6465 6620 5f64 656c 7665 7768  ch..def _delvewh
-00000030: 6565 6c5f 696e 6974 5f70 6174 6368 5f30  eel_init_patch_0
-00000040: 5f30 5f32 3328 293a 0d0a 2020 2020 696d  _0_23():..    im
-00000050: 706f 7274 206f 730d 0a20 2020 2069 6d70  port os..    imp
-00000060: 6f72 7420 7379 730d 0a20 2020 206c 6962  ort sys..    lib
-00000070: 735f 6469 7220 3d20 6f73 2e70 6174 682e  s_dir = os.path.
-00000080: 6162 7370 6174 6828 6f73 2e70 6174 682e  abspath(os.path.
-00000090: 6a6f 696e 286f 732e 7061 7468 2e64 6972  join(os.path.dir
-000000a0: 6e61 6d65 285f 5f66 696c 655f 5f29 2c20  name(__file__), 
-000000b0: 6f73 2e70 6172 6469 722c 2027 6769 6f74  os.pardir, 'giot
-000000c0: 746f 5f74 6461 2e6c 6962 7327 2929 0d0a  to_tda.libs'))..
-000000d0: 2020 2020 6966 2073 7973 2e76 6572 7369      if sys.versi
-000000e0: 6f6e 5f69 6e66 6f5b 3a32 5d20 3e3d 2028  on_info[:2] >= (
-000000f0: 332c 2038 293a 0d0a 2020 2020 2020 2020  3, 8):..        
-00000100: 636f 6e64 615f 776f 726b 6172 6f75 6e64  conda_workaround
-00000110: 203d 206f 732e 7061 7468 2e65 7869 7374   = os.path.exist
-00000120: 7328 6f73 2e70 6174 682e 6a6f 696e 2873  s(os.path.join(s
-00000130: 7973 2e62 6173 655f 7072 6566 6978 2c20  ys.base_prefix, 
-00000140: 2763 6f6e 6461 2d6d 6574 6127 2929 2061  'conda-meta')) a
-00000150: 6e64 2028 7379 732e 7665 7273 696f 6e5f  nd (sys.version_
-00000160: 696e 666f 5b3a 335d 203c 2028 332c 2038  info[:3] < (3, 8
-00000170: 2c20 3133 2920 6f72 2028 332c 2039 2c20  , 13) or (3, 9, 
-00000180: 3029 203c 3d20 7379 732e 7665 7273 696f  0) <= sys.versio
-00000190: 6e5f 696e 666f 5b3a 335d 203c 2028 332c  n_info[:3] < (3,
-000001a0: 2039 2c20 3929 290d 0a20 2020 2020 2020   9, 9))..       
-000001b0: 2069 6620 636f 6e64 615f 776f 726b 6172   if conda_workar
-000001c0: 6f75 6e64 3a0d 0a20 2020 2020 2020 2020  ound:..         
-000001d0: 2020 2023 2062 6163 6b75 7020 7468 6520     # backup the 
-000001e0: 7374 6174 6520 6f66 2074 6865 2065 6e76  state of the env
-000001f0: 6972 6f6e 6d65 6e74 2076 6172 6961 626c  ironment variabl
-00000200: 6520 434f 4e44 415f 444c 4c5f 5345 4152  e CONDA_DLL_SEAR
-00000210: 4348 5f4d 4f44 4946 4943 4154 494f 4e5f  CH_MODIFICATION_
-00000220: 454e 4142 4c45 0d0a 2020 2020 2020 2020  ENABLE..        
-00000230: 2020 2020 636f 6e64 615f 646c 6c5f 7365      conda_dll_se
-00000240: 6172 6368 5f6d 6f64 6966 6963 6174 696f  arch_modificatio
-00000250: 6e5f 656e 6162 6c65 203d 206f 732e 656e  n_enable = os.en
-00000260: 7669 726f 6e2e 6765 7428 2743 4f4e 4441  viron.get('CONDA
-00000270: 5f44 4c4c 5f53 4541 5243 485f 4d4f 4449  _DLL_SEARCH_MODI
-00000280: 4649 4341 5449 4f4e 5f45 4e41 424c 4527  FICATION_ENABLE'
-00000290: 290d 0a20 2020 2020 2020 2020 2020 206f  )..            o
-000002a0: 732e 656e 7669 726f 6e5b 2743 4f4e 4441  s.environ['CONDA
-000002b0: 5f44 4c4c 5f53 4541 5243 485f 4d4f 4449  _DLL_SEARCH_MODI
-000002c0: 4649 4341 5449 4f4e 5f45 4e41 424c 4527  FICATION_ENABLE'
-000002d0: 5d20 3d20 2731 270d 0a20 2020 2020 2020  ] = '1'..       
-000002e0: 206f 732e 6164 645f 646c 6c5f 6469 7265   os.add_dll_dire
-000002f0: 6374 6f72 7928 6c69 6273 5f64 6972 290d  ctory(libs_dir).
-00000300: 0a20 2020 2020 2020 2069 6620 636f 6e64  .        if cond
-00000310: 615f 776f 726b 6172 6f75 6e64 3a0d 0a20  a_workaround:.. 
-00000320: 2020 2020 2020 2020 2020 2023 2072 6573             # res
-00000330: 746f 7265 2074 6865 2073 7461 7465 206f  tore the state o
-00000340: 6620 7468 6520 656e 7669 726f 6e6d 656e  f the environmen
-00000350: 7420 7661 7269 6162 6c65 2043 4f4e 4441  t variable CONDA
-00000360: 5f44 4c4c 5f53 4541 5243 485f 4d4f 4449  _DLL_SEARCH_MODI
-00000370: 4649 4341 5449 4f4e 5f45 4e41 424c 450d  FICATION_ENABLE.
-00000380: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-00000390: 636f 6e64 615f 646c 6c5f 7365 6172 6368  conda_dll_search
-000003a0: 5f6d 6f64 6966 6963 6174 696f 6e5f 656e  _modification_en
-000003b0: 6162 6c65 2069 7320 4e6f 6e65 3a0d 0a20  able is None:.. 
-000003c0: 2020 2020 2020 2020 2020 2020 2020 206f                 o
-000003d0: 732e 656e 7669 726f 6e2e 706f 7028 2743  s.environ.pop('C
-000003e0: 4f4e 4441 5f44 4c4c 5f53 4541 5243 485f  ONDA_DLL_SEARCH_
-000003f0: 4d4f 4449 4649 4341 5449 4f4e 5f45 4e41  MODIFICATION_ENA
-00000400: 424c 4527 2c20 4e6f 6e65 290d 0a20 2020  BLE', None)..   
-00000410: 2020 2020 2020 2020 2065 6c73 653a 0d0a           else:..
-00000420: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000430: 6f73 2e65 6e76 6972 6f6e 5b27 434f 4e44  os.environ['COND
-00000440: 415f 444c 4c5f 5345 4152 4348 5f4d 4f44  A_DLL_SEARCH_MOD
-00000450: 4946 4943 4154 494f 4e5f 454e 4142 4c45  IFICATION_ENABLE
-00000460: 275d 203d 2063 6f6e 6461 5f64 6c6c 5f73  '] = conda_dll_s
-00000470: 6561 7263 685f 6d6f 6469 6669 6361 7469  earch_modificati
-00000480: 6f6e 5f65 6e61 626c 650d 0a20 2020 2065  on_enable..    e
-00000490: 6c73 653a 0d0a 2020 2020 2020 2020 6672  lse:..        fr
-000004a0: 6f6d 2063 7479 7065 7320 696d 706f 7274  om ctypes import
-000004b0: 2057 696e 444c 4c0d 0a20 2020 2020 2020   WinDLL..       
-000004c0: 2077 6974 6820 6f70 656e 286f 732e 7061   with open(os.pa
-000004d0: 7468 2e6a 6f69 6e28 6c69 6273 5f64 6972  th.join(libs_dir
-000004e0: 2c20 272e 6c6f 6164 2d6f 7264 6572 2d67  , '.load-order-g
-000004f0: 696f 7474 6f5f 7464 612d 302e 362e 3027  iotto_tda-0.6.0'
-00000500: 2929 2061 7320 6669 6c65 3a0d 0a20 2020  )) as file:..   
-00000510: 2020 2020 2020 2020 206c 6f61 645f 6f72           load_or
-00000520: 6465 7220 3d20 6669 6c65 2e72 6561 6428  der = file.read(
-00000530: 292e 7370 6c69 7428 290d 0a20 2020 2020  ).split()..     
-00000540: 2020 2066 6f72 206c 6962 2069 6e20 6c6f     for lib in lo
-00000550: 6164 5f6f 7264 6572 3a0d 0a20 2020 2020  ad_order:..     
-00000560: 2020 2020 2020 2057 696e 444c 4c28 6f73         WinDLL(os
-00000570: 2e70 6174 682e 6a6f 696e 286c 6962 735f  .path.join(libs_
-00000580: 6469 722c 206c 6962 2929 0d0a 0d0a 0d0a  dir, lib))......
-00000590: 5f64 656c 7665 7768 6565 6c5f 696e 6974  _delvewheel_init
-000005a0: 5f70 6174 6368 5f30 5f30 5f32 3328 290d  _patch_0_0_23().
-000005b0: 0a64 656c 205f 6465 6c76 6577 6865 656c  .del _delvewheel
-000005c0: 5f69 6e69 745f 7061 7463 685f 305f 305f  _init_patch_0_0_
-000005d0: 3233 0d0a 2320 656e 6420 6465 6c76 6577  23..# end delvew
-000005e0: 6865 656c 2070 6174 6368 0d0a 0d0a 6672  heel patch....fr
-000005f0: 6f6d 202e 5f76 6572 7369 6f6e 2069 6d70  om ._version imp
-00000600: 6f72 7420 5f5f 7665 7273 696f 6e5f 5f0d  ort __version__.
-00000610: 0a0d 0a5f 5f61 6c6c 5f5f 203d 205b 0d0a  ...__all__ = [..
-00000620: 2020 2020 276d 6170 7065 7227 2c0d 0a20      'mapper',.. 
-00000630: 2020 2027 7469 6d65 5f73 6572 6965 7327     'time_series'
-00000640: 2c0d 0a20 2020 2027 6772 6170 6873 272c  ,..    'graphs',
-00000650: 0d0a 2020 2020 2769 6d61 6765 7327 2c0d  ..    'images',.
-00000660: 0a20 2020 2027 706f 696e 745f 636c 6f75  .    'point_clou
-00000670: 6473 272c 0d0a 2020 2020 2768 6f6d 6f6c  ds',..    'homol
-00000680: 6f67 7927 2c0d 0a20 2020 2027 6469 6167  ogy',..    'diag
-00000690: 7261 6d73 272c 0d0a 2020 2020 2763 7572  rams',..    'cur
-000006a0: 7665 7327 2c0d 0a20 2020 2027 706c 6f74  ves',..    'plot
-000006b0: 7469 6e67 272c 0d0a 2020 2020 2765 7874  ting',..    'ext
-000006c0: 6572 6e61 6c73 272c 0d0a 2020 2020 2775  ernals',..    'u
-000006d0: 7469 6c73 272c 0d0a 2020 2020 276d 6574  tils',..    'met
-000006e0: 6165 7374 696d 6174 6f72 7327 2c0d 0a20  aestimators',.. 
-000006f0: 2020 2027 6c6f 6361 6c5f 686f 6d6f 6c6f     'local_homolo
-00000700: 6779 272c 0d0a 2020 2020 275f 5f76 6572  gy',..    '__ver
-00000710: 7369 6f6e 5f5f 270d 0a20 2020 205d 0d0a  sion__'..    ]..
+00000000: 6672 6f6d 202e 5f76 6572 7369 6f6e 2069  from ._version i
+00000010: 6d70 6f72 7420 5f5f 7665 7273 696f 6e5f  mport __version_
+00000020: 5f0a 0a5f 5f61 6c6c 5f5f 203d 205b 0a20  _..__all__ = [. 
+00000030: 2020 2027 6d61 7070 6572 272c 0a20 2020     'mapper',.   
+00000040: 2027 7469 6d65 5f73 6572 6965 7327 2c0a   'time_series',.
+00000050: 2020 2020 2767 7261 7068 7327 2c0a 2020      'graphs',.  
+00000060: 2020 2769 6d61 6765 7327 2c0a 2020 2020    'images',.    
+00000070: 2770 6f69 6e74 5f63 6c6f 7564 7327 2c0a  'point_clouds',.
+00000080: 2020 2020 2768 6f6d 6f6c 6f67 7927 2c0a      'homology',.
+00000090: 2020 2020 2764 6961 6772 616d 7327 2c0a      'diagrams',.
+000000a0: 2020 2020 2763 7572 7665 7327 2c0a 2020      'curves',.  
+000000b0: 2020 2770 6c6f 7474 696e 6727 2c0a 2020    'plotting',.  
+000000c0: 2020 2765 7874 6572 6e61 6c73 272c 0a20    'externals',. 
+000000d0: 2020 2027 7574 696c 7327 2c0a 2020 2020     'utils',.    
+000000e0: 276d 6574 6165 7374 696d 6174 6f72 7327  'metaestimators'
+000000f0: 2c0a 2020 2020 276c 6f63 616c 5f68 6f6d  ,.    'local_hom
+00000100: 6f6c 6f67 7927 2c0a 2020 2020 275f 5f76  ology',.    '__v
+00000110: 6572 7369 6f6e 5f5f 270a 2020 2020 5d0a  ersion__'.    ].
```

## gtda/curves/features.py

```diff
@@ -1,243 +1,243 @@
-"""Feature extraction from curves."""
-# License: GNU AGPLv3
-
-from copy import deepcopy
-from types import FunctionType
-
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils.validation import check_is_fitted, check_array
-
-from ._functions import _AVAILABLE_FUNCTIONS, _implemented_function_recipes, \
-    _parallel_featurization
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.validation import validate_params
-
-
-@adapt_fit_transform_docs
-class StandardFeatures(BaseEstimator, TransformerMixin):
-    """Standard features from multi-channel curves.
-
-    A multi-channel (integer sampled) curve is a 2D array of shape
-    ``(n_channels, n_bins)``, where each row represents the y-values in one of
-    the channels. This transformer applies scalar or vector-valued functions
-    channel-wise to extract features from each multi-channel curve in a
-    collection. The output is always a 2D array such that row ``i`` is the
-    concatenation of the outputs of the chosen functions on the channels in the
-    ``i``-th (multi-)curve in the collection.
-
-    Parameters
-    ----------
-    function : string, callable, list or tuple, optional, default: ``"max"``
-        Function or list/tuple of functions to apply to each channel of each
-        multi-channel curve. Functions can map to scalars or to 1D arrays. If a
-        string (see below) or a callable, then the same function is applied to
-        all channels. Otherwise, `function` is a list/tuple of the same length
-        as the number of entries along axis 1 in the collection passed to
-        :meth:`fit`. Lists/tuples may contain allowed strings (see below),
-        callables, and ``None`` in some positions to indicate that no feature
-        should be extracted from the corresponding channel. Available strings
-        are ``"identity"``, ``"argmin"``, ``"argmax"``, ``"min"``, ``"max"``,
-        ``"mean"``, ``"std"``, ``"median"`` and ``"average"``.
-
-    function_params : dict, None, list or tuple, optional, default: ``None``
-        Additional keyword arguments for the function or functions in
-        `function`. Passing ``None`` is equivalent to passing no arguments.
-        Otherwise, if `function` is a single string or callable then
-        `function_params` must be a dictionary. For functions encoded by
-        allowed strings, the dictionary keys are as follows:
-
-        - If ``function == "average"``, the only key is ``"weights"``
-          (np.ndarray or None, default: ``None``).
-        - Otherwise, there are no allowed keys.
-
-        If `function` is a list or tuple, `function_params` must be a list or
-        tuple of dictionaries (or ``None``) as above, of the same length as
-        `function`.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors. Ignored if `function` is one of the allowed string options.
-
-    Attributes
-    ----------
-    n_channels_ : int
-        Number of channels present in the 3D array passed to :meth:`fit`. Must
-        match the number of channels in the 3D array passed to
-        :meth:`transform`.
-
-    effective_function_ : callable or tuple
-        Callable, or tuple of callables or ``None``, describing the function(s)
-        used to compute features in each available channel. It is a single
-        callable only when `function` was passed as a string.
-
-    effective_function_params_ : dict or tuple
-        Dictionary or tuple of dictionaries containing all information present
-        in `function_params` as well as relevant quantities computed in
-        :meth:`fit`. It is a single dict only when `function` was passed as a
-        string. ``None``s are converted to empty dictionaries.
-
-    """
-    _hyperparameters = {
-        "function": {"type": (str, FunctionType, list, tuple),
-                     "in": tuple(_AVAILABLE_FUNCTIONS.keys()),
-                     "of": {"type": (str, FunctionType, type(None)),
-                            "in": tuple(_AVAILABLE_FUNCTIONS.keys())}},
-        "function_params": {"type": (dict, type(None), list, tuple)},
-        }
-
-    def __init__(self, function="max", function_params=None, n_jobs=None):
-        self.function = function
-        self.function_params = function_params
-        self.n_jobs = n_jobs
-
-    def _validate_params(self):
-        params = self.get_params().copy()
-        _hyperparameters = deepcopy(self._hyperparameters)
-        if not isinstance(self.function, str):
-            _hyperparameters["function"].pop("in")
-        try:
-            validate_params(params, _hyperparameters, exclude=["n_jobs"])
-        # Another go if we fail because function is a list/tuple containing
-        # instances of FunctionType and the "in" key checks fail
-        except ValueError as ve:
-            end_string = f"which is not in " \
-                         f"{tuple(_AVAILABLE_FUNCTIONS.keys())}."
-            function = params["function"]
-            if ve.args[0].endswith(end_string) \
-                    and isinstance(function, (list, tuple)):
-                params["function"] = [f for f in function
-                                      if isinstance(f, str)]
-                validate_params(params, _hyperparameters, exclude=["n_jobs"])
-            else:
-                raise ve
-
-        if isinstance(self.function, (list, tuple)) \
-                and isinstance(self.function_params, dict):
-            raise TypeError("If `function` is a list/tuple then "
-                            "`function_params` must be a list/tuple of dict, "
-                            "or None.")
-        elif isinstance(self.function, (str, FunctionType)) \
-                and isinstance(self.function_params, (list, tuple)):
-            raise TypeError("If `function` is a string or a callable "
-                            "function then `function_params` must be a dict "
-                            "or None.")
-
-    def fit(self, X, y=None):
-        """Compute :attr:`n_channels_` and :attr:`effective_function_params_`.
-        Then, return the estimator.
-
-        This function is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_channels, n_bins)
-            Input data. Collection of multi-channel curves.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_array(X, ensure_2d=False, allow_nd=True)
-        if X.ndim != 3:
-            raise ValueError("Input must be 3-dimensional.")
-        self._validate_params()
-
-        self.n_channels_ = X.shape[1]
-
-        if isinstance(self.function, str):
-            self.effective_function_ = \
-                _implemented_function_recipes[self.function]
-
-            if self.function_params is None:
-                self.effective_function_params_ = {}
-            else:
-                validate_params(self.function_params,
-                                _AVAILABLE_FUNCTIONS[self.function])
-                self.effective_function_params_ = self.function_params.copy()
-
-        elif isinstance(self.function, FunctionType):
-            self.effective_function_ = \
-                tuple([self.function] * self.n_channels_)
-
-            if self.function_params is None:
-                self.effective_function_params_ = \
-                    tuple([{}] * self.n_channels_)
-            else:
-                self.effective_function_params_ = \
-                    tuple([self.function_params.copy()] * self.n_channels_)
-        else:
-            n_functions = len(self.function)
-            if len(self.function) != self.n_channels_:
-                raise ValueError(
-                    f"`function` has length {n_functions} while curves in `X` "
-                    f"have {self.n_channels_} channels."
-                    )
-
-            if self.function_params is None:
-                self._effective_function_params = [{}] * self.n_channels_
-            else:
-                self._effective_function_params = self.function_params
-                n_function_params = len(self._effective_function_params)
-                if n_function_params != self.n_channels_:
-                    raise ValueError(f"`function_params` has length "
-                                     f"{n_function_params} while curves in "
-                                     f"`X` have {self.n_channels_} channels.")
-
-            self.effective_function_ = []
-            self.effective_function_params_ = []
-            for f, p in zip(self.function, self._effective_function_params):
-                if isinstance(f, str):
-                    validate_params(p, _AVAILABLE_FUNCTIONS[f])
-                    self.effective_function_.\
-                        append(_implemented_function_recipes[f])
-                else:
-                    self.effective_function_.append(f)
-                self.effective_function_params_.append({} if p is None
-                                                       else p.copy())
-            self.effective_function_ = tuple(self.effective_function_)
-            self.effective_function_params_ = \
-                tuple(self.effective_function_params_)
-
-        return self
-
-    def transform(self, X, y=None):
-        """Compute features of multi-channel curves.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_channels, n_bins)
-            Input collection of multi-channel curves.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_features)
-            Output collection of features of multi-channel curves.
-            ``n_features`` is the sum of the number of features output by the
-            (non-``None``) functions on their respective channels.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, ensure_2d=False, allow_nd=True)
-        if Xt.ndim != 3:
-            raise ValueError("Input must be 3-dimensional.")
-        if Xt.shape[1] != self.n_channels_:
-            raise ValueError(f"Number of channels must be the same as in "
-                             f"`fit`. Passed {Xt.shape[1]}, expected "
-                             f"{self.n_channels_}.")
-
-        Xt = _parallel_featurization(Xt, self.effective_function_,
-                                     self.effective_function_params_,
-                                     self.n_jobs)
-
-        return Xt
+"""Feature extraction from curves."""
+# License: GNU AGPLv3
+
+from copy import deepcopy
+from typing import Callable
+
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils.validation import check_is_fitted, check_array
+
+from ._functions import _AVAILABLE_FUNCTIONS, _implemented_function_recipes, \
+    _parallel_featurization
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.validation import validate_params
+
+
+@adapt_fit_transform_docs
+class StandardFeatures(BaseEstimator, TransformerMixin):
+    """Standard features from multi-channel curves.
+
+    A multi-channel (integer sampled) curve is a 2D array of shape
+    ``(n_channels, n_bins)``, where each row represents the y-values in one of
+    the channels. This transformer applies scalar or vector-valued functions
+    channel-wise to extract features from each multi-channel curve in a
+    collection. The output is always a 2D array such that row ``i`` is the
+    concatenation of the outputs of the chosen functions on the channels in the
+    ``i``-th (multi-)curve in the collection.
+
+    Parameters
+    ----------
+    function : string, callable, list or tuple, optional, default: ``"max"``
+        Function or list/tuple of functions to apply to each channel of each
+        multi-channel curve. Functions can map to scalars or to 1D arrays. If a
+        string (see below) or a callable, then the same function is applied to
+        all channels. Otherwise, `function` is a list/tuple of the same length
+        as the number of entries along axis 1 in the collection passed to
+        :meth:`fit`. Lists/tuples may contain allowed strings (see below),
+        callables, and ``None`` in some positions to indicate that no feature
+        should be extracted from the corresponding channel. Available strings
+        are ``"identity"``, ``"argmin"``, ``"argmax"``, ``"min"``, ``"max"``,
+        ``"mean"``, ``"std"``, ``"median"`` and ``"average"``.
+
+    function_params : dict, None, list or tuple, optional, default: ``None``
+        Additional keyword arguments for the function or functions in
+        `function`. Passing ``None`` is equivalent to passing no arguments.
+        Otherwise, if `function` is a single string or callable then
+        `function_params` must be a dictionary. For functions encoded by
+        allowed strings, the dictionary keys are as follows:
+
+        - If ``function == "average"``, the only key is ``"weights"``
+          (np.ndarray or None, default: ``None``).
+        - Otherwise, there are no allowed keys.
+
+        If `function` is a list or tuple, `function_params` must be a list or
+        tuple of dictionaries (or ``None``) as above, of the same length as
+        `function`.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors. Ignored if `function` is one of the allowed string options.
+
+    Attributes
+    ----------
+    n_channels_ : int
+        Number of channels present in the 3D array passed to :meth:`fit`. Must
+        match the number of channels in the 3D array passed to
+        :meth:`transform`.
+
+    effective_function_ : callable or tuple
+        Callable, or tuple of callables or ``None``, describing the function(s)
+        used to compute features in each available channel. It is a single
+        callable only when `function` was passed as a string.
+
+    effective_function_params_ : dict or tuple
+        Dictionary or tuple of dictionaries containing all information present
+        in `function_params` as well as relevant quantities computed in
+        :meth:`fit`. It is a single dict only when `function` was passed as a
+        string. ``None``s are converted to empty dictionaries.
+
+    """
+    _hyperparameters = {
+        "function": {"type": (str, Callable, list, tuple),
+                     "in": tuple(_AVAILABLE_FUNCTIONS.keys()),
+                     "of": {"type": (str, Callable, type(None)),
+                            "in": tuple(_AVAILABLE_FUNCTIONS.keys())}},
+        "function_params": {"type": (dict, type(None), list, tuple)},
+        }
+
+    def __init__(self, function="max", function_params=None, n_jobs=None):
+        self.function = function
+        self.function_params = function_params
+        self.n_jobs = n_jobs
+
+    def _validate_params(self):
+        params = self.get_params().copy()
+        _hyperparameters = deepcopy(self._hyperparameters)
+        if not isinstance(self.function, str):
+            _hyperparameters["function"].pop("in")
+        try:
+            validate_params(params, _hyperparameters, exclude=["n_jobs"])
+        # Another go if we fail because function is a list/tuple containing
+        # callables and the "in" key checks fail
+        except ValueError as ve:
+            end_string = f"which is not in " \
+                         f"{tuple(_AVAILABLE_FUNCTIONS.keys())}."
+            function = params["function"]
+            if ve.args[0].endswith(end_string) \
+                    and isinstance(function, (list, tuple)):
+                params["function"] = [f for f in function
+                                      if isinstance(f, str)]
+                validate_params(params, _hyperparameters, exclude=["n_jobs"])
+            else:
+                raise ve
+
+        if isinstance(self.function, (list, tuple)) \
+                and isinstance(self.function_params, dict):
+            raise TypeError("If `function` is a list/tuple then "
+                            "`function_params` must be a list/tuple of dict, "
+                            "or None.")
+        elif isinstance(self.function, (str, Callable)) \
+                and isinstance(self.function_params, (list, tuple)):
+            raise TypeError("If `function` is a string or a callable "
+                            "function then `function_params` must be a dict "
+                            "or None.")
+
+    def fit(self, X, y=None):
+        """Compute :attr:`n_channels_` and :attr:`effective_function_params_`.
+        Then, return the estimator.
+
+        This function is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_channels, n_bins)
+            Input data. Collection of multi-channel curves.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_array(X, ensure_2d=False, allow_nd=True)
+        if X.ndim != 3:
+            raise ValueError("Input must be 3-dimensional.")
+        self._validate_params()
+
+        self.n_channels_ = X.shape[1]
+
+        if isinstance(self.function, str):
+            self.effective_function_ = \
+                _implemented_function_recipes[self.function]
+
+            if self.function_params is None:
+                self.effective_function_params_ = {}
+            else:
+                validate_params(self.function_params,
+                                _AVAILABLE_FUNCTIONS[self.function])
+                self.effective_function_params_ = self.function_params.copy()
+
+        elif isinstance(self.function, Callable):
+            self.effective_function_ = \
+                tuple([self.function] * self.n_channels_)
+
+            if self.function_params is None:
+                self.effective_function_params_ = \
+                    tuple([{}] * self.n_channels_)
+            else:
+                self.effective_function_params_ = \
+                    tuple([self.function_params.copy()] * self.n_channels_)
+        else:
+            n_functions = len(self.function)
+            if len(self.function) != self.n_channels_:
+                raise ValueError(
+                    f"`function` has length {n_functions} while curves in `X` "
+                    f"have {self.n_channels_} channels."
+                    )
+
+            if self.function_params is None:
+                self._effective_function_params = [{}] * self.n_channels_
+            else:
+                self._effective_function_params = self.function_params
+                n_function_params = len(self._effective_function_params)
+                if n_function_params != self.n_channels_:
+                    raise ValueError(f"`function_params` has length "
+                                     f"{n_function_params} while curves in "
+                                     f"`X` have {self.n_channels_} channels.")
+
+            self.effective_function_ = []
+            self.effective_function_params_ = []
+            for f, p in zip(self.function, self._effective_function_params):
+                if isinstance(f, str):
+                    validate_params(p, _AVAILABLE_FUNCTIONS[f])
+                    self.effective_function_.\
+                        append(_implemented_function_recipes[f])
+                else:
+                    self.effective_function_.append(f)
+                self.effective_function_params_.append({} if p is None
+                                                       else p.copy())
+            self.effective_function_ = tuple(self.effective_function_)
+            self.effective_function_params_ = \
+                tuple(self.effective_function_params_)
+
+        return self
+
+    def transform(self, X, y=None):
+        """Compute features of multi-channel curves.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_channels, n_bins)
+            Input collection of multi-channel curves.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_features)
+            Output collection of features of multi-channel curves.
+            ``n_features`` is the sum of the number of features output by the
+            (non-``None``) functions on their respective channels.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, ensure_2d=False, allow_nd=True)
+        if Xt.ndim != 3:
+            raise ValueError("Input must be 3-dimensional.")
+        if Xt.shape[1] != self.n_channels_:
+            raise ValueError(f"Number of channels must be the same as in "
+                             f"`fit`. Passed {Xt.shape[1]}, expected "
+                             f"{self.n_channels_}.")
+
+        Xt = _parallel_featurization(Xt, self.effective_function_,
+                                     self.effective_function_params_,
+                                     self.n_jobs)
+
+        return Xt
```

## gtda/curves/preprocessing.py

 * *Ordering differences only*

```diff
@@ -1,199 +1,199 @@
-"""Preprocessing transformers for curves."""
-# License: GNU AGPLv3
-
-import numpy as np
-from joblib import Parallel, delayed, effective_n_jobs
-from plotly.graph_objs import Figure, Scatter
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils import gen_even_slices
-from sklearn.utils.validation import check_is_fitted, check_array
-
-from ..base import PlotterMixin
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.intervals import Interval
-from ..utils.validation import validate_params
-
-
-@adapt_fit_transform_docs
-class Derivative(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Derivatives of multi-channel curves.
-
-    A multi-channel (integer sampled) curve is a 2D array of shape
-    ``(n_channels, n_bins)``, where each row represents the y-values in one of
-    the channels. This transformer computes the n-th order derivative of each
-    channel in each multi-channel curve in a collection, by discrete
-    differences. The output is another collection of multi-channel curves.
-
-    Parameters
-    ----------
-    order : int, optional, default: ``1``
-        Order of the derivative to be taken.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    n_channels_ : int
-        Number of channels present in the 3D array passed to :meth:`fit`.
-
-    """
-    _hyperparameters = {
-        'order': {'type': int, 'in': Interval(1, np.inf, closed='left')},
-    }
-
-    def __init__(self, order=1, n_jobs=None):
-        self.order = order
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Compute :attr:`n_channels_`. Then, return the estimator.
-
-        This function is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_channels, n_bins)
-            Input data. Collection of multi-channel curves.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_array(X, ensure_2d=False, allow_nd=True)
-        if X.ndim != 3:
-            raise ValueError("Input must be 3-dimensional.")
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        n_bins = X.shape[2]
-        if self.order >= n_bins:
-            raise ValueError(
-                f"Input channels have length {n_bins} but they must have at "
-                f"least length {self.order + 1} to calculate derivatives of "
-                f"order {self.order}."
-                )
-
-        self.n_channels_ = X.shape[1]
-
-        return self
-
-    def transform(self, X, y=None):
-        """Compute derivatives of multi-channel curves.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_channels, n_bins)
-            Input collection of multi-channel curves.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_channels, n_bins - order)
-            Output collection of multi-channel curves given by taking discrete
-            differences of order `order` in each channel in the curves in `X`.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, ensure_2d=False, allow_nd=True)
-        if Xt.ndim != 3:
-            raise ValueError("Input must be 3-dimensional.")
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(np.diff)(Xt[s], n=self.order, axis=-1)
-            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs))
-            )
-        Xt = np.concatenate(Xt)
-
-        return Xt
-
-    def plot(self, Xt, sample=0, channels=None, plotly_params=None):
-        """Plot a sample from a collection of derivatives of multi-channel
-        curves arranged as in the output of :meth:`transform`.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_channels, n_bins)
-            Collection of multi-channel curves, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        channels : list, tuple or None, optional, default: ``None``
-            Which channels to include in the plot. ``None`` means plotting the
-            first :attr:`n_channels_` channels.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        check_is_fitted(self)
-
-        layout_axes_common = {
-            "type": "linear",
-            "ticks": "outside",
-            "showline": True,
-            "zeroline": True,
-            "linewidth": 1,
-            "linecolor": "black",
-            "mirror": False,
-            "showexponent": "all",
-            "exponentformat": "e"
-            }
-        layout = {
-            "xaxis1": {
-                "title": "Sample",
-                "side": "bottom",
-                "anchor": "y1",
-                **layout_axes_common
-                },
-            "yaxis1": {
-                "title": "Derivative",
-                "side": "left",
-                "anchor": "x1",
-                **layout_axes_common
-                },
-            "plot_bgcolor": "white",
-            "title": f"Derivative of sample {sample}"
-            }
-
-        fig = Figure(layout=layout)
-
-        if channels is None:
-            channels = range(self.n_channels_)
-
-        samplings = np.arange(Xt[sample].shape[0])
-        for ix, channel in enumerate(channels):
-            fig.add_trace(Scatter(x=samplings,
-                                  y=Xt[sample][ix],
-                                  mode="lines",
-                                  showlegend=True,
-                                  name=f"Channel {channel}"))
-
-        # Update traces and layout according to user input
-        if plotly_params:
-            fig.update_traces(plotly_params.get("traces", None))
-            fig.update_layout(plotly_params.get("layout", None))
-
-        return fig
+"""Preprocessing transformers for curves."""
+# License: GNU AGPLv3
+
+import numpy as np
+from joblib import Parallel, delayed, effective_n_jobs
+from plotly.graph_objs import Figure, Scatter
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils import gen_even_slices
+from sklearn.utils.validation import check_is_fitted, check_array
+
+from ..base import PlotterMixin
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.intervals import Interval
+from ..utils.validation import validate_params
+
+
+@adapt_fit_transform_docs
+class Derivative(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Derivatives of multi-channel curves.
+
+    A multi-channel (integer sampled) curve is a 2D array of shape
+    ``(n_channels, n_bins)``, where each row represents the y-values in one of
+    the channels. This transformer computes the n-th order derivative of each
+    channel in each multi-channel curve in a collection, by discrete
+    differences. The output is another collection of multi-channel curves.
+
+    Parameters
+    ----------
+    order : int, optional, default: ``1``
+        Order of the derivative to be taken.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    n_channels_ : int
+        Number of channels present in the 3D array passed to :meth:`fit`.
+
+    """
+    _hyperparameters = {
+        'order': {'type': int, 'in': Interval(1, np.inf, closed='left')},
+    }
+
+    def __init__(self, order=1, n_jobs=None):
+        self.order = order
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Compute :attr:`n_channels_`. Then, return the estimator.
+
+        This function is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_channels, n_bins)
+            Input data. Collection of multi-channel curves.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_array(X, ensure_2d=False, allow_nd=True)
+        if X.ndim != 3:
+            raise ValueError("Input must be 3-dimensional.")
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        n_bins = X.shape[2]
+        if self.order >= n_bins:
+            raise ValueError(
+                f"Input channels have length {n_bins} but they must have at "
+                f"least length {self.order + 1} to calculate derivatives of "
+                f"order {self.order}."
+                )
+
+        self.n_channels_ = X.shape[1]
+
+        return self
+
+    def transform(self, X, y=None):
+        """Compute derivatives of multi-channel curves.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_channels, n_bins)
+            Input collection of multi-channel curves.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_channels, n_bins - order)
+            Output collection of multi-channel curves given by taking discrete
+            differences of order `order` in each channel in the curves in `X`.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, ensure_2d=False, allow_nd=True)
+        if Xt.ndim != 3:
+            raise ValueError("Input must be 3-dimensional.")
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(np.diff)(Xt[s], n=self.order, axis=-1)
+            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs))
+            )
+        Xt = np.concatenate(Xt)
+
+        return Xt
+
+    def plot(self, Xt, sample=0, channels=None, plotly_params=None):
+        """Plot a sample from a collection of derivatives of multi-channel
+        curves arranged as in the output of :meth:`transform`.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_channels, n_bins)
+            Collection of multi-channel curves, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        channels : list, tuple or None, optional, default: ``None``
+            Which channels to include in the plot. ``None`` means plotting the
+            first :attr:`n_channels_` channels.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        check_is_fitted(self)
+
+        layout_axes_common = {
+            "type": "linear",
+            "ticks": "outside",
+            "showline": True,
+            "zeroline": True,
+            "linewidth": 1,
+            "linecolor": "black",
+            "mirror": False,
+            "showexponent": "all",
+            "exponentformat": "e"
+            }
+        layout = {
+            "xaxis1": {
+                "title": "Sample",
+                "side": "bottom",
+                "anchor": "y1",
+                **layout_axes_common
+                },
+            "yaxis1": {
+                "title": "Derivative",
+                "side": "left",
+                "anchor": "x1",
+                **layout_axes_common
+                },
+            "plot_bgcolor": "white",
+            "title": f"Derivative of sample {sample}"
+            }
+
+        fig = Figure(layout=layout)
+
+        if channels is None:
+            channels = range(self.n_channels_)
+
+        samplings = np.arange(Xt[sample].shape[0])
+        for ix, channel in enumerate(channels):
+            fig.add_trace(Scatter(x=samplings,
+                                  y=Xt[sample][ix],
+                                  mode="lines",
+                                  showlegend=True,
+                                  name=f"Channel {channel}"))
+
+        # Update traces and layout according to user input
+        if plotly_params:
+            fig.update_traces(plotly_params.get("traces", None))
+            fig.update_layout(plotly_params.get("layout", None))
+
+        return fig
```

## gtda/curves/_functions.py

 * *Ordering differences only*

```diff
@@ -1,52 +1,52 @@
-# License: GNU AGPLv3
-
-import warnings
-from itertools import product
-
-import numpy as np
-from joblib import Parallel, delayed
-
-_AVAILABLE_FUNCTIONS = {
-    "identity": {},
-    "argmax": {},
-    "argmin": {},
-    "min": {},
-    "max": {},
-    "mean": {},
-    "std": {},
-    "median": {},
-    "average": {"weights": {"type": np.ndarray}}
-    }
-
-_implemented_function_recipes = {
-    "identity": lambda X, axis: X.reshape(len(X), -1),
-    "argmax": np.argmax,
-    "argmin": np.argmin,
-    "min": np.min,
-    "max": np.max,
-    "mean": np.mean,
-    "std": np.std,
-    "median": np.median,
-    "average": np.average
-    }
-
-
-def _parallel_featurization(Xt, function, function_params, n_jobs):
-    if callable(function):
-        return function(Xt, axis=-1, **function_params)
-    else:  # Assume function is a list or tuple of functions or None
-        channel_idx = [j for j, f in enumerate(function) if f is not None]
-        n_samples = len(Xt)
-        index_pairs = product(range(n_samples), channel_idx)
-        Xt = Parallel(n_jobs=n_jobs)(
-            delayed(function[j])(Xt[i, j], **function_params[j])
-            for i, j in index_pairs
-            )
-        with warnings.catch_warnings():
-            warnings.simplefilter("ignore",
-                                  category=np.VisibleDeprecationWarning)
-            Xt = np.array(Xt)
-        if Xt.dtype == np.dtype('object'):
-            Xt = np.concatenate(list(map(np.ravel, Xt)))
-
-        return Xt.reshape(n_samples, -1)
+# License: GNU AGPLv3
+
+import warnings
+from itertools import product
+
+import numpy as np
+from joblib import Parallel, delayed
+
+_AVAILABLE_FUNCTIONS = {
+    "identity": {},
+    "argmax": {},
+    "argmin": {},
+    "min": {},
+    "max": {},
+    "mean": {},
+    "std": {},
+    "median": {},
+    "average": {"weights": {"type": np.ndarray}}
+    }
+
+_implemented_function_recipes = {
+    "identity": lambda X, axis: X.reshape(len(X), -1),
+    "argmax": np.argmax,
+    "argmin": np.argmin,
+    "min": np.min,
+    "max": np.max,
+    "mean": np.mean,
+    "std": np.std,
+    "median": np.median,
+    "average": np.average
+    }
+
+
+def _parallel_featurization(Xt, function, function_params, n_jobs):
+    if callable(function):
+        return function(Xt, axis=-1, **function_params)
+    else:  # Assume function is a list or tuple of functions or None
+        channel_idx = [j for j, f in enumerate(function) if f is not None]
+        n_samples = len(Xt)
+        index_pairs = product(range(n_samples), channel_idx)
+        Xt = Parallel(n_jobs=n_jobs)(
+            delayed(function[j])(Xt[i, j], **function_params[j])
+            for i, j in index_pairs
+            )
+        with warnings.catch_warnings():
+            warnings.simplefilter("ignore",
+                                  category=np.VisibleDeprecationWarning)
+            Xt = np.array(Xt)
+        if Xt.dtype == np.dtype('object'):
+            Xt = np.concatenate(list(map(np.ravel, Xt)))
+
+        return Xt.reshape(n_samples, -1)
```

## gtda/curves/__init__.py

 * *Ordering differences only*

```diff
@@ -1,10 +1,10 @@
-"""The module :mod:`gtda.curves` implements transformers to postprocess
-curves."""
-
-from .preprocessing import Derivative
-from .features import StandardFeatures
-
-__all__ = [
-    "Derivative",
-    "StandardFeatures"
-    ]
+"""The module :mod:`gtda.curves` implements transformers to postprocess
+curves."""
+
+from .preprocessing import Derivative
+from .features import StandardFeatures
+
+__all__ = [
+    "Derivative",
+    "StandardFeatures"
+    ]
```

## gtda/diagrams/distance.py

 * *Ordering differences only*

```diff
@@ -1,242 +1,242 @@
-"""Pairwise distance calculations for persistence diagrams."""
-# License: GNU AGPLv3
-
-from numbers import Real
-
-import numpy as np
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils.validation import check_is_fitted
-
-from ._metrics import _AVAILABLE_METRICS, _parallel_pairwise
-from ._utils import _bin, _homology_dimensions_to_sorted_ints
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.intervals import Interval
-from ..utils.validation import check_diagrams, validate_params
-
-
-@adapt_fit_transform_docs
-class PairwiseDistance(BaseEstimator, TransformerMixin):
-    """:ref:`Distances <wasserstein_and_bottleneck_distance>` between pairs
-    of persistence diagrams.
-
-    Given two collections of persistence diagrams consisting of
-    birth-death-dimension triples [b, d, q], a collection of distance
-    matrices or a single distance matrix between pairs of diagrams is
-    calculated according to the following steps:
-
-        1. All diagrams are partitioned into subdiagrams corresponding to
-           distinct homology dimensions.
-        2. Pairwise distances between subdiagrams of equal homology
-           dimension are calculated according to the parameters `metric` and
-           `metric_params`. This gives a collection of distance matrices,
-           :math:`\\mathbf{D} = (D_{q_1}, \\ldots, D_{q_n})`.
-        3. The final result is either :math:`\\mathbf{D}` itself as a
-           three-dimensional array, or a single distance matrix constructed
-           by taking norms of the vectors of distances between diagram pairs.
-
-    **Important notes**:
-
-        - Input collections of persistence diagrams for this transformer must
-          satisfy certain requirements, see e.g. :meth:`fit`.
-        - The shape of outputs of :meth:`transform` depends on the value of the
-          `order` parameter.
-
-    Parameters
-    ----------
-    metric : ``'bottleneck'`` | ``'wasserstein'`` | ``'betti'`` | \
-        ``'landscape'`` | ``'silhouette'`` | ``'heat'`` | \
-        ``'persistence_image'``, optional, default: ``'landscape'``
-        Distance or dissimilarity function between subdiagrams:
-
-        - ``'bottleneck'`` and ``'wasserstein'`` refer to the identically named
-          perfect-matching--based notions of distance.
-        - ``'betti'`` refers to the :math:`L^p` distance between Betti curves.
-        - ``'landscape'`` refers to the :math:`L^p` distance between
-          persistence landscapes.
-        - ``'silhouette'`` refers to the :math:`L^p` distance between
-          silhouettes.
-        - ``'heat'`` refers to the :math:`L^p` distance between
-          Gaussian-smoothed diagrams.
-        - ``'persistence_image'`` refers to the :math:`L^p` distance between
-          Gaussian-smoothed diagrams represented on birth-persistence axes.
-
-    metric_params : dict or None, optional, default: ``None``
-        Additional keyword arguments for the metric function (passing
-        ``None`` is equivalent to passing the defaults described below):
-
-        - If ``metric == 'bottleneck'`` the only argument is `delta` (float,
-          default: ``0.01``). When equal to ``0.``, an exact algorithm is used;
-          otherwise, a faster approximate algorithm is used and symmetry is not
-          guaranteed.
-        - If ``metric == 'wasserstein'`` the available arguments are `p`
-          (float, default: ``2.``) and `delta` (float, default: ``0.01``).
-          Unlike the case of ``'bottleneck'``, `delta` cannot be set to ``0.``
-          and an exact algorithm is not available.
-        - If ``metric == 'betti'`` the available arguments are `p` (float,
-          default: ``2.``) and `n_bins` (int, default: ``100``).
-        - If ``metric == 'landscape'`` the available arguments are `p` (float,
-          default: ``2.``), `n_bins` (int, default: ``100``) and `n_layers`
-          (int, default: ``1``).
-        - If ``metric == 'silhouette'`` the available arguments are `p` (float,
-          default: ``2.``), `power` (float, default: ``1.``) and `n_bins` (int,
-          default: ``100``).
-        - If ``metric == 'heat'`` the available arguments are `p` (float,
-          default: ``2.``), `sigma` (float, default: ``0.1``) and `n_bins`
-          (int, default: ``100``).
-        - If ``metric == 'persistence_image'`` the available arguments are `p`
-          (float, default: ``2.``), `sigma` (float, default: ``0.1``), `n_bins`
-          (int, default: ``100``) and `weight_function` (callable or None,
-          default: ``None``).
-
-    order : float or None, optional, default: ``2.``
-        If ``None``, :meth:`transform` returns for each pair of diagrams a
-        vector of distances corresponding to the dimensions in
-        :attr:`homology_dimensions_`. Otherwise, the :math:`p`-norm of
-        these vectors with :math:`p` equal to `order` is taken.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    effective_metric_params_ : dict
-        Dictionary containing all information present in `metric_params` as
-        well as relevant quantities computed in :meth:`fit`.
-
-    homology_dimensions_ : tuple
-        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
-
-    See also
-    --------
-    Amplitude, Scaler, Filtering, BettiCurve, PersistenceLandscape, \
-    PersistenceImage, HeatKernel, Silhouette, \
-    gtda.homology.VietorisRipsPersistence
-
-    Notes
-    -----
-    To compute distances without first splitting the computation between
-    different homology dimensions, data should be first transformed by an
-    instance of :class:`ForgetDimension`.
-
-    `Hera <https://bitbucket.org/grey_narn/hera>`_ is used as a C++ backend
-    for computing bottleneck and Wasserstein distances between persistence
-    diagrams. Python bindings were modified for performance from the
-    `Dyonisus 2 <https://mrzv.org/software/dionysus2/>`_ package.
-
-    """
-
-    _hyperparameters = {
-        'metric': {'type': str, 'in': _AVAILABLE_METRICS.keys()},
-        'order': {'type': (Real, type(None)),
-                  'in': Interval(0, np.inf, closed='right')},
-        'metric_params': {'type': (dict, type(None))}
-        }
-
-    def __init__(self, metric='landscape', metric_params=None, order=2.,
-                 n_jobs=None):
-        self.metric = metric
-        self.metric_params = metric_params
-        self.order = order
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Store all observed homology dimensions in
-        :attr:`homology_dimensions_` and compute
-        :attr:`effective_metric_params_`. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples_fit, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of `X`.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_diagrams(X)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        if self.metric_params is None:
-            self.effective_metric_params_ = {}
-        else:
-            self.effective_metric_params_ = self.metric_params.copy()
-        validate_params(
-            self.effective_metric_params_, _AVAILABLE_METRICS[self.metric])
-
-        # Find the unique homology dimensions in the 3D array X passed to `fit`
-        # assuming that they can all be found in its zero-th entry
-        homology_dimensions_fit = np.unique(X[0, :, 2])
-        self.homology_dimensions_ = \
-            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
-
-        self.effective_metric_params_['samplings'], \
-            self.effective_metric_params_['step_sizes'] = \
-            _bin(X, self.metric, **self.effective_metric_params_)
-
-        if self.metric == 'persistence_image':
-            weight_function = self.effective_metric_params_.get(
-                'weight_function', None
-                )
-            weight_function = \
-                np.ones_like if weight_function is None else weight_function
-            self.effective_metric_params_['weight_function'] = weight_function
-
-        self._X = X
-        return self
-
-    def transform(self, X, y=None):
-        """Computes a distance or vector of distances between the diagrams in
-        `X` and the diagrams seen in :meth:`fit`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of `X`.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_samples_fit, \
-            n_homology_dimensions) if `order` is ``None``, else \
-            (n_samples, n_samples_fit)
-            Distance matrix or collection of distance matrices between
-            diagrams in `X` and diagrams seen in :meth:`fit`. In the
-            second case, index i along axis 2 corresponds to the i-th
-            homology dimension in :attr:`homology_dimensions_`.
-
-        """
-        check_is_fitted(self)
-        Xt = check_diagrams(X, copy=True)
-
-        Xt = _parallel_pairwise(Xt, self._X, self.metric,
-                                self.effective_metric_params_,
-                                self.homology_dimensions_,
-                                self.n_jobs)
-        if self.order is not None:
-            Xt = np.linalg.norm(Xt, axis=2, ord=self.order)
-
-        return Xt
+"""Pairwise distance calculations for persistence diagrams."""
+# License: GNU AGPLv3
+
+from numbers import Real
+
+import numpy as np
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils.validation import check_is_fitted
+
+from ._metrics import _AVAILABLE_METRICS, _parallel_pairwise
+from ._utils import _bin, _homology_dimensions_to_sorted_ints
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.intervals import Interval
+from ..utils.validation import check_diagrams, validate_params
+
+
+@adapt_fit_transform_docs
+class PairwiseDistance(BaseEstimator, TransformerMixin):
+    """:ref:`Distances <wasserstein_and_bottleneck_distance>` between pairs
+    of persistence diagrams.
+
+    Given two collections of persistence diagrams consisting of
+    birth-death-dimension triples [b, d, q], a collection of distance
+    matrices or a single distance matrix between pairs of diagrams is
+    calculated according to the following steps:
+
+        1. All diagrams are partitioned into subdiagrams corresponding to
+           distinct homology dimensions.
+        2. Pairwise distances between subdiagrams of equal homology
+           dimension are calculated according to the parameters `metric` and
+           `metric_params`. This gives a collection of distance matrices,
+           :math:`\\mathbf{D} = (D_{q_1}, \\ldots, D_{q_n})`.
+        3. The final result is either :math:`\\mathbf{D}` itself as a
+           three-dimensional array, or a single distance matrix constructed
+           by taking norms of the vectors of distances between diagram pairs.
+
+    **Important notes**:
+
+        - Input collections of persistence diagrams for this transformer must
+          satisfy certain requirements, see e.g. :meth:`fit`.
+        - The shape of outputs of :meth:`transform` depends on the value of the
+          `order` parameter.
+
+    Parameters
+    ----------
+    metric : ``'bottleneck'`` | ``'wasserstein'`` | ``'betti'`` | \
+        ``'landscape'`` | ``'silhouette'`` | ``'heat'`` | \
+        ``'persistence_image'``, optional, default: ``'landscape'``
+        Distance or dissimilarity function between subdiagrams:
+
+        - ``'bottleneck'`` and ``'wasserstein'`` refer to the identically named
+          perfect-matching--based notions of distance.
+        - ``'betti'`` refers to the :math:`L^p` distance between Betti curves.
+        - ``'landscape'`` refers to the :math:`L^p` distance between
+          persistence landscapes.
+        - ``'silhouette'`` refers to the :math:`L^p` distance between
+          silhouettes.
+        - ``'heat'`` refers to the :math:`L^p` distance between
+          Gaussian-smoothed diagrams.
+        - ``'persistence_image'`` refers to the :math:`L^p` distance between
+          Gaussian-smoothed diagrams represented on birth-persistence axes.
+
+    metric_params : dict or None, optional, default: ``None``
+        Additional keyword arguments for the metric function (passing
+        ``None`` is equivalent to passing the defaults described below):
+
+        - If ``metric == 'bottleneck'`` the only argument is `delta` (float,
+          default: ``0.01``). When equal to ``0.``, an exact algorithm is used;
+          otherwise, a faster approximate algorithm is used and symmetry is not
+          guaranteed.
+        - If ``metric == 'wasserstein'`` the available arguments are `p`
+          (float, default: ``2.``) and `delta` (float, default: ``0.01``).
+          Unlike the case of ``'bottleneck'``, `delta` cannot be set to ``0.``
+          and an exact algorithm is not available.
+        - If ``metric == 'betti'`` the available arguments are `p` (float,
+          default: ``2.``) and `n_bins` (int, default: ``100``).
+        - If ``metric == 'landscape'`` the available arguments are `p` (float,
+          default: ``2.``), `n_bins` (int, default: ``100``) and `n_layers`
+          (int, default: ``1``).
+        - If ``metric == 'silhouette'`` the available arguments are `p` (float,
+          default: ``2.``), `power` (float, default: ``1.``) and `n_bins` (int,
+          default: ``100``).
+        - If ``metric == 'heat'`` the available arguments are `p` (float,
+          default: ``2.``), `sigma` (float, default: ``0.1``) and `n_bins`
+          (int, default: ``100``).
+        - If ``metric == 'persistence_image'`` the available arguments are `p`
+          (float, default: ``2.``), `sigma` (float, default: ``0.1``), `n_bins`
+          (int, default: ``100``) and `weight_function` (callable or None,
+          default: ``None``).
+
+    order : float or None, optional, default: ``2.``
+        If ``None``, :meth:`transform` returns for each pair of diagrams a
+        vector of distances corresponding to the dimensions in
+        :attr:`homology_dimensions_`. Otherwise, the :math:`p`-norm of
+        these vectors with :math:`p` equal to `order` is taken.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    effective_metric_params_ : dict
+        Dictionary containing all information present in `metric_params` as
+        well as relevant quantities computed in :meth:`fit`.
+
+    homology_dimensions_ : tuple
+        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
+
+    See also
+    --------
+    Amplitude, Scaler, Filtering, BettiCurve, PersistenceLandscape, \
+    PersistenceImage, HeatKernel, Silhouette, \
+    gtda.homology.VietorisRipsPersistence
+
+    Notes
+    -----
+    To compute distances without first splitting the computation between
+    different homology dimensions, data should be first transformed by an
+    instance of :class:`ForgetDimension`.
+
+    `Hera <https://bitbucket.org/grey_narn/hera>`_ is used as a C++ backend
+    for computing bottleneck and Wasserstein distances between persistence
+    diagrams. Python bindings were modified for performance from the
+    `Dyonisus 2 <https://mrzv.org/software/dionysus2/>`_ package.
+
+    """
+
+    _hyperparameters = {
+        'metric': {'type': str, 'in': _AVAILABLE_METRICS.keys()},
+        'order': {'type': (Real, type(None)),
+                  'in': Interval(0, np.inf, closed='right')},
+        'metric_params': {'type': (dict, type(None))}
+        }
+
+    def __init__(self, metric='landscape', metric_params=None, order=2.,
+                 n_jobs=None):
+        self.metric = metric
+        self.metric_params = metric_params
+        self.order = order
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Store all observed homology dimensions in
+        :attr:`homology_dimensions_` and compute
+        :attr:`effective_metric_params_`. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples_fit, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of `X`.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_diagrams(X)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        if self.metric_params is None:
+            self.effective_metric_params_ = {}
+        else:
+            self.effective_metric_params_ = self.metric_params.copy()
+        validate_params(
+            self.effective_metric_params_, _AVAILABLE_METRICS[self.metric])
+
+        # Find the unique homology dimensions in the 3D array X passed to `fit`
+        # assuming that they can all be found in its zero-th entry
+        homology_dimensions_fit = np.unique(X[0, :, 2])
+        self.homology_dimensions_ = \
+            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
+
+        self.effective_metric_params_['samplings'], \
+            self.effective_metric_params_['step_sizes'] = \
+            _bin(X, self.metric, **self.effective_metric_params_)
+
+        if self.metric == 'persistence_image':
+            weight_function = self.effective_metric_params_.get(
+                'weight_function', None
+                )
+            weight_function = \
+                np.ones_like if weight_function is None else weight_function
+            self.effective_metric_params_['weight_function'] = weight_function
+
+        self._X = X
+        return self
+
+    def transform(self, X, y=None):
+        """Computes a distance or vector of distances between the diagrams in
+        `X` and the diagrams seen in :meth:`fit`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of `X`.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_samples_fit, \
+            n_homology_dimensions) if `order` is ``None``, else \
+            (n_samples, n_samples_fit)
+            Distance matrix or collection of distance matrices between
+            diagrams in `X` and diagrams seen in :meth:`fit`. In the
+            second case, index i along axis 2 corresponds to the i-th
+            homology dimension in :attr:`homology_dimensions_`.
+
+        """
+        check_is_fitted(self)
+        Xt = check_diagrams(X, copy=True)
+
+        Xt = _parallel_pairwise(Xt, self._X, self.metric,
+                                self.effective_metric_params_,
+                                self.homology_dimensions_,
+                                self.n_jobs)
+        if self.order is not None:
+            Xt = np.linalg.norm(Xt, axis=2, ord=self.order)
+
+        return Xt
```

## gtda/diagrams/features.py

```diff
@@ -1,723 +1,723 @@
-"""Feature extraction from persistence diagrams."""
-# License: GNU AGPLv3
-
-from numbers import Real
-
-import numpy as np
-from joblib import Parallel, delayed, effective_n_jobs
-from scipy.stats import entropy
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils import gen_even_slices
-from sklearn.utils.validation import check_is_fitted
-
-from ._metrics import _AVAILABLE_AMPLITUDE_METRICS, _parallel_amplitude
-from ._features import _AVAILABLE_POLYNOMIALS, _implemented_polynomial_recipes
-from ._utils import _subdiagrams, _bin, _homology_dimensions_to_sorted_ints
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.intervals import Interval
-from ..utils.validation import validate_params, check_diagrams
-
-
-@adapt_fit_transform_docs
-class PersistenceEntropy(BaseEstimator, TransformerMixin):
-    """:ref:`Persistence entropies <persistence_entropy>` of persistence
-    diagrams.
-
-    Based on ideas in [1]_. Given a persistence diagram consisting of
-    birth-death-dimension triples [b, d, q], subdiagrams corresponding to
-    distinct homology dimensions are considered separately, and their
-    respective persistence entropies are calculated as the (base 2) Shannon
-    entropies of the collections of differences d - b ("lifetimes"), normalized
-    by the sum of all such differences. Optionally, these entropies can be
-    normalized according to a simple heuristic, see `normalize`.
-
-    **Important notes**:
-
-        - Input collections of persistence diagrams for this transformer must
-          satisfy certain requirements, see e.g. :meth:`fit`.
-        - By default, persistence subdiagrams containing only triples with zero
-          lifetime will have corresponding (normalized) entropies computed as
-          ``numpy.nan``. To avoid this, set a value of `nan_fill_value`
-          different from ``None``.
-
-    Parameters
-    ----------
-    normalize : bool, optional, default: ``False``
-        When ``True``, the persistence entropy of each diagram is normalized by
-        the logarithm of the sum of lifetimes of all points in the diagram.
-        Can aid comparison between diagrams in an input collection when these
-        have different numbers of (non-trivial) points. See [2]_.
-
-    nan_fill_value : float or None, optional, default: ``-1.``
-        If a float, (normalized) persistence entropies initially computed as
-        ``numpy.nan`` are replaced with this value. If ``None``, these values
-        are left as ``numpy.nan``.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    homology_dimensions_ : tuple
-        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
-
-    See also
-    --------
-    NumberOfPoints, Amplitude, BettiCurve, PersistenceLandscape, HeatKernel, \
-    Silhouette, PersistenceImage
-
-    References
-    ----------
-    .. [1] H. Chintakunta et al, "An entropy-based persistence barcode";
-           *Pattern Recognition* **48**, 2, 2015;
-           `DOI: 10.1016/j.patcog.2014.06.023
-           <https://doi.org/10.1016/j.patcog.2014.06.023>`_.
-
-    .. [2] A. Myers, E. Munch, and F. A. Khasawneh, "Persistent Homology of
-           Complex Networks for Dynamic State Detection"; *Phys. Rev. E*
-           **100**, 022314, 2019; `DOI: 10.1103/PhysRevE.100.022314
-           <https://doi.org/10.1103/PhysRevE.100.022314>`_.
-
-    """
-
-    _hyperparameters = {
-        'normalize': {'type': bool},
-        'nan_fill_value': {'type': (Real, type(None))}
-        }
-
-    def __init__(self, normalize=False, nan_fill_value=-1., n_jobs=None):
-        self.normalize = normalize
-        self.nan_fill_value = nan_fill_value
-        self.n_jobs = n_jobs
-
-    @staticmethod
-    def _persistence_entropy(X, normalize=False, nan_fill_value=None):
-        X_lifespan = X[:, :, 1] - X[:, :, 0]
-        X_entropy = entropy(X_lifespan, base=2, axis=1)
-        if normalize:
-            lifespan_sums = np.sum(X_lifespan, axis=1)
-            X_entropy /= np.log2(lifespan_sums)
-        if nan_fill_value is not None:
-            np.nan_to_num(X_entropy, nan=nan_fill_value, copy=False)
-        X_entropy = X_entropy[:, None]
-        return X_entropy
-
-    def fit(self, X, y=None):
-        """Store all observed homology dimensions in
-        :attr:`homology_dimensions_`. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of `X`.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_diagrams(X)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        # Find the unique homology dimensions in the 3D array X passed to `fit`
-        # assuming that they can all be found in its zero-th entry
-        homology_dimensions_fit = np.unique(X[0, :, 2])
-        self.homology_dimensions_ = \
-            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
-        self._n_dimensions = len(self.homology_dimensions_)
-
-        return self
-
-    def transform(self, X, y=None):
-        """Compute the persistence entropies of diagrams in `X`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of `X`.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_homology_dimensions)
-            Persistence entropies: one value per sample and per homology
-            dimension seen in :meth:`fit`. Index i along axis 1 corresponds to
-            the i-th homology dimension in :attr:`homology_dimensions_`.
-
-        """
-        check_is_fitted(self)
-        X = check_diagrams(X)
-
-        with np.errstate(divide='ignore', invalid='ignore'):
-            Xt = Parallel(n_jobs=self.n_jobs)(
-                delayed(self._persistence_entropy)(
-                    _subdiagrams(X[s], [dim]),
-                    normalize=self.normalize,
-                    nan_fill_value=self.nan_fill_value
-                    )
-                for dim in self.homology_dimensions_
-                for s in gen_even_slices(len(X), effective_n_jobs(self.n_jobs))
-                )
-        Xt = np.concatenate(Xt).reshape(self._n_dimensions, len(X)).T
-
-        return Xt
-
-
-@adapt_fit_transform_docs
-class Amplitude(BaseEstimator, TransformerMixin):
-    """:ref:`Amplitudes <vectorization_amplitude_and_kernel>` of persistence
-    diagrams.
-
-    For each persistence diagram in a collection, a vector of amplitudes or a
-    single scalar amplitude is calculated according to the following steps:
-
-        1. The diagram is partitioned into subdiagrams according to homology
-           dimension.
-        2. The amplitude of each subdiagram is calculated according to the
-           parameters `metric` and `metric_params`. This gives a vector of
-           amplitudes, :math:`\\mathbf{a} = (a_{q_1}, \\ldots, a_{q_n})` where
-           the :math:`q_i` range over the available homology dimensions.
-        3. The final result is either :math:`\\mathbf{a}` itself or a norm of
-           :math:`\\mathbf{a}`, specified by the parameter `order`.
-
-    **Important notes**:
-
-        - Input collections of persistence diagrams for this transformer must
-          satisfy certain requirements, see e.g. :meth:`fit`.
-        - The shape of outputs of :meth:`transform` depends on the value of the
-          `order` parameter.
-
-    Parameters
-    ----------
-    metric : ``'bottleneck'`` | ``'wasserstein'`` | ``'betti'`` | \
-        ``'landscape'`` | ``'silhouette'`` | ``'heat'`` | \
-        ``'persistence_image'``, optional, default: ``'landscape'``
-        Distance or dissimilarity function used to define the amplitude of a
-        subdiagram as its distance from the (trivial) diagonal diagram:
-
-        - ``'bottleneck'`` and ``'wasserstein'`` refer to the identically named
-          perfect-matching--based notions of distance.
-        - ``'betti'`` refers to the :math:`L^p` distance between Betti curves.
-        - ``'landscape'`` refers to the :math:`L^p` distance between
-          persistence landscapes.
-        - ``'silhouette'`` refers to the :math:`L^p` distance between
-          silhouettes.
-        - ``'heat'`` refers to the :math:`L^p` distance between
-          Gaussian-smoothed diagrams.
-        - ``'persistence_image'`` refers to the :math:`L^p` distance between
-          Gaussian-smoothed diagrams represented on birth-persistence axes.
-
-    metric_params : dict or None, optional, default: ``None``
-        Additional keyword arguments for the metric function (passing ``None``
-        is equivalent to passing the defaults described below):
-
-        - If ``metric == 'bottleneck'`` there are no available arguments.
-        - If ``metric == 'wasserstein'`` the only argument is `p` (float,
-          default: ``2.``).
-        - If ``metric == 'betti'`` the available arguments are `p` (float,
-          default: ``2.``) and `n_bins` (int, default: ``100``).
-        - If ``metric == 'landscape'`` the available arguments are `p` (float,
-          default: ``2.``), `n_bins` (int, default: ``100``) and `n_layers`
-          (int, default: ``1``).
-        - If ``metric == 'silhouette'`` the available arguments are `p` (float,
-          default: ``2.``), `power` (float, default: ``1.``) and `n_bins` (int,
-          default: ``100``).
-        - If ``metric == 'heat'`` the available arguments are `p` (float,
-          default: ``2.``), `sigma` (float, default: ``0.1``) and `n_bins`
-          (int, default: ``100``).
-        - If ``metric == 'persistence_image'`` the available arguments are `p`
-          (float, default: ``2.``), `sigma` (float, default: ``0.1``), `n_bins`
-          (int, default: ``100``) and `weight_function` (callable or None,
-          default: ``None``).
-
-    order : float or None, optional, default: ``None``
-        If ``None``, :meth:`transform` returns for each diagram a vector of
-        amplitudes corresponding to the dimensions in
-        :attr:`homology_dimensions_`. Otherwise, the :math:`p`-norm of these
-        vectors with :math:`p` equal to `order` is taken.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    effective_metric_params_ : dict
-        Dictionary containing all information present in `metric_params` as
-        well as relevant quantities computed in :meth:`fit`.
-
-    homology_dimensions_ : tuple
-        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
-
-    See also
-    --------
-    NumberOfPoints, PersistenceEntropy, PairwiseDistance, Scaler, Filtering, \
-    BettiCurve, PersistenceLandscape, HeatKernel, Silhouette, PersistenceImage
-
-    Notes
-    -----
-    To compute amplitudes without first splitting the computation between
-    different homology dimensions, data should be first transformed by an
-    instance of :class:`ForgetDimension`.
-
-    """
-
-    _hyperparameters = {
-        'metric': {'type': str, 'in': _AVAILABLE_AMPLITUDE_METRICS.keys()},
-        'order': {'type': (Real, type(None)),
-                  'in': Interval(0, np.inf, closed='right')},
-        'metric_params': {'type': (dict, type(None))}
-        }
-
-    def __init__(self, metric='landscape', metric_params=None, order=None,
-                 n_jobs=None):
-        self.metric = metric
-        self.metric_params = metric_params
-        self.order = order
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Store all observed homology dimensions in
-        :attr:`homology_dimensions_` and compute
-        :attr:`effective_metric_params`. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_diagrams(X)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        if self.metric_params is None:
-            self.effective_metric_params_ = {}
-        else:
-            self.effective_metric_params_ = self.metric_params.copy()
-        validate_params(self.effective_metric_params_,
-                        _AVAILABLE_AMPLITUDE_METRICS[self.metric])
-
-        # Find the unique homology dimensions in the 3D array X passed to `fit`
-        # assuming that they can all be found in its zero-th entry
-        homology_dimensions_fit = np.unique(X[0, :, 2])
-        self.homology_dimensions_ = \
-            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
-
-        self.effective_metric_params_['samplings'], \
-            self.effective_metric_params_['step_sizes'] = \
-            _bin(X, self.metric, **self.effective_metric_params_)
-
-        if self.metric == 'persistence_image':
-            weight_function = self.effective_metric_params_.get(
-                'weight_function', None
-                )
-            weight_function = \
-                np.ones_like if weight_function is None else weight_function
-            self.effective_metric_params_['weight_function'] = weight_function
-
-        return self
-
-    def transform(self, X, y=None):
-        """Compute the amplitudes or amplitude vectors of diagrams in `X`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_homology_dimensions) if `order` \
-            is ``None``, else (n_samples, 1)
-            Amplitudes or amplitude vectors of the diagrams in `X`. In the
-            second case, index i along axis 1 corresponds to the i-th homology
-            dimension in :attr:`homology_dimensions_`.
-
-        """
-        check_is_fitted(self)
-        Xt = check_diagrams(X, copy=True)
-
-        Xt = _parallel_amplitude(Xt, self.metric,
-                                 self.effective_metric_params_,
-                                 self.homology_dimensions_,
-                                 self.n_jobs)
-        if self.order is not None:
-            Xt = np.linalg.norm(Xt, axis=1, ord=self.order).reshape(-1, 1)
-
-        return Xt
-
-
-@adapt_fit_transform_docs
-class NumberOfPoints(BaseEstimator, TransformerMixin):
-    """Number of off-diagonal points in persistence diagrams, per homology
-    dimension.
-
-    Given a persistence diagram consisting of birth-death-dimension triples
-    [b, d, q], subdiagrams corresponding to distinct homology dimensions are
-    considered separately, and their respective numbers of off-diagonal points
-    are calculated.
-
-    **Important note**:
-
-        - Input collections of persistence diagrams for this transformer must
-          satisfy certain requirements, see e.g. :meth:`fit`.
-
-    Parameters
-    ----------
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    homology_dimensions_ : list
-        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
-
-    See also
-    --------
-    PersistenceEntropy, Amplitude, BettiCurve, PersistenceLandscape,
-    HeatKernel, Silhouette, PersistenceImage
-
-    """
-
-    def __init__(self, n_jobs=None):
-        self.n_jobs = n_jobs
-
-    @staticmethod
-    def _number_points(X):
-        return np.count_nonzero(X[:, :, 1] - X[:, :, 0], axis=1)
-
-    def fit(self, X, y=None):
-        """Store all observed homology dimensions in
-        :attr:`homology_dimensions_`. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of `X`.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_diagrams(X)
-
-        # Find the unique homology dimensions in the 3D array X passed to `fit`
-        # assuming that they can all be found in its zero-th entry
-        homology_dimensions_fit = np.unique(X[0, :, 2])
-        self.homology_dimensions_ = \
-            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
-        self._n_dimensions = len(self.homology_dimensions_)
-
-        return self
-
-    def transform(self, X, y=None):
-        """Compute a vector of numbers of off-diagonal points for each diagram
-        in `X`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of `X`.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_homology_dimensions)
-            Number of points: one value per sample and per homology dimension
-            seen in :meth:`fit`. Index i along axis 1 corresponds to the i-th
-            homology dimension in :attr:`homology_dimensions_`.
-
-        """
-        check_is_fitted(self)
-        X = check_diagrams(X)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._number_points)(_subdiagrams(X, [dim])[s])
-            for dim in self.homology_dimensions_
-            for s in gen_even_slices(len(X), effective_n_jobs(self.n_jobs))
-            )
-        Xt = np.concatenate(Xt).reshape(self._n_dimensions, len(X)).T
-
-        return Xt
-
-
-@adapt_fit_transform_docs
-class ComplexPolynomial(BaseEstimator, TransformerMixin):
-    """Coefficients of complex polynomials whose roots are obtained from points
-    in persistence diagrams.
-
-    Given a persistence diagram consisting of birth-death-dimension triples
-    [b, d, q], subdiagrams corresponding to distinct homology dimensions are
-    first considered separately. For each subdiagram, the polynomial whose
-    roots are complex numbers obtained from its birth-death pairs is
-    computed, and its :attr:`n_coefficients_` highest-degree complex
-    coefficients excluding the top one are stored into a single real vector
-    by concatenating the vector of all real parts with the vector of all
-    imaginary parts [1]_ (if not enough coefficients are available to form a
-    vector of the required length, padding with zeros is performed). Finally,
-    all such vectors coming from different subdiagrams are concatenated to
-    yield a single vector for the diagram.
-
-    There are three possibilities for mapping birth-death pairs :math:`(b, d)`
-    to complex polynomial roots. They are:
-
-    .. math::
-       :nowrap:
-
-       \\begin{gather*}
-       R(b, d) = b + \\mathrm{i} d, \\\\
-       S(b, d) = \\frac{d - b}{\\sqrt{2} r} (b + \\mathrm{i} d), \\\\
-       T(b, d) = \\frac{d - b}{2} [\\cos{r} - \\sin{r} + \
-       \\mathrm{i}(\\cos{r} + \\sin{r})],
-       \\end{gather*}
-
-    where :math:`r = \\sqrt{b^2 + d^2}`.
-
-    **Important note**:
-
-        - Input collections of persistence diagrams for this transformer must
-          satisfy certain requirements, see e.g. :meth:`fit`.
-
-    Parameters
-    ----------
-    polynomial_type : ``'R'`` | ``'S'`` | ``'T'``, optional, default: ``'R'``
-        Type of complex polynomial to compute.
-
-    n_coefficients : list, int or None, optional, default: ``10``
-        Number of complex coefficients per homology dimension. If an int then
-        the number of coefficients will be equal to that value for each
-        homology dimension. If ``None`` then, for each homology dimension in
-        the collection of persistence diagrams seen in :meth:`fit`, the number
-        of complex coefficients is defined to be the largest number of
-        off-diagonal points seen among all subdiagrams in that homology
-        dimension, minus one.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    homology_dimensions_ : list
-        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
-
-    n_coefficients_ : list
-        Effective number of complex coefficients per homology dimension. Set in
-        :meth:`fit`.
-
-    See also
-    --------
-    Amplitude, PersistenceEntropy
-
-    References
-    ----------
-    .. [1] B. Di Fabio and M. Ferri, "Comparing Persistence Diagrams Through
-           Complex Vectors"; in *Image Analysis and Processing  ICIAP 2015*,
-           2015; `DOI: 10.1007/978-3-319-23231-7_27
-           <https://doi.org/10.1007/978-3-319-23231-7_27>_.
-
-    """
-    _hyperparameters = {
-        'n_coefficients': {'type': (int, type(None), list),
-                           'in': Interval(1, np.inf, closed='left'),
-                           'of': {'type': int,
-                                  'in': Interval(1, np.inf, closed='left')}},
-        'polynomial_type': {'type': str,
-                            'in': _AVAILABLE_POLYNOMIALS.keys()}
-        }
-
-    def __init__(self, n_coefficients=10, polynomial_type='R', n_jobs=None):
-        self.n_coefficients = n_coefficients
-        self.polynomial_type = polynomial_type
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Store all observed homology dimensions in
-        :attr:`homology_dimensions_` and compute :attr:`n_coefficients_`. Then,
-        return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of `X`.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-        X = check_diagrams(X)
-
-        # Find the unique homology dimensions in the 3D array X passed to `fit`
-        # assuming that they can all be found in its zero-th entry
-        homology_dimensions_fit, counts = np.unique(X[0, :, 2],
-                                                    return_counts=True)
-        self.homology_dimensions_ = \
-            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
-
-        _n_homology_dimensions = len(self.homology_dimensions_)
-        _homology_dimensions_counts = dict(zip(homology_dimensions_fit,
-                                               counts))
-        if self.n_coefficients is None:
-            self.n_coefficients_ = [_homology_dimensions_counts[dim]
-                                    for dim in self.homology_dimensions_]
-        elif type(self.n_coefficients) == list:
-            if len(self.n_coefficients) != _n_homology_dimensions:
-                raise ValueError(
-                    f'`n_coefficients` has been passed as a list of length '
-                    f'{len(self.n_coefficients)} while diagrams in `X` have '
-                    f'{_n_homology_dimensions} homology dimensions.'
-                    )
-            self.n_coefficients_ = self.n_coefficients
-        else:
-            self.n_coefficients_ = \
-                [self.n_coefficients] * _n_homology_dimensions
-
-        self._polynomial_function = \
-            _implemented_polynomial_recipes[self.polynomial_type]
-
-        return self
-
-    def _complex_polynomial(self, X, n_coefficients):
-        Xt = np.zeros(2 * n_coefficients,)
-        X = X[X[:, 0] != X[:, 1]]
-
-        roots = self._polynomial_function(X)
-        coefficients = np.poly(roots)
-
-        coefficients = np.array(coefficients[1:])
-        dimension = min(n_coefficients, coefficients.shape[0])
-        Xt[:dimension] = coefficients[:dimension].real
-        Xt[n_coefficients:n_coefficients + dimension] = \
-            coefficients[:dimension].imag
-
-        return Xt
-
-    def transform(self, X, y=None):
-        """Compute vectors of real and imaginary parts of coefficients of
-        complex polynomials obtained from each diagram in `X`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of `X`.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_homology_dimensions * 2 \
-            * n_coefficients_)
-            Polynomial coefficients: real and imaginary parts of the complex
-            polynomials obtained in each homology dimension from each diagram
-            in `X`.
-
-        """
-        check_is_fitted(self)
-        Xt = check_diagrams(X, copy=True)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._complex_polynomial)(
-                _subdiagrams(Xt[[s]], [dim], remove_dim=True)[0],
-                self.n_coefficients_[d])
-            for s in range(len(X))
-            for d, dim in enumerate(self.homology_dimensions_)
-            )
-        Xt = np.concatenate(Xt).reshape(len(X), -1)
-
-        return Xt
+"""Feature extraction from persistence diagrams."""
+# License: GNU AGPLv3
+
+from numbers import Real
+
+import numpy as np
+from joblib import Parallel, delayed, effective_n_jobs
+from scipy.stats import entropy
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils import gen_even_slices
+from sklearn.utils.validation import check_is_fitted
+
+from ._metrics import _AVAILABLE_AMPLITUDE_METRICS, _parallel_amplitude
+from ._features import _AVAILABLE_POLYNOMIALS, _implemented_polynomial_recipes
+from ._utils import _subdiagrams, _bin, _homology_dimensions_to_sorted_ints
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.intervals import Interval
+from ..utils.validation import validate_params, check_diagrams
+
+
+@adapt_fit_transform_docs
+class PersistenceEntropy(BaseEstimator, TransformerMixin):
+    """:ref:`Persistence entropies <persistence_entropy>` of persistence
+    diagrams.
+
+    Based on ideas in [1]_. Given a persistence diagram consisting of
+    birth-death-dimension triples [b, d, q], subdiagrams corresponding to
+    distinct homology dimensions are considered separately, and their
+    respective persistence entropies are calculated as the (base 2) Shannon
+    entropies of the collections of differences d - b ("lifetimes"), normalized
+    by the sum of all such differences. Optionally, these entropies can be
+    normalized according to a simple heuristic, see `normalize`.
+
+    **Important notes**:
+
+        - Input collections of persistence diagrams for this transformer must
+          satisfy certain requirements, see e.g. :meth:`fit`.
+        - By default, persistence subdiagrams containing only triples with zero
+          lifetime will have corresponding (normalized) entropies computed as
+          ``numpy.nan``. To avoid this, set a value of `nan_fill_value`
+          different from ``None``.
+
+    Parameters
+    ----------
+    normalize : bool, optional, default: ``False``
+        When ``True``, the persistence entropy of each diagram is normalized by
+        the logarithm of the sum of lifetimes of all points in the diagram.
+        Can aid comparison between diagrams in an input collection when these
+        have different numbers of (non-trivial) points. See [2]_.
+
+    nan_fill_value : float or None, optional, default: ``-1.``
+        If a float, (normalized) persistence entropies initially computed as
+        ``numpy.nan`` are replaced with this value. If ``None``, these values
+        are left as ``numpy.nan``.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    homology_dimensions_ : tuple
+        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
+
+    See also
+    --------
+    NumberOfPoints, Amplitude, BettiCurve, PersistenceLandscape, HeatKernel, \
+    Silhouette, PersistenceImage
+
+    References
+    ----------
+    .. [1] H. Chintakunta et al, "An entropy-based persistence barcode";
+           *Pattern Recognition* **48**, 2, 2015;
+           `DOI: 10.1016/j.patcog.2014.06.023
+           <https://doi.org/10.1016/j.patcog.2014.06.023>`_.
+
+    .. [2] A. Myers, E. Munch, and F. A. Khasawneh, "Persistent Homology of
+           Complex Networks for Dynamic State Detection"; *Phys. Rev. E*
+           **100**, 022314, 2019; `DOI: 10.1103/PhysRevE.100.022314
+           <https://doi.org/10.1103/PhysRevE.100.022314>`_.
+
+    """
+
+    _hyperparameters = {
+        'normalize': {'type': bool},
+        'nan_fill_value': {'type': (Real, type(None))}
+        }
+
+    def __init__(self, normalize=False, nan_fill_value=-1., n_jobs=None):
+        self.normalize = normalize
+        self.nan_fill_value = nan_fill_value
+        self.n_jobs = n_jobs
+
+    @staticmethod
+    def _persistence_entropy(X, normalize=False, nan_fill_value=None):
+        X_lifespan = X[:, :, 1] - X[:, :, 0]
+        X_entropy = entropy(X_lifespan, base=2, axis=1)
+        if normalize:
+            lifespan_sums = np.sum(X_lifespan, axis=1)
+            X_entropy /= np.log2(lifespan_sums)
+        if nan_fill_value is not None:
+            np.nan_to_num(X_entropy, nan=nan_fill_value, copy=False)
+        X_entropy = X_entropy[:, None]
+        return X_entropy
+
+    def fit(self, X, y=None):
+        """Store all observed homology dimensions in
+        :attr:`homology_dimensions_`. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of `X`.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_diagrams(X)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        # Find the unique homology dimensions in the 3D array X passed to `fit`
+        # assuming that they can all be found in its zero-th entry
+        homology_dimensions_fit = np.unique(X[0, :, 2])
+        self.homology_dimensions_ = \
+            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
+        self._n_dimensions = len(self.homology_dimensions_)
+
+        return self
+
+    def transform(self, X, y=None):
+        """Compute the persistence entropies of diagrams in `X`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of `X`.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_homology_dimensions)
+            Persistence entropies: one value per sample and per homology
+            dimension seen in :meth:`fit`. Index i along axis 1 corresponds to
+            the i-th homology dimension in :attr:`homology_dimensions_`.
+
+        """
+        check_is_fitted(self)
+        X = check_diagrams(X)
+
+        with np.errstate(divide='ignore', invalid='ignore'):
+            Xt = Parallel(n_jobs=self.n_jobs)(
+                delayed(self._persistence_entropy)(
+                    _subdiagrams(X[s], [dim]),
+                    normalize=self.normalize,
+                    nan_fill_value=self.nan_fill_value
+                    )
+                for dim in self.homology_dimensions_
+                for s in gen_even_slices(len(X), effective_n_jobs(self.n_jobs))
+                )
+        Xt = np.concatenate(Xt).reshape(self._n_dimensions, len(X)).T
+
+        return Xt
+
+
+@adapt_fit_transform_docs
+class Amplitude(BaseEstimator, TransformerMixin):
+    """:ref:`Amplitudes <vectorization_amplitude_and_kernel>` of persistence
+    diagrams.
+
+    For each persistence diagram in a collection, a vector of amplitudes or a
+    single scalar amplitude is calculated according to the following steps:
+
+        1. The diagram is partitioned into subdiagrams according to homology
+           dimension.
+        2. The amplitude of each subdiagram is calculated according to the
+           parameters `metric` and `metric_params`. This gives a vector of
+           amplitudes, :math:`\\mathbf{a} = (a_{q_1}, \\ldots, a_{q_n})` where
+           the :math:`q_i` range over the available homology dimensions.
+        3. The final result is either :math:`\\mathbf{a}` itself or a norm of
+           :math:`\\mathbf{a}`, specified by the parameter `order`.
+
+    **Important notes**:
+
+        - Input collections of persistence diagrams for this transformer must
+          satisfy certain requirements, see e.g. :meth:`fit`.
+        - The shape of outputs of :meth:`transform` depends on the value of the
+          `order` parameter.
+
+    Parameters
+    ----------
+    metric : ``'bottleneck'`` | ``'wasserstein'`` | ``'betti'`` | \
+        ``'landscape'`` | ``'silhouette'`` | ``'heat'`` | \
+        ``'persistence_image'``, optional, default: ``'landscape'``
+        Distance or dissimilarity function used to define the amplitude of a
+        subdiagram as its distance from the (trivial) diagonal diagram:
+
+        - ``'bottleneck'`` and ``'wasserstein'`` refer to the identically named
+          perfect-matching--based notions of distance.
+        - ``'betti'`` refers to the :math:`L^p` distance between Betti curves.
+        - ``'landscape'`` refers to the :math:`L^p` distance between
+          persistence landscapes.
+        - ``'silhouette'`` refers to the :math:`L^p` distance between
+          silhouettes.
+        - ``'heat'`` refers to the :math:`L^p` distance between
+          Gaussian-smoothed diagrams.
+        - ``'persistence_image'`` refers to the :math:`L^p` distance between
+          Gaussian-smoothed diagrams represented on birth-persistence axes.
+
+    metric_params : dict or None, optional, default: ``None``
+        Additional keyword arguments for the metric function (passing ``None``
+        is equivalent to passing the defaults described below):
+
+        - If ``metric == 'bottleneck'`` there are no available arguments.
+        - If ``metric == 'wasserstein'`` the only argument is `p` (float,
+          default: ``2.``).
+        - If ``metric == 'betti'`` the available arguments are `p` (float,
+          default: ``2.``) and `n_bins` (int, default: ``100``).
+        - If ``metric == 'landscape'`` the available arguments are `p` (float,
+          default: ``2.``), `n_bins` (int, default: ``100``) and `n_layers`
+          (int, default: ``1``).
+        - If ``metric == 'silhouette'`` the available arguments are `p` (float,
+          default: ``2.``), `power` (float, default: ``1.``) and `n_bins` (int,
+          default: ``100``).
+        - If ``metric == 'heat'`` the available arguments are `p` (float,
+          default: ``2.``), `sigma` (float, default: ``0.1``) and `n_bins`
+          (int, default: ``100``).
+        - If ``metric == 'persistence_image'`` the available arguments are `p`
+          (float, default: ``2.``), `sigma` (float, default: ``0.1``), `n_bins`
+          (int, default: ``100``) and `weight_function` (callable or None,
+          default: ``None``).
+
+    order : float or None, optional, default: ``None``
+        If ``None``, :meth:`transform` returns for each diagram a vector of
+        amplitudes corresponding to the dimensions in
+        :attr:`homology_dimensions_`. Otherwise, the :math:`p`-norm of these
+        vectors with :math:`p` equal to `order` is taken.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    effective_metric_params_ : dict
+        Dictionary containing all information present in `metric_params` as
+        well as relevant quantities computed in :meth:`fit`.
+
+    homology_dimensions_ : tuple
+        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
+
+    See also
+    --------
+    NumberOfPoints, PersistenceEntropy, PairwiseDistance, Scaler, Filtering, \
+    BettiCurve, PersistenceLandscape, HeatKernel, Silhouette, PersistenceImage
+
+    Notes
+    -----
+    To compute amplitudes without first splitting the computation between
+    different homology dimensions, data should be first transformed by an
+    instance of :class:`ForgetDimension`.
+
+    """
+
+    _hyperparameters = {
+        'metric': {'type': str, 'in': _AVAILABLE_AMPLITUDE_METRICS.keys()},
+        'order': {'type': (Real, type(None)),
+                  'in': Interval(0, np.inf, closed='right')},
+        'metric_params': {'type': (dict, type(None))}
+        }
+
+    def __init__(self, metric='landscape', metric_params=None, order=None,
+                 n_jobs=None):
+        self.metric = metric
+        self.metric_params = metric_params
+        self.order = order
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Store all observed homology dimensions in
+        :attr:`homology_dimensions_` and compute
+        :attr:`effective_metric_params`. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_diagrams(X)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        if self.metric_params is None:
+            self.effective_metric_params_ = {}
+        else:
+            self.effective_metric_params_ = self.metric_params.copy()
+        validate_params(self.effective_metric_params_,
+                        _AVAILABLE_AMPLITUDE_METRICS[self.metric])
+
+        # Find the unique homology dimensions in the 3D array X passed to `fit`
+        # assuming that they can all be found in its zero-th entry
+        homology_dimensions_fit = np.unique(X[0, :, 2])
+        self.homology_dimensions_ = \
+            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
+
+        self.effective_metric_params_['samplings'], \
+            self.effective_metric_params_['step_sizes'] = \
+            _bin(X, self.metric, **self.effective_metric_params_)
+
+        if self.metric == 'persistence_image':
+            weight_function = self.effective_metric_params_.get(
+                'weight_function', None
+                )
+            weight_function = \
+                np.ones_like if weight_function is None else weight_function
+            self.effective_metric_params_['weight_function'] = weight_function
+
+        return self
+
+    def transform(self, X, y=None):
+        """Compute the amplitudes or amplitude vectors of diagrams in `X`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_homology_dimensions) if `order` \
+            is ``None``, else (n_samples, 1)
+            Amplitudes or amplitude vectors of the diagrams in `X`. In the
+            second case, index i along axis 1 corresponds to the i-th homology
+            dimension in :attr:`homology_dimensions_`.
+
+        """
+        check_is_fitted(self)
+        Xt = check_diagrams(X, copy=True)
+
+        Xt = _parallel_amplitude(Xt, self.metric,
+                                 self.effective_metric_params_,
+                                 self.homology_dimensions_,
+                                 self.n_jobs)
+        if self.order is not None:
+            Xt = np.linalg.norm(Xt, axis=1, ord=self.order).reshape(-1, 1)
+
+        return Xt
+
+
+@adapt_fit_transform_docs
+class NumberOfPoints(BaseEstimator, TransformerMixin):
+    """Number of off-diagonal points in persistence diagrams, per homology
+    dimension.
+
+    Given a persistence diagram consisting of birth-death-dimension triples
+    [b, d, q], subdiagrams corresponding to distinct homology dimensions are
+    considered separately, and their respective numbers of off-diagonal points
+    are calculated.
+
+    **Important note**:
+
+        - Input collections of persistence diagrams for this transformer must
+          satisfy certain requirements, see e.g. :meth:`fit`.
+
+    Parameters
+    ----------
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    homology_dimensions_ : list
+        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
+
+    See also
+    --------
+    PersistenceEntropy, Amplitude, BettiCurve, PersistenceLandscape,
+    HeatKernel, Silhouette, PersistenceImage
+
+    """
+
+    def __init__(self, n_jobs=None):
+        self.n_jobs = n_jobs
+
+    @staticmethod
+    def _number_points(X):
+        return np.count_nonzero(X[:, :, 1] - X[:, :, 0], axis=1)
+
+    def fit(self, X, y=None):
+        """Store all observed homology dimensions in
+        :attr:`homology_dimensions_`. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of `X`.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_diagrams(X)
+
+        # Find the unique homology dimensions in the 3D array X passed to `fit`
+        # assuming that they can all be found in its zero-th entry
+        homology_dimensions_fit = np.unique(X[0, :, 2])
+        self.homology_dimensions_ = \
+            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
+        self._n_dimensions = len(self.homology_dimensions_)
+
+        return self
+
+    def transform(self, X, y=None):
+        """Compute a vector of numbers of off-diagonal points for each diagram
+        in `X`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of `X`.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_homology_dimensions)
+            Number of points: one value per sample and per homology dimension
+            seen in :meth:`fit`. Index i along axis 1 corresponds to the i-th
+            homology dimension in :attr:`homology_dimensions_`.
+
+        """
+        check_is_fitted(self)
+        X = check_diagrams(X)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._number_points)(_subdiagrams(X, [dim])[s])
+            for dim in self.homology_dimensions_
+            for s in gen_even_slices(len(X), effective_n_jobs(self.n_jobs))
+            )
+        Xt = np.concatenate(Xt).reshape(self._n_dimensions, len(X)).T
+
+        return Xt
+
+
+@adapt_fit_transform_docs
+class ComplexPolynomial(BaseEstimator, TransformerMixin):
+    """Coefficients of complex polynomials whose roots are obtained from points
+    in persistence diagrams.
+
+    Given a persistence diagram consisting of birth-death-dimension triples
+    [b, d, q], subdiagrams corresponding to distinct homology dimensions are
+    first considered separately. For each subdiagram, the polynomial whose
+    roots are complex numbers obtained from its birth-death pairs is
+    computed, and its :attr:`n_coefficients_` highest-degree complex
+    coefficients excluding the top one are stored into a single real vector
+    by concatenating the vector of all real parts with the vector of all
+    imaginary parts [1]_ (if not enough coefficients are available to form a
+    vector of the required length, padding with zeros is performed). Finally,
+    all such vectors coming from different subdiagrams are concatenated to
+    yield a single vector for the diagram.
+
+    There are three possibilities for mapping birth-death pairs :math:`(b, d)`
+    to complex polynomial roots. They are:
+
+    .. math::
+       :nowrap:
+
+       \\begin{gather*}
+       R(b, d) = b + \\mathrm{i} d, \\\\
+       S(b, d) = \\frac{d - b}{\\sqrt{2} r} (b + \\mathrm{i} d), \\\\
+       T(b, d) = \\frac{d - b}{2} [\\cos{r} - \\sin{r} + \
+       \\mathrm{i}(\\cos{r} + \\sin{r})],
+       \\end{gather*}
+
+    where :math:`r = \\sqrt{b^2 + d^2}`.
+
+    **Important note**:
+
+        - Input collections of persistence diagrams for this transformer must
+          satisfy certain requirements, see e.g. :meth:`fit`.
+
+    Parameters
+    ----------
+    polynomial_type : ``'R'`` | ``'S'`` | ``'T'``, optional, default: ``'R'``
+        Type of complex polynomial to compute.
+
+    n_coefficients : list, int or None, optional, default: ``10``
+        Number of complex coefficients per homology dimension. If an int then
+        the number of coefficients will be equal to that value for each
+        homology dimension. If ``None`` then, for each homology dimension in
+        the collection of persistence diagrams seen in :meth:`fit`, the number
+        of complex coefficients is defined to be the largest number of
+        off-diagonal points seen among all subdiagrams in that homology
+        dimension, minus one.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    homology_dimensions_ : list
+        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
+
+    n_coefficients_ : list
+        Effective number of complex coefficients per homology dimension. Set in
+        :meth:`fit`.
+
+    See also
+    --------
+    Amplitude, PersistenceEntropy
+
+    References
+    ----------
+    .. [1] B. Di Fabio and M. Ferri, "Comparing Persistence Diagrams Through
+           Complex Vectors"; in *Image Analysis and Processing  ICIAP 2015*,
+           2015; `DOI: 10.1007/978-3-319-23231-7_27
+           <https://doi.org/10.1007/978-3-319-23231-7_27>_.
+
+    """
+    _hyperparameters = {
+        'n_coefficients': {'type': (int, type(None), list),
+                           'in': Interval(1, np.inf, closed='left'),
+                           'of': {'type': int,
+                                  'in': Interval(1, np.inf, closed='left')}},
+        'polynomial_type': {'type': str,
+                            'in': _AVAILABLE_POLYNOMIALS.keys()}
+        }
+
+    def __init__(self, n_coefficients=10, polynomial_type='R', n_jobs=None):
+        self.n_coefficients = n_coefficients
+        self.polynomial_type = polynomial_type
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Store all observed homology dimensions in
+        :attr:`homology_dimensions_` and compute :attr:`n_coefficients_`. Then,
+        return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of `X`.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+        X = check_diagrams(X)
+
+        # Find the unique homology dimensions in the 3D array X passed to `fit`
+        # assuming that they can all be found in its zero-th entry
+        homology_dimensions_fit, counts = np.unique(X[0, :, 2],
+                                                    return_counts=True)
+        self.homology_dimensions_ = \
+            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
+
+        _n_homology_dimensions = len(self.homology_dimensions_)
+        _homology_dimensions_counts = dict(zip(homology_dimensions_fit,
+                                               counts))
+        if self.n_coefficients is None:
+            self.n_coefficients_ = [_homology_dimensions_counts[dim]
+                                    for dim in self.homology_dimensions_]
+        elif isinstance(self.n_coefficients, list):
+            if len(self.n_coefficients) != _n_homology_dimensions:
+                raise ValueError(
+                    f'`n_coefficients` has been passed as a list of length '
+                    f'{len(self.n_coefficients)} while diagrams in `X` have '
+                    f'{_n_homology_dimensions} homology dimensions.'
+                    )
+            self.n_coefficients_ = self.n_coefficients
+        else:
+            self.n_coefficients_ = \
+                [self.n_coefficients] * _n_homology_dimensions
+
+        self._polynomial_function = \
+            _implemented_polynomial_recipes[self.polynomial_type]
+
+        return self
+
+    def _complex_polynomial(self, X, n_coefficients):
+        Xt = np.zeros(2 * n_coefficients,)
+        X = X[X[:, 0] != X[:, 1]]
+
+        roots = self._polynomial_function(X)
+        coefficients = np.poly(roots)
+
+        coefficients = np.array(coefficients[1:])
+        dimension = min(n_coefficients, coefficients.shape[0])
+        Xt[:dimension] = coefficients[:dimension].real
+        Xt[n_coefficients:n_coefficients + dimension] = \
+            coefficients[:dimension].imag
+
+        return Xt
+
+    def transform(self, X, y=None):
+        """Compute vectors of real and imaginary parts of coefficients of
+        complex polynomials obtained from each diagram in `X`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of `X`.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_homology_dimensions * 2 \
+            * n_coefficients_)
+            Polynomial coefficients: real and imaginary parts of the complex
+            polynomials obtained in each homology dimension from each diagram
+            in `X`.
+
+        """
+        check_is_fitted(self)
+        Xt = check_diagrams(X, copy=True)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._complex_polynomial)(
+                _subdiagrams(Xt[[s]], [dim], remove_dim=True)[0],
+                self.n_coefficients_[d])
+            for s in range(len(X))
+            for d, dim in enumerate(self.homology_dimensions_)
+            )
+        Xt = np.concatenate(Xt).reshape(len(X), -1)
+
+        return Xt
```

## gtda/diagrams/preprocessing.py

```diff
@@ -1,521 +1,521 @@
-"""Persistence diagram preprocessing."""
-# License: GNU AGPLv3
-
-from numbers import Real
-from types import FunctionType
-
-import numpy as np
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils.validation import check_is_fitted
-
-from ._metrics import _AVAILABLE_AMPLITUDE_METRICS, _parallel_amplitude
-from ._utils import _filter, _bin, _homology_dimensions_to_sorted_ints
-from ..base import PlotterMixin
-from ..plotting.persistence_diagrams import plot_diagram
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.intervals import Interval
-from ..utils.validation import check_diagrams, validate_params
-
-
-@adapt_fit_transform_docs
-class ForgetDimension(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Replaces all homology dimensions in persistence diagrams with
-    ``numpy.inf``.
-
-    Useful when downstream tasks require the use of topological features all at
-    once -- and not separated between different homology dimensions.
-
-    See also
-    --------
-    PairwiseDistance, Amplitude, Scaler, Filtering
-
-    """
-
-    def __init__(self):
-        pass
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_diagrams(X)
-
-        self._is_fitted = True
-        return self
-
-    def transform(self, X, y=None):
-        """Replace all homology dimensions in `X` with ``numpy.inf``.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Output persistence diagram.
-
-        """
-        check_is_fitted(self, '_is_fitted')
-        Xt = check_diagrams(X, copy=True)
-
-        Xt[:, :, 2] = np.inf
-        # TODO: for plotting, replace the dimension with a tag
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, plotly_params=None):
-        """Plot a sample from a collection of persistence diagrams.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_points, 3)
-            Collection of persistence diagrams, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_diagram(
-            Xt[sample], homology_dimensions=[np.inf],
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class Scaler(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Linear scaling of persistence diagrams.
-
-    A positive scale factor :attr:`scale_` is calculated during :meth:`fit` by
-    considering all available persistence diagrams partitioned according to
-    homology dimensions. During :meth:`transform`, all birth-death pairs are
-    divided by :attr:`scale_`.
-
-    The value of :attr:`scale_` depends on two things:
-
-        - A way of computing, for each homology dimension, the :ref:`amplitude
-          <vectorization_amplitude_and_kernel>` in that dimension of a
-          persistence diagram consisting of birth-death-dimension triples
-          [b, d, q]. Together, `metric` and `metric_params` define this in the
-          same way as in :class:`Amplitude`.
-        - A scalar-valued function which is applied to the resulting
-          two-dimensional array of amplitudes (one per diagram and homology
-          dimension) to obtain :attr:`scale_`.
-
-    **Important note**:
-
-        - Input collections of persistence diagrams for this transformer must
-          satisfy certain requirements, see e.g. :meth:`fit`.
-
-    Parameters
-    ----------
-    metric : ``'bottleneck'`` | ``'wasserstein'`` | ``'betti'`` | \
-        ``'landscape'`` |``'silhouette'`` |  ``'heat'`` | \
-        ``'persistence_image'``, optional, default: ``'bottleneck'``
-        See the corresponding parameter in :class:`Amplitude`.
-
-    metric_params : dict or None, optional, default: ``None``
-        See the corresponding parameter in :class:`Amplitude`.
-
-    function : callable, optional, default: ``numpy.max``
-        Function used to extract a positive scalar from the collection of
-        amplitude vectors in :meth:`fit`. Must map 2D arrays to scalars.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    effective_metric_params_ : dict
-        Dictionary containing all information present in `metric_params` as
-        well as relevant quantities computed in :meth:`fit`.
-
-    homology_dimensions_ : tuple
-        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
-
-    scale_ : float
-        Value by which to rescale diagrams.
-
-    See also
-    --------
-    PairwiseDistance, ForgetDimension, Filtering, Amplitude
-
-    Notes
-    -----
-    When `metric` is ``'bottleneck'`` and `function` is ``numpy.max``,
-    :meth:`fit_transform` has the effect of making the lifetime of the most
-    persistent point across all diagrams and homology dimensions equal to 2.
-
-    To compute scaling factors without first splitting the computation between
-    different homology dimensions, data should be first transformed by an
-    instance of :class:`ForgetDimension`.
-
-    """
-
-    _hyperparameters = {
-        'metric': {'type': str, 'in': _AVAILABLE_AMPLITUDE_METRICS.keys()},
-        'metric_params': {'type': (dict, type(None))},
-        'function': {'type': (FunctionType, type(None))}
-        }
-
-    def __init__(self, metric='bottleneck', metric_params=None,
-                 function=np.max, n_jobs=None):
-        self.metric = metric
-        self.metric_params = metric_params
-        self.function = function
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Store all observed homology dimensions in
-        :attr:`homology_dimensions_` and compute :attr:`scale_`.
-        Then, return the estimator.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_diagrams(X)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        if self.metric_params is None:
-            self.effective_metric_params_ = {}
-        else:
-            self.effective_metric_params_ = self.metric_params.copy()
-        validate_params(self.effective_metric_params_,
-                        _AVAILABLE_AMPLITUDE_METRICS[self.metric])
-
-        # Find the unique homology dimensions in the 3D array X passed to `fit`
-        # assuming that they can all be found in its zero-th entry
-        homology_dimensions_fit = np.unique(X[0, :, 2])
-        self.homology_dimensions_ = \
-            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
-
-        self.effective_metric_params_['samplings'], \
-            self.effective_metric_params_['step_sizes'] = \
-            _bin(X, self.metric, **self.effective_metric_params_)
-
-        if self.metric == 'persistence_image':
-            weight_function = self.effective_metric_params_.get(
-                'weight_function', None
-                )
-            weight_function = \
-                np.ones_like if weight_function is None else weight_function
-            self.effective_metric_params_['weight_function'] = weight_function
-
-        amplitude_array = _parallel_amplitude(X, self.metric,
-                                              self.effective_metric_params_,
-                                              self.homology_dimensions_,
-                                              self.n_jobs)
-        self.scale_ = self.function(amplitude_array)
-
-        return self
-
-    def transform(self, X, y=None):
-        """Divide all birth and death values in `X` by :attr:`scale_`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xs : ndarray of shape (n_samples, n_features, 3)
-            Rescaled diagrams.
-
-        """
-        check_is_fitted(self)
-
-        Xs = check_diagrams(X, copy=True)
-        Xs[:, :, :2] /= self.scale_
-        return Xs
-
-    def inverse_transform(self, X):
-        """Scale back the data to the original representation. Multiplies by
-        the scale found in :meth:`fit`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Data to apply the inverse transform to, c.f. :meth:`transform`.
-
-        Returns
-        -------
-        Xs : ndarray of shape (n_samples, n_features, 3)
-            Rescaled diagrams.
-
-        """
-        check_is_fitted(self)
-
-        Xs = check_diagrams(X, copy=True)
-        Xs[:, :, :2] *= self.scale_
-        return Xs
-
-    def plot(self, Xt, sample=0, homology_dimensions=None, plotly_params=None):
-        """Plot a sample from a collection of persistence diagrams, with
-        homology in multiple dimensions.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_points, 3)
-            Collection of persistence diagrams, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        homology_dimensions : list, tuple or None, optional, default: ``None``
-            Which homology dimensions to include in the plot. ``None`` is
-            equivalent to passing :attr:`homology_dimensions_`.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        if homology_dimensions is None:
-            _homology_dimensions = self.homology_dimensions_
-        else:
-            _homology_dimensions = homology_dimensions
-
-        return plot_diagram(
-            Xt[sample], homology_dimensions=_homology_dimensions,
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class Filtering(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Filtering of persistence diagrams.
-
-    Filtering a diagram means discarding all points [b, d, q] representing
-    non-trivial topological features whose lifetime d - b is less than or equal
-    to a cutoff value. Points on the diagonal (i.e. for which b and d are
-    equal) may still appear in the output for padding purposes, but carry no
-    information.
-
-    **Important note**:
-
-        - Input collections of persistence diagrams for this transformer must
-          satisfy certain requirements, see e.g. :meth:`fit`.
-
-    Parameters
-    ----------
-    homology_dimensions : list, tuple, or None, optional, default: ``None``
-        When set to ``None``, subdiagrams corresponding to all homology
-        dimensions seen in :meth:`fit` will be filtered. Otherwise, it contains
-        the homology dimensions (as non-negative integers) at which filtering
-        should occur.
-
-    epsilon : float, optional, default: ``0.01``
-        The cutoff value controlling the amount of filtering.
-
-    Attributes
-    ----------
-    homology_dimensions_ : tuple
-        If `homology_dimensions` is set to ``None``, contains the homology
-        dimensions seen in :meth:`fit`, sorted in ascending order. Otherwise,
-        it is a similarly sorted version of `homology_dimensions`.
-
-    See also
-    --------
-    PairwiseDistance, ForgetDimension, Scaler, Amplitude
-
-    """
-
-    _hyperparameters = {
-        'homology_dimensions': {
-            'type': (list, tuple, type(None)),
-            'of': {'type': int, 'in': Interval(0, np.inf, closed='left')}
-            },
-        'epsilon': {'type': Real, 'in': Interval(0, np.inf, closed='left')}
-        }
-
-    def __init__(self, homology_dimensions=None, epsilon=0.01):
-        self.homology_dimensions = homology_dimensions
-        self.epsilon = epsilon
-
-    def fit(self, X, y=None):
-        """Store relevant homology dimensions in
-        :attr:`homology_dimensions_`. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of `X`.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_diagrams(X)
-        validate_params(
-            self.get_params(), self._hyperparameters)
-
-        if self.homology_dimensions is None:
-            # Find the unique homology dimensions in the 3D array X passed to
-            # `fit` assuming that they can all be found in its zero-th entry
-            homology_dimensions = np.unique(X[0, :, 2])
-        else:
-            homology_dimensions = self.homology_dimensions
-        self.homology_dimensions_ = \
-            _homology_dimensions_to_sorted_ints(homology_dimensions)
-
-        return self
-
-    def transform(self, X, y=None):
-        """Filter all relevant persistence subdiagrams.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_features_filtered, 3)
-            Filtered persistence diagrams. Only the subdiagrams corresponding
-            to dimensions in :attr:`homology_dimensions_` are filtered.
-            ``n_features_filtered`` is less than or equal to ``n_features``.
-
-        """
-        check_is_fitted(self)
-        X = check_diagrams(X)
-
-        Xt = _filter(X, self.homology_dimensions_, self.epsilon)
-        return Xt
-
-    def plot(self, Xt, sample=0, homology_dimensions=None, plotly_params=None):
-        """Plot a sample from a collection of persistence diagrams, with
-        homology in multiple dimensions.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_points, 3)
-            Collection of persistence diagrams, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        homology_dimensions : list, tuple or None, optional, default: ``None``
-            Which homology dimensions to include in the plot. ``None`` is
-            equivalent to passing :attr:`homology_dimensions_`.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        if homology_dimensions is None:
-            _homology_dimensions = self.homology_dimensions_
-        else:
-            _homology_dimensions = homology_dimensions
-
-        return plot_diagram(
-            Xt[sample], homology_dimensions=_homology_dimensions,
-            plotly_params=plotly_params
-            )
+"""Persistence diagram preprocessing."""
+# License: GNU AGPLv3
+
+from numbers import Real
+from typing import Callable
+
+import numpy as np
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils.validation import check_is_fitted
+
+from ._metrics import _AVAILABLE_AMPLITUDE_METRICS, _parallel_amplitude
+from ._utils import _filter, _bin, _homology_dimensions_to_sorted_ints
+from ..base import PlotterMixin
+from ..plotting.persistence_diagrams import plot_diagram
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.intervals import Interval
+from ..utils.validation import check_diagrams, validate_params
+
+
+@adapt_fit_transform_docs
+class ForgetDimension(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Replaces all homology dimensions in persistence diagrams with
+    ``numpy.inf``.
+
+    Useful when downstream tasks require the use of topological features all at
+    once -- and not separated between different homology dimensions.
+
+    See also
+    --------
+    PairwiseDistance, Amplitude, Scaler, Filtering
+
+    """
+
+    def __init__(self):
+        pass
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_diagrams(X)
+
+        self._is_fitted = True
+        return self
+
+    def transform(self, X, y=None):
+        """Replace all homology dimensions in `X` with ``numpy.inf``.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Output persistence diagram.
+
+        """
+        check_is_fitted(self, '_is_fitted')
+        Xt = check_diagrams(X, copy=True)
+
+        Xt[:, :, 2] = np.inf
+        # TODO: for plotting, replace the dimension with a tag
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, plotly_params=None):
+        """Plot a sample from a collection of persistence diagrams.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_points, 3)
+            Collection of persistence diagrams, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_diagram(
+            Xt[sample], homology_dimensions=[np.inf],
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class Scaler(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Linear scaling of persistence diagrams.
+
+    A positive scale factor :attr:`scale_` is calculated during :meth:`fit` by
+    considering all available persistence diagrams partitioned according to
+    homology dimensions. During :meth:`transform`, all birth-death pairs are
+    divided by :attr:`scale_`.
+
+    The value of :attr:`scale_` depends on two things:
+
+        - A way of computing, for each homology dimension, the :ref:`amplitude
+          <vectorization_amplitude_and_kernel>` in that dimension of a
+          persistence diagram consisting of birth-death-dimension triples
+          [b, d, q]. Together, `metric` and `metric_params` define this in the
+          same way as in :class:`Amplitude`.
+        - A scalar-valued function which is applied to the resulting
+          two-dimensional array of amplitudes (one per diagram and homology
+          dimension) to obtain :attr:`scale_`.
+
+    **Important note**:
+
+        - Input collections of persistence diagrams for this transformer must
+          satisfy certain requirements, see e.g. :meth:`fit`.
+
+    Parameters
+    ----------
+    metric : ``'bottleneck'`` | ``'wasserstein'`` | ``'betti'`` | \
+        ``'landscape'`` |``'silhouette'`` |  ``'heat'`` | \
+        ``'persistence_image'``, optional, default: ``'bottleneck'``
+        See the corresponding parameter in :class:`Amplitude`.
+
+    metric_params : dict or None, optional, default: ``None``
+        See the corresponding parameter in :class:`Amplitude`.
+
+    function : callable, optional, default: ``numpy.max``
+        Function used to extract a positive scalar from the collection of
+        amplitude vectors in :meth:`fit`. Must map 2D arrays to scalars.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    effective_metric_params_ : dict
+        Dictionary containing all information present in `metric_params` as
+        well as relevant quantities computed in :meth:`fit`.
+
+    homology_dimensions_ : tuple
+        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
+
+    scale_ : float
+        Value by which to rescale diagrams.
+
+    See also
+    --------
+    PairwiseDistance, ForgetDimension, Filtering, Amplitude
+
+    Notes
+    -----
+    When `metric` is ``'bottleneck'`` and `function` is ``numpy.max``,
+    :meth:`fit_transform` has the effect of making the lifetime of the most
+    persistent point across all diagrams and homology dimensions equal to 2.
+
+    To compute scaling factors without first splitting the computation between
+    different homology dimensions, data should be first transformed by an
+    instance of :class:`ForgetDimension`.
+
+    """
+
+    _hyperparameters = {
+        'metric': {'type': str, 'in': _AVAILABLE_AMPLITUDE_METRICS.keys()},
+        'metric_params': {'type': (dict, type(None))},
+        'function': {'type': (Callable, type(None))}
+        }
+
+    def __init__(self, metric='bottleneck', metric_params=None,
+                 function=np.max, n_jobs=None):
+        self.metric = metric
+        self.metric_params = metric_params
+        self.function = function
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Store all observed homology dimensions in
+        :attr:`homology_dimensions_` and compute :attr:`scale_`.
+        Then, return the estimator.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_diagrams(X)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        if self.metric_params is None:
+            self.effective_metric_params_ = {}
+        else:
+            self.effective_metric_params_ = self.metric_params.copy()
+        validate_params(self.effective_metric_params_,
+                        _AVAILABLE_AMPLITUDE_METRICS[self.metric])
+
+        # Find the unique homology dimensions in the 3D array X passed to `fit`
+        # assuming that they can all be found in its zero-th entry
+        homology_dimensions_fit = np.unique(X[0, :, 2])
+        self.homology_dimensions_ = \
+            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
+
+        self.effective_metric_params_['samplings'], \
+            self.effective_metric_params_['step_sizes'] = \
+            _bin(X, self.metric, **self.effective_metric_params_)
+
+        if self.metric == 'persistence_image':
+            weight_function = self.effective_metric_params_.get(
+                'weight_function', None
+                )
+            weight_function = \
+                np.ones_like if weight_function is None else weight_function
+            self.effective_metric_params_['weight_function'] = weight_function
+
+        amplitude_array = _parallel_amplitude(X, self.metric,
+                                              self.effective_metric_params_,
+                                              self.homology_dimensions_,
+                                              self.n_jobs)
+        self.scale_ = self.function(amplitude_array)
+
+        return self
+
+    def transform(self, X, y=None):
+        """Divide all birth and death values in `X` by :attr:`scale_`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xs : ndarray of shape (n_samples, n_features, 3)
+            Rescaled diagrams.
+
+        """
+        check_is_fitted(self)
+
+        Xs = check_diagrams(X, copy=True)
+        Xs[:, :, :2] /= self.scale_
+        return Xs
+
+    def inverse_transform(self, X):
+        """Scale back the data to the original representation. Multiplies by
+        the scale found in :meth:`fit`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Data to apply the inverse transform to, c.f. :meth:`transform`.
+
+        Returns
+        -------
+        Xs : ndarray of shape (n_samples, n_features, 3)
+            Rescaled diagrams.
+
+        """
+        check_is_fitted(self)
+
+        Xs = check_diagrams(X, copy=True)
+        Xs[:, :, :2] *= self.scale_
+        return Xs
+
+    def plot(self, Xt, sample=0, homology_dimensions=None, plotly_params=None):
+        """Plot a sample from a collection of persistence diagrams, with
+        homology in multiple dimensions.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_points, 3)
+            Collection of persistence diagrams, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        homology_dimensions : list, tuple or None, optional, default: ``None``
+            Which homology dimensions to include in the plot. ``None`` is
+            equivalent to passing :attr:`homology_dimensions_`.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        if homology_dimensions is None:
+            _homology_dimensions = self.homology_dimensions_
+        else:
+            _homology_dimensions = homology_dimensions
+
+        return plot_diagram(
+            Xt[sample], homology_dimensions=_homology_dimensions,
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class Filtering(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Filtering of persistence diagrams.
+
+    Filtering a diagram means discarding all points [b, d, q] representing
+    non-trivial topological features whose lifetime d - b is less than or equal
+    to a cutoff value. Points on the diagonal (i.e. for which b and d are
+    equal) may still appear in the output for padding purposes, but carry no
+    information.
+
+    **Important note**:
+
+        - Input collections of persistence diagrams for this transformer must
+          satisfy certain requirements, see e.g. :meth:`fit`.
+
+    Parameters
+    ----------
+    homology_dimensions : list, tuple, or None, optional, default: ``None``
+        When set to ``None``, subdiagrams corresponding to all homology
+        dimensions seen in :meth:`fit` will be filtered. Otherwise, it contains
+        the homology dimensions (as non-negative integers) at which filtering
+        should occur.
+
+    epsilon : float, optional, default: ``0.01``
+        The cutoff value controlling the amount of filtering.
+
+    Attributes
+    ----------
+    homology_dimensions_ : tuple
+        If `homology_dimensions` is set to ``None``, contains the homology
+        dimensions seen in :meth:`fit`, sorted in ascending order. Otherwise,
+        it is a similarly sorted version of `homology_dimensions`.
+
+    See also
+    --------
+    PairwiseDistance, ForgetDimension, Scaler, Amplitude
+
+    """
+
+    _hyperparameters = {
+        'homology_dimensions': {
+            'type': (list, tuple, type(None)),
+            'of': {'type': int, 'in': Interval(0, np.inf, closed='left')}
+            },
+        'epsilon': {'type': Real, 'in': Interval(0, np.inf, closed='left')}
+        }
+
+    def __init__(self, homology_dimensions=None, epsilon=0.01):
+        self.homology_dimensions = homology_dimensions
+        self.epsilon = epsilon
+
+    def fit(self, X, y=None):
+        """Store relevant homology dimensions in
+        :attr:`homology_dimensions_`. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of `X`.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_diagrams(X)
+        validate_params(
+            self.get_params(), self._hyperparameters)
+
+        if self.homology_dimensions is None:
+            # Find the unique homology dimensions in the 3D array X passed to
+            # `fit` assuming that they can all be found in its zero-th entry
+            homology_dimensions = np.unique(X[0, :, 2])
+        else:
+            homology_dimensions = self.homology_dimensions
+        self.homology_dimensions_ = \
+            _homology_dimensions_to_sorted_ints(homology_dimensions)
+
+        return self
+
+    def transform(self, X, y=None):
+        """Filter all relevant persistence subdiagrams.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_features_filtered, 3)
+            Filtered persistence diagrams. Only the subdiagrams corresponding
+            to dimensions in :attr:`homology_dimensions_` are filtered.
+            ``n_features_filtered`` is less than or equal to ``n_features``.
+
+        """
+        check_is_fitted(self)
+        X = check_diagrams(X)
+
+        Xt = _filter(X, self.homology_dimensions_, self.epsilon)
+        return Xt
+
+    def plot(self, Xt, sample=0, homology_dimensions=None, plotly_params=None):
+        """Plot a sample from a collection of persistence diagrams, with
+        homology in multiple dimensions.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_points, 3)
+            Collection of persistence diagrams, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        homology_dimensions : list, tuple or None, optional, default: ``None``
+            Which homology dimensions to include in the plot. ``None`` is
+            equivalent to passing :attr:`homology_dimensions_`.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        if homology_dimensions is None:
+            _homology_dimensions = self.homology_dimensions_
+        else:
+            _homology_dimensions = homology_dimensions
+
+        return plot_diagram(
+            Xt[sample], homology_dimensions=_homology_dimensions,
+            plotly_params=plotly_params
+            )
```

## gtda/diagrams/representations.py

```diff
@@ -1,1225 +1,1225 @@
-"""Vector representations of persistence diagrams."""
-# License: GNU AGPLv3
-
-import types
-from numbers import Real
-
-import numpy as np
-from joblib import Parallel, delayed, effective_n_jobs
-from plotly.graph_objects import Figure, Scatter
-from plotly.subplots import make_subplots
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils import gen_even_slices
-from sklearn.utils.validation import check_is_fitted
-
-from ._metrics import betti_curves, landscapes, heats, \
-    persistence_images, silhouettes
-from ._utils import _subdiagrams, _bin, _make_homology_dimensions_mapping, \
-    _homology_dimensions_to_sorted_ints
-from ..base import PlotterMixin
-from ..plotting import plot_heatmap
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.intervals import Interval
-from ..utils.validation import validate_params, check_diagrams
-
-
-@adapt_fit_transform_docs
-class BettiCurve(BaseEstimator, TransformerMixin, PlotterMixin):
-    """:ref:`Betti curves <betti_curve>` of persistence diagrams.
-
-    Given a persistence diagram consisting of birth-death-dimension triples
-    [b, d, q], subdiagrams corresponding to distinct homology dimensions are
-    considered separately, and their respective Betti curves are obtained by
-    evenly sampling the :ref:`filtration parameter <filtered_complex>`.
-
-    **Important note**:
-
-        - Input collections of persistence diagrams for this transformer must
-          satisfy certain requirements, see e.g. :meth:`fit`.
-
-    Parameters
-    ----------
-    n_bins : int, optional, default: ``100``
-        The number of filtration parameter values, per available homology
-        dimension, to sample during :meth:`fit`.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    homology_dimensions_ : tuple
-        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
-
-    samplings_ : dict
-        For each number in `homology_dimensions_`, a discrete sampling of
-        filtration parameters, calculated during :meth:`fit` according to the
-        minimum birth and maximum death values observed across all samples.
-
-    See also
-    --------
-    PersistenceLandscape, PersistenceEntropy, HeatKernel, Amplitude, \
-    PairwiseDistance, Silhouette, PersistenceImage, \
-    gtda.homology.VietorisRipsPersistence
-
-    Notes
-    -----
-    The samplings in :attr:`samplings_` are in general different between
-    different homology dimensions. This means that the j-th entry of a Betti
-    curve in homology dimension q typically arises from a different parameter
-    values to the j-th entry of a curve in dimension q'.
-
-    """
-
-    _hyperparameters = {
-        "n_bins": {"type": int, "in": Interval(1, np.inf, closed="left")}
-        }
-
-    def __init__(self, n_bins=100, n_jobs=None):
-        self.n_bins = n_bins
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Store all observed homology dimensions in
-        :attr:`homology_dimensions_` and, for each dimension separately,
-        store evenly sample filtration parameter values in :attr:`samplings_`.
-        Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_diagrams(X)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
-
-        # Find the unique homology dimensions in the 3D array X passed to `fit`
-        # assuming that they can all be found in its zero-th entry
-        homology_dimensions_fit = np.unique(X[0, :, 2])
-        self.homology_dimensions_ = \
-            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
-        self._n_dimensions = len(self.homology_dimensions_)
-
-        self._samplings, _ = _bin(
-            X, "betti", n_bins=self.n_bins,
-            homology_dimensions=self.homology_dimensions_
-            )
-        self.samplings_ = {dim: s.flatten()
-                           for dim, s in self._samplings.items()}
-
-        return self
-
-    def transform(self, X, y=None):
-        """Compute the Betti curves of diagrams in `X`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins)
-            Betti curves: one curve (represented as a one-dimensional array
-            of integer values) per sample and per homology dimension seen
-            in :meth:`fit`. Index i along axis 1 corresponds to the i-th
-            homology dimension in :attr:`homology_dimensions_`.
-
-        """
-        check_is_fitted(self)
-        X = check_diagrams(X)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(delayed(betti_curves)(
-                _subdiagrams(X[s], [dim], remove_dim=True),
-                self._samplings[dim])
-            for dim in self.homology_dimensions_
-            for s in gen_even_slices(len(X), effective_n_jobs(self.n_jobs)))
-        Xt = np.concatenate(Xt).\
-            reshape(self._n_dimensions, len(X), -1).\
-            transpose((1, 0, 2))
-
-        return Xt
-
-    def plot(self, Xt, sample=0, homology_dimensions=None, plotly_params=None):
-        """Plot a sample from a collection of Betti curves arranged as in the
-        output of :meth:`transform`. Include homology in multiple dimensions.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins)
-            Collection of Betti curves, such as returned by :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        homology_dimensions : list, tuple or None, optional, default: ``None``
-            Which homology dimensions to include in the plot. ``None`` means
-            plotting all dimensions present in :attr:`homology_dimensions_`.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        check_is_fitted(self)
-
-        homology_dimensions_mapping = _make_homology_dimensions_mapping(
-            homology_dimensions, self.homology_dimensions_
-            )
-
-        layout_axes_common = {
-            "type": "linear",
-            "ticks": "outside",
-            "showline": True,
-            "zeroline": True,
-            "linewidth": 1,
-            "linecolor": "black",
-            "mirror": False,
-            "showexponent": "all",
-            "exponentformat": "e"
-            }
-        layout = {
-            "xaxis1": {
-                "title": "Filtration parameter",
-                "side": "bottom",
-                "anchor": "y1",
-                **layout_axes_common
-                },
-            "yaxis1": {
-                "title": "Betti number",
-                "side": "left",
-                "anchor": "x1",
-                **layout_axes_common
-                },
-            "plot_bgcolor": "white",
-            "title": f"Betti curves from diagram {sample}"
-            }
-
-        fig = Figure(layout=layout)
-
-        for ix, dim in homology_dimensions_mapping:
-            fig.add_trace(Scatter(x=self.samplings_[dim],
-                                  y=Xt[sample][ix],
-                                  mode="lines",
-                                  showlegend=True,
-                                  name=f"H{dim}"))
-
-        # Update traces and layout according to user input
-        if plotly_params:
-            fig.update_traces(plotly_params.get("traces", None))
-            fig.update_layout(plotly_params.get("layout", None))
-
-        return fig
-
-
-@adapt_fit_transform_docs
-class PersistenceLandscape(BaseEstimator, TransformerMixin, PlotterMixin):
-    """:ref:`Persistence landscapes <persistence_landscape>` of persistence
-    diagrams.
-
-    Given a persistence diagram consisting of birth-death-dimension triples
-    [b, d, q], subdiagrams corresponding to distinct homology dimensions are
-    considered separately, and layers of their respective persistence
-    landscapes are obtained by evenly sampling the :ref:`filtration parameter
-    <filtered_complex>`.
-
-    **Important note**:
-
-        - Input collections of persistence diagrams for this transformer must
-          satisfy certain requirements, see e.g. :meth:`fit`.
-
-    Parameters
-    ----------
-    n_layers : int, optional, default: ``1``
-        How many layers to consider in the persistence landscape.
-
-    n_bins : int, optional, default: ``100``
-        The number of filtration parameter values, per available
-        homology dimension, to sample during :meth:`fit`.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    homology_dimensions_ : tuple
-        Homology dimensions seen in :meth:`fit`.
-
-    samplings_ : dict
-        For each number in `homology_dimensions_`, a discrete sampling of
-        filtration parameters, calculated during :meth:`fit` according to the
-        minimum birth and maximum death values observed across all samples.
-
-    See also
-    --------
-    BettiCurve, PersistenceEntropy, HeatKernel, Amplitude, PairwiseDistance, \
-    Silhouette, PersistenceImage, gtda.homology.VietorisRipsPersistence
-
-    Notes
-    -----
-    The samplings in :attr:`samplings_` are in general different between
-    different homology dimensions. This means that the j-th entry of the
-    k-layer of a persistence landscape in homology dimension q typically arises
-    from a different parameter value to the j-th entry of a k-layer in
-    dimension q'.
-
-    """
-
-    _hyperparameters = {
-        "n_bins": {"type": int, "in": Interval(1, np.inf, closed="left")},
-        "n_layers": {"type": int, "in": Interval(1, np.inf, closed="left")}
-        }
-
-    def __init__(self, n_layers=1, n_bins=100, n_jobs=None):
-        self.n_layers = n_layers
-        self.n_bins = n_bins
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Store all observed homology dimensions in
-        :attr:`homology_dimensions_` and, for each dimension separately, store
-        evenly sample filtration parameter values in :attr:`samplings_`. Then,
-        return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_diagrams(X)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
-
-        # Find the unique homology dimensions in the 3D array X passed to `fit`
-        # assuming that they can all be found in its zero-th entry
-        homology_dimensions_fit = np.unique(X[0, :, 2])
-        self.homology_dimensions_ = \
-            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
-        self._n_dimensions = len(self.homology_dimensions_)
-
-        self._samplings, _ = _bin(
-            X, "landscape", n_bins=self.n_bins,
-            homology_dimensions=self.homology_dimensions_
-            )
-        self.samplings_ = {dim: s.flatten()
-                           for dim, s in self._samplings.items()}
-
-        return self
-
-    def transform(self, X, y=None):
-        """Compute the persistence landscapes of diagrams in `X`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_homology_dimensions * n_layers, \
-            n_bins)
-            Persistence landscapes, where ``n_homology_dimensions`` is the
-            number of distinct homology dimensions seen in :meth:`fit`.
-            Landscapes coming from different homology dimensions are stacked
-            for each sample, so layer ``k`` of the landscape in the ``j``-th
-            homology dimension in :attr:`homology_dimensions_` is
-            ``X[i, n_homology_dimensions * j + k]``.
-
-        """
-        check_is_fitted(self)
-        X = check_diagrams(X)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(landscapes)(_subdiagrams(X[s], [dim], remove_dim=True),
-                                self._samplings[dim],
-                                self.n_layers)
-            for dim in self.homology_dimensions_
-            for s in gen_even_slices(len(X), effective_n_jobs(self.n_jobs))
-            )
-        Xt = np.concatenate(Xt).\
-            reshape(self._n_dimensions, len(X), self.n_layers, self.n_bins).\
-            transpose((1, 0, 2, 3)).\
-            reshape(len(X), self._n_dimensions * self.n_layers, self.n_bins)
-
-        return Xt
-
-    def plot(self, Xt, sample=0, homology_dimensions=None, plotly_params=None):
-        """Plot a sample from a collection of persistence landscapes arranged
-        as in the output of :meth:`transform`. Include homology in multiple
-        dimensions.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_layers, \
-            n_bins
-            Collection of persistence landscapes, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        homology_dimensions : list, tuple or None, optional, default: ``None``
-            Homology dimensions for which the landscape should be plotted.
-            ``None`` means plotting all dimensions present in
-            :attr:`homology_dimensions_`.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        check_is_fitted(self)
-
-        homology_dimensions_mapping = _make_homology_dimensions_mapping(
-            homology_dimensions, self.homology_dimensions_
-            )
-
-        layout_axes_common = {
-            "type": "linear",
-            "ticks": "outside",
-            "showline": True,
-            "zeroline": True,
-            "linewidth": 1,
-            "linecolor": "black",
-            "mirror": False,
-            "showexponent": "all",
-            "exponentformat": "e"
-            }
-        layout = {
-            "xaxis1": {
-                "side": "bottom",
-                "anchor": "y1",
-                **layout_axes_common
-                },
-            "yaxis1": {
-                "side": "left",
-                "anchor": "x1",
-                **layout_axes_common
-                },
-            "plot_bgcolor": "white",
-            }
-
-        Xt_sample = Xt[sample]
-        n_dims = len(self.homology_dimensions_)
-        n_layers = Xt_sample.shape[0] // n_dims
-        subplot_titles = [f"H{dim}" for _, dim in homology_dimensions_mapping]
-        fig = make_subplots(rows=len(homology_dimensions_mapping), cols=1,
-                            subplot_titles=subplot_titles)
-        has_many_homology_dim = len(homology_dimensions_mapping) - 1
-        for i, (inv_idx, dim) in enumerate(homology_dimensions_mapping):
-            hom_dim_str = \
-                f" ({subplot_titles[i]})" if has_many_homology_dim else ""
-            for layer in range(n_layers):
-                fig.add_trace(
-                    Scatter(x=self.samplings_[dim],
-                            y=Xt_sample[inv_idx * n_layers + layer],
-                            mode="lines",
-                            showlegend=True,
-                            hoverinfo="none",
-                            name=f"Layer {layer + 1}{hom_dim_str}"),
-                    row=i + 1,
-                    col=1
-                    )
-
-        fig.update_layout(
-            title_text=f"Landscape representations of diagram {sample}",
-            **layout.copy()
-            )
-
-        # Update traces and layout according to user input
-        if plotly_params:
-            fig.update_traces(plotly_params.get("traces", None))
-            fig.update_layout(plotly_params.get("layout", None))
-
-        return fig
-
-
-@adapt_fit_transform_docs
-class HeatKernel(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Convolution of persistence diagrams with a Gaussian kernel.
-
-    Based on ideas in [1]_. Given a persistence diagram consisting of
-    birth-death-dimension triples [b, d, q], subdiagrams corresponding to
-    distinct homology dimensions are considered separately and regarded as sums
-    of Dirac deltas. Then, the convolution with a Gaussian kernel is computed
-    over a rectangular grid of locations evenly sampled from appropriate
-    ranges of the :ref:`filtration parameter <filtered_complex>`. The
-    same is done with the reflected images of the subdiagrams about the
-    diagonal, and the difference between the results of the two convolutions is
-    computed. The result can be thought of as a (multi-channel) raster image.
-
-    **Important note**:
-
-        - Input collections of persistence diagrams for this transformer must
-          satisfy certain requirements, see e.g. :meth:`fit`.
-
-    Parameters
-    ----------
-    sigma : float, optional default ``0.1``
-        Standard deviation for Gaussian kernel.
-
-    n_bins : int, optional, default: ``100``
-        The number of filtration parameter values, per available homology
-        dimension, to sample during :meth:`fit`.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    homology_dimensions_ : tuple
-        Homology dimensions seen in :meth:`fit`.
-
-    samplings_ : dict
-        For each number in `homology_dimensions_`, a discrete sampling of
-        filtration parameters, calculated during :meth:`fit` according to the
-        minimum birth and maximum death values observed across all samples.
-
-    See also
-    --------
-    BettiCurve, PersistenceLandscape, PersistenceEntropy, Amplitude, \
-    PairwiseDistance, Silhouette, PersistenceImage, \
-    gtda.homology.VietorisRipsPersistence
-
-    Notes
-    -----
-    The samplings in :attr:`samplings_` are in general different between
-    different homology dimensions. This means that the (i, j)-th pixel
-    of an image in homology dimension q typically arises from a different
-    pair of parameter values to the (i, j)-th pixel of an image in
-    dimension q'.
-
-    References
-    ----------
-    .. [1] J. Reininghaus, S. Huber, U. Bauer, and R. Kwitt, "A Stable
-           Multi-Scale Kernel for Topological Machine Learning"; *2015 IEEE
-           Conference on Computer Vision and Pattern Recognition (CVPR)*,
-           pp. 4741--4748, 2015; `DOI: 10.1109/CVPR.2015.7299106
-           <http://dx.doi.org/10.1109/CVPR.2015.7299106>`_.
-
-    """
-
-    _hyperparameters = {
-        "n_bins": {"type": int, "in": Interval(1, np.inf, closed="left")},
-        "sigma": {"type": Real, "in": Interval(0, np.inf, closed="neither")}
-        }
-
-    def __init__(self, sigma=0.1, n_bins=100, n_jobs=None):
-        self.sigma = sigma
-        self.n_bins = n_bins
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Store all observed homology dimensions in
-        :attr:`homology_dimensions_` and, for each dimension separately,
-        store evenly sample filtration parameter values in :attr:`samplings_`.
-        Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_diagrams(X)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
-
-        # Find the unique homology dimensions in the 3D array X passed to `fit`
-        # assuming that they can all be found in its zero-th entry
-        homology_dimensions_fit = np.unique(X[0, :, 2])
-        self.homology_dimensions_ = \
-            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
-        self._n_dimensions = len(self.homology_dimensions_)
-
-        self._samplings, self._step_size = _bin(
-            X, "heat", n_bins=self.n_bins,
-            homology_dimensions=self.homology_dimensions_
-            )
-        self.samplings_ = {dim: s.flatten()
-                           for dim, s in self._samplings.items()}
-
-        return self
-
-    def transform(self, X, y=None):
-        """Compute multi-channel raster images from diagrams in `X` by
-        convolution with a Gaussian kernel and reflection about the diagonal.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins, \
-            n_bins)
-            Multi-channel raster images: one image per sample and one
-            channel per homology dimension seen in :meth:`fit`. Index i
-            along axis 1 corresponds to the i-th homology dimension in
-            :attr:`homology_dimensions_`.
-
-        """
-        check_is_fitted(self)
-        X = check_diagrams(X, copy=True)
-
-        Xt = Parallel(n_jobs=self.n_jobs, mmap_mode="c")(delayed(
-            heats)(_subdiagrams(X[s], [dim], remove_dim=True),
-                   self._samplings[dim], self._step_size[dim], self.sigma)
-            for dim in self.homology_dimensions_
-            for s in gen_even_slices(len(X), effective_n_jobs(self.n_jobs)))
-        Xt = np.concatenate(Xt).\
-            reshape(self._n_dimensions, len(X), self.n_bins, self.n_bins).\
-            transpose((1, 0, 2, 3))
-        return Xt
-
-    def plot(self, Xt, sample=0, homology_dimension_idx=0, colorscale="blues",
-             plotly_params=None):
-        """Plot a single channel - corresponding to a given homology
-        dimension -- in a sample from a collection of heat kernel images.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins, \
-            n_bins)
-            Collection of multi-channel raster images, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be selected.
-
-        homology_dimension_idx : int, optional, default: ``0``
-            Index of the channel in the selected sample to be plotted. If `Xt`
-            is the result of a call to :meth:`transform` and this index is i,
-            the plot corresponds to the homology dimension given by the i-th
-            entry in :attr:`homology_dimensions_`.
-
-        colorscale : str, optional, default: ``"blues"``
-            Color scale to be used in the heat map. Can be anything allowed by
-            :class:`plotly.graph_objects.Heatmap`.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        check_is_fitted(self)
-        homology_dimension = self.homology_dimensions_[homology_dimension_idx]
-        if homology_dimension != np.inf:
-            homology_dimension = int(homology_dimension)
-        x = self.samplings_[homology_dimension]
-
-        return plot_heatmap(
-            Xt[sample][homology_dimension_idx], x=x, y=x[::-1],
-            colorscale=colorscale, origin="lower",
-            title=f"Heat kernel representation of diagram {sample} in "
-                  f"homology dimension {homology_dimension}",
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class PersistenceImage(BaseEstimator, TransformerMixin, PlotterMixin):
-    """:ref:`Persistence images <TODO>` of persistence
-    diagrams.
-
-    Based on ideas in [1]_. Given a persistence diagram consisting of
-    birth-death-dimension triples [b, d, q], the equivalent diagrams of
-    birth-persistence-dimension [b, d-b, q] triples are computed and
-    subdiagrams corresponding to distinct homology dimensions are considered
-    separately and regarded as sums of Dirac deltas. Then, the convolution
-    with a Gaussian kernel is computed over a rectangular grid of locations
-    evenly sampled from appropriate ranges of the :ref:`filtration parameter
-    <filtered_complex>`. The result can be thought of as a (multi-channel)
-    raster image.
-
-    **Important note**:
-
-        - Input collections of persistence diagrams for this transformer must
-          satisfy certain requirements, see e.g. :meth:`fit`.
-
-    Parameters
-    ----------
-    sigma : float, optional default ``0.1``
-        Standard deviation for Gaussian kernel.
-
-    n_bins : int, optional, default: ``100``
-        The number of filtration parameter values, per available homology
-        dimension, to sample during :meth:`fit`.
-
-    weight_function : callable or None, default: ``None``
-        Function mapping the 1D array of sampled persistence values (see
-        :attr:`samplings_`) to a 1D array of weights. ``None`` is equivalent to
-        passing ``numpy.ones_like``. More weight can be given to regions of
-        high persistence by passing a monotonic function, e.g. the identity.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    effective_weight_function_ : callable
-        Effective function corresponding to `weight_function`. Set in
-        :meth:`fit`.
-
-    homology_dimensions_ : tuple
-        Homology dimensions seen in :meth:`fit`.
-
-    samplings_ : dict
-        For each dimension in `homology_dimensions_`, a discrete sampling of
-        birth parameters and one of persistence values, calculated during
-        :meth:`fit` according to the minimum birth and maximum death values
-        observed across all samples.
-
-    weights_ : dict
-        For each number in `homology_dimensions_`, an array of weights
-        corresponding to the persistence values obtained from `samplings_`
-        calculated during :meth:`fit` using the `weight_function`.
-
-    See also
-    --------
-    BettiCurve, PersistenceLandscape, PersistenceEntropy, HeatKernel, \
-    Amplitude, PairwiseDistance, gtda.homology.VietorisRipsPersistence
-
-    Notes
-    -----
-    The samplings in :attr:`samplings_` are in general different between
-    different homology dimensions. This means that the (i, j)-th pixel of a
-    persistence image in homology dimension q typically arises from a different
-    pair of parameter values to the (i, j)-th pixel of a persistence image in
-    dimension q'.
-
-    References
-    ----------
-    .. [1] H. Adams, T. Emerson, M. Kirby, R. Neville, C. Peterson, P. Shipman,
-           S. Chepushtanova, E. Hanson, F. Motta, and L. Ziegelmeier,
-           "Persistence Images: A Stable Vector Representation of Persistent
-           Homology"; *Journal of Machine Learning Research 18, 1*,
-           pp. 218-252, 2017; `DOI: 10.5555/3122009.3122017
-           <http://dx.doi.org/10.5555/3122009.3122017>`_.
-
-    """
-
-    _hyperparameters = {
-        "n_bins": {"type": int, "in": Interval(1, np.inf, closed="left")},
-        "sigma": {"type": Real, "in": Interval(0, np.inf, closed="neither")},
-        "weight_function": {"type": (types.FunctionType, type(None))}
-        }
-
-    def __init__(self, sigma=0.1, n_bins=100, weight_function=None,
-                 n_jobs=None):
-        self.sigma = sigma
-        self.n_bins = n_bins
-        self.weight_function = weight_function
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Store all observed homology dimensions in
-        :attr:`homology_dimensions_` and, for each dimension separately,
-        store evenly sample filtration parameter values in :attr:`samplings_`.
-        Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_diagrams(X)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
-
-        if self.weight_function is None:
-            self.effective_weight_function_ = np.ones_like
-        else:
-            self.effective_weight_function_ = self.weight_function
-
-        # Find the unique homology dimensions in the 3D array X passed to `fit`
-        # assuming that they can all be found in its zero-th entry
-        homology_dimensions_fit = np.unique(X[0, :, 2])
-        self.homology_dimensions_ = \
-            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
-        self._n_dimensions = len(self.homology_dimensions_)
-
-        self._samplings, self._step_size = _bin(
-            X, "persistence_image",  n_bins=self.n_bins,
-            homology_dimensions=self.homology_dimensions_
-            )
-        self.weights_ = {
-            dim: self.effective_weight_function_(samplings_dim[:, 1])
-            for dim, samplings_dim in self._samplings.items()
-            }
-        self.samplings_ = {dim: s.T for dim, s in self._samplings.items()}
-
-        return self
-
-    def transform(self, X, y=None):
-        """Compute multi-channel raster images from diagrams in `X` by
-        convolution with a Gaussian kernel.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins, \
-            n_bins)
-            Multi-channel raster images: one image per sample and one channel
-            per homology dimension seen in :meth:`fit`. Index i along axis 1
-            corresponds to the i-th homology dimension in
-            :attr:`homology_dimensions_`.
-
-        """
-        check_is_fitted(self)
-        X = check_diagrams(X, copy=True)
-
-        Xt = Parallel(n_jobs=self.n_jobs, mmap_mode="c")(
-            delayed(persistence_images)(
-                _subdiagrams(X[s], [dim], remove_dim=True),
-                self._samplings[dim],
-                self._step_size[dim],
-                self.sigma,
-                self.weights_[dim]
-                )
-            for dim in self.homology_dimensions_
-            for s in gen_even_slices(len(X), effective_n_jobs(self.n_jobs))
-            )
-        Xt = np.concatenate(Xt).\
-            reshape(self._n_dimensions, len(X), self.n_bins, self.n_bins).\
-            transpose((1, 0, 2, 3))
-        return Xt
-
-    def plot(self, Xt, sample=0, homology_dimension_idx=0, colorscale="blues",
-             plotly_params=None):
-        """Plot a single channel - corresponding to a given homology
-        dimension - in a sample from a collection of persistence images.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins, \
-            n_bins)
-            Collection of multi-channel raster images, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be selected.
-
-        homology_dimension_idx : int, optional, default: ``0``
-            Index of the channel in the selected sample to be plotted. If `Xt`
-            is the result of a call to :meth:`transform` and this index is i,
-            the plot corresponds to the homology dimension given by the i-th
-            entry in :attr:`homology_dimensions_`.
-
-        colorscale : str, optional, default: ``"blues"``
-            Color scale to be used in the heat map. Can be anything allowed by
-            :class:`plotly.graph_objects.Heatmap`.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        check_is_fitted(self)
-        homology_dimension = self.homology_dimensions_[homology_dimension_idx]
-        if homology_dimension != np.inf:
-            homology_dimension = int(homology_dimension)
-        samplings_x, samplings_y = self.samplings_[homology_dimension]
-
-        return plot_heatmap(
-            Xt[sample][homology_dimension_idx],
-            x=samplings_x,
-            y=samplings_y[::-1],
-            colorscale=colorscale,
-            origin="lower",
-            title=f"Persistence image representation of diagram {sample} in "
-                  f"homology dimension {homology_dimension}",
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class Silhouette(BaseEstimator, TransformerMixin, PlotterMixin):
-    """:ref:`Power-weighted silhouettes <weighted_silhouette>` of persistence
-    diagrams.
-
-    Based on ideas in [1]_. Given a persistence diagram consisting of
-    birth-death-dimension triples [b, d, q], subdiagrams corresponding to
-    distinct homology dimensions are considered separately, and their
-    respective silhouettes are obtained by sampling the silhouette function
-    over evenly spaced locations from appropriate ranges of the
-    :ref:`filtration parameter <filtered_complex>`.
-
-    **Important note**:
-
-        - Input collections of persistence diagrams for this transformer must
-          satisfy certain requirements, see e.g. :meth:`fit`.
-
-    Parameters
-    ----------
-    power: float, optional, default: ``1.``
-        The power to which persistence values are raised to define the
-        :ref:`power-weighted silhouettes <weighted_silhouette>`.
-
-    n_bins : int, optional, default: ``100``
-        The number of filtration parameter values, per available homology
-        dimension, to sample during :meth:`fit`.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    homology_dimensions_ : tuple
-        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
-
-    samplings_ : dict
-        For each number in `homology_dimensions_`, a discrete sampling of
-        filtration parameters, calculated during :meth:`fit` according to the
-        minimum birth and maximum death values observed across all samples.
-
-    See also
-    --------
-    PersistenceLandscape, PersistenceEntropy, HeatKernel, Amplitude, \
-    PairwiseDistance, BettiCurve, gtda.homology.VietorisRipsPersistence
-
-    Notes
-    -----
-    The samplings in :attr:`samplings_` are in general different between
-    different homology dimensions. This means that the j-th entry of a
-    silhouette in homology dimension q typically arises from a different
-    parameter values to the j-th entry of a curve in dimension q'.
-
-    References
-    ----------
-    .. [1] F. Chazal, B. T. Fasy, F. Lecci, A. Rinaldo, and L. Wasserman,
-           "Stochastic Convergence of Persistence Landscapes and Silhouettes";
-           *In Proceedings of the thirtieth annual symposium on Computational
-           Geometry*, Kyoto, Japan, 2014, pp. 474483;
-           `DOI: 10.1145/2582112.2582128
-           <http://dx.doi.org/10.1145/2582112.2582128>`_.
-
-    """
-
-    _hyperparameters = {
-        "power": {"type": Real, "in": Interval(0, np.inf, closed="right")},
-        "n_bins": {"type": int, "in": Interval(1, np.inf, closed="left")}
-        }
-
-    def __init__(self, power=1., n_bins=100, n_jobs=None):
-        self.power = power
-        self.n_bins = n_bins
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Store all observed homology dimensions in
-        :attr:`homology_dimensions_` and, for each dimension separately,
-        store evenly sample filtration parameter values in :attr:`samplings_`.
-        Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_diagrams(X)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
-
-        # Find the unique homology dimensions in the 3D array X passed to `fit`
-        # assuming that they can all be found in its zero-th entry
-        homology_dimensions_fit = np.unique(X[0, :, 2])
-        self.homology_dimensions_ = \
-            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
-        self._n_dimensions = len(self.homology_dimensions_)
-
-        self._samplings, _ = _bin(
-            X, "silhouette", n_bins=self.n_bins,
-            homology_dimensions=self.homology_dimensions_
-            )
-        self.samplings_ = {dim: s.flatten()
-                           for dim, s in self._samplings.items()}
-
-        return self
-
-    def transform(self, X, y=None):
-        """Compute silhouettes of diagrams in `X`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features, 3)
-            Input data. Array of persistence diagrams, each a collection of
-            triples [b, d, q] representing persistent topological features
-            through their birth (b), death (d) and homology dimension (q).
-            It is important that, for each possible homology dimension, the
-            number of triples for which q equals that homology dimension is
-            constants across the entries of X.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins)
-            One silhouette (represented as a one-dimensional array)
-            per sample and per homology dimension seen
-            in :meth:`fit`. Index i along axis 1 corresponds to the i-th
-            homology dimension in :attr:`homology_dimensions_`.
-
-        """
-        check_is_fitted(self)
-        X = check_diagrams(X)
-
-        Xt = (Parallel(n_jobs=self.n_jobs)
-              (delayed(silhouettes)(_subdiagrams(X[s], [dim], remove_dim=True),
-                                    self._samplings[dim], power=self.power)
-              for dim in self.homology_dimensions_
-              for s in gen_even_slices(len(X), effective_n_jobs(self.n_jobs))))
-
-        Xt = np.concatenate(Xt).\
-            reshape(self._n_dimensions, len(X), -1).\
-            transpose((1, 0, 2))
-        return Xt
-
-    def plot(self, Xt, sample=0, homology_dimensions=None, plotly_params=None):
-        """Plot a sample from a collection of silhouettes arranged as in the
-        output of :meth:`transform`. Include homology in multiple dimensions.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins)
-            Collection of silhouettes, such as returned by :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        homology_dimensions : list, tuple or None, optional, default: ``None``
-            Which homology dimensions to include in the plot. ``None`` means
-            plotting all dimensions present in :attr:`homology_dimensions_`.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        check_is_fitted(self)
-
-        homology_dimensions_mapping = _make_homology_dimensions_mapping(
-            homology_dimensions, self.homology_dimensions_
-            )
-
-        layout_axes_common = {
-            "type": "linear",
-            "ticks": "outside",
-            "showline": True,
-            "zeroline": True,
-            "linewidth": 1,
-            "linecolor": "black",
-            "mirror": False,
-            "showexponent": "all",
-            "exponentformat": "e"
-            }
-        layout = {
-            "xaxis1": {
-                "title": "Filtration parameter",
-                "side": "bottom",
-                "anchor": "y1",
-                **layout_axes_common
-                },
-            "yaxis1": {
-                "side": "left",
-                "anchor": "x1",
-                **layout_axes_common
-                },
-            "plot_bgcolor": "white",
-            "title": f"Silhouette representation of diagram {sample}"
-            }
-
-        fig = Figure(layout=layout)
-
-        for ix, dim in homology_dimensions_mapping:
-            fig.add_trace(Scatter(x=self.samplings_[dim],
-                                  y=Xt[sample][ix],
-                                  mode="lines",
-                                  showlegend=True,
-                                  hoverinfo="none",
-                                  name=f"H{dim}"))
-
-        # Update traces and layout according to user input
-        if plotly_params:
-            fig.update_traces(plotly_params.get("traces", None))
-            fig.update_layout(plotly_params.get("layout", None))
-
-        return fig
+"""Vector representations of persistence diagrams."""
+# License: GNU AGPLv3
+
+from typing import Callable
+from numbers import Real
+
+import numpy as np
+from joblib import Parallel, delayed, effective_n_jobs
+from plotly.graph_objects import Figure, Scatter
+from plotly.subplots import make_subplots
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils import gen_even_slices
+from sklearn.utils.validation import check_is_fitted
+
+from ._metrics import betti_curves, landscapes, heats, \
+    persistence_images, silhouettes
+from ._utils import _subdiagrams, _bin, _make_homology_dimensions_mapping, \
+    _homology_dimensions_to_sorted_ints
+from ..base import PlotterMixin
+from ..plotting import plot_heatmap
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.intervals import Interval
+from ..utils.validation import validate_params, check_diagrams
+
+
+@adapt_fit_transform_docs
+class BettiCurve(BaseEstimator, TransformerMixin, PlotterMixin):
+    """:ref:`Betti curves <betti_curve>` of persistence diagrams.
+
+    Given a persistence diagram consisting of birth-death-dimension triples
+    [b, d, q], subdiagrams corresponding to distinct homology dimensions are
+    considered separately, and their respective Betti curves are obtained by
+    evenly sampling the :ref:`filtration parameter <filtered_complex>`.
+
+    **Important note**:
+
+        - Input collections of persistence diagrams for this transformer must
+          satisfy certain requirements, see e.g. :meth:`fit`.
+
+    Parameters
+    ----------
+    n_bins : int, optional, default: ``100``
+        The number of filtration parameter values, per available homology
+        dimension, to sample during :meth:`fit`.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    homology_dimensions_ : tuple
+        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
+
+    samplings_ : dict
+        For each number in `homology_dimensions_`, a discrete sampling of
+        filtration parameters, calculated during :meth:`fit` according to the
+        minimum birth and maximum death values observed across all samples.
+
+    See also
+    --------
+    PersistenceLandscape, PersistenceEntropy, HeatKernel, Amplitude, \
+    PairwiseDistance, Silhouette, PersistenceImage, \
+    gtda.homology.VietorisRipsPersistence
+
+    Notes
+    -----
+    The samplings in :attr:`samplings_` are in general different between
+    different homology dimensions. This means that the j-th entry of a Betti
+    curve in homology dimension q typically arises from a different parameter
+    values to the j-th entry of a curve in dimension q'.
+
+    """
+
+    _hyperparameters = {
+        "n_bins": {"type": int, "in": Interval(1, np.inf, closed="left")}
+        }
+
+    def __init__(self, n_bins=100, n_jobs=None):
+        self.n_bins = n_bins
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Store all observed homology dimensions in
+        :attr:`homology_dimensions_` and, for each dimension separately,
+        store evenly sample filtration parameter values in :attr:`samplings_`.
+        Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_diagrams(X)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
+
+        # Find the unique homology dimensions in the 3D array X passed to `fit`
+        # assuming that they can all be found in its zero-th entry
+        homology_dimensions_fit = np.unique(X[0, :, 2])
+        self.homology_dimensions_ = \
+            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
+        self._n_dimensions = len(self.homology_dimensions_)
+
+        self._samplings, _ = _bin(
+            X, "betti", n_bins=self.n_bins,
+            homology_dimensions=self.homology_dimensions_
+            )
+        self.samplings_ = {dim: s.flatten()
+                           for dim, s in self._samplings.items()}
+
+        return self
+
+    def transform(self, X, y=None):
+        """Compute the Betti curves of diagrams in `X`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins)
+            Betti curves: one curve (represented as a one-dimensional array
+            of integer values) per sample and per homology dimension seen
+            in :meth:`fit`. Index i along axis 1 corresponds to the i-th
+            homology dimension in :attr:`homology_dimensions_`.
+
+        """
+        check_is_fitted(self)
+        X = check_diagrams(X)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(delayed(betti_curves)(
+                _subdiagrams(X[s], [dim], remove_dim=True),
+                self._samplings[dim])
+            for dim in self.homology_dimensions_
+            for s in gen_even_slices(len(X), effective_n_jobs(self.n_jobs)))
+        Xt = np.concatenate(Xt).\
+            reshape(self._n_dimensions, len(X), -1).\
+            transpose((1, 0, 2))
+
+        return Xt
+
+    def plot(self, Xt, sample=0, homology_dimensions=None, plotly_params=None):
+        """Plot a sample from a collection of Betti curves arranged as in the
+        output of :meth:`transform`. Include homology in multiple dimensions.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins)
+            Collection of Betti curves, such as returned by :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        homology_dimensions : list, tuple or None, optional, default: ``None``
+            Which homology dimensions to include in the plot. ``None`` means
+            plotting all dimensions present in :attr:`homology_dimensions_`.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        check_is_fitted(self)
+
+        homology_dimensions_mapping = _make_homology_dimensions_mapping(
+            homology_dimensions, self.homology_dimensions_
+            )
+
+        layout_axes_common = {
+            "type": "linear",
+            "ticks": "outside",
+            "showline": True,
+            "zeroline": True,
+            "linewidth": 1,
+            "linecolor": "black",
+            "mirror": False,
+            "showexponent": "all",
+            "exponentformat": "e"
+            }
+        layout = {
+            "xaxis1": {
+                "title": "Filtration parameter",
+                "side": "bottom",
+                "anchor": "y1",
+                **layout_axes_common
+                },
+            "yaxis1": {
+                "title": "Betti number",
+                "side": "left",
+                "anchor": "x1",
+                **layout_axes_common
+                },
+            "plot_bgcolor": "white",
+            "title": f"Betti curves from diagram {sample}"
+            }
+
+        fig = Figure(layout=layout)
+
+        for ix, dim in homology_dimensions_mapping:
+            fig.add_trace(Scatter(x=self.samplings_[dim],
+                                  y=Xt[sample][ix],
+                                  mode="lines",
+                                  showlegend=True,
+                                  name=f"H{dim}"))
+
+        # Update traces and layout according to user input
+        if plotly_params:
+            fig.update_traces(plotly_params.get("traces", None))
+            fig.update_layout(plotly_params.get("layout", None))
+
+        return fig
+
+
+@adapt_fit_transform_docs
+class PersistenceLandscape(BaseEstimator, TransformerMixin, PlotterMixin):
+    """:ref:`Persistence landscapes <persistence_landscape>` of persistence
+    diagrams.
+
+    Given a persistence diagram consisting of birth-death-dimension triples
+    [b, d, q], subdiagrams corresponding to distinct homology dimensions are
+    considered separately, and layers of their respective persistence
+    landscapes are obtained by evenly sampling the :ref:`filtration parameter
+    <filtered_complex>`.
+
+    **Important note**:
+
+        - Input collections of persistence diagrams for this transformer must
+          satisfy certain requirements, see e.g. :meth:`fit`.
+
+    Parameters
+    ----------
+    n_layers : int, optional, default: ``1``
+        How many layers to consider in the persistence landscape.
+
+    n_bins : int, optional, default: ``100``
+        The number of filtration parameter values, per available
+        homology dimension, to sample during :meth:`fit`.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    homology_dimensions_ : tuple
+        Homology dimensions seen in :meth:`fit`.
+
+    samplings_ : dict
+        For each number in `homology_dimensions_`, a discrete sampling of
+        filtration parameters, calculated during :meth:`fit` according to the
+        minimum birth and maximum death values observed across all samples.
+
+    See also
+    --------
+    BettiCurve, PersistenceEntropy, HeatKernel, Amplitude, PairwiseDistance, \
+    Silhouette, PersistenceImage, gtda.homology.VietorisRipsPersistence
+
+    Notes
+    -----
+    The samplings in :attr:`samplings_` are in general different between
+    different homology dimensions. This means that the j-th entry of the
+    k-layer of a persistence landscape in homology dimension q typically arises
+    from a different parameter value to the j-th entry of a k-layer in
+    dimension q'.
+
+    """
+
+    _hyperparameters = {
+        "n_bins": {"type": int, "in": Interval(1, np.inf, closed="left")},
+        "n_layers": {"type": int, "in": Interval(1, np.inf, closed="left")}
+        }
+
+    def __init__(self, n_layers=1, n_bins=100, n_jobs=None):
+        self.n_layers = n_layers
+        self.n_bins = n_bins
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Store all observed homology dimensions in
+        :attr:`homology_dimensions_` and, for each dimension separately, store
+        evenly sample filtration parameter values in :attr:`samplings_`. Then,
+        return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_diagrams(X)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
+
+        # Find the unique homology dimensions in the 3D array X passed to `fit`
+        # assuming that they can all be found in its zero-th entry
+        homology_dimensions_fit = np.unique(X[0, :, 2])
+        self.homology_dimensions_ = \
+            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
+        self._n_dimensions = len(self.homology_dimensions_)
+
+        self._samplings, _ = _bin(
+            X, "landscape", n_bins=self.n_bins,
+            homology_dimensions=self.homology_dimensions_
+            )
+        self.samplings_ = {dim: s.flatten()
+                           for dim, s in self._samplings.items()}
+
+        return self
+
+    def transform(self, X, y=None):
+        """Compute the persistence landscapes of diagrams in `X`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_homology_dimensions * n_layers, \
+            n_bins)
+            Persistence landscapes, where ``n_homology_dimensions`` is the
+            number of distinct homology dimensions seen in :meth:`fit`.
+            Landscapes coming from different homology dimensions are stacked
+            for each sample, so layer ``k`` of the landscape in the ``j``-th
+            homology dimension in :attr:`homology_dimensions_` is
+            ``X[i, n_homology_dimensions * j + k]``.
+
+        """
+        check_is_fitted(self)
+        X = check_diagrams(X)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(landscapes)(_subdiagrams(X[s], [dim], remove_dim=True),
+                                self._samplings[dim],
+                                self.n_layers)
+            for dim in self.homology_dimensions_
+            for s in gen_even_slices(len(X), effective_n_jobs(self.n_jobs))
+            )
+        Xt = np.concatenate(Xt).\
+            reshape(self._n_dimensions, len(X), self.n_layers, self.n_bins).\
+            transpose((1, 0, 2, 3)).\
+            reshape(len(X), self._n_dimensions * self.n_layers, self.n_bins)
+
+        return Xt
+
+    def plot(self, Xt, sample=0, homology_dimensions=None, plotly_params=None):
+        """Plot a sample from a collection of persistence landscapes arranged
+        as in the output of :meth:`transform`. Include homology in multiple
+        dimensions.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_layers, \
+            n_bins
+            Collection of persistence landscapes, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        homology_dimensions : list, tuple or None, optional, default: ``None``
+            Homology dimensions for which the landscape should be plotted.
+            ``None`` means plotting all dimensions present in
+            :attr:`homology_dimensions_`.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        check_is_fitted(self)
+
+        homology_dimensions_mapping = _make_homology_dimensions_mapping(
+            homology_dimensions, self.homology_dimensions_
+            )
+
+        layout_axes_common = {
+            "type": "linear",
+            "ticks": "outside",
+            "showline": True,
+            "zeroline": True,
+            "linewidth": 1,
+            "linecolor": "black",
+            "mirror": False,
+            "showexponent": "all",
+            "exponentformat": "e"
+            }
+        layout = {
+            "xaxis1": {
+                "side": "bottom",
+                "anchor": "y1",
+                **layout_axes_common
+                },
+            "yaxis1": {
+                "side": "left",
+                "anchor": "x1",
+                **layout_axes_common
+                },
+            "plot_bgcolor": "white",
+            }
+
+        Xt_sample = Xt[sample]
+        n_dims = len(self.homology_dimensions_)
+        n_layers = Xt_sample.shape[0] // n_dims
+        subplot_titles = [f"H{dim}" for _, dim in homology_dimensions_mapping]
+        fig = make_subplots(rows=len(homology_dimensions_mapping), cols=1,
+                            subplot_titles=subplot_titles)
+        has_many_homology_dim = len(homology_dimensions_mapping) - 1
+        for i, (inv_idx, dim) in enumerate(homology_dimensions_mapping):
+            hom_dim_str = \
+                f" ({subplot_titles[i]})" if has_many_homology_dim else ""
+            for layer in range(n_layers):
+                fig.add_trace(
+                    Scatter(x=self.samplings_[dim],
+                            y=Xt_sample[inv_idx * n_layers + layer],
+                            mode="lines",
+                            showlegend=True,
+                            hoverinfo="none",
+                            name=f"Layer {layer + 1}{hom_dim_str}"),
+                    row=i + 1,
+                    col=1
+                    )
+
+        fig.update_layout(
+            title_text=f"Landscape representations of diagram {sample}",
+            **layout.copy()
+            )
+
+        # Update traces and layout according to user input
+        if plotly_params:
+            fig.update_traces(plotly_params.get("traces", None))
+            fig.update_layout(plotly_params.get("layout", None))
+
+        return fig
+
+
+@adapt_fit_transform_docs
+class HeatKernel(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Convolution of persistence diagrams with a Gaussian kernel.
+
+    Based on ideas in [1]_. Given a persistence diagram consisting of
+    birth-death-dimension triples [b, d, q], subdiagrams corresponding to
+    distinct homology dimensions are considered separately and regarded as sums
+    of Dirac deltas. Then, the convolution with a Gaussian kernel is computed
+    over a rectangular grid of locations evenly sampled from appropriate
+    ranges of the :ref:`filtration parameter <filtered_complex>`. The
+    same is done with the reflected images of the subdiagrams about the
+    diagonal, and the difference between the results of the two convolutions is
+    computed. The result can be thought of as a (multi-channel) raster image.
+
+    **Important note**:
+
+        - Input collections of persistence diagrams for this transformer must
+          satisfy certain requirements, see e.g. :meth:`fit`.
+
+    Parameters
+    ----------
+    sigma : float, optional default ``0.1``
+        Standard deviation for Gaussian kernel.
+
+    n_bins : int, optional, default: ``100``
+        The number of filtration parameter values, per available homology
+        dimension, to sample during :meth:`fit`.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    homology_dimensions_ : tuple
+        Homology dimensions seen in :meth:`fit`.
+
+    samplings_ : dict
+        For each number in `homology_dimensions_`, a discrete sampling of
+        filtration parameters, calculated during :meth:`fit` according to the
+        minimum birth and maximum death values observed across all samples.
+
+    See also
+    --------
+    BettiCurve, PersistenceLandscape, PersistenceEntropy, Amplitude, \
+    PairwiseDistance, Silhouette, PersistenceImage, \
+    gtda.homology.VietorisRipsPersistence
+
+    Notes
+    -----
+    The samplings in :attr:`samplings_` are in general different between
+    different homology dimensions. This means that the (i, j)-th pixel
+    of an image in homology dimension q typically arises from a different
+    pair of parameter values to the (i, j)-th pixel of an image in
+    dimension q'.
+
+    References
+    ----------
+    .. [1] J. Reininghaus, S. Huber, U. Bauer, and R. Kwitt, "A Stable
+           Multi-Scale Kernel for Topological Machine Learning"; *2015 IEEE
+           Conference on Computer Vision and Pattern Recognition (CVPR)*,
+           pp. 4741--4748, 2015; `DOI: 10.1109/CVPR.2015.7299106
+           <http://dx.doi.org/10.1109/CVPR.2015.7299106>`_.
+
+    """
+
+    _hyperparameters = {
+        "n_bins": {"type": int, "in": Interval(1, np.inf, closed="left")},
+        "sigma": {"type": Real, "in": Interval(0, np.inf, closed="neither")}
+        }
+
+    def __init__(self, sigma=0.1, n_bins=100, n_jobs=None):
+        self.sigma = sigma
+        self.n_bins = n_bins
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Store all observed homology dimensions in
+        :attr:`homology_dimensions_` and, for each dimension separately,
+        store evenly sample filtration parameter values in :attr:`samplings_`.
+        Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_diagrams(X)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
+
+        # Find the unique homology dimensions in the 3D array X passed to `fit`
+        # assuming that they can all be found in its zero-th entry
+        homology_dimensions_fit = np.unique(X[0, :, 2])
+        self.homology_dimensions_ = \
+            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
+        self._n_dimensions = len(self.homology_dimensions_)
+
+        self._samplings, self._step_size = _bin(
+            X, "heat", n_bins=self.n_bins,
+            homology_dimensions=self.homology_dimensions_
+            )
+        self.samplings_ = {dim: s.flatten()
+                           for dim, s in self._samplings.items()}
+
+        return self
+
+    def transform(self, X, y=None):
+        """Compute multi-channel raster images from diagrams in `X` by
+        convolution with a Gaussian kernel and reflection about the diagonal.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins, \
+            n_bins)
+            Multi-channel raster images: one image per sample and one
+            channel per homology dimension seen in :meth:`fit`. Index i
+            along axis 1 corresponds to the i-th homology dimension in
+            :attr:`homology_dimensions_`.
+
+        """
+        check_is_fitted(self)
+        X = check_diagrams(X, copy=True)
+
+        Xt = Parallel(n_jobs=self.n_jobs, mmap_mode="c")(delayed(
+            heats)(_subdiagrams(X[s], [dim], remove_dim=True),
+                   self._samplings[dim], self._step_size[dim], self.sigma)
+            for dim in self.homology_dimensions_
+            for s in gen_even_slices(len(X), effective_n_jobs(self.n_jobs)))
+        Xt = np.concatenate(Xt).\
+            reshape(self._n_dimensions, len(X), self.n_bins, self.n_bins).\
+            transpose((1, 0, 2, 3))
+        return Xt
+
+    def plot(self, Xt, sample=0, homology_dimension_idx=0, colorscale="blues",
+             plotly_params=None):
+        """Plot a single channel - corresponding to a given homology
+        dimension -- in a sample from a collection of heat kernel images.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins, \
+            n_bins)
+            Collection of multi-channel raster images, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be selected.
+
+        homology_dimension_idx : int, optional, default: ``0``
+            Index of the channel in the selected sample to be plotted. If `Xt`
+            is the result of a call to :meth:`transform` and this index is i,
+            the plot corresponds to the homology dimension given by the i-th
+            entry in :attr:`homology_dimensions_`.
+
+        colorscale : str, optional, default: ``"blues"``
+            Color scale to be used in the heat map. Can be anything allowed by
+            :class:`plotly.graph_objects.Heatmap`.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        check_is_fitted(self)
+        homology_dimension = self.homology_dimensions_[homology_dimension_idx]
+        if homology_dimension != np.inf:
+            homology_dimension = int(homology_dimension)
+        x = self.samplings_[homology_dimension]
+
+        return plot_heatmap(
+            Xt[sample][homology_dimension_idx], x=x, y=x[::-1],
+            colorscale=colorscale, origin="lower",
+            title=f"Heat kernel representation of diagram {sample} in "
+                  f"homology dimension {homology_dimension}",
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class PersistenceImage(BaseEstimator, TransformerMixin, PlotterMixin):
+    """:ref:`Persistence images <TODO>` of persistence
+    diagrams.
+
+    Based on ideas in [1]_. Given a persistence diagram consisting of
+    birth-death-dimension triples [b, d, q], the equivalent diagrams of
+    birth-persistence-dimension [b, d-b, q] triples are computed and
+    subdiagrams corresponding to distinct homology dimensions are considered
+    separately and regarded as sums of Dirac deltas. Then, the convolution
+    with a Gaussian kernel is computed over a rectangular grid of locations
+    evenly sampled from appropriate ranges of the :ref:`filtration parameter
+    <filtered_complex>`. The result can be thought of as a (multi-channel)
+    raster image.
+
+    **Important note**:
+
+        - Input collections of persistence diagrams for this transformer must
+          satisfy certain requirements, see e.g. :meth:`fit`.
+
+    Parameters
+    ----------
+    sigma : float, optional default ``0.1``
+        Standard deviation for Gaussian kernel.
+
+    n_bins : int, optional, default: ``100``
+        The number of filtration parameter values, per available homology
+        dimension, to sample during :meth:`fit`.
+
+    weight_function : callable or None, default: ``None``
+        Function mapping the 1D array of sampled persistence values (see
+        :attr:`samplings_`) to a 1D array of weights. ``None`` is equivalent to
+        passing ``numpy.ones_like``. More weight can be given to regions of
+        high persistence by passing a monotonic function, e.g. the identity.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    effective_weight_function_ : callable
+        Effective function corresponding to `weight_function`. Set in
+        :meth:`fit`.
+
+    homology_dimensions_ : tuple
+        Homology dimensions seen in :meth:`fit`.
+
+    samplings_ : dict
+        For each dimension in `homology_dimensions_`, a discrete sampling of
+        birth parameters and one of persistence values, calculated during
+        :meth:`fit` according to the minimum birth and maximum death values
+        observed across all samples.
+
+    weights_ : dict
+        For each number in `homology_dimensions_`, an array of weights
+        corresponding to the persistence values obtained from `samplings_`
+        calculated during :meth:`fit` using the `weight_function`.
+
+    See also
+    --------
+    BettiCurve, PersistenceLandscape, PersistenceEntropy, HeatKernel, \
+    Amplitude, PairwiseDistance, gtda.homology.VietorisRipsPersistence
+
+    Notes
+    -----
+    The samplings in :attr:`samplings_` are in general different between
+    different homology dimensions. This means that the (i, j)-th pixel of a
+    persistence image in homology dimension q typically arises from a different
+    pair of parameter values to the (i, j)-th pixel of a persistence image in
+    dimension q'.
+
+    References
+    ----------
+    .. [1] H. Adams, T. Emerson, M. Kirby, R. Neville, C. Peterson, P. Shipman,
+           S. Chepushtanova, E. Hanson, F. Motta, and L. Ziegelmeier,
+           "Persistence Images: A Stable Vector Representation of Persistent
+           Homology"; *Journal of Machine Learning Research 18, 1*,
+           pp. 218-252, 2017; `DOI: 10.5555/3122009.3122017
+           <http://dx.doi.org/10.5555/3122009.3122017>`_.
+
+    """
+
+    _hyperparameters = {
+        "n_bins": {"type": int, "in": Interval(1, np.inf, closed="left")},
+        "sigma": {"type": Real, "in": Interval(0, np.inf, closed="neither")},
+        "weight_function": {"type": (Callable, type(None))}
+        }
+
+    def __init__(self, sigma=0.1, n_bins=100, weight_function=None,
+                 n_jobs=None):
+        self.sigma = sigma
+        self.n_bins = n_bins
+        self.weight_function = weight_function
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Store all observed homology dimensions in
+        :attr:`homology_dimensions_` and, for each dimension separately,
+        store evenly sample filtration parameter values in :attr:`samplings_`.
+        Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_diagrams(X)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
+
+        if self.weight_function is None:
+            self.effective_weight_function_ = np.ones_like
+        else:
+            self.effective_weight_function_ = self.weight_function
+
+        # Find the unique homology dimensions in the 3D array X passed to `fit`
+        # assuming that they can all be found in its zero-th entry
+        homology_dimensions_fit = np.unique(X[0, :, 2])
+        self.homology_dimensions_ = \
+            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
+        self._n_dimensions = len(self.homology_dimensions_)
+
+        self._samplings, self._step_size = _bin(
+            X, "persistence_image",  n_bins=self.n_bins,
+            homology_dimensions=self.homology_dimensions_
+            )
+        self.weights_ = {
+            dim: self.effective_weight_function_(samplings_dim[:, 1])
+            for dim, samplings_dim in self._samplings.items()
+            }
+        self.samplings_ = {dim: s.T for dim, s in self._samplings.items()}
+
+        return self
+
+    def transform(self, X, y=None):
+        """Compute multi-channel raster images from diagrams in `X` by
+        convolution with a Gaussian kernel.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins, \
+            n_bins)
+            Multi-channel raster images: one image per sample and one channel
+            per homology dimension seen in :meth:`fit`. Index i along axis 1
+            corresponds to the i-th homology dimension in
+            :attr:`homology_dimensions_`.
+
+        """
+        check_is_fitted(self)
+        X = check_diagrams(X, copy=True)
+
+        Xt = Parallel(n_jobs=self.n_jobs, mmap_mode="c")(
+            delayed(persistence_images)(
+                _subdiagrams(X[s], [dim], remove_dim=True),
+                self._samplings[dim],
+                self._step_size[dim],
+                self.sigma,
+                self.weights_[dim]
+                )
+            for dim in self.homology_dimensions_
+            for s in gen_even_slices(len(X), effective_n_jobs(self.n_jobs))
+            )
+        Xt = np.concatenate(Xt).\
+            reshape(self._n_dimensions, len(X), self.n_bins, self.n_bins).\
+            transpose((1, 0, 2, 3))
+        return Xt
+
+    def plot(self, Xt, sample=0, homology_dimension_idx=0, colorscale="blues",
+             plotly_params=None):
+        """Plot a single channel - corresponding to a given homology
+        dimension - in a sample from a collection of persistence images.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins, \
+            n_bins)
+            Collection of multi-channel raster images, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be selected.
+
+        homology_dimension_idx : int, optional, default: ``0``
+            Index of the channel in the selected sample to be plotted. If `Xt`
+            is the result of a call to :meth:`transform` and this index is i,
+            the plot corresponds to the homology dimension given by the i-th
+            entry in :attr:`homology_dimensions_`.
+
+        colorscale : str, optional, default: ``"blues"``
+            Color scale to be used in the heat map. Can be anything allowed by
+            :class:`plotly.graph_objects.Heatmap`.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        check_is_fitted(self)
+        homology_dimension = self.homology_dimensions_[homology_dimension_idx]
+        if homology_dimension != np.inf:
+            homology_dimension = int(homology_dimension)
+        samplings_x, samplings_y = self.samplings_[homology_dimension]
+
+        return plot_heatmap(
+            Xt[sample][homology_dimension_idx],
+            x=samplings_x,
+            y=samplings_y[::-1],
+            colorscale=colorscale,
+            origin="lower",
+            title=f"Persistence image representation of diagram {sample} in "
+                  f"homology dimension {homology_dimension}",
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class Silhouette(BaseEstimator, TransformerMixin, PlotterMixin):
+    """:ref:`Power-weighted silhouettes <weighted_silhouette>` of persistence
+    diagrams.
+
+    Based on ideas in [1]_. Given a persistence diagram consisting of
+    birth-death-dimension triples [b, d, q], subdiagrams corresponding to
+    distinct homology dimensions are considered separately, and their
+    respective silhouettes are obtained by sampling the silhouette function
+    over evenly spaced locations from appropriate ranges of the
+    :ref:`filtration parameter <filtered_complex>`.
+
+    **Important note**:
+
+        - Input collections of persistence diagrams for this transformer must
+          satisfy certain requirements, see e.g. :meth:`fit`.
+
+    Parameters
+    ----------
+    power: float, optional, default: ``1.``
+        The power to which persistence values are raised to define the
+        :ref:`power-weighted silhouettes <weighted_silhouette>`.
+
+    n_bins : int, optional, default: ``100``
+        The number of filtration parameter values, per available homology
+        dimension, to sample during :meth:`fit`.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    homology_dimensions_ : tuple
+        Homology dimensions seen in :meth:`fit`, sorted in ascending order.
+
+    samplings_ : dict
+        For each number in `homology_dimensions_`, a discrete sampling of
+        filtration parameters, calculated during :meth:`fit` according to the
+        minimum birth and maximum death values observed across all samples.
+
+    See also
+    --------
+    PersistenceLandscape, PersistenceEntropy, HeatKernel, Amplitude, \
+    PairwiseDistance, BettiCurve, gtda.homology.VietorisRipsPersistence
+
+    Notes
+    -----
+    The samplings in :attr:`samplings_` are in general different between
+    different homology dimensions. This means that the j-th entry of a
+    silhouette in homology dimension q typically arises from a different
+    parameter values to the j-th entry of a curve in dimension q'.
+
+    References
+    ----------
+    .. [1] F. Chazal, B. T. Fasy, F. Lecci, A. Rinaldo, and L. Wasserman,
+           "Stochastic Convergence of Persistence Landscapes and Silhouettes";
+           *In Proceedings of the thirtieth annual symposium on Computational
+           Geometry*, Kyoto, Japan, 2014, pp. 474483;
+           `DOI: 10.1145/2582112.2582128
+           <http://dx.doi.org/10.1145/2582112.2582128>`_.
+
+    """
+
+    _hyperparameters = {
+        "power": {"type": Real, "in": Interval(0, np.inf, closed="right")},
+        "n_bins": {"type": int, "in": Interval(1, np.inf, closed="left")}
+        }
+
+    def __init__(self, power=1., n_bins=100, n_jobs=None):
+        self.power = power
+        self.n_bins = n_bins
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Store all observed homology dimensions in
+        :attr:`homology_dimensions_` and, for each dimension separately,
+        store evenly sample filtration parameter values in :attr:`samplings_`.
+        Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_diagrams(X)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
+
+        # Find the unique homology dimensions in the 3D array X passed to `fit`
+        # assuming that they can all be found in its zero-th entry
+        homology_dimensions_fit = np.unique(X[0, :, 2])
+        self.homology_dimensions_ = \
+            _homology_dimensions_to_sorted_ints(homology_dimensions_fit)
+        self._n_dimensions = len(self.homology_dimensions_)
+
+        self._samplings, _ = _bin(
+            X, "silhouette", n_bins=self.n_bins,
+            homology_dimensions=self.homology_dimensions_
+            )
+        self.samplings_ = {dim: s.flatten()
+                           for dim, s in self._samplings.items()}
+
+        return self
+
+    def transform(self, X, y=None):
+        """Compute silhouettes of diagrams in `X`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features, 3)
+            Input data. Array of persistence diagrams, each a collection of
+            triples [b, d, q] representing persistent topological features
+            through their birth (b), death (d) and homology dimension (q).
+            It is important that, for each possible homology dimension, the
+            number of triples for which q equals that homology dimension is
+            constants across the entries of X.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins)
+            One silhouette (represented as a one-dimensional array)
+            per sample and per homology dimension seen
+            in :meth:`fit`. Index i along axis 1 corresponds to the i-th
+            homology dimension in :attr:`homology_dimensions_`.
+
+        """
+        check_is_fitted(self)
+        X = check_diagrams(X)
+
+        Xt = (Parallel(n_jobs=self.n_jobs)
+              (delayed(silhouettes)(_subdiagrams(X[s], [dim], remove_dim=True),
+                                    self._samplings[dim], power=self.power)
+              for dim in self.homology_dimensions_
+              for s in gen_even_slices(len(X), effective_n_jobs(self.n_jobs))))
+
+        Xt = np.concatenate(Xt).\
+            reshape(self._n_dimensions, len(X), -1).\
+            transpose((1, 0, 2))
+        return Xt
+
+    def plot(self, Xt, sample=0, homology_dimensions=None, plotly_params=None):
+        """Plot a sample from a collection of silhouettes arranged as in the
+        output of :meth:`transform`. Include homology in multiple dimensions.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_homology_dimensions, n_bins)
+            Collection of silhouettes, such as returned by :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        homology_dimensions : list, tuple or None, optional, default: ``None``
+            Which homology dimensions to include in the plot. ``None`` means
+            plotting all dimensions present in :attr:`homology_dimensions_`.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        check_is_fitted(self)
+
+        homology_dimensions_mapping = _make_homology_dimensions_mapping(
+            homology_dimensions, self.homology_dimensions_
+            )
+
+        layout_axes_common = {
+            "type": "linear",
+            "ticks": "outside",
+            "showline": True,
+            "zeroline": True,
+            "linewidth": 1,
+            "linecolor": "black",
+            "mirror": False,
+            "showexponent": "all",
+            "exponentformat": "e"
+            }
+        layout = {
+            "xaxis1": {
+                "title": "Filtration parameter",
+                "side": "bottom",
+                "anchor": "y1",
+                **layout_axes_common
+                },
+            "yaxis1": {
+                "side": "left",
+                "anchor": "x1",
+                **layout_axes_common
+                },
+            "plot_bgcolor": "white",
+            "title": f"Silhouette representation of diagram {sample}"
+            }
+
+        fig = Figure(layout=layout)
+
+        for ix, dim in homology_dimensions_mapping:
+            fig.add_trace(Scatter(x=self.samplings_[dim],
+                                  y=Xt[sample][ix],
+                                  mode="lines",
+                                  showlegend=True,
+                                  hoverinfo="none",
+                                  name=f"H{dim}"))
+
+        # Update traces and layout according to user input
+        if plotly_params:
+            fig.update_traces(plotly_params.get("traces", None))
+            fig.update_layout(plotly_params.get("layout", None))
+
+        return fig
```

## gtda/diagrams/_features.py

 * *Ordering differences only*

```diff
@@ -1,42 +1,42 @@
-# License: GNU AGPLv3
-
-import numpy as np
-
-
-_AVAILABLE_POLYNOMIALS = {'R': {},
-                          'S': {},
-                          'T': {}}
-
-
-def R_polynomial(Xd):
-    roots = Xd[:, 0] + 1j * Xd[:, 1]
-
-    return roots
-
-
-def S_polynomial(Xd):
-    alpha = np.linalg.norm(Xd, axis=1)
-    alpha = np.where(alpha == 0, np.ones(Xd.shape[0]), alpha)
-    roots = np.multiply(
-        np.multiply(
-            (Xd[:, 0] + 1j * Xd[:, 1]), (Xd[:, 1] - Xd[:, 0])
-            ),
-        1. / (np.sqrt(2) * alpha)
-        )
-
-    return roots
-
-
-def T_polynomial(Xd):
-    alpha = np.linalg.norm(Xd, axis=1)
-    roots = np.multiply(
-        (Xd[:, 1] - Xd[:, 0]) / 2, np.cos(alpha) - np.sin(alpha)
-        + 1j * (np.cos(alpha) + np.sin(alpha))
-        )
-
-    return roots
-
-
-_implemented_polynomial_recipes = {'R': R_polynomial,
-                                   'S': S_polynomial,
-                                   'T': T_polynomial}
+# License: GNU AGPLv3
+
+import numpy as np
+
+
+_AVAILABLE_POLYNOMIALS = {'R': {},
+                          'S': {},
+                          'T': {}}
+
+
+def R_polynomial(Xd):
+    roots = Xd[:, 0] + 1j * Xd[:, 1]
+
+    return roots
+
+
+def S_polynomial(Xd):
+    alpha = np.linalg.norm(Xd, axis=1)
+    alpha = np.where(alpha == 0, np.ones(Xd.shape[0]), alpha)
+    roots = np.multiply(
+        np.multiply(
+            (Xd[:, 0] + 1j * Xd[:, 1]), (Xd[:, 1] - Xd[:, 0])
+            ),
+        1. / (np.sqrt(2) * alpha)
+        )
+
+    return roots
+
+
+def T_polynomial(Xd):
+    alpha = np.linalg.norm(Xd, axis=1)
+    roots = np.multiply(
+        (Xd[:, 1] - Xd[:, 0]) / 2, np.cos(alpha) - np.sin(alpha)
+        + 1j * (np.cos(alpha) + np.sin(alpha))
+        )
+
+    return roots
+
+
+_implemented_polynomial_recipes = {'R': R_polynomial,
+                                   'S': S_polynomial,
+                                   'T': T_polynomial}
```

## gtda/diagrams/_metrics.py

```diff
@@ -1,444 +1,444 @@
-# License: GNU AGPLv3
-
-from numbers import Real
-from types import FunctionType
-
-import numpy as np
-from joblib import Parallel, delayed, effective_n_jobs
-from scipy.ndimage import gaussian_filter
-from scipy.spatial.distance import cdist, pdist, squareform
-from sklearn.utils import gen_even_slices
-from sklearn.utils.validation import _num_samples
-
-from ._utils import _subdiagrams, _sample_image
-from ..externals.modules.gtda_bottleneck import bottleneck_distance
-from ..externals.modules.gtda_wasserstein import wasserstein_distance
-from ..utils.intervals import Interval
-
-_AVAILABLE_METRICS = {
-    'bottleneck': {
-        'delta': {'type': Real, 'in': Interval(0, 1, closed='both')}
-        },
-    'wasserstein': {
-        'p': {'type': Real, 'in': Interval(1, np.inf, closed='left')},
-        'delta': {'type': Real, 'in': Interval(0, 1, closed='right')}
-        },
-    'betti': {
-        'p': {'type': Real, 'in': Interval(1, np.inf, closed='both')},
-        'n_bins': {'type': int, 'in': Interval(1, np.inf, closed='left')}
-        },
-    'landscape': {
-        'p': {'type': Real, 'in': Interval(1, np.inf, closed='both')},
-        'n_bins': {'type': int, 'in': Interval(1, np.inf, closed='left')},
-        'n_layers': {'type': int, 'in': Interval(1, np.inf, closed='left')}
-        },
-    'heat': {
-        'p': {'type': Real, 'in': Interval(1, np.inf, closed='both')},
-        'n_bins': {'type': int, 'in': Interval(1, np.inf, closed='left')},
-        'sigma': {'type': Real, 'in': Interval(0, np.inf, closed='neither')}
-        },
-    'persistence_image': {
-        'p': {'type': Real, 'in': Interval(1, np.inf, closed='both')},
-        'n_bins': {'type': int, 'in': Interval(1, np.inf, closed='left')},
-        'sigma': {'type': Real, 'in': Interval(0, np.inf, closed='neither')},
-        'weight_function': {'type': (FunctionType, type(None))}
-        },
-    'silhouette': {
-        'power': {'type': Real, 'in': Interval(0, np.inf, closed='right')},
-        'p': {'type': Real, 'in': Interval(1, np.inf, closed='both')},
-        'n_bins': {'type': int, 'in': Interval(1, np.inf, closed='left')}
-        }
-    }
-
-_AVAILABLE_AMPLITUDE_METRICS = {}
-for _metric, _metric_params in _AVAILABLE_METRICS.items():
-    if _metric not in ['bottleneck', 'wasserstein']:
-        _AVAILABLE_AMPLITUDE_METRICS[_metric] = _metric_params.copy()
-    else:
-        _AVAILABLE_AMPLITUDE_METRICS[_metric] = \
-            {name: descr for name, descr in _metric_params.items()
-             if name != 'delta'}
-
-
-def betti_curves(diagrams, sampling):
-    born = sampling >= diagrams[:, :, 0]
-    not_dead = sampling < diagrams[:, :, 1]
-    alive = np.logical_and(born, not_dead)
-    betti = np.sum(alive, axis=2).T
-    return betti
-
-
-def landscapes(diagrams, sampling, n_layers):
-    n_points = diagrams.shape[1]
-    midpoints = (diagrams[:, :, 1] + diagrams[:, :, 0]) / 2.
-    heights = (diagrams[:, :, 1] - diagrams[:, :, 0]) / 2.
-    fibers = np.maximum(-np.abs(sampling - midpoints) + heights, 0)
-    top_pos = range(-min(n_layers, n_points), 0)
-    fibers.partition(top_pos, axis=2)
-    fibers = np.flip(fibers[:, :, -n_layers:], axis=2)
-    fibers = np.transpose(fibers, (1, 2, 0))
-    pad_with = ((0, 0), (0, max(0, n_layers - n_points)), (0, 0))
-    fibers = np.pad(fibers, pad_with, "constant", constant_values=0)
-    return fibers
-
-
-def heats(diagrams, sampling, step_size, sigma):
-    # WARNING: modifies `diagrams` in place
-    heats_ = \
-        np.zeros((len(diagrams), len(sampling), len(sampling)), dtype=float)
-    # If the step size is zero, we return a trivial image
-    if step_size == 0:
-        return heats_
-
-    # Set the values outside of the sampling range
-    first_sampling, last_sampling = sampling[0, 0, 0], sampling[-1, 0, 0]
-    diagrams[diagrams < first_sampling] = first_sampling
-    diagrams[diagrams > last_sampling] = last_sampling
-
-    # Calculate the value of `sigma` in pixel units, threshold for numerical
-    # reasons if it's too large.
-    sigma_pixel = sigma / step_size
-    sigma_pixel = min(sigma_pixel, 10**5 * len(sampling))
-
-    for i, diagram in enumerate(diagrams):
-        nontrivial_points_idx = np.flatnonzero(diagram[:, 1] != diagram[:, 0])
-        diagram_nontrivial_pixel_coords = np.array(
-            (diagram - first_sampling) / step_size, dtype=int
-            )[nontrivial_points_idx]
-        image = heats_[i]
-        _sample_image(image, diagram_nontrivial_pixel_coords)
-        gaussian_filter(image, sigma_pixel, mode="constant", output=image)
-
-    heats_ -= np.transpose(heats_, (0, 2, 1))
-    heats_ /= (step_size ** 2)
-    heats_ = np.rot90(heats_, k=1, axes=(1, 2))
-    return heats_
-
-
-def persistence_images(diagrams, sampling, step_size, sigma, weights):
-    # For persistence images, `sampling` is a tall matrix with two columns
-    # (the first for birth and the second for persistence), and `step_size` is
-    # a 2d array
-    # WARNING: modifies `diagrams` in place
-    persistence_images_ = \
-        np.zeros((len(diagrams), len(sampling), len(sampling)), dtype=float)
-    # If either step size is zero, we return a trivial image
-    if (step_size == 0).any():
-        return persistence_images_
-
-    # Transform diagrams from (birth, death, dim) to (birth, persistence, dim)
-    diagrams[:, :, 1] -= diagrams[:, :, 0]
-
-    sigma_pixel = []
-    first_samplings = sampling[0]
-    last_samplings = sampling[-1]
-    for ax in [0, 1]:
-        diagrams_ax = diagrams[:, :, ax]
-        # Set the values outside of the sampling range
-        diagrams_ax[diagrams_ax < first_samplings[ax]] = first_samplings[ax]
-        diagrams_ax[diagrams_ax > last_samplings[ax]] = last_samplings[ax]
-        # Calculate the value of the component of `sigma` in pixel units
-        sigma_pixel.append(sigma / step_size[ax])
-
-    # Sample the image, apply the weights, smoothen
-    for i, diagram in enumerate(diagrams):
-        nontrivial_points_idx = np.flatnonzero(diagram[:, 1])
-        diagram_nontrivial_pixel_coords = np.array(
-            (diagram - first_samplings) / step_size, dtype=int
-            )[nontrivial_points_idx]
-        image = persistence_images_[i]
-        _sample_image(image, diagram_nontrivial_pixel_coords)
-        image *= weights
-        gaussian_filter(image, sigma_pixel, mode="constant", output=image)
-
-    persistence_images_ = np.rot90(persistence_images_, k=1, axes=(1, 2))
-    persistence_images_ /= np.product(step_size)
-    return persistence_images_
-
-
-def silhouettes(diagrams, sampling, power, **kwargs):
-    """Input: a batch of persistence diagrams with a sampling (3d array
-    returned by _bin) of a one-dimensional range.
-    """
-    sampling = np.transpose(sampling, axes=(1, 2, 0))
-    weights = np.diff(diagrams, axis=2)
-    if power > 8.:
-        weights = weights / np.max(weights, axis=1, keepdims=True)
-    weights = weights ** power
-    total_weights = np.sum(weights, axis=1)
-    # Next line is a trick to avoid NaNs when computing `fibers_weighted_sum`
-    total_weights[total_weights == 0.] = np.inf
-    midpoints = (diagrams[:, :, [1]] + diagrams[:, :, [0]]) / 2.
-    heights = (diagrams[:, :, [1]] - diagrams[:, :, [0]]) / 2.
-    fibers = np.maximum(-np.abs(sampling - midpoints) + heights, 0)
-    fibers_weighted_sum = np.sum(weights * fibers, axis=1) / total_weights
-    return fibers_weighted_sum
-
-
-def bottleneck_distances(diagrams_1, diagrams_2, delta=0.01, **kwargs):
-    return np.array([[bottleneck_distance(
-        diagram_1[diagram_1[:, 0] != diagram_1[:, 1]],
-        diagram_2[diagram_2[:, 0] != diagram_2[:, 1]],
-        delta) for diagram_2 in diagrams_2] for diagram_1 in diagrams_1])
-
-
-def wasserstein_distances(diagrams_1, diagrams_2, p=2, delta=0.01, **kwargs):
-    return np.array([[wasserstein_distance(
-        diagram_1[diagram_1[:, 0] != diagram_1[:, 1]],
-        diagram_2[diagram_2[:, 0] != diagram_2[:, 1]],
-        p, delta,) for diagram_2 in diagrams_2] for diagram_1 in diagrams_1])
-
-
-def betti_distances(
-        diagrams_1, diagrams_2, sampling, step_size, p=2., **kwargs
-        ):
-    step_size_factor = step_size ** (1 / p)
-    are_arrays_equal = np.array_equal(diagrams_1, diagrams_2)
-    betti_curves_1 = betti_curves(diagrams_1, sampling)
-    if are_arrays_equal:
-        distances = pdist(betti_curves_1, "minkowski", p=p)
-        distances *= step_size_factor
-        return squareform(distances)
-    betti_curves_2 = betti_curves(diagrams_2, sampling)
-    distances = cdist(betti_curves_1, betti_curves_2, "minkowski", p=p)
-    distances *= step_size_factor
-    return distances
-
-
-def landscape_distances(
-        diagrams_1, diagrams_2, sampling, step_size, p=2., n_layers=1,
-        **kwargs
-        ):
-    step_size_factor = step_size ** (1 / p)
-    n_samples_1, n_points_1 = diagrams_1.shape[:2]
-    n_layers_1 = min(n_layers, n_points_1)
-    if np.array_equal(diagrams_1, diagrams_2):
-        ls_1 = landscapes(diagrams_1, sampling, n_layers_1).\
-            reshape(n_samples_1, -1)
-        distances = pdist(ls_1, "minkowski", p=p)
-        distances *= step_size_factor
-        return squareform(distances)
-    n_samples_2, n_points_2 = diagrams_2.shape[:2]
-    n_layers_2 = min(n_layers, n_points_2)
-    n_layers = max(n_layers_1, n_layers_2)
-    ls_1 = landscapes(diagrams_1, sampling, n_layers).\
-        reshape(n_samples_1, -1)
-    ls_2 = landscapes(diagrams_2, sampling, n_layers).\
-        reshape(n_samples_2, -1)
-    distances = cdist(ls_1, ls_2, "minkowski", p=p)
-    distances *= step_size_factor
-    return distances
-
-
-def heat_distances(
-        diagrams_1, diagrams_2, sampling, step_size, sigma=0.1, p=2., **kwargs
-        ):
-    # WARNING: `heats` modifies `diagrams` in place
-    step_size_factor = step_size ** (2 / p)
-    are_arrays_equal = np.array_equal(diagrams_1, diagrams_2)
-    heats_1 = heats(diagrams_1, sampling, step_size, sigma).\
-        reshape(len(diagrams_1), -1)
-    if are_arrays_equal:
-        distances = pdist(heats_1, "minkowski", p=p)
-        distances *= step_size_factor
-        return squareform(distances)
-    heats_2 = heats(diagrams_2, sampling, step_size, sigma).\
-        reshape(len(diagrams_2), -1)
-    distances = cdist(heats_1, heats_2, "minkowski", p=p)
-    distances *= step_size_factor
-    return distances
-
-
-def persistence_image_distances(
-        diagrams_1, diagrams_2, sampling, step_size, sigma=0.1,
-        weight_function=np.ones_like, p=2., **kwargs
-        ):
-    # For persistence images, `sampling` is a tall matrix with two columns
-    # (the first for birth and the second for persistence), and `step_size` is
-    # a 2d array
-    weights = weight_function(sampling[:, 1])
-    step_sizes_factor = np.product(step_size) ** (1 / p)
-    # WARNING: `persistence_images` modifies `diagrams` in place
-    are_arrays_equal = np.array_equal(diagrams_1, diagrams_2)
-    persistence_images_1 = \
-        persistence_images(diagrams_1, sampling, step_size, sigma, weights).\
-        reshape(len(diagrams_1), -1)
-    if are_arrays_equal:
-        distances = pdist(persistence_images_1, "minkowski", p=p)
-        distances *= step_sizes_factor
-        return squareform(distances)
-    persistence_images_2 = persistence_images(
-        diagrams_2, sampling, step_size, sigma, weights
-        ).reshape(len(diagrams_2), -1)
-    distances = cdist(
-        persistence_images_1, persistence_images_2, "minkowski", p=p
-        )
-    distances *= step_sizes_factor
-    return distances
-
-
-def silhouette_distances(
-        diagrams_1, diagrams_2, sampling, step_size, power=1., p=2., **kwargs
-        ):
-    step_size_factor = step_size ** (1 / p)
-    are_arrays_equal = np.array_equal(diagrams_1, diagrams_2)
-    silhouettes_1 = silhouettes(diagrams_1, sampling, power)
-    if are_arrays_equal:
-        distances = pdist(silhouettes_1, 'minkowski', p=p)
-        distances *= step_size_factor
-        return squareform(distances)
-    silhouettes_2 = silhouettes(diagrams_2, sampling, power)
-    distances = cdist(silhouettes_1, silhouettes_2, 'minkowski', p=p)
-    distances *= step_size_factor
-    return distances
-
-
-implemented_metric_recipes = {
-    "bottleneck": bottleneck_distances,
-    "wasserstein": wasserstein_distances,
-    "landscape": landscape_distances,
-    "betti": betti_distances,
-    "heat": heat_distances,
-    "persistence_image": persistence_image_distances,
-    'silhouette': silhouette_distances
-    }
-
-
-def _parallel_pairwise(
-        X1, X2, metric, metric_params, homology_dimensions, n_jobs
-        ):
-    metric_func = implemented_metric_recipes[metric]
-    effective_metric_params = metric_params.copy()
-    none_dict = {dim: None for dim in homology_dimensions}
-    samplings = effective_metric_params.pop("samplings", none_dict)
-    step_sizes = effective_metric_params.pop("step_sizes", none_dict)
-    if metric in ["heat", "persistence_image"]:
-        parallel_kwargs = {"mmap_mode": "c"}
-    else:
-        parallel_kwargs = {}
-
-    n_columns = len(X2)
-    distance_matrices = Parallel(n_jobs=n_jobs, **parallel_kwargs)(
-        delayed(metric_func)(
-            _subdiagrams(X1, [dim], remove_dim=True),
-            _subdiagrams(X2[s], [dim], remove_dim=True),
-            sampling=samplings[dim],
-            step_size=step_sizes[dim],
-            **effective_metric_params
-            )
-        for dim in homology_dimensions
-        for s in gen_even_slices(n_columns, effective_n_jobs(n_jobs))
-        )
-
-    distance_matrices = np.concatenate(distance_matrices, axis=1)
-    distance_matrices = np.stack(
-        [distance_matrices[:, i * n_columns:(i + 1) * n_columns]
-         for i in range(len(homology_dimensions))],
-        axis=2)
-    return distance_matrices
-
-
-def bottleneck_amplitudes(diagrams, **kwargs):
-    half_lifetimes = (diagrams[:, :, 1] - diagrams[:, :, 0]) / 2.
-    return np.linalg.norm(half_lifetimes, axis=1, ord=np.inf)
-
-
-def wasserstein_amplitudes(diagrams, p=2., **kwargs):
-    half_lifetimes = (diagrams[:, :, 1] - diagrams[:, :, 0]) / 2.
-    return np.linalg.norm(half_lifetimes, axis=1, ord=p)
-
-
-def betti_amplitudes(diagrams, sampling, step_size, p=2., **kwargs):
-    step_size_factor = step_size ** (1 / p)
-    bcs = betti_curves(diagrams, sampling)
-    amplitudes = np.linalg.norm(bcs, axis=1, ord=p)
-    amplitudes *= step_size_factor
-    return amplitudes
-
-
-def landscape_amplitudes(
-        diagrams, sampling, step_size, p=2., n_layers=1, **kwargs
-        ):
-    step_size_factor = step_size ** (1 / p)
-    ls = landscapes(diagrams, sampling, n_layers).\
-        reshape(len(diagrams), -1)
-    amplitudes = np.linalg.norm(ls, axis=1, ord=p)
-    amplitudes *= step_size_factor
-    return amplitudes
-
-
-def heat_amplitudes(diagrams, sampling, step_size, sigma=0.1, p=2., **kwargs):
-    # WARNING: `heats` modifies `diagrams` in place
-    step_size_factor = step_size ** (2 / p)
-    heats_ = heats(diagrams, sampling, step_size, sigma).\
-        reshape(len(diagrams), -1)
-    amplitudes = np.linalg.norm(heats_, axis=1, ord=p)
-    amplitudes *= step_size_factor
-    return amplitudes
-
-
-def persistence_image_amplitudes(
-        diagrams, sampling, step_size, sigma=0.1, weight_function=np.ones_like,
-        p=2., **kwargs
-        ):
-    # For persistence images, `sampling` is a tall matrix with two columns
-    # (the first for birth and the second for persistence), and `step_size` is
-    # a 2d array
-    weights = weight_function(sampling[:, 1])
-    step_sizes_factor = np.product(step_size) ** (1 / p)
-    # WARNING: `persistence_images` modifies `diagrams` in place
-    persistence_images_ = persistence_images(
-        diagrams, sampling, step_size, sigma, weights
-        ).reshape(len(diagrams), -1)
-    amplitudes = np.linalg.norm(persistence_images_, axis=1, ord=p)
-    amplitudes *= step_sizes_factor
-    return amplitudes
-
-
-def silhouette_amplitudes(
-        diagrams, sampling, step_size, power=1., p=2., **kwargs
-        ):
-    step_size_factor = step_size ** (1 / p)
-    silhouettes_ = silhouettes(diagrams, sampling, power)
-    amplitudes = np.linalg.norm(silhouettes_, axis=1, ord=p)
-    amplitudes *= step_size_factor
-    return amplitudes
-
-
-implemented_amplitude_recipes = {
-    "bottleneck": bottleneck_amplitudes,
-    "wasserstein": wasserstein_amplitudes,
-    "landscape": landscape_amplitudes,
-    "betti": betti_amplitudes,
-    "heat": heat_amplitudes,
-    "persistence_image": persistence_image_amplitudes,
-    'silhouette': silhouette_amplitudes
-    }
-
-
-def _parallel_amplitude(X, metric, metric_params, homology_dimensions, n_jobs):
-    amplitude_func = implemented_amplitude_recipes[metric]
-    effective_metric_params = metric_params.copy()
-    none_dict = {dim: None for dim in homology_dimensions}
-    samplings = effective_metric_params.pop("samplings", none_dict)
-    step_sizes = effective_metric_params.pop("step_sizes", none_dict)
-    if metric in ["heat", "persistence_image"]:
-        parallel_kwargs = {"mmap_mode": "c"}
-    else:
-        parallel_kwargs = {}
-
-    amplitude_arrays = Parallel(n_jobs=n_jobs, **parallel_kwargs)(
-        delayed(amplitude_func)(
-            _subdiagrams(X[s], [dim], remove_dim=True),
-            sampling=samplings[dim],
-            step_size=step_sizes[dim],
-            **effective_metric_params
-            )
-        for dim in homology_dimensions
-        for s in gen_even_slices(_num_samples(X), effective_n_jobs(n_jobs))
-        )
-
-    amplitude_arrays = np.concatenate(amplitude_arrays).\
-        reshape(len(homology_dimensions), len(X)).T
-
-    return amplitude_arrays
+# License: GNU AGPLv3
+
+from numbers import Real
+from typing import Callable
+
+import numpy as np
+from joblib import Parallel, delayed, effective_n_jobs
+from scipy.ndimage import gaussian_filter
+from scipy.spatial.distance import cdist, pdist, squareform
+from sklearn.utils import gen_even_slices
+from sklearn.utils.validation import _num_samples
+
+from ._utils import _subdiagrams, _sample_image
+from ..externals.modules.gtda_bottleneck import bottleneck_distance
+from ..externals.modules.gtda_wasserstein import wasserstein_distance
+from ..utils.intervals import Interval
+
+_AVAILABLE_METRICS = {
+    'bottleneck': {
+        'delta': {'type': Real, 'in': Interval(0, 1, closed='both')}
+        },
+    'wasserstein': {
+        'p': {'type': Real, 'in': Interval(1, np.inf, closed='left')},
+        'delta': {'type': Real, 'in': Interval(0, 1, closed='right')}
+        },
+    'betti': {
+        'p': {'type': Real, 'in': Interval(1, np.inf, closed='both')},
+        'n_bins': {'type': int, 'in': Interval(1, np.inf, closed='left')}
+        },
+    'landscape': {
+        'p': {'type': Real, 'in': Interval(1, np.inf, closed='both')},
+        'n_bins': {'type': int, 'in': Interval(1, np.inf, closed='left')},
+        'n_layers': {'type': int, 'in': Interval(1, np.inf, closed='left')}
+        },
+    'heat': {
+        'p': {'type': Real, 'in': Interval(1, np.inf, closed='both')},
+        'n_bins': {'type': int, 'in': Interval(1, np.inf, closed='left')},
+        'sigma': {'type': Real, 'in': Interval(0, np.inf, closed='neither')}
+        },
+    'persistence_image': {
+        'p': {'type': Real, 'in': Interval(1, np.inf, closed='both')},
+        'n_bins': {'type': int, 'in': Interval(1, np.inf, closed='left')},
+        'sigma': {'type': Real, 'in': Interval(0, np.inf, closed='neither')},
+        'weight_function': {'type': (Callable, type(None))}
+        },
+    'silhouette': {
+        'power': {'type': Real, 'in': Interval(0, np.inf, closed='right')},
+        'p': {'type': Real, 'in': Interval(1, np.inf, closed='both')},
+        'n_bins': {'type': int, 'in': Interval(1, np.inf, closed='left')}
+        }
+    }
+
+_AVAILABLE_AMPLITUDE_METRICS = {}
+for _metric, _metric_params in _AVAILABLE_METRICS.items():
+    if _metric not in ['bottleneck', 'wasserstein']:
+        _AVAILABLE_AMPLITUDE_METRICS[_metric] = _metric_params.copy()
+    else:
+        _AVAILABLE_AMPLITUDE_METRICS[_metric] = \
+            {name: descr for name, descr in _metric_params.items()
+             if name != 'delta'}
+
+
+def betti_curves(diagrams, sampling):
+    born = sampling >= diagrams[:, :, 0]
+    not_dead = sampling < diagrams[:, :, 1]
+    alive = np.logical_and(born, not_dead)
+    betti = np.sum(alive, axis=2).T
+    return betti
+
+
+def landscapes(diagrams, sampling, n_layers):
+    n_points = diagrams.shape[1]
+    midpoints = (diagrams[:, :, 1] + diagrams[:, :, 0]) / 2.
+    heights = (diagrams[:, :, 1] - diagrams[:, :, 0]) / 2.
+    fibers = np.maximum(-np.abs(sampling - midpoints) + heights, 0)
+    top_pos = range(-min(n_layers, n_points), 0)
+    fibers.partition(top_pos, axis=2)
+    fibers = np.flip(fibers[:, :, -n_layers:], axis=2)
+    fibers = np.transpose(fibers, (1, 2, 0))
+    pad_with = ((0, 0), (0, max(0, n_layers - n_points)), (0, 0))
+    fibers = np.pad(fibers, pad_with, "constant", constant_values=0)
+    return fibers
+
+
+def heats(diagrams, sampling, step_size, sigma):
+    # WARNING: modifies `diagrams` in place
+    heats_ = \
+        np.zeros((len(diagrams), len(sampling), len(sampling)), dtype=float)
+    # If the step size is zero, we return a trivial image
+    if step_size == 0:
+        return heats_
+
+    # Set the values outside of the sampling range
+    first_sampling, last_sampling = sampling[0, 0, 0], sampling[-1, 0, 0]
+    diagrams[diagrams < first_sampling] = first_sampling
+    diagrams[diagrams > last_sampling] = last_sampling
+
+    # Calculate the value of `sigma` in pixel units, threshold for numerical
+    # reasons if it's too large.
+    sigma_pixel = sigma / step_size
+    sigma_pixel = min(sigma_pixel, 10**5 * len(sampling))
+
+    for i, diagram in enumerate(diagrams):
+        nontrivial_points_idx = np.flatnonzero(diagram[:, 1] != diagram[:, 0])
+        diagram_nontrivial_pixel_coords = np.array(
+            (diagram - first_sampling) / step_size, dtype=int
+            )[nontrivial_points_idx]
+        image = heats_[i]
+        _sample_image(image, diagram_nontrivial_pixel_coords)
+        gaussian_filter(image, sigma_pixel, mode="constant", output=image)
+
+    heats_ -= np.transpose(heats_, (0, 2, 1))
+    heats_ /= (step_size ** 2)
+    heats_ = np.rot90(heats_, k=1, axes=(1, 2))
+    return heats_
+
+
+def persistence_images(diagrams, sampling, step_size, sigma, weights):
+    # For persistence images, `sampling` is a tall matrix with two columns
+    # (the first for birth and the second for persistence), and `step_size` is
+    # a 2d array
+    # WARNING: modifies `diagrams` in place
+    persistence_images_ = \
+        np.zeros((len(diagrams), len(sampling), len(sampling)), dtype=float)
+    # If either step size is zero, we return a trivial image
+    if (step_size == 0).any():
+        return persistence_images_
+
+    # Transform diagrams from (birth, death, dim) to (birth, persistence, dim)
+    diagrams[:, :, 1] -= diagrams[:, :, 0]
+
+    sigma_pixel = []
+    first_samplings = sampling[0]
+    last_samplings = sampling[-1]
+    for ax in [0, 1]:
+        diagrams_ax = diagrams[:, :, ax]
+        # Set the values outside of the sampling range
+        diagrams_ax[diagrams_ax < first_samplings[ax]] = first_samplings[ax]
+        diagrams_ax[diagrams_ax > last_samplings[ax]] = last_samplings[ax]
+        # Calculate the value of the component of `sigma` in pixel units
+        sigma_pixel.append(sigma / step_size[ax])
+
+    # Sample the image, apply the weights, smoothen
+    for i, diagram in enumerate(diagrams):
+        nontrivial_points_idx = np.flatnonzero(diagram[:, 1])
+        diagram_nontrivial_pixel_coords = np.array(
+            (diagram - first_samplings) / step_size, dtype=int
+            )[nontrivial_points_idx]
+        image = persistence_images_[i]
+        _sample_image(image, diagram_nontrivial_pixel_coords)
+        image *= weights
+        gaussian_filter(image, sigma_pixel, mode="constant", output=image)
+
+    persistence_images_ = np.rot90(persistence_images_, k=1, axes=(1, 2))
+    persistence_images_ /= np.product(step_size)
+    return persistence_images_
+
+
+def silhouettes(diagrams, sampling, power, **kwargs):
+    """Input: a batch of persistence diagrams with a sampling (3d array
+    returned by _bin) of a one-dimensional range.
+    """
+    sampling = np.transpose(sampling, axes=(1, 2, 0))
+    weights = np.diff(diagrams, axis=2)
+    if power > 8.:
+        weights = weights / np.max(weights, axis=1, keepdims=True)
+    weights = weights ** power
+    total_weights = np.sum(weights, axis=1)
+    # Next line is a trick to avoid NaNs when computing `fibers_weighted_sum`
+    total_weights[total_weights == 0.] = np.inf
+    midpoints = (diagrams[:, :, [1]] + diagrams[:, :, [0]]) / 2.
+    heights = (diagrams[:, :, [1]] - diagrams[:, :, [0]]) / 2.
+    fibers = np.maximum(-np.abs(sampling - midpoints) + heights, 0)
+    fibers_weighted_sum = np.sum(weights * fibers, axis=1) / total_weights
+    return fibers_weighted_sum
+
+
+def bottleneck_distances(diagrams_1, diagrams_2, delta=0.01, **kwargs):
+    return np.array([[bottleneck_distance(
+        diagram_1[diagram_1[:, 0] != diagram_1[:, 1]],
+        diagram_2[diagram_2[:, 0] != diagram_2[:, 1]],
+        delta) for diagram_2 in diagrams_2] for diagram_1 in diagrams_1])
+
+
+def wasserstein_distances(diagrams_1, diagrams_2, p=2, delta=0.01, **kwargs):
+    return np.array([[wasserstein_distance(
+        diagram_1[diagram_1[:, 0] != diagram_1[:, 1]],
+        diagram_2[diagram_2[:, 0] != diagram_2[:, 1]],
+        p, delta,) for diagram_2 in diagrams_2] for diagram_1 in diagrams_1])
+
+
+def betti_distances(
+        diagrams_1, diagrams_2, sampling, step_size, p=2., **kwargs
+        ):
+    step_size_factor = step_size ** (1 / p)
+    are_arrays_equal = np.array_equal(diagrams_1, diagrams_2)
+    betti_curves_1 = betti_curves(diagrams_1, sampling)
+    if are_arrays_equal:
+        distances = pdist(betti_curves_1, "minkowski", p=p)
+        distances *= step_size_factor
+        return squareform(distances)
+    betti_curves_2 = betti_curves(diagrams_2, sampling)
+    distances = cdist(betti_curves_1, betti_curves_2, "minkowski", p=p)
+    distances *= step_size_factor
+    return distances
+
+
+def landscape_distances(
+        diagrams_1, diagrams_2, sampling, step_size, p=2., n_layers=1,
+        **kwargs
+        ):
+    step_size_factor = step_size ** (1 / p)
+    n_samples_1, n_points_1 = diagrams_1.shape[:2]
+    n_layers_1 = min(n_layers, n_points_1)
+    if np.array_equal(diagrams_1, diagrams_2):
+        ls_1 = landscapes(diagrams_1, sampling, n_layers_1).\
+            reshape(n_samples_1, -1)
+        distances = pdist(ls_1, "minkowski", p=p)
+        distances *= step_size_factor
+        return squareform(distances)
+    n_samples_2, n_points_2 = diagrams_2.shape[:2]
+    n_layers_2 = min(n_layers, n_points_2)
+    n_layers = max(n_layers_1, n_layers_2)
+    ls_1 = landscapes(diagrams_1, sampling, n_layers).\
+        reshape(n_samples_1, -1)
+    ls_2 = landscapes(diagrams_2, sampling, n_layers).\
+        reshape(n_samples_2, -1)
+    distances = cdist(ls_1, ls_2, "minkowski", p=p)
+    distances *= step_size_factor
+    return distances
+
+
+def heat_distances(
+        diagrams_1, diagrams_2, sampling, step_size, sigma=0.1, p=2., **kwargs
+        ):
+    # WARNING: `heats` modifies `diagrams` in place
+    step_size_factor = step_size ** (2 / p)
+    are_arrays_equal = np.array_equal(diagrams_1, diagrams_2)
+    heats_1 = heats(diagrams_1, sampling, step_size, sigma).\
+        reshape(len(diagrams_1), -1)
+    if are_arrays_equal:
+        distances = pdist(heats_1, "minkowski", p=p)
+        distances *= step_size_factor
+        return squareform(distances)
+    heats_2 = heats(diagrams_2, sampling, step_size, sigma).\
+        reshape(len(diagrams_2), -1)
+    distances = cdist(heats_1, heats_2, "minkowski", p=p)
+    distances *= step_size_factor
+    return distances
+
+
+def persistence_image_distances(
+        diagrams_1, diagrams_2, sampling, step_size, sigma=0.1,
+        weight_function=np.ones_like, p=2., **kwargs
+        ):
+    # For persistence images, `sampling` is a tall matrix with two columns
+    # (the first for birth and the second for persistence), and `step_size` is
+    # a 2d array
+    weights = weight_function(sampling[:, 1])
+    step_sizes_factor = np.product(step_size) ** (1 / p)
+    # WARNING: `persistence_images` modifies `diagrams` in place
+    are_arrays_equal = np.array_equal(diagrams_1, diagrams_2)
+    persistence_images_1 = \
+        persistence_images(diagrams_1, sampling, step_size, sigma, weights).\
+        reshape(len(diagrams_1), -1)
+    if are_arrays_equal:
+        distances = pdist(persistence_images_1, "minkowski", p=p)
+        distances *= step_sizes_factor
+        return squareform(distances)
+    persistence_images_2 = persistence_images(
+        diagrams_2, sampling, step_size, sigma, weights
+        ).reshape(len(diagrams_2), -1)
+    distances = cdist(
+        persistence_images_1, persistence_images_2, "minkowski", p=p
+        )
+    distances *= step_sizes_factor
+    return distances
+
+
+def silhouette_distances(
+        diagrams_1, diagrams_2, sampling, step_size, power=1., p=2., **kwargs
+        ):
+    step_size_factor = step_size ** (1 / p)
+    are_arrays_equal = np.array_equal(diagrams_1, diagrams_2)
+    silhouettes_1 = silhouettes(diagrams_1, sampling, power)
+    if are_arrays_equal:
+        distances = pdist(silhouettes_1, 'minkowski', p=p)
+        distances *= step_size_factor
+        return squareform(distances)
+    silhouettes_2 = silhouettes(diagrams_2, sampling, power)
+    distances = cdist(silhouettes_1, silhouettes_2, 'minkowski', p=p)
+    distances *= step_size_factor
+    return distances
+
+
+implemented_metric_recipes = {
+    "bottleneck": bottleneck_distances,
+    "wasserstein": wasserstein_distances,
+    "landscape": landscape_distances,
+    "betti": betti_distances,
+    "heat": heat_distances,
+    "persistence_image": persistence_image_distances,
+    'silhouette': silhouette_distances
+    }
+
+
+def _parallel_pairwise(
+        X1, X2, metric, metric_params, homology_dimensions, n_jobs
+        ):
+    metric_func = implemented_metric_recipes[metric]
+    effective_metric_params = metric_params.copy()
+    none_dict = {dim: None for dim in homology_dimensions}
+    samplings = effective_metric_params.pop("samplings", none_dict)
+    step_sizes = effective_metric_params.pop("step_sizes", none_dict)
+    if metric in ["heat", "persistence_image"]:
+        parallel_kwargs = {"mmap_mode": "c"}
+    else:
+        parallel_kwargs = {}
+
+    n_columns = len(X2)
+    distance_matrices = Parallel(n_jobs=n_jobs, **parallel_kwargs)(
+        delayed(metric_func)(
+            _subdiagrams(X1, [dim], remove_dim=True),
+            _subdiagrams(X2[s], [dim], remove_dim=True),
+            sampling=samplings[dim],
+            step_size=step_sizes[dim],
+            **effective_metric_params
+            )
+        for dim in homology_dimensions
+        for s in gen_even_slices(n_columns, effective_n_jobs(n_jobs))
+        )
+
+    distance_matrices = np.concatenate(distance_matrices, axis=1)
+    distance_matrices = np.stack(
+        [distance_matrices[:, i * n_columns:(i + 1) * n_columns]
+         for i in range(len(homology_dimensions))],
+        axis=2)
+    return distance_matrices
+
+
+def bottleneck_amplitudes(diagrams, **kwargs):
+    half_lifetimes = (diagrams[:, :, 1] - diagrams[:, :, 0]) / 2.
+    return np.linalg.norm(half_lifetimes, axis=1, ord=np.inf)
+
+
+def wasserstein_amplitudes(diagrams, p=2., **kwargs):
+    half_lifetimes = (diagrams[:, :, 1] - diagrams[:, :, 0]) / 2.
+    return np.linalg.norm(half_lifetimes, axis=1, ord=p)
+
+
+def betti_amplitudes(diagrams, sampling, step_size, p=2., **kwargs):
+    step_size_factor = step_size ** (1 / p)
+    bcs = betti_curves(diagrams, sampling)
+    amplitudes = np.linalg.norm(bcs, axis=1, ord=p)
+    amplitudes *= step_size_factor
+    return amplitudes
+
+
+def landscape_amplitudes(
+        diagrams, sampling, step_size, p=2., n_layers=1, **kwargs
+        ):
+    step_size_factor = step_size ** (1 / p)
+    ls = landscapes(diagrams, sampling, n_layers).\
+        reshape(len(diagrams), -1)
+    amplitudes = np.linalg.norm(ls, axis=1, ord=p)
+    amplitudes *= step_size_factor
+    return amplitudes
+
+
+def heat_amplitudes(diagrams, sampling, step_size, sigma=0.1, p=2., **kwargs):
+    # WARNING: `heats` modifies `diagrams` in place
+    step_size_factor = step_size ** (2 / p)
+    heats_ = heats(diagrams, sampling, step_size, sigma).\
+        reshape(len(diagrams), -1)
+    amplitudes = np.linalg.norm(heats_, axis=1, ord=p)
+    amplitudes *= step_size_factor
+    return amplitudes
+
+
+def persistence_image_amplitudes(
+        diagrams, sampling, step_size, sigma=0.1, weight_function=np.ones_like,
+        p=2., **kwargs
+        ):
+    # For persistence images, `sampling` is a tall matrix with two columns
+    # (the first for birth and the second for persistence), and `step_size` is
+    # a 2d array
+    weights = weight_function(sampling[:, 1])
+    step_sizes_factor = np.product(step_size) ** (1 / p)
+    # WARNING: `persistence_images` modifies `diagrams` in place
+    persistence_images_ = persistence_images(
+        diagrams, sampling, step_size, sigma, weights
+        ).reshape(len(diagrams), -1)
+    amplitudes = np.linalg.norm(persistence_images_, axis=1, ord=p)
+    amplitudes *= step_sizes_factor
+    return amplitudes
+
+
+def silhouette_amplitudes(
+        diagrams, sampling, step_size, power=1., p=2., **kwargs
+        ):
+    step_size_factor = step_size ** (1 / p)
+    silhouettes_ = silhouettes(diagrams, sampling, power)
+    amplitudes = np.linalg.norm(silhouettes_, axis=1, ord=p)
+    amplitudes *= step_size_factor
+    return amplitudes
+
+
+implemented_amplitude_recipes = {
+    "bottleneck": bottleneck_amplitudes,
+    "wasserstein": wasserstein_amplitudes,
+    "landscape": landscape_amplitudes,
+    "betti": betti_amplitudes,
+    "heat": heat_amplitudes,
+    "persistence_image": persistence_image_amplitudes,
+    'silhouette': silhouette_amplitudes
+    }
+
+
+def _parallel_amplitude(X, metric, metric_params, homology_dimensions, n_jobs):
+    amplitude_func = implemented_amplitude_recipes[metric]
+    effective_metric_params = metric_params.copy()
+    none_dict = {dim: None for dim in homology_dimensions}
+    samplings = effective_metric_params.pop("samplings", none_dict)
+    step_sizes = effective_metric_params.pop("step_sizes", none_dict)
+    if metric in ["heat", "persistence_image"]:
+        parallel_kwargs = {"mmap_mode": "c"}
+    else:
+        parallel_kwargs = {}
+
+    amplitude_arrays = Parallel(n_jobs=n_jobs, **parallel_kwargs)(
+        delayed(amplitude_func)(
+            _subdiagrams(X[s], [dim], remove_dim=True),
+            sampling=samplings[dim],
+            step_size=step_sizes[dim],
+            **effective_metric_params
+            )
+        for dim in homology_dimensions
+        for s in gen_even_slices(_num_samples(X), effective_n_jobs(n_jobs))
+        )
+
+    amplitude_arrays = np.concatenate(amplitude_arrays).\
+        reshape(len(homology_dimensions), len(X)).T
+
+    return amplitude_arrays
```

## gtda/diagrams/_utils.py

 * *Ordering differences only*

```diff
@@ -1,186 +1,186 @@
-"""Utility functions for diagrams."""
-# License: GNU AGPLv3
-
-import numpy as np
-
-
-def _homology_dimensions_to_sorted_ints(homology_dimensions):
-    return tuple(
-        sorted([int(dim) if dim != np.inf else dim
-                for dim in homology_dimensions])
-        )
-
-
-def _subdiagrams(X, homology_dimensions, remove_dim=False):
-    """For each diagram in a collection, extract the subdiagrams in a given
-    list of homology dimensions. It is assumed that all diagrams in X contain
-    the same number of points in each homology dimension."""
-    n_samples = len(X)
-    X_0 = X[0]
-
-    def _subdiagrams_single_homology_dimension(homology_dimension):
-        n_features_in_dim = np.sum(X_0[:, 2] == homology_dimension)
-        try:
-            # In this case, reshape ensures copy
-            Xs = X[X[:, :, 2] == homology_dimension].\
-                reshape(n_samples, n_features_in_dim, 3)
-            return Xs
-        except ValueError as e:
-            if e.args[0].lower().startswith("cannot reshape array"):
-                raise ValueError(
-                    f"All persistence diagrams in the collection must have "
-                    f"the same number of birth-death-dimension triples in any "
-                    f"given homology dimension. This is not true in homology "
-                    f"dimension {homology_dimension}. Trivial triples for "
-                    f"which birth = death may be added or removed to fulfill "
-                    f"this requirement."
-                )
-            else:
-                raise e
-
-    if len(homology_dimensions) == 1:
-        Xs = _subdiagrams_single_homology_dimension(homology_dimensions[0])
-    else:
-        # np.concatenate will also create a copy
-        Xs = np.concatenate(
-            [_subdiagrams_single_homology_dimension(dim)
-             for dim in homology_dimensions],
-            axis=1
-            )
-    if remove_dim:
-        Xs = Xs[:, :, :2]
-    return Xs
-
-
-def _sample_image(image, diagram_pixel_coords):
-    # WARNING: Modifies `image` in-place
-    unique, counts = \
-        np.unique(diagram_pixel_coords, axis=0, return_counts=True)
-    unique = tuple(tuple(row) for row in unique.astype(int).T)
-    image[unique] = counts
-
-
-def _multirange(counts):
-    """Given a 1D array of positive integers, generate an array equal to
-    np.concatenate([np.arange(c) for c in counts]), but in a faster and more
-    memory-efficient way."""
-    cumsum = np.cumsum(counts)
-    reset_index = cumsum[:-1]
-    incr = np.ones(cumsum[-1], dtype=int)
-    incr[0] = 0
-
-    # For each index in reset_index, we insert the negative value necessary
-    # to offset the cumsum in the last line
-    incr[reset_index] = 1 - counts[:-1]
-    incr.cumsum(out=incr)
-
-    return incr
-
-
-def _filter(X, filtered_homology_dimensions, cutoff):
-    n = len(X)
-    homology_dimensions = sorted(np.unique(X[0, :, 2]))
-    unfiltered_homology_dimensions = [dim for dim in homology_dimensions if
-                                      dim not in filtered_homology_dimensions]
-
-    if len(unfiltered_homology_dimensions) == 0:
-        Xuf = np.empty((n, 0, 3), dtype=X.dtype)
-    else:
-        Xuf = _subdiagrams(X, unfiltered_homology_dimensions)
-
-    # Compute a global 2D cutoff mask once
-    cutoff_mask = X[:, :, 1] - X[:, :, 0] > cutoff
-    Xf = []
-    for dim in filtered_homology_dimensions:
-        # Compute a 2D mask for persistence pairs in dimension dim
-        dim_mask = X[:, :, 2] == dim
-        # Need the indices relative to X of persistence triples in dimension
-        # dim surviving the cutoff
-        indices = np.nonzero(np.logical_and(dim_mask, cutoff_mask))
-        if not indices[0].size:
-            Xdim = np.tile([0., 0., dim], (n, 1, 1))
-        else:
-            # A unique element k is repeated N times *consecutively* in
-            # indices[0] iff there are exactly N valid persistence triples
-            # in the k-th diagram
-            unique, counts = np.unique(indices[0], return_counts=True)
-            max_n_points = np.max(counts)
-            # Make a global 2D array of all valid triples
-            X_indices = X[indices]
-            min_value = np.min(X_indices[:, 0])  # For padding
-            # Initialise the array of filtered subdiagrams in dimension m
-            Xdim = np.tile([min_value, min_value, dim], (n, max_n_points, 1))
-            # Since repeated indices in indices[0] are consecutive and we know
-            # the counts per unique index, we can fill the top portion of
-            # each 2D array entry of Xdim with the filtered triples from the
-            # corresponding entry of X
-            Xdim[indices[0], _multirange(counts)] = X_indices
-        Xf.append(Xdim)
-
-    Xf.append(Xuf)
-    Xf = np.concatenate(Xf, axis=1)
-    return Xf
-
-
-def _bin(X, metric, n_bins=100, homology_dimensions=None, **kw_args):
-    if homology_dimensions is None:
-        homology_dimensions = sorted(np.unique(X[0, :, 2]))
-    # For some vectorizations, we force the values to be the same + widest
-    sub_diags = {dim: _subdiagrams(X, [dim], remove_dim=True)
-                 for dim in homology_dimensions}
-    # For persistence images, move into birth-persistence
-    if metric == 'persistence_image':
-        for dim in homology_dimensions:
-            sub_diags[dim][:, :, [1]] = sub_diags[dim][:, :, [1]] \
-                - sub_diags[dim][:, :, [0]]
-    min_vals = {dim: np.min(sub_diags[dim], axis=(0, 1))
-                for dim in homology_dimensions}
-    max_vals = {dim: np.max(sub_diags[dim], axis=(0, 1))
-                for dim in homology_dimensions}
-
-    if metric in ['landscape', 'betti', 'heat', 'silhouette']:
-        #  Taking the min(resp. max) of a tuple `m` amounts to extracting
-        #  the birth (resp. death) value
-        min_vals = {d: np.array(2*[np.min(m)]) for d, m in min_vals.items()}
-        max_vals = {d: np.array(2*[np.max(m)]) for d, m in max_vals.items()}
-
-    # Scales between axes should be kept the same, but not between dimension
-    all_max_values = np.stack(list(max_vals.values()))
-    if len(homology_dimensions) == 1:
-        all_max_values = all_max_values.reshape(1, -1)
-    global_max_val = np.max(all_max_values, axis=0)
-    max_vals = {dim: np.array([max_vals[dim][k] if
-                               (max_vals[dim][k] != min_vals[dim][k])
-                               else global_max_val[k] for k in range(2)])
-                for dim in homology_dimensions}
-
-    samplings = {}
-    step_sizes = {}
-    for dim in homology_dimensions:
-        samplings[dim], step_sizes[dim] = np.linspace(
-            min_vals[dim], max_vals[dim], retstep=True, num=n_bins
-            )
-    if metric in ['landscape', 'betti', 'heat', 'silhouette']:
-        for dim in homology_dimensions:
-            samplings[dim] = samplings[dim][:, [0], None]
-            step_sizes[dim] = step_sizes[dim][0]
-    return samplings, step_sizes
-
-
-def _make_homology_dimensions_mapping(homology_dimensions,
-                                      homology_dimensions_ref):
-    """`homology_dimensions_ref` is assumed to be a sorted tuple as is e.g.
-    :attr:`homology_dimensions_` for several transformers."""
-    if homology_dimensions is None:
-        homology_dimensions_mapping = list(enumerate(homology_dimensions_ref))
-    else:
-        homology_dimensions_mapping = []
-        for dim in homology_dimensions:
-            if dim not in homology_dimensions_ref:
-                raise ValueError(f"All homology dimensions must be in "
-                                 f"{homology_dimensions_ref}; {dim} is not.")
-            else:
-                homology_dimensions_arr = np.array(homology_dimensions_ref)
-                inv_idx = np.flatnonzero(homology_dimensions_arr == dim)[0]
-                homology_dimensions_mapping.append((inv_idx, dim))
-    return homology_dimensions_mapping
+"""Utility functions for diagrams."""
+# License: GNU AGPLv3
+
+import numpy as np
+
+
+def _homology_dimensions_to_sorted_ints(homology_dimensions):
+    return tuple(
+        sorted([int(dim) if dim != np.inf else dim
+                for dim in homology_dimensions])
+        )
+
+
+def _subdiagrams(X, homology_dimensions, remove_dim=False):
+    """For each diagram in a collection, extract the subdiagrams in a given
+    list of homology dimensions. It is assumed that all diagrams in X contain
+    the same number of points in each homology dimension."""
+    n_samples = len(X)
+    X_0 = X[0]
+
+    def _subdiagrams_single_homology_dimension(homology_dimension):
+        n_features_in_dim = np.sum(X_0[:, 2] == homology_dimension)
+        try:
+            # In this case, reshape ensures copy
+            Xs = X[X[:, :, 2] == homology_dimension].\
+                reshape(n_samples, n_features_in_dim, 3)
+            return Xs
+        except ValueError as e:
+            if e.args[0].lower().startswith("cannot reshape array"):
+                raise ValueError(
+                    f"All persistence diagrams in the collection must have "
+                    f"the same number of birth-death-dimension triples in any "
+                    f"given homology dimension. This is not true in homology "
+                    f"dimension {homology_dimension}. Trivial triples for "
+                    f"which birth = death may be added or removed to fulfill "
+                    f"this requirement."
+                )
+            else:
+                raise e
+
+    if len(homology_dimensions) == 1:
+        Xs = _subdiagrams_single_homology_dimension(homology_dimensions[0])
+    else:
+        # np.concatenate will also create a copy
+        Xs = np.concatenate(
+            [_subdiagrams_single_homology_dimension(dim)
+             for dim in homology_dimensions],
+            axis=1
+            )
+    if remove_dim:
+        Xs = Xs[:, :, :2]
+    return Xs
+
+
+def _sample_image(image, diagram_pixel_coords):
+    # WARNING: Modifies `image` in-place
+    unique, counts = \
+        np.unique(diagram_pixel_coords, axis=0, return_counts=True)
+    unique = tuple(tuple(row) for row in unique.astype(int).T)
+    image[unique] = counts
+
+
+def _multirange(counts):
+    """Given a 1D array of positive integers, generate an array equal to
+    np.concatenate([np.arange(c) for c in counts]), but in a faster and more
+    memory-efficient way."""
+    cumsum = np.cumsum(counts)
+    reset_index = cumsum[:-1]
+    incr = np.ones(cumsum[-1], dtype=int)
+    incr[0] = 0
+
+    # For each index in reset_index, we insert the negative value necessary
+    # to offset the cumsum in the last line
+    incr[reset_index] = 1 - counts[:-1]
+    incr.cumsum(out=incr)
+
+    return incr
+
+
+def _filter(X, filtered_homology_dimensions, cutoff):
+    n = len(X)
+    homology_dimensions = sorted(np.unique(X[0, :, 2]))
+    unfiltered_homology_dimensions = [dim for dim in homology_dimensions if
+                                      dim not in filtered_homology_dimensions]
+
+    if len(unfiltered_homology_dimensions) == 0:
+        Xuf = np.empty((n, 0, 3), dtype=X.dtype)
+    else:
+        Xuf = _subdiagrams(X, unfiltered_homology_dimensions)
+
+    # Compute a global 2D cutoff mask once
+    cutoff_mask = X[:, :, 1] - X[:, :, 0] > cutoff
+    Xf = []
+    for dim in filtered_homology_dimensions:
+        # Compute a 2D mask for persistence pairs in dimension dim
+        dim_mask = X[:, :, 2] == dim
+        # Need the indices relative to X of persistence triples in dimension
+        # dim surviving the cutoff
+        indices = np.nonzero(np.logical_and(dim_mask, cutoff_mask))
+        if not indices[0].size:
+            Xdim = np.tile([0., 0., dim], (n, 1, 1))
+        else:
+            # A unique element k is repeated N times *consecutively* in
+            # indices[0] iff there are exactly N valid persistence triples
+            # in the k-th diagram
+            unique, counts = np.unique(indices[0], return_counts=True)
+            max_n_points = np.max(counts)
+            # Make a global 2D array of all valid triples
+            X_indices = X[indices]
+            min_value = np.min(X_indices[:, 0])  # For padding
+            # Initialise the array of filtered subdiagrams in dimension m
+            Xdim = np.tile([min_value, min_value, dim], (n, max_n_points, 1))
+            # Since repeated indices in indices[0] are consecutive and we know
+            # the counts per unique index, we can fill the top portion of
+            # each 2D array entry of Xdim with the filtered triples from the
+            # corresponding entry of X
+            Xdim[indices[0], _multirange(counts)] = X_indices
+        Xf.append(Xdim)
+
+    Xf.append(Xuf)
+    Xf = np.concatenate(Xf, axis=1)
+    return Xf
+
+
+def _bin(X, metric, n_bins=100, homology_dimensions=None, **kw_args):
+    if homology_dimensions is None:
+        homology_dimensions = sorted(np.unique(X[0, :, 2]))
+    # For some vectorizations, we force the values to be the same + widest
+    sub_diags = {dim: _subdiagrams(X, [dim], remove_dim=True)
+                 for dim in homology_dimensions}
+    # For persistence images, move into birth-persistence
+    if metric == 'persistence_image':
+        for dim in homology_dimensions:
+            sub_diags[dim][:, :, [1]] = sub_diags[dim][:, :, [1]] \
+                - sub_diags[dim][:, :, [0]]
+    min_vals = {dim: np.min(sub_diags[dim], axis=(0, 1))
+                for dim in homology_dimensions}
+    max_vals = {dim: np.max(sub_diags[dim], axis=(0, 1))
+                for dim in homology_dimensions}
+
+    if metric in ['landscape', 'betti', 'heat', 'silhouette']:
+        #  Taking the min(resp. max) of a tuple `m` amounts to extracting
+        #  the birth (resp. death) value
+        min_vals = {d: np.array(2*[np.min(m)]) for d, m in min_vals.items()}
+        max_vals = {d: np.array(2*[np.max(m)]) for d, m in max_vals.items()}
+
+    # Scales between axes should be kept the same, but not between dimension
+    all_max_values = np.stack(list(max_vals.values()))
+    if len(homology_dimensions) == 1:
+        all_max_values = all_max_values.reshape(1, -1)
+    global_max_val = np.max(all_max_values, axis=0)
+    max_vals = {dim: np.array([max_vals[dim][k] if
+                               (max_vals[dim][k] != min_vals[dim][k])
+                               else global_max_val[k] for k in range(2)])
+                for dim in homology_dimensions}
+
+    samplings = {}
+    step_sizes = {}
+    for dim in homology_dimensions:
+        samplings[dim], step_sizes[dim] = np.linspace(
+            min_vals[dim], max_vals[dim], retstep=True, num=n_bins
+            )
+    if metric in ['landscape', 'betti', 'heat', 'silhouette']:
+        for dim in homology_dimensions:
+            samplings[dim] = samplings[dim][:, [0], None]
+            step_sizes[dim] = step_sizes[dim][0]
+    return samplings, step_sizes
+
+
+def _make_homology_dimensions_mapping(homology_dimensions,
+                                      homology_dimensions_ref):
+    """`homology_dimensions_ref` is assumed to be a sorted tuple as is e.g.
+    :attr:`homology_dimensions_` for several transformers."""
+    if homology_dimensions is None:
+        homology_dimensions_mapping = list(enumerate(homology_dimensions_ref))
+    else:
+        homology_dimensions_mapping = []
+        for dim in homology_dimensions:
+            if dim not in homology_dimensions_ref:
+                raise ValueError(f"All homology dimensions must be in "
+                                 f"{homology_dimensions_ref}; {dim} is not.")
+            else:
+                homology_dimensions_arr = np.array(homology_dimensions_ref)
+                inv_idx = np.flatnonzero(homology_dimensions_arr == dim)[0]
+                homology_dimensions_mapping.append((inv_idx, dim))
+    return homology_dimensions_mapping
```

## gtda/diagrams/__init__.py

 * *Ordering differences only*

```diff
@@ -1,26 +1,26 @@
-"""The module :mod:`gtda.diagrams` implements transformers to preprocess
-persistence diagrams, extract features from them, or compute pairwise distances
-between diagrams."""
-
-from .preprocessing import ForgetDimension, Scaler, Filtering
-from .distance import PairwiseDistance
-from .features import PersistenceEntropy, Amplitude, NumberOfPoints, \
-    ComplexPolynomial
-from .representations import BettiCurve, PersistenceLandscape, HeatKernel, \
-    Silhouette, PersistenceImage
-
-__all__ = [
-    'ForgetDimension',
-    'Scaler',
-    'Filtering',
-    'PairwiseDistance',
-    'PersistenceEntropy',
-    'Amplitude',
-    'NumberOfPoints',
-    'ComplexPolynomial',
-    'BettiCurve',
-    'PersistenceLandscape',
-    'HeatKernel',
-    'Silhouette',
-    'PersistenceImage'
-    ]
+"""The module :mod:`gtda.diagrams` implements transformers to preprocess
+persistence diagrams, extract features from them, or compute pairwise distances
+between diagrams."""
+
+from .preprocessing import ForgetDimension, Scaler, Filtering
+from .distance import PairwiseDistance
+from .features import PersistenceEntropy, Amplitude, NumberOfPoints, \
+    ComplexPolynomial
+from .representations import BettiCurve, PersistenceLandscape, HeatKernel, \
+    Silhouette, PersistenceImage
+
+__all__ = [
+    'ForgetDimension',
+    'Scaler',
+    'Filtering',
+    'PairwiseDistance',
+    'PersistenceEntropy',
+    'Amplitude',
+    'NumberOfPoints',
+    'ComplexPolynomial',
+    'BettiCurve',
+    'PersistenceLandscape',
+    'HeatKernel',
+    'Silhouette',
+    'PersistenceImage'
+    ]
```

## gtda/externals/__init__.py

```diff
@@ -1,21 +1,22 @@
-""""Python bindings for external dependencies."""
-# License: GNU AGPLv3
-
-from .modules.gtda_bottleneck import bottleneck_distance
-from .modules.gtda_wasserstein import wasserstein_distance
-from .python import RipsComplex, SparseRipsComplex, CechComplex, \
-    CubicalComplex, PeriodicCubicalComplex, SimplexTree, WitnessComplex, \
-    StrongWitnessComplex
-
-__all__ = [
-    'bottleneck_distance',
-    'wasserstein_distance',
-    'RipsComplex',
-    'SparseRipsComplex',
-    'CechComplex',
-    'CubicalComplex',
-    'PeriodicCubicalComplex',
-    'SimplexTree',
-    'WitnessComplex',
-    'StrongWitnessComplex'
-    ]
+""""Python bindings for external dependencies."""
+# License: GNU AGPLv3
+
+from .modules.gtda_bottleneck import bottleneck_distance
+from .modules.gtda_wasserstein import wasserstein_distance
+from .python import RipsComplex, SparseRipsComplex, CechComplex, \
+    CubicalComplex, PeriodicCubicalComplex, SimplexTree, WitnessComplex, \
+    StrongWitnessComplex
+
+__all__ = [
+    'bottleneck_distance',
+    'wasserstein_distance',
+    'RipsComplex',
+    'SparseRipsComplex',
+    'CechComplex',
+    'CubicalComplex',
+    'PeriodicCubicalComplex',
+    'SimplexTree',
+    'WitnessComplex',
+    'StrongWitnessComplex',
+    'modules'
+    ]
```

## gtda/externals/python/cech_complex_interface.py

 * *Ordering differences only*

```diff
@@ -1,34 +1,34 @@
-from ..modules.gtda_cech_complex import Cech_complex_interface
-from . import SimplexTree
-
-
-# CechComplex python interface
-class CechComplex:
-    """ The data structure is a proximity graph, containing edges when the edge
-    length is less or equal * to a given max_radius. The set of all simplices
-    is filtered by the radius of their minimal enclosing ball.
-    """
-    def __init__(self, points, max_radius=0):
-        """CechComplex constructor.
-        :param points: A list of points in d-Dimension.
-        :type points: list of coordinates of double
-        :param max_radius: A distance matrix (full square or lower
-            triangular).
-        """
-        self.thisref = Cech_complex_interface(points, max_radius)
-
-    def __del__(self):
-        if self.thisref is not None:
-            del self.thisref
-
-    def create_simplex_tree(self, max_dimension=1):
-        """
-        :param max_dimension: graph expansion for rips until this given maximal
-            dimension.
-        :type max_dimension: int
-        :returns: A simplex tree created from the Delaunay Triangulation.
-        :rtype: SimplexTree
-        """
-        simplex_tree = SimplexTree()
-        self.thisref.create_simplex_tree(simplex_tree.thisptr, max_dimension)
-        return simplex_tree
+from ..modules.gtda_cech_complex import Cech_complex_interface
+from . import SimplexTree
+
+
+# CechComplex python interface
+class CechComplex:
+    """ The data structure is a proximity graph, containing edges when the edge
+    length is less or equal * to a given max_radius. The set of all simplices
+    is filtered by the radius of their minimal enclosing ball.
+    """
+    def __init__(self, points, max_radius=0):
+        """CechComplex constructor.
+        :param points: A list of points in d-Dimension.
+        :type points: list of coordinates of double
+        :param max_radius: A distance matrix (full square or lower
+            triangular).
+        """
+        self.thisref = Cech_complex_interface(points, max_radius)
+
+    def __del__(self):
+        if self.thisref is not None:
+            del self.thisref
+
+    def create_simplex_tree(self, max_dimension=1):
+        """
+        :param max_dimension: graph expansion for rips until this given maximal
+            dimension.
+        :type max_dimension: int
+        :returns: A simplex tree created from the Delaunay Triangulation.
+        :rtype: SimplexTree
+        """
+        simplex_tree = SimplexTree()
+        self.thisref.create_simplex_tree(simplex_tree.thisptr, max_dimension)
+        return simplex_tree
```

## gtda/externals/python/cubical_complex_interface.py

 * *Ordering differences only*

```diff
@@ -1,157 +1,157 @@
-import os
-import numpy as np
-from ..modules.gtda_cubical_complex \
-    import Cubical_complex_interface \
-    as Bitmap_cubical_complex_base_interface
-from ..modules.gtda_persistent_cohomology \
-    import Persistent_cohomology_interface \
-    as Cubical_complex_persistence_interface
-
-
-class CubicalComplex:
-    """The CubicalComplex is an example of a structured complex useful in
-    computational mathematics (specially rigorous numerics) and image
-    analysis.
-    """
-    # Bitmap_cubical_complex_base_interface * thisptr
-    # cdef Cubical_complex_persistence_interface * pcohptr
-
-    def __init__(self, dimensions=None, top_dimensional_cells=None,
-                 perseus_file=''):
-        """CubicalComplex constructor from dimensions and
-        top_dimensional_cells or from a Perseus-style file name.
-        :param dimensions: A list of number of top dimensional cells.
-        :type dimensions: list of int
-        :param top_dimensional_cells: A list of cells filtration values.
-        :type top_dimensional_cells: list of double
-        Or
-        :param perseus_file: A Perseus-style file name.
-        :type perseus_file: string
-        """
-        self.thisptr = None
-        self.pcohptr = None
-        if (dimensions is not None) and \
-                (top_dimensional_cells is not None) and \
-                (perseus_file == ''):
-            self.thisptr = \
-                Bitmap_cubical_complex_base_interface(dimensions,
-                                                      top_dimensional_cells)
-        elif (dimensions is None) and \
-             (top_dimensional_cells is None) and (perseus_file != ''):
-            if os.path.isfile(perseus_file):
-                self.thisptr = Bitmap_cubical_complex_base_interface(
-                    str.encode(perseus_file))
-            else:
-                print("file " + perseus_file + " not found.")
-        else:
-            print("CubicalComplex can be constructed from dimensions and "
-                  "top_dimensional_cells or from a Perseus-style file name.")
-
-    def __del__(self):
-        if self.thisptr is not None:
-            del self.thisptr
-        if self.pcohptr is not None:
-            del self.pcohptr
-
-    def __is_defined(self):
-        """Returns true if CubicalComplex pointer is not NULL.
-         """
-        if self.thisptr is not None:
-            return True
-        return False
-
-    def __is_persistence_defined(self):
-        """Returns true if Persistence pointer is not NULL.
-         """
-        if self.pcohptr is not None:
-            return True
-        return False
-
-    def num_simplices(self):
-        """This function returns the number of all cubes in the complex.
-        :returns:  int -- the number of all cubes in the complex.
-        """
-        return self.thisptr.num_simplices()
-
-    def dimension(self):
-        """This function returns the dimension of the complex.
-        :returns:  int -- the complex dimension.
-        """
-        return self.thisptr.dimension()
-
-    def persistence(self, homology_coeff_field=11, min_persistence=0):
-        """This function returns the persistence of the complex.
-        :param homology_coeff_field: The homology coefficient field. Must be a
-            prime number
-        :type homology_coeff_field: int.
-        :param min_persistence: The minimum persistence value to take into
-            account (strictly greater than min_persistence). Default value is
-            0.0.
-            Sets min_persistence to -1.0 to see all values.
-        :type min_persistence: float.
-        :returns: list of pairs(dimension, pair(birth, death)) -- the
-            persistence of the complex.
-        """
-        if self.pcohptr is not None:
-            del self.pcohptr
-        if self.thisptr is not None:
-            pass
-            self.pcohptr = Cubical_complex_persistence_interface(self.thisptr,
-                                                                 True)
-        persistence_result = []
-        if self.pcohptr is not None:
-            self.pcohptr.compute_persistence(homology_coeff_field,
-                                             min_persistence)
-            persistence_result = self.pcohptr.get_persistence()
-        return persistence_result
-
-    def betti_numbers(self):
-        """This function returns the Betti numbers of the complex.
-        :returns: list of int -- The Betti numbers ([B0, B1, ..., Bn]).
-        :note: betti_numbers function requires persistence function to be
-            launched first.
-        :note: betti_numbers function always returns [1, 0, 0, ...] as infinity
-            filtration cubes are not removed from the complex.
-        """
-        bn_result = []
-        if self.pcohptr is not None:
-            bn_result = self.pcohptr.betti_numbers()
-        return bn_result
-
-    def persistent_betti_numbers(self, from_value, to_value):
-        """This function returns the persistent Betti numbers of the complex.
-        :param from_value: The persistence birth limit to be added in the
-            numbers (persistent birth <= from_value).
-        :type from_value: float.
-        :param to_value: The persistence death limit to be added in the
-            numbers (persistent death > to_value).
-        :type to_value: float.
-        :returns: list of int -- The persistent Betti numbers ([B0, B1, ...,
-            Bn]).
-        :note: persistent_betti_numbers function requires persistence
-            function to be launched first.
-        """
-        pbn_result = []
-        if self.pcohptr is not None:
-            # pbn_result = self.pcohptr.persistent_betti_numbers(<double>from_value, <double>to_value)
-            pbn_result = self.pcohptr.persistent_betti_numbers(from_value,
-                                                               to_value)
-        return pbn_result
-
-    def persistence_intervals_in_dimension(self, dimension):
-        """This function returns the persistence intervals of the complex in a
-        specific dimension.
-        :param dimension: The specific dimension.
-        :type dimension: int.
-        :returns: The persistence intervals.
-        :rtype:  numpy array of dimension 2
-        :note: intervals_in_dim function requires persistence function to be
-            launched first.
-        """
-        intervals_result = [[]]
-        if self.pcohptr is not None:
-            intervals_result = self.pcohptr.intervals_in_dimension(dimension)
-        else:
-            print("intervals_in_dim function requires persistence function"
-                  " to be launched first.")
-        return np.array(intervals_result)
+import os
+import numpy as np
+from ..modules.gtda_cubical_complex \
+    import Cubical_complex_interface \
+    as Bitmap_cubical_complex_base_interface
+from ..modules.gtda_persistent_cohomology \
+    import Persistent_cohomology_interface \
+    as Cubical_complex_persistence_interface
+
+
+class CubicalComplex:
+    """The CubicalComplex is an example of a structured complex useful in
+    computational mathematics (specially rigorous numerics) and image
+    analysis.
+    """
+    # Bitmap_cubical_complex_base_interface * thisptr
+    # cdef Cubical_complex_persistence_interface * pcohptr
+
+    def __init__(self, dimensions=None, top_dimensional_cells=None,
+                 perseus_file=''):
+        """CubicalComplex constructor from dimensions and
+        top_dimensional_cells or from a Perseus-style file name.
+        :param dimensions: A list of number of top dimensional cells.
+        :type dimensions: list of int
+        :param top_dimensional_cells: A list of cells filtration values.
+        :type top_dimensional_cells: list of double
+        Or
+        :param perseus_file: A Perseus-style file name.
+        :type perseus_file: string
+        """
+        self.thisptr = None
+        self.pcohptr = None
+        if (dimensions is not None) and \
+                (top_dimensional_cells is not None) and \
+                (perseus_file == ''):
+            self.thisptr = \
+                Bitmap_cubical_complex_base_interface(dimensions,
+                                                      top_dimensional_cells)
+        elif (dimensions is None) and \
+             (top_dimensional_cells is None) and (perseus_file != ''):
+            if os.path.isfile(perseus_file):
+                self.thisptr = Bitmap_cubical_complex_base_interface(
+                    str.encode(perseus_file))
+            else:
+                print("file " + perseus_file + " not found.")
+        else:
+            print("CubicalComplex can be constructed from dimensions and "
+                  "top_dimensional_cells or from a Perseus-style file name.")
+
+    def __del__(self):
+        if self.thisptr is not None:
+            del self.thisptr
+        if self.pcohptr is not None:
+            del self.pcohptr
+
+    def __is_defined(self):
+        """Returns true if CubicalComplex pointer is not NULL.
+         """
+        if self.thisptr is not None:
+            return True
+        return False
+
+    def __is_persistence_defined(self):
+        """Returns true if Persistence pointer is not NULL.
+         """
+        if self.pcohptr is not None:
+            return True
+        return False
+
+    def num_simplices(self):
+        """This function returns the number of all cubes in the complex.
+        :returns:  int -- the number of all cubes in the complex.
+        """
+        return self.thisptr.num_simplices()
+
+    def dimension(self):
+        """This function returns the dimension of the complex.
+        :returns:  int -- the complex dimension.
+        """
+        return self.thisptr.dimension()
+
+    def persistence(self, homology_coeff_field=11, min_persistence=0):
+        """This function returns the persistence of the complex.
+        :param homology_coeff_field: The homology coefficient field. Must be a
+            prime number
+        :type homology_coeff_field: int.
+        :param min_persistence: The minimum persistence value to take into
+            account (strictly greater than min_persistence). Default value is
+            0.0.
+            Sets min_persistence to -1.0 to see all values.
+        :type min_persistence: float.
+        :returns: list of pairs(dimension, pair(birth, death)) -- the
+            persistence of the complex.
+        """
+        if self.pcohptr is not None:
+            del self.pcohptr
+        if self.thisptr is not None:
+            pass
+            self.pcohptr = Cubical_complex_persistence_interface(self.thisptr,
+                                                                 True)
+        persistence_result = []
+        if self.pcohptr is not None:
+            self.pcohptr.compute_persistence(homology_coeff_field,
+                                             min_persistence)
+            persistence_result = self.pcohptr.get_persistence()
+        return persistence_result
+
+    def betti_numbers(self):
+        """This function returns the Betti numbers of the complex.
+        :returns: list of int -- The Betti numbers ([B0, B1, ..., Bn]).
+        :note: betti_numbers function requires persistence function to be
+            launched first.
+        :note: betti_numbers function always returns [1, 0, 0, ...] as infinity
+            filtration cubes are not removed from the complex.
+        """
+        bn_result = []
+        if self.pcohptr is not None:
+            bn_result = self.pcohptr.betti_numbers()
+        return bn_result
+
+    def persistent_betti_numbers(self, from_value, to_value):
+        """This function returns the persistent Betti numbers of the complex.
+        :param from_value: The persistence birth limit to be added in the
+            numbers (persistent birth <= from_value).
+        :type from_value: float.
+        :param to_value: The persistence death limit to be added in the
+            numbers (persistent death > to_value).
+        :type to_value: float.
+        :returns: list of int -- The persistent Betti numbers ([B0, B1, ...,
+            Bn]).
+        :note: persistent_betti_numbers function requires persistence
+            function to be launched first.
+        """
+        pbn_result = []
+        if self.pcohptr is not None:
+            # pbn_result = self.pcohptr.persistent_betti_numbers(<double>from_value, <double>to_value)
+            pbn_result = self.pcohptr.persistent_betti_numbers(from_value,
+                                                               to_value)
+        return pbn_result
+
+    def persistence_intervals_in_dimension(self, dimension):
+        """This function returns the persistence intervals of the complex in a
+        specific dimension.
+        :param dimension: The specific dimension.
+        :type dimension: int.
+        :returns: The persistence intervals.
+        :rtype:  numpy array of dimension 2
+        :note: intervals_in_dim function requires persistence function to be
+            launched first.
+        """
+        intervals_result = [[]]
+        if self.pcohptr is not None:
+            intervals_result = self.pcohptr.intervals_in_dimension(dimension)
+        else:
+            print("intervals_in_dim function requires persistence function"
+                  " to be launched first.")
+        return np.array(intervals_result)
```

## gtda/externals/python/periodic_cubical_complex_interface.py

 * *Ordering differences only*

```diff
@@ -1,156 +1,156 @@
-import os
-import numpy as np
-from ..modules.gtda_periodic_cubical_complex import \
-    Periodic_cubical_complex_base_interface, \
-    Periodic_cubical_complex_persistence_interface
-
-
-# PeriodicCubicalComplex python interface
-class PeriodicCubicalComplex:
-
-    """The PeriodicCubicalComplex is an example of a structured complex useful
-    in computational mathematics (especially rigorous numerics) and image
-    analysis.
-    """
-    def __init__(self, dimensions=None, top_dimensional_cells=None,
-                 periodic_dimensions=None, perseus_file=''):
-        """PeriodicCubicalComplex constructor from dimensions and
-        top_dimensional_cells or from a Perseus-style file name.
-        :param dimensions: A list of number of top dimensional cells.
-        :type dimensions: list of int
-        :param top_dimensional_cells: A list of cells filtration values.
-        :type top_dimensional_cells: list of double
-        :param periodic_dimensions: A list of top dimensional cells periodicity
-        value.
-        :type periodic_dimensions: list of boolean
-        Or
-        :param perseus_file: A Perseus-style file name.
-        :type perseus_file: string
-        """
-        self.thisptr = None
-        self.pcohptr = None
-        if (dimensions is not None) and (top_dimensional_cells is not None) \
-           and (periodic_dimensions is not None) and (perseus_file == ''):
-            self.thisptr = \
-                Periodic_cubical_complex_base_interface(dimensions,
-                                                        top_dimensional_cells,
-                                                        periodic_dimensions)
-        elif (dimensions is None) and (top_dimensional_cells is None) and \
-             (periodic_dimensions is None) and (perseus_file != ''):
-            if os.path.isfile(perseus_file):
-                self.thisptr = \
-                    Periodic_cubical_complex_base_interface(
-                        str.encode(perseus_file))
-            else:
-                print("file " + perseus_file + " not found.")
-        else:
-            print("CubicalComplex can be constructed from dimensions and "
-                  "top_dimensional_cells or from a Perseus-style file name.")
-
-    def __del__(self):
-        if self.thisptr is not None:
-            del self.thisptr
-        if self.pcohptr is not None:
-            del self.pcohptr
-
-    def __is_defined(self):
-        """Returns true if PeriodicCubicalComplex pointer is not NULL.
-         """
-        if self.thisptr is not None:
-            return True
-        return False
-
-    def __is_persistence_defined(self):
-        """Returns true if Persistence pointer is not NULL.
-         """
-        if self.pcohptr is not None:
-            return True
-        return False
-
-    def num_simplices(self):
-        """This function returns the number of all cubes in the complex.
-        :returns:  int -- the number of all cubes in the complex.
-        """
-        return self.thisptr.num_simplices()
-
-    def dimension(self):
-        """This function returns the dimension of the complex.
-        :returns:  int -- the complex dimension.
-        """
-        return self.thisptr.dimension()
-
-    def persistence(self, homology_coeff_field=11, min_persistence=0):
-        """This function returns the persistence of the complex.
-        :param homology_coeff_field: The homology coefficient field. Must be a
-            prime number
-        :type homology_coeff_field: int.
-        :param min_persistence: The minimum persistence value to take into
-            account (strictly greater than min_persistence). Default value is
-            0.0.
-            Sets min_persistence to -1.0 to see all values.
-        :type min_persistence: float.
-        :returns: list of pairs(dimension, pair(birth, death)) -- the
-            persistence of the complex.
-        """
-        if self.pcohptr is not None:
-            del self.pcohptr
-        if self.thisptr is not None:
-            self.pcohptr = \
-                Periodic_cubical_complex_persistence_interface(self.thisptr,
-                                                               True)
-        persistence_result = []
-        if self.pcohptr is not None:
-            self.pcohptr.compute_persistence(homology_coeff_field,
-                                             min_persistence)
-            persistence_result = self.pcohptr.get_persistence()
-        return persistence_result
-
-    def betti_numbers(self):
-        """This function returns the Betti numbers of the complex.
-        :returns: list of int -- The Betti numbers ([B0, B1, ..., Bn]).
-        :note: betti_numbers function requires persistence function to be
-            launched first.
-        :note: betti_numbers function always returns [1, 0, 0, ...] as infinity
-            filtration cubes are not removed from the complex.
-        """
-        bn_result = []
-        if self.pcohptr is not None:
-            bn_result = self.pcohptr.betti_numbers()
-        return bn_result
-
-    def persistent_betti_numbers(self, from_value, to_value):
-        """This function returns the persistent Betti numbers of the complex.
-        :param from_value: The persistence birth limit to be added in the
-            numbers (persistent birth <= from_value).
-        :type from_value: float.
-        :param to_value: The persistence death limit to be added in the
-            numbers (persistent death > to_value).
-        :type to_value: float.
-        :returns: list of int -- The persistent Betti numbers ([B0, B1, ...,
-            Bn]).
-        :note: persistent_betti_numbers function requires persistence
-            function to be launched first.
-        """
-        pbn_result = []
-        if self.pcohptr is not None:
-            pbn_result = \
-                self.pcohptr.persistent_betti_numbers(from_value, to_value)
-        return pbn_result
-
-    def persistence_intervals_in_dimension(self, dimension):
-        """This function returns the persistence intervals of the complex in a
-        specific dimension.
-        :param dimension: The specific dimension.
-        :type dimension: int.
-        :returns: The persistence intervals.
-        :rtype:  numpy array of dimension 2
-        :note: intervals_in_dim function requires persistence function to be
-            launched first.
-        """
-        intervals_result = []
-        if self.pcohptr is not None:
-            intervals_result = self.pcohptr.intervals_in_dimension(dimension)
-        else:
-            print("intervals_in_dim function requires persistence function"
-                  " to be launched first.")
-        return np.array(intervals_result)
+import os
+import numpy as np
+from ..modules.gtda_periodic_cubical_complex import \
+    Periodic_cubical_complex_base_interface, \
+    Periodic_cubical_complex_persistence_interface
+
+
+# PeriodicCubicalComplex python interface
+class PeriodicCubicalComplex:
+
+    """The PeriodicCubicalComplex is an example of a structured complex useful
+    in computational mathematics (especially rigorous numerics) and image
+    analysis.
+    """
+    def __init__(self, dimensions=None, top_dimensional_cells=None,
+                 periodic_dimensions=None, perseus_file=''):
+        """PeriodicCubicalComplex constructor from dimensions and
+        top_dimensional_cells or from a Perseus-style file name.
+        :param dimensions: A list of number of top dimensional cells.
+        :type dimensions: list of int
+        :param top_dimensional_cells: A list of cells filtration values.
+        :type top_dimensional_cells: list of double
+        :param periodic_dimensions: A list of top dimensional cells periodicity
+        value.
+        :type periodic_dimensions: list of boolean
+        Or
+        :param perseus_file: A Perseus-style file name.
+        :type perseus_file: string
+        """
+        self.thisptr = None
+        self.pcohptr = None
+        if (dimensions is not None) and (top_dimensional_cells is not None) \
+           and (periodic_dimensions is not None) and (perseus_file == ''):
+            self.thisptr = \
+                Periodic_cubical_complex_base_interface(dimensions,
+                                                        top_dimensional_cells,
+                                                        periodic_dimensions)
+        elif (dimensions is None) and (top_dimensional_cells is None) and \
+             (periodic_dimensions is None) and (perseus_file != ''):
+            if os.path.isfile(perseus_file):
+                self.thisptr = \
+                    Periodic_cubical_complex_base_interface(
+                        str.encode(perseus_file))
+            else:
+                print("file " + perseus_file + " not found.")
+        else:
+            print("CubicalComplex can be constructed from dimensions and "
+                  "top_dimensional_cells or from a Perseus-style file name.")
+
+    def __del__(self):
+        if self.thisptr is not None:
+            del self.thisptr
+        if self.pcohptr is not None:
+            del self.pcohptr
+
+    def __is_defined(self):
+        """Returns true if PeriodicCubicalComplex pointer is not NULL.
+         """
+        if self.thisptr is not None:
+            return True
+        return False
+
+    def __is_persistence_defined(self):
+        """Returns true if Persistence pointer is not NULL.
+         """
+        if self.pcohptr is not None:
+            return True
+        return False
+
+    def num_simplices(self):
+        """This function returns the number of all cubes in the complex.
+        :returns:  int -- the number of all cubes in the complex.
+        """
+        return self.thisptr.num_simplices()
+
+    def dimension(self):
+        """This function returns the dimension of the complex.
+        :returns:  int -- the complex dimension.
+        """
+        return self.thisptr.dimension()
+
+    def persistence(self, homology_coeff_field=11, min_persistence=0):
+        """This function returns the persistence of the complex.
+        :param homology_coeff_field: The homology coefficient field. Must be a
+            prime number
+        :type homology_coeff_field: int.
+        :param min_persistence: The minimum persistence value to take into
+            account (strictly greater than min_persistence). Default value is
+            0.0.
+            Sets min_persistence to -1.0 to see all values.
+        :type min_persistence: float.
+        :returns: list of pairs(dimension, pair(birth, death)) -- the
+            persistence of the complex.
+        """
+        if self.pcohptr is not None:
+            del self.pcohptr
+        if self.thisptr is not None:
+            self.pcohptr = \
+                Periodic_cubical_complex_persistence_interface(self.thisptr,
+                                                               True)
+        persistence_result = []
+        if self.pcohptr is not None:
+            self.pcohptr.compute_persistence(homology_coeff_field,
+                                             min_persistence)
+            persistence_result = self.pcohptr.get_persistence()
+        return persistence_result
+
+    def betti_numbers(self):
+        """This function returns the Betti numbers of the complex.
+        :returns: list of int -- The Betti numbers ([B0, B1, ..., Bn]).
+        :note: betti_numbers function requires persistence function to be
+            launched first.
+        :note: betti_numbers function always returns [1, 0, 0, ...] as infinity
+            filtration cubes are not removed from the complex.
+        """
+        bn_result = []
+        if self.pcohptr is not None:
+            bn_result = self.pcohptr.betti_numbers()
+        return bn_result
+
+    def persistent_betti_numbers(self, from_value, to_value):
+        """This function returns the persistent Betti numbers of the complex.
+        :param from_value: The persistence birth limit to be added in the
+            numbers (persistent birth <= from_value).
+        :type from_value: float.
+        :param to_value: The persistence death limit to be added in the
+            numbers (persistent death > to_value).
+        :type to_value: float.
+        :returns: list of int -- The persistent Betti numbers ([B0, B1, ...,
+            Bn]).
+        :note: persistent_betti_numbers function requires persistence
+            function to be launched first.
+        """
+        pbn_result = []
+        if self.pcohptr is not None:
+            pbn_result = \
+                self.pcohptr.persistent_betti_numbers(from_value, to_value)
+        return pbn_result
+
+    def persistence_intervals_in_dimension(self, dimension):
+        """This function returns the persistence intervals of the complex in a
+        specific dimension.
+        :param dimension: The specific dimension.
+        :type dimension: int.
+        :returns: The persistence intervals.
+        :rtype:  numpy array of dimension 2
+        :note: intervals_in_dim function requires persistence function to be
+            launched first.
+        """
+        intervals_result = []
+        if self.pcohptr is not None:
+            intervals_result = self.pcohptr.intervals_in_dimension(dimension)
+        else:
+            print("intervals_in_dim function requires persistence function"
+                  " to be launched first.")
+        return np.array(intervals_result)
```

## gtda/externals/python/rips_complex_interface.py

 * *Ordering differences only*

```diff
@@ -1,100 +1,100 @@
-from ..modules.gtda_sparse_rips_complex \
-    import Rips_complex_interface
-from . import SimplexTree
-
-
-# RipsComplex python interface
-class RipsComplex:
-    """The data structure is a one skeleton graph, or Rips graph, containing
-    edges when the edge length is less or equal to a given threshold. Edge
-    length is computed from a user given point cloud with a given distance
-    function, or a distance matrix.
-    """
-    def __init__(self, points=None, distance_matrix=None,
-                 max_edge_length=float('inf')):
-        """RipsComplex constructor.
-        :param max_edge_length: Rips value.
-        :type max_edge_length: float
-        :param points: A list of points in d-Dimension.
-        :type points: list of list of double
-        Or
-        :param distance_matrix: A distance matrix (full square or lower
-            triangular).
-        :type points: list of list of double
-        """
-        self.thisref = Rips_complex_interface()
-
-        if distance_matrix is not None:
-            self.thisref.init_matrix(distance_matrix, max_edge_length)
-        else:
-            if points is None:
-                # Empty Rips construction
-                points = []
-            self.thisref.init_points(points, max_edge_length)
-
-    def create_simplex_tree(self, max_dimension=1):
-        """
-        :param max_dimension: graph expansion for rips until this given maximal
-            dimension.
-        :type max_dimension: int
-        :returns: A simplex tree created from the Delaunay Triangulation.
-        :rtype: SimplexTree
-        """
-        simplex_tree = SimplexTree()
-        self.thisref.create_simplex_tree(simplex_tree.thisptr, max_dimension)
-        return simplex_tree
-
-
-# SparseRipsComplex python interface
-class SparseRipsComplex:
-    """The data structure is a one skeleton graph, or Rips graph, containing
-    edges when the edge length is less or equal to a given threshold. Edge
-    length is computed from a user given point cloud with a given distance
-    function, or a distance matrix.
-    Even truncated in filtration value and dimension, the Rips complex remains
-    quite large. However, it is possible to approximate it by a much smaller
-    filtered simplicial complex
-    (linear size, with constants that depend on  and the doubling dimension of
-    the space) that is (1+O())interleaved with it (in particular, their
-    persistence diagrams are at log-bottleneck distance at most O()).
-    """
-    def __init__(self, points=None, distance_matrix=None,
-                 max_edge_length=float('inf'), sparse=0.0):
-        """SparseRipsComplex constructor.
-        :param max_edge_length: Rips value.
-        :type max_edge_length: float
-        :param points: A list of points in d-Dimension.
-        :type points: list of list of double
-        Or
-        :param distance_matrix: A distance matrix (full square or lower
-            triangular).
-        :type points: list of list of double
-        And in both cases
-        :param sparse: If this is not None, it switches to building a sparse
-            Rips and represents the approximation parameter epsilon.
-        :type sparse: float
-        """
-        self.thisref = Rips_complex_interface()
-
-        if distance_matrix is not None:
-            self.thisref.init_matrix_sparse(distance_matrix,
-                                            max_edge_length,
-                                            sparse)
-        else:
-            if points is None:
-                # Empty Rips construction
-                points = []
-            self.thisref.init_points_sparse(points, max_edge_length,
-                                            sparse)
-
-    def create_simplex_tree(self, max_dimension=1):
-        """
-        :param max_dimension: graph expansion for rips until this given maximal
-            dimension.
-        :type max_dimension: int
-        :returns: A simplex tree created from the Delaunay Triangulation.
-        :rtype: SimplexTree
-        """
-        simplex_tree = SimplexTree()
-        self.thisref.create_simplex_tree(simplex_tree.thisptr, max_dimension)
-        return simplex_tree
+from ..modules.gtda_sparse_rips_complex \
+    import Rips_complex_interface
+from . import SimplexTree
+
+
+# RipsComplex python interface
+class RipsComplex:
+    """The data structure is a one skeleton graph, or Rips graph, containing
+    edges when the edge length is less or equal to a given threshold. Edge
+    length is computed from a user given point cloud with a given distance
+    function, or a distance matrix.
+    """
+    def __init__(self, points=None, distance_matrix=None,
+                 max_edge_length=float('inf')):
+        """RipsComplex constructor.
+        :param max_edge_length: Rips value.
+        :type max_edge_length: float
+        :param points: A list of points in d-Dimension.
+        :type points: list of list of double
+        Or
+        :param distance_matrix: A distance matrix (full square or lower
+            triangular).
+        :type points: list of list of double
+        """
+        self.thisref = Rips_complex_interface()
+
+        if distance_matrix is not None:
+            self.thisref.init_matrix(distance_matrix, max_edge_length)
+        else:
+            if points is None:
+                # Empty Rips construction
+                points = []
+            self.thisref.init_points(points, max_edge_length)
+
+    def create_simplex_tree(self, max_dimension=1):
+        """
+        :param max_dimension: graph expansion for rips until this given maximal
+            dimension.
+        :type max_dimension: int
+        :returns: A simplex tree created from the Delaunay Triangulation.
+        :rtype: SimplexTree
+        """
+        simplex_tree = SimplexTree()
+        self.thisref.create_simplex_tree(simplex_tree.thisptr, max_dimension)
+        return simplex_tree
+
+
+# SparseRipsComplex python interface
+class SparseRipsComplex:
+    """The data structure is a one skeleton graph, or Rips graph, containing
+    edges when the edge length is less or equal to a given threshold. Edge
+    length is computed from a user given point cloud with a given distance
+    function, or a distance matrix.
+    Even truncated in filtration value and dimension, the Rips complex remains
+    quite large. However, it is possible to approximate it by a much smaller
+    filtered simplicial complex
+    (linear size, with constants that depend on  and the doubling dimension of
+    the space) that is (1+O())interleaved with it (in particular, their
+    persistence diagrams are at log-bottleneck distance at most O()).
+    """
+    def __init__(self, points=None, distance_matrix=None,
+                 max_edge_length=float('inf'), sparse=0.0):
+        """SparseRipsComplex constructor.
+        :param max_edge_length: Rips value.
+        :type max_edge_length: float
+        :param points: A list of points in d-Dimension.
+        :type points: list of list of double
+        Or
+        :param distance_matrix: A distance matrix (full square or lower
+            triangular).
+        :type points: list of list of double
+        And in both cases
+        :param sparse: If this is not None, it switches to building a sparse
+            Rips and represents the approximation parameter epsilon.
+        :type sparse: float
+        """
+        self.thisref = Rips_complex_interface()
+
+        if distance_matrix is not None:
+            self.thisref.init_matrix_sparse(distance_matrix,
+                                            max_edge_length,
+                                            sparse)
+        else:
+            if points is None:
+                # Empty Rips construction
+                points = []
+            self.thisref.init_points_sparse(points, max_edge_length,
+                                            sparse)
+
+    def create_simplex_tree(self, max_dimension=1):
+        """
+        :param max_dimension: graph expansion for rips until this given maximal
+            dimension.
+        :type max_dimension: int
+        :returns: A simplex tree created from the Delaunay Triangulation.
+        :rtype: SimplexTree
+        """
+        simplex_tree = SimplexTree()
+        self.thisref.create_simplex_tree(simplex_tree.thisptr, max_dimension)
+        return simplex_tree
```

## gtda/externals/python/simplex_tree_interface.py

 * *Ordering differences only*

```diff
@@ -1,410 +1,410 @@
-import numpy as np
-from ..modules.gtda_simplex_tree import *
-
-
-class SimplexTree:
-    """The simplex tree is an efficient and flexible data structure for
-    representing general (filtered) simplicial complexes. The data structure
-    is described in Jean-Daniel Boissonnat and Clment Maria. The Simplex
-    Tree: An Efficient Data Structure for General Simplicial Complexes.
-    Algorithmica, pages 122, 2014.
-    This class is a filtered, with keys, and non contiguous vertices version
-    of the simplex tree.
-    """
-    # cdef Simplex_tree_interface_full_featured * thisptr
-    # cdef Simplex_tree_persistence_interface * pcohptr
-
-    # Fake constructor that does nothing but documenting the constructor
-    def __init__(self):
-        "SimplexTree constructor."
-        self.thisptr = Simplex_tree_interface_full_featured()
-        self.pcohptr = None
-
-    def __del__(self):
-        if self.thisptr is not None:
-            del self.thisptr
-        if self.pcohptr is not None:
-            del self.pcohptr
-
-    def __is_defined(self):
-        "Return True if SimplexTree pointer is not NULL."
-        if self.thisptr is not None:
-            return True
-        return False
-
-    def __is_persistence_defined(self):
-        """Return True if Persistence pointer is not NULL."""
-        if self.pcohptr is not None:
-            return True
-        return False
-
-    def filtration(self, simplex):
-        """Return the filtration value for a given N-simplex in this simplicial
-        complex, or +infinity if it is not in the complex.
-        :param simplex: The N-simplex, represented by a list of vertex.
-        :type simplex: list of int.
-        :returns:  The simplicial complex filtration value.
-        :rtype:  float
-        """
-        return self.thisptr.simplex_filtration(simplex)
-
-    def assign_filtration(self, simplex, filtration):
-        """Assign the simplicial complex filtration value for a given
-        N-simplex.
-        :param simplex: The N-simplex, represented by a list of vertex.
-        :type simplex: list of int.
-        :param filtration:  The simplicial complex filtration value.
-        :type filtration:  float
-        """
-        self.thisptr.assign_simplex_filtration(simplex, filtration)
-
-    def initialize_filtration(self):
-        """Initialize and sort the simplicial complex filtration vector.
-        .. note::
-            This function must be launched before
-            :func:`persistence()<gudhi.SimplexTree.persistence>`,
-            :func:`betti_numbers()<gudhi.SimplexTree.betti_numbers>`,
-            :func:`persistent_betti_numbers()<gudhi.SimplexTree.persistent_betti_numbers>`,
-            or :func:`get_filtration()<gudhi.SimplexTree.get_filtration>`
-            after :func:`inserting<gudhi.SimplexTree.insert>` or
-            :func:`removing<gudhi.SimplexTree.remove_maximal_simplex>`
-            simplices.
-        """
-        self.thisptr.initialize_filtration()
-
-    def num_vertices(self):
-        """Return the number of vertices of the simplicial complex.
-        :returns:  The simplicial complex number of vertices.
-        :rtype:  int
-        """
-        return self.thisptr.num_vertices()
-
-    def num_simplices(self):
-        """Return the number of simplices of the simplicial complex.
-        :returns:  the simplicial complex number of simplices.
-        :rtype:  int
-        """
-        return self.thisptr.num_simplices()
-
-    def dimension(self):
-        """Return the dimension of the simplicial complex.
-        :returns:  the simplicial complex dimension.
-        :rtype:  int
-        .. note::
-            This function is not constant time because it can recompute
-            dimension if required (can be triggered by
-            :func:`remove_maximal_simplex()<gudhi.SimplexTree.remove_maximal_simplex>`
-            or
-            :func:`prune_above_filtration()<gudhi.SimplexTree.prune_above_filtration>`
-            methods).
-        """
-        return self.thisptr.dimension()
-
-    def upper_bound_dimension(self):
-        """Return a valid dimension upper bound of the simplicial complex.
-        :returns:  an upper bound on the dimension of the simplicial complex.
-        :rtype:  int
-        """
-        return self.thisptr.upper_bound_dimension()
-
-    def set_dimension(self, dimension):
-        """Set the dimension of the simplicial complex.
-        :param dimension: The new dimension value.
-        :type dimension: int.
-        .. note::
-            This function must be used with caution because it disables
-            dimension recomputation when required
-            (this recomputation can be triggered by
-            :func:`remove_maximal_simplex()<gudhi.SimplexTree.remove_maximal_simplex>`
-            or
-            :func:`prune_above_filtration()<gudhi.SimplexTree.prune_above_filtration>`
-            ).
-        """
-        self.thisptr.set_dimension(dimension)
-
-    def find(self, simplex):
-        """Return if the N-simplex was found in the simplicial complex or not.
-        :param simplex: The N-simplex to find, represented by a list of vertex.
-        :type simplex: list of int.
-        :returns:  true if the simplex was found, false otherwise.
-        :rtype:  bool
-        """
-        csimplex = [i for i in simplex]
-        return self.thisptr.find_simplex(csimplex)
-
-    def insert(self, simplex, filtration=0.0):
-        """Insert the given N-simplex and its subfaces with the given
-        filtration value (default value is '0.0'). If some of those simplices
-        are already present with a higher filtration value, their filtration
-        value is lowered.
-        :param simplex: The N-simplex to insert, represented by a list of
-            vertex.
-        :type simplex: list of int.
-        :param filtration: The filtration value of the simplex.
-        :type filtration: float.
-        :returns:  true if the simplex was not yet in the complex, false
-            otherwise (whatever its original filtration value).
-        :rtype:  bool
-        """
-        csimplex = [i for i in simplex]
-        return self.thisptr.insert_simplex_and_subfaces(csimplex,
-                                                        filtration)
-
-    def get_filtration(self):
-        """Return a list of all simplices with their given filtration values.
-        :returns:  The simplices sorted by increasing filtration values.
-        :rtype:  list of tuples(simplex, filtration)
-        """
-        filtration = self.thisptr.get_filtration()
-        ct = []
-        for filtered_complex in filtration:
-            v = [vertex for vertex in filtered_complex[0]]
-            ct.append((v, filtered_complex[1]))
-        return ct
-
-    def get_skeleton(self, dimension):
-        """Return the (simplices of the) skeleton of a maximum given dimension.
-        :param dimension: The skeleton dimension value.
-        :type dimension: int.
-        :returns:  The (simplices of the) skeleton of a maximum dimension.
-        :rtype:  list of tuples(simplex, filtration)
-        """
-        skeleton = self.thisptr.get_skeleton(dimension)
-        ct = []
-        for filtered_simplex in skeleton:
-            v = [vertex for vertex in filtered_simplex[0]]
-            ct.append((v, filtered_simplex[1]))
-        return ct
-
-    def get_star(self, simplex):
-        """Return the star of a given N-simplex.
-        :param simplex: The N-simplex, represented by a list of vertex.
-        :type simplex: list of int.
-        :returns:  The (simplices of the) star of a simplex.
-        :rtype:  list of tuples(simplex, filtration)
-        """
-        csimplex = [i for i in simplex]
-        star = self.thisptr.get_star(csimplex)
-        ct = []
-        for filtered_simplex in star:
-            v = [vertex for vertex in filtered_simplex[0]]
-            ct.append((v, filtered_simplex[1]))
-        return ct
-
-    def get_cofaces(self, simplex, codimension):
-        """Return the cofaces of a given N-simplex with a given codimension.
-        :param simplex: The N-simplex, represented by a list of vertex.
-        :type simplex: list of int.
-        :param codimension: The codimension. If codimension = 0, all cofaces
-            are returned (equivalent of get_star function)
-        :type codimension: int.
-        :returns:  The (simplices of the) cofaces of a simplex
-        :rtype:  list of tuples(simplex, filtration)
-        """
-        csimplex = [i for i in simplex]
-        cofaces = self.thisptr.get_cofaces(csimplex, codimension)
-        ct = []
-        for filtered_simplex in cofaces:
-            v = [vertex for vertex in filtered_simplex[0]]
-            ct.append((v, filtered_simplex[1]))
-        return ct
-
-    def remove_maximal_simplex(self, simplex):
-        """Remove a given maximal N-simplex from the simplicial complex.
-        :param simplex: The N-simplex, represented by a list of vertex.
-        :type simplex: list of int.
-        .. note::
-            Be aware that removing is shifting data in a flat_map
-            (:func:`initialize_filtration()<gudhi.SimplexTree.initialize_filtration>`
-            to be done).
-        .. note::
-            The dimension of the simplicial complex may be lower after calling
-            remove_maximal_simplex than it was before. However,
-            :func:`upper_bound_dimension()<gudhi.SimplexTree.upper_bound_dimension>`
-            method will return the old value, which
-            remains a valid upper bound. If you care, you can call
-            :func:`dimension()<gudhi.SimplexTree.dimension>`
-            to recompute the exact dimension.
-        """
-        self.thisptr.remove_maximal_simplex(simplex)
-
-    def prune_above_filtration(self, filtration):
-        """Prune above filtration value given as parameter.
-        :param filtration: Maximum threshold value.
-        :type filtration: float.
-        :returns: The filtration modification information.
-        :rtype: bool
-        .. note::
-            Some simplex tree functions require the filtration to be valid.
-            prune_above_filtration function is not launching
-            :func:`initialize_filtration()<gudhi.SimplexTree.initialize_filtration>`
-            but returns the filtration modification
-            information. If the complex has changed , please call
-            :func:`initialize_filtration()<gudhi.SimplexTree.initialize_filtration>`
-            to recompute it.
-        .. note::
-            Note that the dimension of the simplicial complex may be lower
-            after calling
-            :func:`prune_above_filtration()<gudhi.SimplexTree.prune_above_filtration>`
-            than it was before. However,
-            :func:`upper_bound_dimension()<gudhi.SimplexTree.upper_bound_dimension>`
-            will return the old value, which remains a
-            valid upper bound. If you care, you can call
-            :func:`dimension()<gudhi.SimplexTree.dimension>`
-            method to recompute the exact dimension.
-        """
-        return self.thisptr.prune_above_filtration(filtration)
-
-    def expansion(self, max_dim):
-        """Expand the Simplex_tree containing only its one skeleton until
-        dimension max_dim.
-        The expanded simplicial complex until dimension :math:`d`
-        attached to a graph :math:`G` is the maximal simplicial complex of
-        dimension at most :math:`d` admitting the graph :math:`G` as
-        :math:`1`-skeleton.
-        The filtration value assigned to a simplex is the maximal filtration
-        value of one of its edges.
-        The Simplex_tree must contain no simplex of dimension bigger than
-        1 when calling the method.
-        :param max_dim: The maximal dimension.
-        :type max_dim: int.
-        """
-        self.thisptr.expansion(max_dim)
-
-    def make_filtration_non_decreasing(self):
-        """Ensure that each simplex has a higher filtration value than its
-        faces by increasing the filtration values.
-        :returns: True if any filtration value was modified,
-        False if the filtration was already non-decreasing.
-        :rtype: bool
-        .. note::
-            Some simplex tree functions require the filtration to be valid.
-            make_filtration_non_decreasing function is not launching
-            :func:`initialize_filtration()<gudhi.SimplexTree.initialize_filtration>`
-            but returns the filtration modification
-            information. If the complex has changed , please call
-            :func:`initialize_filtration()<gudhi.SimplexTree.initialize_filtration>`
-            to recompute it.
-        """
-        return self.thisptr.make_filtration_non_decreasing()
-
-    def persistence(self, homology_coeff_field=11, min_persistence=0,
-                    persistence_dim_max=False):
-        """Return the persistence of the simplicial complex.
-        :param homology_coeff_field: The homology coefficient field. Must be a
-            prime number. Default value is 11.
-        :type homology_coeff_field: int.
-        :param min_persistence: The minimum persistence value to take into
-            account (strictly greater than min_persistence). Default value is
-            0.0.
-            Sets min_persistence to -1.0 to see all values.
-        :type min_persistence: float.
-        :param persistence_dim_max: If true, the persistent homology for the
-            maximal dimension in the complex is computed. If false, it is
-            ignored. Default is false.
-        :type persistence_dim_max: bool
-        :returns: The persistence of the simplicial complex.
-        :rtype:  list of pairs(dimension, pair(birth, death))
-        """
-        if self.pcohptr is not None:
-            del self.pcohptr
-        self.pcohptr = Simplex_tree_persistence_interface(self.thisptr,
-                                                          persistence_dim_max)
-        persistence_result = []
-        if self.pcohptr is not None:
-            self.pcohptr.compute_persistence(homology_coeff_field,
-                                             min_persistence)
-            persistence_result = self.pcohptr.get_persistence()
-        return persistence_result
-
-    def betti_numbers(self):
-        """Return the Betti numbers of the simplicial complex.
-        :returns: The Betti numbers ([B0, B1, ..., Bn]).
-        :rtype:  list of int
-        :note: betti_numbers function requires
-            :func:`persistence()<gudhi.SimplexTree.persistence>`
-            function to be launched first.
-        """
-        bn_result = []
-        if self.pcohptr is not None:
-            bn_result = self.pcohptr.betti_numbers()
-        else:
-            print("`betti_numbers` requires persistence function to be "
-                  "launched first.")
-        return bn_result
-
-    def persistent_betti_numbers(self, from_value, to_value):
-        """Return the persistent Betti numbers of the simplicial complex.
-        :param from_value: The persistence birth limit to be added in the
-            numbers (persistent birth <= from_value).
-        :type from_value: float.
-        :param to_value: The persistence death limit to be added in the
-            numbers (persistent death > to_value).
-        :type to_value: float.
-        :returns: The persistent Betti numbers ([B0, B1, ..., Bn]).
-        :rtype:  list of int
-        :note: persistent_betti_numbers function requires
-            :func:`persistence()<gudhi.SimplexTree.persistence>`
-            function to be launched first.
-        """
-        pbn_result = []
-        if self.pcohptr is not None:
-            pbn_result = self.pcohptr.persistent_betti_numbers(from_value,
-                                                               to_value)
-        else:
-            print("`persistent_betti_numbers` requires persistence function "
-                  "to be launched first.")
-        return pbn_result
-
-    def persistence_intervals_in_dimension(self, dimension):
-        """Return the persistence intervals of the simplicial complex in a
-        specific dimension.
-        :param dimension: The specific dimension.
-        :type dimension: int.
-        :returns: The persistence intervals.
-        :rtype:  numpy array of dimension 2
-        :note: intervals_in_dim function requires
-            :func:`persistence()<gudhi.SimplexTree.persistence>`
-            function to be launched first.
-        """
-        intervals_result = []
-        if self.pcohptr is not None:
-            intervals_result = self.pcohptr.intervals_in_dimension(dimension)
-        else:
-            print("`intervals_in_dim` requires persistence function to be "
-                  "launched first.")
-        return np.array(intervals_result)
-
-    def persistence_pairs(self):
-        """Return a list of persistence birth and death simplex pairs.
-        :returns: A list of persistence simplices intervals.
-        :rtype:  list of pair of list of int
-        :note: persistence_pairs function requires
-            :func:`persistence()<gudhi.SimplexTree.persistence>`
-            function to be launched first.
-        """
-        persistence_pairs_result = []
-        if self.pcohptr is not None:
-            persistence_pairs_result = self.pcohptr.persistence_pairs()
-        else:
-            print("`persistence_pairs` requires persistence function to be "
-                  "launched first.")
-        return persistence_pairs_result
-
-    def write_persistence_diagram(self, persistence_file=''):
-        """Write the persistence intervals of the simplicial complex in a
-        user-given file name.
-        :param persistence_file: The specific dimension.
-        :type persistence_file: string.
-        :note: intervals_in_dim function requires
-            :func:`persistence()<gudhi.SimplexTree.persistence>`
-            function to be launched first.
-        """
-        if self.pcohptr is not None:
-            if persistence_file != '':
-                self.pcohptr.write_output_diagram(str.encode(persistence_file))
-            else:
-                print("`persistence_file` must be specified")
-        else:
-            print("`intervals_in_dim` requires persistence function to be "
-                  "launched first.")
+import numpy as np
+from ..modules.gtda_simplex_tree import *
+
+
+class SimplexTree:
+    """The simplex tree is an efficient and flexible data structure for
+    representing general (filtered) simplicial complexes. The data structure
+    is described in Jean-Daniel Boissonnat and Clment Maria. The Simplex
+    Tree: An Efficient Data Structure for General Simplicial Complexes.
+    Algorithmica, pages 122, 2014.
+    This class is a filtered, with keys, and non contiguous vertices version
+    of the simplex tree.
+    """
+    # cdef Simplex_tree_interface_full_featured * thisptr
+    # cdef Simplex_tree_persistence_interface * pcohptr
+
+    # Fake constructor that does nothing but documenting the constructor
+    def __init__(self):
+        "SimplexTree constructor."
+        self.thisptr = Simplex_tree_interface_full_featured()
+        self.pcohptr = None
+
+    def __del__(self):
+        if self.thisptr is not None:
+            del self.thisptr
+        if self.pcohptr is not None:
+            del self.pcohptr
+
+    def __is_defined(self):
+        "Return True if SimplexTree pointer is not NULL."
+        if self.thisptr is not None:
+            return True
+        return False
+
+    def __is_persistence_defined(self):
+        """Return True if Persistence pointer is not NULL."""
+        if self.pcohptr is not None:
+            return True
+        return False
+
+    def filtration(self, simplex):
+        """Return the filtration value for a given N-simplex in this simplicial
+        complex, or +infinity if it is not in the complex.
+        :param simplex: The N-simplex, represented by a list of vertex.
+        :type simplex: list of int.
+        :returns:  The simplicial complex filtration value.
+        :rtype:  float
+        """
+        return self.thisptr.simplex_filtration(simplex)
+
+    def assign_filtration(self, simplex, filtration):
+        """Assign the simplicial complex filtration value for a given
+        N-simplex.
+        :param simplex: The N-simplex, represented by a list of vertex.
+        :type simplex: list of int.
+        :param filtration:  The simplicial complex filtration value.
+        :type filtration:  float
+        """
+        self.thisptr.assign_simplex_filtration(simplex, filtration)
+
+    def initialize_filtration(self):
+        """Initialize and sort the simplicial complex filtration vector.
+        .. note::
+            This function must be launched before
+            :func:`persistence()<gudhi.SimplexTree.persistence>`,
+            :func:`betti_numbers()<gudhi.SimplexTree.betti_numbers>`,
+            :func:`persistent_betti_numbers()<gudhi.SimplexTree.persistent_betti_numbers>`,
+            or :func:`get_filtration()<gudhi.SimplexTree.get_filtration>`
+            after :func:`inserting<gudhi.SimplexTree.insert>` or
+            :func:`removing<gudhi.SimplexTree.remove_maximal_simplex>`
+            simplices.
+        """
+        self.thisptr.initialize_filtration()
+
+    def num_vertices(self):
+        """Return the number of vertices of the simplicial complex.
+        :returns:  The simplicial complex number of vertices.
+        :rtype:  int
+        """
+        return self.thisptr.num_vertices()
+
+    def num_simplices(self):
+        """Return the number of simplices of the simplicial complex.
+        :returns:  the simplicial complex number of simplices.
+        :rtype:  int
+        """
+        return self.thisptr.num_simplices()
+
+    def dimension(self):
+        """Return the dimension of the simplicial complex.
+        :returns:  the simplicial complex dimension.
+        :rtype:  int
+        .. note::
+            This function is not constant time because it can recompute
+            dimension if required (can be triggered by
+            :func:`remove_maximal_simplex()<gudhi.SimplexTree.remove_maximal_simplex>`
+            or
+            :func:`prune_above_filtration()<gudhi.SimplexTree.prune_above_filtration>`
+            methods).
+        """
+        return self.thisptr.dimension()
+
+    def upper_bound_dimension(self):
+        """Return a valid dimension upper bound of the simplicial complex.
+        :returns:  an upper bound on the dimension of the simplicial complex.
+        :rtype:  int
+        """
+        return self.thisptr.upper_bound_dimension()
+
+    def set_dimension(self, dimension):
+        """Set the dimension of the simplicial complex.
+        :param dimension: The new dimension value.
+        :type dimension: int.
+        .. note::
+            This function must be used with caution because it disables
+            dimension recomputation when required
+            (this recomputation can be triggered by
+            :func:`remove_maximal_simplex()<gudhi.SimplexTree.remove_maximal_simplex>`
+            or
+            :func:`prune_above_filtration()<gudhi.SimplexTree.prune_above_filtration>`
+            ).
+        """
+        self.thisptr.set_dimension(dimension)
+
+    def find(self, simplex):
+        """Return if the N-simplex was found in the simplicial complex or not.
+        :param simplex: The N-simplex to find, represented by a list of vertex.
+        :type simplex: list of int.
+        :returns:  true if the simplex was found, false otherwise.
+        :rtype:  bool
+        """
+        csimplex = [i for i in simplex]
+        return self.thisptr.find_simplex(csimplex)
+
+    def insert(self, simplex, filtration=0.0):
+        """Insert the given N-simplex and its subfaces with the given
+        filtration value (default value is '0.0'). If some of those simplices
+        are already present with a higher filtration value, their filtration
+        value is lowered.
+        :param simplex: The N-simplex to insert, represented by a list of
+            vertex.
+        :type simplex: list of int.
+        :param filtration: The filtration value of the simplex.
+        :type filtration: float.
+        :returns:  true if the simplex was not yet in the complex, false
+            otherwise (whatever its original filtration value).
+        :rtype:  bool
+        """
+        csimplex = [i for i in simplex]
+        return self.thisptr.insert_simplex_and_subfaces(csimplex,
+                                                        filtration)
+
+    def get_filtration(self):
+        """Return a list of all simplices with their given filtration values.
+        :returns:  The simplices sorted by increasing filtration values.
+        :rtype:  list of tuples(simplex, filtration)
+        """
+        filtration = self.thisptr.get_filtration()
+        ct = []
+        for filtered_complex in filtration:
+            v = [vertex for vertex in filtered_complex[0]]
+            ct.append((v, filtered_complex[1]))
+        return ct
+
+    def get_skeleton(self, dimension):
+        """Return the (simplices of the) skeleton of a maximum given dimension.
+        :param dimension: The skeleton dimension value.
+        :type dimension: int.
+        :returns:  The (simplices of the) skeleton of a maximum dimension.
+        :rtype:  list of tuples(simplex, filtration)
+        """
+        skeleton = self.thisptr.get_skeleton(dimension)
+        ct = []
+        for filtered_simplex in skeleton:
+            v = [vertex for vertex in filtered_simplex[0]]
+            ct.append((v, filtered_simplex[1]))
+        return ct
+
+    def get_star(self, simplex):
+        """Return the star of a given N-simplex.
+        :param simplex: The N-simplex, represented by a list of vertex.
+        :type simplex: list of int.
+        :returns:  The (simplices of the) star of a simplex.
+        :rtype:  list of tuples(simplex, filtration)
+        """
+        csimplex = [i for i in simplex]
+        star = self.thisptr.get_star(csimplex)
+        ct = []
+        for filtered_simplex in star:
+            v = [vertex for vertex in filtered_simplex[0]]
+            ct.append((v, filtered_simplex[1]))
+        return ct
+
+    def get_cofaces(self, simplex, codimension):
+        """Return the cofaces of a given N-simplex with a given codimension.
+        :param simplex: The N-simplex, represented by a list of vertex.
+        :type simplex: list of int.
+        :param codimension: The codimension. If codimension = 0, all cofaces
+            are returned (equivalent of get_star function)
+        :type codimension: int.
+        :returns:  The (simplices of the) cofaces of a simplex
+        :rtype:  list of tuples(simplex, filtration)
+        """
+        csimplex = [i for i in simplex]
+        cofaces = self.thisptr.get_cofaces(csimplex, codimension)
+        ct = []
+        for filtered_simplex in cofaces:
+            v = [vertex for vertex in filtered_simplex[0]]
+            ct.append((v, filtered_simplex[1]))
+        return ct
+
+    def remove_maximal_simplex(self, simplex):
+        """Remove a given maximal N-simplex from the simplicial complex.
+        :param simplex: The N-simplex, represented by a list of vertex.
+        :type simplex: list of int.
+        .. note::
+            Be aware that removing is shifting data in a flat_map
+            (:func:`initialize_filtration()<gudhi.SimplexTree.initialize_filtration>`
+            to be done).
+        .. note::
+            The dimension of the simplicial complex may be lower after calling
+            remove_maximal_simplex than it was before. However,
+            :func:`upper_bound_dimension()<gudhi.SimplexTree.upper_bound_dimension>`
+            method will return the old value, which
+            remains a valid upper bound. If you care, you can call
+            :func:`dimension()<gudhi.SimplexTree.dimension>`
+            to recompute the exact dimension.
+        """
+        self.thisptr.remove_maximal_simplex(simplex)
+
+    def prune_above_filtration(self, filtration):
+        """Prune above filtration value given as parameter.
+        :param filtration: Maximum threshold value.
+        :type filtration: float.
+        :returns: The filtration modification information.
+        :rtype: bool
+        .. note::
+            Some simplex tree functions require the filtration to be valid.
+            prune_above_filtration function is not launching
+            :func:`initialize_filtration()<gudhi.SimplexTree.initialize_filtration>`
+            but returns the filtration modification
+            information. If the complex has changed , please call
+            :func:`initialize_filtration()<gudhi.SimplexTree.initialize_filtration>`
+            to recompute it.
+        .. note::
+            Note that the dimension of the simplicial complex may be lower
+            after calling
+            :func:`prune_above_filtration()<gudhi.SimplexTree.prune_above_filtration>`
+            than it was before. However,
+            :func:`upper_bound_dimension()<gudhi.SimplexTree.upper_bound_dimension>`
+            will return the old value, which remains a
+            valid upper bound. If you care, you can call
+            :func:`dimension()<gudhi.SimplexTree.dimension>`
+            method to recompute the exact dimension.
+        """
+        return self.thisptr.prune_above_filtration(filtration)
+
+    def expansion(self, max_dim):
+        """Expand the Simplex_tree containing only its one skeleton until
+        dimension max_dim.
+        The expanded simplicial complex until dimension :math:`d`
+        attached to a graph :math:`G` is the maximal simplicial complex of
+        dimension at most :math:`d` admitting the graph :math:`G` as
+        :math:`1`-skeleton.
+        The filtration value assigned to a simplex is the maximal filtration
+        value of one of its edges.
+        The Simplex_tree must contain no simplex of dimension bigger than
+        1 when calling the method.
+        :param max_dim: The maximal dimension.
+        :type max_dim: int.
+        """
+        self.thisptr.expansion(max_dim)
+
+    def make_filtration_non_decreasing(self):
+        """Ensure that each simplex has a higher filtration value than its
+        faces by increasing the filtration values.
+        :returns: True if any filtration value was modified,
+        False if the filtration was already non-decreasing.
+        :rtype: bool
+        .. note::
+            Some simplex tree functions require the filtration to be valid.
+            make_filtration_non_decreasing function is not launching
+            :func:`initialize_filtration()<gudhi.SimplexTree.initialize_filtration>`
+            but returns the filtration modification
+            information. If the complex has changed , please call
+            :func:`initialize_filtration()<gudhi.SimplexTree.initialize_filtration>`
+            to recompute it.
+        """
+        return self.thisptr.make_filtration_non_decreasing()
+
+    def persistence(self, homology_coeff_field=11, min_persistence=0,
+                    persistence_dim_max=False):
+        """Return the persistence of the simplicial complex.
+        :param homology_coeff_field: The homology coefficient field. Must be a
+            prime number. Default value is 11.
+        :type homology_coeff_field: int.
+        :param min_persistence: The minimum persistence value to take into
+            account (strictly greater than min_persistence). Default value is
+            0.0.
+            Sets min_persistence to -1.0 to see all values.
+        :type min_persistence: float.
+        :param persistence_dim_max: If true, the persistent homology for the
+            maximal dimension in the complex is computed. If false, it is
+            ignored. Default is false.
+        :type persistence_dim_max: bool
+        :returns: The persistence of the simplicial complex.
+        :rtype:  list of pairs(dimension, pair(birth, death))
+        """
+        if self.pcohptr is not None:
+            del self.pcohptr
+        self.pcohptr = Simplex_tree_persistence_interface(self.thisptr,
+                                                          persistence_dim_max)
+        persistence_result = []
+        if self.pcohptr is not None:
+            self.pcohptr.compute_persistence(homology_coeff_field,
+                                             min_persistence)
+            persistence_result = self.pcohptr.get_persistence()
+        return persistence_result
+
+    def betti_numbers(self):
+        """Return the Betti numbers of the simplicial complex.
+        :returns: The Betti numbers ([B0, B1, ..., Bn]).
+        :rtype:  list of int
+        :note: betti_numbers function requires
+            :func:`persistence()<gudhi.SimplexTree.persistence>`
+            function to be launched first.
+        """
+        bn_result = []
+        if self.pcohptr is not None:
+            bn_result = self.pcohptr.betti_numbers()
+        else:
+            print("`betti_numbers` requires persistence function to be "
+                  "launched first.")
+        return bn_result
+
+    def persistent_betti_numbers(self, from_value, to_value):
+        """Return the persistent Betti numbers of the simplicial complex.
+        :param from_value: The persistence birth limit to be added in the
+            numbers (persistent birth <= from_value).
+        :type from_value: float.
+        :param to_value: The persistence death limit to be added in the
+            numbers (persistent death > to_value).
+        :type to_value: float.
+        :returns: The persistent Betti numbers ([B0, B1, ..., Bn]).
+        :rtype:  list of int
+        :note: persistent_betti_numbers function requires
+            :func:`persistence()<gudhi.SimplexTree.persistence>`
+            function to be launched first.
+        """
+        pbn_result = []
+        if self.pcohptr is not None:
+            pbn_result = self.pcohptr.persistent_betti_numbers(from_value,
+                                                               to_value)
+        else:
+            print("`persistent_betti_numbers` requires persistence function "
+                  "to be launched first.")
+        return pbn_result
+
+    def persistence_intervals_in_dimension(self, dimension):
+        """Return the persistence intervals of the simplicial complex in a
+        specific dimension.
+        :param dimension: The specific dimension.
+        :type dimension: int.
+        :returns: The persistence intervals.
+        :rtype:  numpy array of dimension 2
+        :note: intervals_in_dim function requires
+            :func:`persistence()<gudhi.SimplexTree.persistence>`
+            function to be launched first.
+        """
+        intervals_result = []
+        if self.pcohptr is not None:
+            intervals_result = self.pcohptr.intervals_in_dimension(dimension)
+        else:
+            print("`intervals_in_dim` requires persistence function to be "
+                  "launched first.")
+        return np.array(intervals_result)
+
+    def persistence_pairs(self):
+        """Return a list of persistence birth and death simplex pairs.
+        :returns: A list of persistence simplices intervals.
+        :rtype:  list of pair of list of int
+        :note: persistence_pairs function requires
+            :func:`persistence()<gudhi.SimplexTree.persistence>`
+            function to be launched first.
+        """
+        persistence_pairs_result = []
+        if self.pcohptr is not None:
+            persistence_pairs_result = self.pcohptr.persistence_pairs()
+        else:
+            print("`persistence_pairs` requires persistence function to be "
+                  "launched first.")
+        return persistence_pairs_result
+
+    def write_persistence_diagram(self, persistence_file=''):
+        """Write the persistence intervals of the simplicial complex in a
+        user-given file name.
+        :param persistence_file: The specific dimension.
+        :type persistence_file: string.
+        :note: intervals_in_dim function requires
+            :func:`persistence()<gudhi.SimplexTree.persistence>`
+            function to be launched first.
+        """
+        if self.pcohptr is not None:
+            if persistence_file != '':
+                self.pcohptr.write_output_diagram(str.encode(persistence_file))
+            else:
+                print("`persistence_file` must be specified")
+        else:
+            print("`intervals_in_dim` requires persistence function to be "
+                  "launched first.")
```

## gtda/externals/python/strong_witness_complex_interface.py

 * *Ordering differences only*

```diff
@@ -1,53 +1,53 @@
-from ..modules.gtda_strong_witness_complex \
-    import Strong_witness_complex_interface
-from . import SimplexTree
-
-
-# StrongWitnessComplex python interface
-class StrongWitnessComplex:
-    """Constructs (strong) witness complex for a given table of nearest
-    landmarks with respect to witnesses.
-    """
-
-    def __init__(self, nearest_landmark_table=None):
-        """StrongWitnessComplex constructor.
-        :param nearest_landmark_table: A list of lists of nearest landmarks and
-        their distances.  `nearest_landmark_table[w][k]==(l,d)` means that l is
-        the k-th nearest landmark to
-            witness w, and d is the (squared) distance between l and w.
-        :type nearest_landmark_table: list of list of pair of int and float
-        """
-        self.thisptr = None
-
-        if nearest_landmark_table is not None:
-            self.thisptr = \
-                Strong_witness_complex_interface(nearest_landmark_table)
-
-    def __del__(self):
-        if self.thisptr is not None:
-            del self.thisptr
-
-    def __is_defined(self):
-        """Returns true if WitnessComplex pointer is not NULL.
-         """
-        if self.thisptr is not None:
-            return True
-        return False
-
-    def create_simplex_tree(self, max_alpha_square=float('inf'),
-                            limit_dimension=-1):
-        """
-        :param max_alpha_square: The maximum relaxation parameter.
-            Default is set to infinity.
-        :type max_alpha_square: float
-        :returns: A simplex tree created from the Delaunay Triangulation.
-        :rtype: SimplexTree
-        """
-        stree = SimplexTree()
-        stree_int_ptr = stree.thisptr
-        if limit_dimension != -1:
-            self.thisptr.create_simplex_tree(stree_int_ptr, max_alpha_square,
-                                             limit_dimension)
-        else:
-            self.thisptr.create_simplex_tree(stree_int_ptr, max_alpha_square)
-        return stree
+from ..modules.gtda_strong_witness_complex \
+    import Strong_witness_complex_interface
+from . import SimplexTree
+
+
+# StrongWitnessComplex python interface
+class StrongWitnessComplex:
+    """Constructs (strong) witness complex for a given table of nearest
+    landmarks with respect to witnesses.
+    """
+
+    def __init__(self, nearest_landmark_table=None):
+        """StrongWitnessComplex constructor.
+        :param nearest_landmark_table: A list of lists of nearest landmarks and
+        their distances.  `nearest_landmark_table[w][k]==(l,d)` means that l is
+        the k-th nearest landmark to
+            witness w, and d is the (squared) distance between l and w.
+        :type nearest_landmark_table: list of list of pair of int and float
+        """
+        self.thisptr = None
+
+        if nearest_landmark_table is not None:
+            self.thisptr = \
+                Strong_witness_complex_interface(nearest_landmark_table)
+
+    def __del__(self):
+        if self.thisptr is not None:
+            del self.thisptr
+
+    def __is_defined(self):
+        """Returns true if WitnessComplex pointer is not NULL.
+         """
+        if self.thisptr is not None:
+            return True
+        return False
+
+    def create_simplex_tree(self, max_alpha_square=float('inf'),
+                            limit_dimension=-1):
+        """
+        :param max_alpha_square: The maximum relaxation parameter.
+            Default is set to infinity.
+        :type max_alpha_square: float
+        :returns: A simplex tree created from the Delaunay Triangulation.
+        :rtype: SimplexTree
+        """
+        stree = SimplexTree()
+        stree_int_ptr = stree.thisptr
+        if limit_dimension != -1:
+            self.thisptr.create_simplex_tree(stree_int_ptr, max_alpha_square,
+                                             limit_dimension)
+        else:
+            self.thisptr.create_simplex_tree(stree_int_ptr, max_alpha_square)
+        return stree
```

## gtda/externals/python/witness_complex_interface.py

 * *Ordering differences only*

```diff
@@ -1,53 +1,53 @@
-from ..modules.gtda_witness_complex \
-    import Witness_complex_interface
-from . import SimplexTree
-
-
-# WitnessComplex python interface
-class WitnessComplex:
-    """Constructs (weak) witness complex for a given table of nearest landmarks
-    with respect to witnesses.
-    """
-
-    def __init__(self, nearest_landmark_table=None):
-        """WitnessComplex constructor.
-        :param nearest_landmark_table: A list of lists of nearest landmarks and
-        their distances.  `nearest_landmark_table[w][k]==(l,d)` means that l is
-        the k-th nearest landmark to witness w, and d is the (squared) distance
-        between l and w.
-        :type nearest_landmark_table: list of list of pair of int and float
-        """
-
-        self.thisptr = None
-
-        if nearest_landmark_table is not None:
-            self.thisptr = Witness_complex_interface(nearest_landmark_table)
-
-    def __del__(self):
-        if self.thisptr is not None:
-            del self.thisptr
-
-    def __is_defined(self):
-        """Returns true if WitnessComplex pointer is not NULL.
-         """
-        if self.thisptr is not None:
-            return True
-        return False
-
-    def create_simplex_tree(self, max_alpha_square=float('inf'),
-                            limit_dimension=-1):
-        """
-        :param max_alpha_square: The maximum relaxation parameter.
-            Default is set to infinity.
-        :type max_alpha_square: float
-        :returns: A simplex tree created from the Delaunay Triangulation.
-        :rtype: SimplexTree
-        """
-        stree = SimplexTree()
-        stree_int_ptr = stree.thisptr
-        if limit_dimension != -1:
-            self.thisptr.create_simplex_tree(stree_int_ptr, max_alpha_square,
-                                             limit_dimension)
-        else:
-            self.thisptr.create_simplex_tree(stree_int_ptr, max_alpha_square)
-        return stree
+from ..modules.gtda_witness_complex \
+    import Witness_complex_interface
+from . import SimplexTree
+
+
+# WitnessComplex python interface
+class WitnessComplex:
+    """Constructs (weak) witness complex for a given table of nearest landmarks
+    with respect to witnesses.
+    """
+
+    def __init__(self, nearest_landmark_table=None):
+        """WitnessComplex constructor.
+        :param nearest_landmark_table: A list of lists of nearest landmarks and
+        their distances.  `nearest_landmark_table[w][k]==(l,d)` means that l is
+        the k-th nearest landmark to witness w, and d is the (squared) distance
+        between l and w.
+        :type nearest_landmark_table: list of list of pair of int and float
+        """
+
+        self.thisptr = None
+
+        if nearest_landmark_table is not None:
+            self.thisptr = Witness_complex_interface(nearest_landmark_table)
+
+    def __del__(self):
+        if self.thisptr is not None:
+            del self.thisptr
+
+    def __is_defined(self):
+        """Returns true if WitnessComplex pointer is not NULL.
+         """
+        if self.thisptr is not None:
+            return True
+        return False
+
+    def create_simplex_tree(self, max_alpha_square=float('inf'),
+                            limit_dimension=-1):
+        """
+        :param max_alpha_square: The maximum relaxation parameter.
+            Default is set to infinity.
+        :type max_alpha_square: float
+        :returns: A simplex tree created from the Delaunay Triangulation.
+        :rtype: SimplexTree
+        """
+        stree = SimplexTree()
+        stree_int_ptr = stree.thisptr
+        if limit_dimension != -1:
+            self.thisptr.create_simplex_tree(stree_int_ptr, max_alpha_square,
+                                             limit_dimension)
+        else:
+            self.thisptr.create_simplex_tree(stree_int_ptr, max_alpha_square)
+        return stree
```

## gtda/externals/python/__init__.py

 * *Ordering differences only*

```diff
@@ -1,7 +1,7 @@
-from .cubical_complex_interface import CubicalComplex
-from .simplex_tree_interface import SimplexTree
-from .periodic_cubical_complex_interface import PeriodicCubicalComplex
-from .witness_complex_interface import WitnessComplex
-from .strong_witness_complex_interface import StrongWitnessComplex
-from .rips_complex_interface import RipsComplex, SparseRipsComplex
-from .cech_complex_interface import CechComplex
+from .cubical_complex_interface import CubicalComplex
+from .simplex_tree_interface import SimplexTree
+from .periodic_cubical_complex_interface import PeriodicCubicalComplex
+from .witness_complex_interface import WitnessComplex
+from .strong_witness_complex_interface import StrongWitnessComplex
+from .rips_complex_interface import RipsComplex, SparseRipsComplex
+from .cech_complex_interface import CechComplex
```

## gtda/graphs/geodesic_distance.py

 * *Ordering differences only*

```diff
@@ -1,220 +1,220 @@
-"""Graph geodesic distance calculations."""
-# License: GNU AGPLv3
-
-from functools import reduce
-from operator import and_
-from warnings import warn
-
-import numpy as np
-from joblib import Parallel, delayed
-from numpy.ma import masked_invalid
-from numpy.ma.core import MaskedArray
-from scipy.sparse import issparse, isspmatrix_csr
-from scipy.sparse.csgraph import shortest_path
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils.validation import check_is_fitted
-
-from ..base import PlotterMixin
-from ..plotting import plot_heatmap
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.validation import check_graph
-
-
-@adapt_fit_transform_docs
-class GraphGeodesicDistance(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Distance matrices arising from geodesic distances on graphs.
-
-    For each (possibly weighted and/or directed) graph in a collection, this
-    transformer calculates the length of the shortest (directed or undirected)
-    path between any two of its vertices, setting it to ``numpy.inf`` when two
-    vertices cannot be connected by a path.
-
-    The graphs are represented by their adjacency matrices which can be dense
-    arrays, sparse matrices or masked arrays. The following rules apply:
-
-    - In dense arrays of Boolean type, entries which are ``False`` represent
-      absent edges.
-    - In dense arrays of integer or float type, zero entries represent edges
-      of length 0. Absent edges must be indicated by ``numpy.inf``.
-    - In sparse matrices, non-stored values represent absent edges. Explicitly
-      stored zero or ``False`` edges represent edges of length 0.
-
-    Parameters
-    ----------
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    directed : bool, optional, default: ``True``
-        If ``True`` (default), then find the shortest path on a directed graph.
-        If ``False``, then find the shortest path on an undirected graph.
-
-    unweighted : bool, optional, default: ``False``
-        If ``True``, then find unweighted distances. That is, rather than
-        finding the path between each point such that the sum of weights is
-        minimized, find the path such that the number of edges is minimized.
-
-    method : ``'auto'`` | ``'FW'`` | ``'D'`` | ``'BF'`` | ``'J'``, optional, \
-        default: ``'auto'``
-        Algorithm to use for shortest paths. See the `scipy documentation \
-        <https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.\
-        csgraph.shortest_path.html>`_.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from gtda.graphs import TransitionGraph, GraphGeodesicDistance
-    >>> X = np.arange(4).reshape(1, -1, 1)
-    >>> X_tg = TransitionGraph(func=None).fit_transform(X)
-    >>> print(X_tg[0].toarray())
-    [[0 1 0 0]
-     [0 0 1 0]
-     [0 0 0 1]
-     [0 0 0 0]]
-    >>> X_ggd = GraphGeodesicDistance(directed=False).fit_transform(X_tg)
-    >>> print(X_ggd[0])
-    [[0. 1. 2. 3.]
-     [1. 0. 1. 2.]
-     [2. 1. 0. 1.]
-     [3. 2. 1. 0.]]
-
-    See also
-    --------
-    TransitionGraph, KNeighborsGraph
-
-    """
-
-    def __init__(self, n_jobs=None, directed=False, unweighted=False,
-                 method='auto'):
-        self.n_jobs = n_jobs
-        self.directed = directed
-        self.unweighted = unweighted
-        self.method = method
-
-    def _geodesic_distance(self, X, i=None):
-        method_ = self.method
-        if not issparse(X):
-            diag = np.eye(X.shape[0], dtype=bool)
-            if np.any(~np.logical_or(X, diag)):
-                if self.method in ['auto', 'FW']:
-                    if np.any(X < 0):
-                        method_ = 'J'
-                    else:
-                        method_ = 'D'
-                    warn(
-                        f"Methods 'auto' and 'FW' are not supported when "
-                        f"some edge weights are zero. Using '{method_}' "
-                        f"instead for graph {i}."
-                        )
-            if not isinstance(X, MaskedArray):
-                # Convert to a masked array with mask given by positions in
-                # which infs or NaNs occur.
-                if X.dtype != bool:
-                    X = masked_invalid(X)
-        elif X.shape[0] != X.shape[1]:
-            n_vertices = max(X.shape)
-            X = X.copy() if isspmatrix_csr(X) else X.tocsr()
-            X.resize(n_vertices, n_vertices)
-
-        return shortest_path(X, directed=self.directed,
-                             unweighted=self.unweighted, method=method_)
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : list of length n_samples, or ndarray of shape (n_samples, \
-            n_vertices, n_vertices)
-            Input data: a collection of adjacency matrices of graphs. Each
-            adjacency matrix may be a dense or a sparse array.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_graph(X)
-
-        self._is_fitted = True
-        return self
-
-    def transform(self, X, y=None):
-        """Compute the lengths of graph shortest paths between any two
-        vertices.
-
-        Parameters
-        ----------
-        X : list of length n_samples, or ndarray of shape (n_samples, \
-            n_vertices, n_vertices)
-            Input data: a collection of ``n_samples`` adjacency matrices of
-            graphs. Each adjacency matrix may be a dense array, a sparse
-            matrix, or a masked array.
-
-        y : None
-            Ignored.
-
-        Returns
-        -------
-        Xt : list of length n_samples, or ndarray of shape (n_samples, \
-            n_vertices, n_vertices)
-            Output collection of dense distance matrices. If the distance
-            matrices all have the same shape, a single 3D ndarray is returned.
-
-        """
-        check_is_fitted(self, '_is_fitted')
-        X = check_graph(X)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._geodesic_distance)(x, i=i) for i, x in enumerate(X))
-
-        x0_shape = Xt[0].shape
-        if reduce(and_, (x.shape == x0_shape for x in Xt), True):
-            Xt = np.asarray(Xt)
-
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, colorscale='blues', plotly_params=None):
-        """Plot a sample from a collection of distance matrices.
-
-        Parameters
-        ----------
-        Xt : list of length n_samples, or ndarray of shape (n_samples, \
-            n_vertices, n_vertices)
-            Collection of distance matrices, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample to be plotted.
-
-        colorscale : str, optional, default: ``'blues'``
-            Color scale to be used in the heat map. Can be anything allowed by
-            :class:`plotly.graph_objects.Heatmap`.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_heatmap(
-            Xt[sample], colorscale=colorscale,
-            title=f"{sample}-th geodesic distance matrix",
-            plotly_params=plotly_params
-            )
+"""Graph geodesic distance calculations."""
+# License: GNU AGPLv3
+
+from functools import reduce
+from operator import and_
+from warnings import warn
+
+import numpy as np
+from joblib import Parallel, delayed
+from numpy.ma import masked_invalid
+from numpy.ma.core import MaskedArray
+from scipy.sparse import issparse, isspmatrix_csr
+from scipy.sparse.csgraph import shortest_path
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils.validation import check_is_fitted
+
+from ..base import PlotterMixin
+from ..plotting import plot_heatmap
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.validation import check_graph
+
+
+@adapt_fit_transform_docs
+class GraphGeodesicDistance(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Distance matrices arising from geodesic distances on graphs.
+
+    For each (possibly weighted and/or directed) graph in a collection, this
+    transformer calculates the length of the shortest (directed or undirected)
+    path between any two of its vertices, setting it to ``numpy.inf`` when two
+    vertices cannot be connected by a path.
+
+    The graphs are represented by their adjacency matrices which can be dense
+    arrays, sparse matrices or masked arrays. The following rules apply:
+
+    - In dense arrays of Boolean type, entries which are ``False`` represent
+      absent edges.
+    - In dense arrays of integer or float type, zero entries represent edges
+      of length 0. Absent edges must be indicated by ``numpy.inf``.
+    - In sparse matrices, non-stored values represent absent edges. Explicitly
+      stored zero or ``False`` edges represent edges of length 0.
+
+    Parameters
+    ----------
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    directed : bool, optional, default: ``True``
+        If ``True`` (default), then find the shortest path on a directed graph.
+        If ``False``, then find the shortest path on an undirected graph.
+
+    unweighted : bool, optional, default: ``False``
+        If ``True``, then find unweighted distances. That is, rather than
+        finding the path between each point such that the sum of weights is
+        minimized, find the path such that the number of edges is minimized.
+
+    method : ``'auto'`` | ``'FW'`` | ``'D'`` | ``'BF'`` | ``'J'``, optional, \
+        default: ``'auto'``
+        Algorithm to use for shortest paths. See the `scipy documentation \
+        <https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.\
+        csgraph.shortest_path.html>`_.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from gtda.graphs import TransitionGraph, GraphGeodesicDistance
+    >>> X = np.arange(4).reshape(1, -1, 1)
+    >>> X_tg = TransitionGraph(func=None).fit_transform(X)
+    >>> print(X_tg[0].toarray())
+    [[0 1 0 0]
+     [0 0 1 0]
+     [0 0 0 1]
+     [0 0 0 0]]
+    >>> X_ggd = GraphGeodesicDistance(directed=False).fit_transform(X_tg)
+    >>> print(X_ggd[0])
+    [[0. 1. 2. 3.]
+     [1. 0. 1. 2.]
+     [2. 1. 0. 1.]
+     [3. 2. 1. 0.]]
+
+    See also
+    --------
+    TransitionGraph, KNeighborsGraph
+
+    """
+
+    def __init__(self, n_jobs=None, directed=False, unweighted=False,
+                 method='auto'):
+        self.n_jobs = n_jobs
+        self.directed = directed
+        self.unweighted = unweighted
+        self.method = method
+
+    def _geodesic_distance(self, X, i=None):
+        method_ = self.method
+        if not issparse(X):
+            diag = np.eye(X.shape[0], dtype=bool)
+            if np.any(~np.logical_or(X, diag)):
+                if self.method in ['auto', 'FW']:
+                    if np.any(X < 0):
+                        method_ = 'J'
+                    else:
+                        method_ = 'D'
+                    warn(
+                        f"Methods 'auto' and 'FW' are not supported when "
+                        f"some edge weights are zero. Using '{method_}' "
+                        f"instead for graph {i}."
+                        )
+            if not isinstance(X, MaskedArray):
+                # Convert to a masked array with mask given by positions in
+                # which infs or NaNs occur.
+                if X.dtype != bool:
+                    X = masked_invalid(X)
+        elif X.shape[0] != X.shape[1]:
+            n_vertices = max(X.shape)
+            X = X.copy() if isspmatrix_csr(X) else X.tocsr()
+            X.resize(n_vertices, n_vertices)
+
+        return shortest_path(X, directed=self.directed,
+                             unweighted=self.unweighted, method=method_)
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : list of length n_samples, or ndarray of shape (n_samples, \
+            n_vertices, n_vertices)
+            Input data: a collection of adjacency matrices of graphs. Each
+            adjacency matrix may be a dense or a sparse array.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_graph(X)
+
+        self._is_fitted = True
+        return self
+
+    def transform(self, X, y=None):
+        """Compute the lengths of graph shortest paths between any two
+        vertices.
+
+        Parameters
+        ----------
+        X : list of length n_samples, or ndarray of shape (n_samples, \
+            n_vertices, n_vertices)
+            Input data: a collection of ``n_samples`` adjacency matrices of
+            graphs. Each adjacency matrix may be a dense array, a sparse
+            matrix, or a masked array.
+
+        y : None
+            Ignored.
+
+        Returns
+        -------
+        Xt : list of length n_samples, or ndarray of shape (n_samples, \
+            n_vertices, n_vertices)
+            Output collection of dense distance matrices. If the distance
+            matrices all have the same shape, a single 3D ndarray is returned.
+
+        """
+        check_is_fitted(self, '_is_fitted')
+        X = check_graph(X)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._geodesic_distance)(x, i=i) for i, x in enumerate(X))
+
+        x0_shape = Xt[0].shape
+        if reduce(and_, (x.shape == x0_shape for x in Xt), True):
+            Xt = np.asarray(Xt)
+
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, colorscale='blues', plotly_params=None):
+        """Plot a sample from a collection of distance matrices.
+
+        Parameters
+        ----------
+        Xt : list of length n_samples, or ndarray of shape (n_samples, \
+            n_vertices, n_vertices)
+            Collection of distance matrices, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample to be plotted.
+
+        colorscale : str, optional, default: ``'blues'``
+            Color scale to be used in the heat map. Can be anything allowed by
+            :class:`plotly.graph_objects.Heatmap`.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_heatmap(
+            Xt[sample], colorscale=colorscale,
+            title=f"{sample}-th geodesic distance matrix",
+            plotly_params=plotly_params
+            )
```

## gtda/graphs/kneighbors.py

 * *Ordering differences only*

```diff
@@ -1,162 +1,162 @@
-"""kNN graphs from point cloud data."""
-# License: GNU AGPLv3
-
-from functools import partial
-
-from joblib import Parallel, delayed
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.neighbors import kneighbors_graph
-from sklearn.utils.validation import check_is_fitted
-
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.validation import check_point_clouds
-
-
-@adapt_fit_transform_docs
-class KNeighborsGraph(BaseEstimator, TransformerMixin):
-    """Adjacency matrices of :math:`k`-nearest neighbor graphs.
-
-    Given a two-dimensional array of row vectors seen as points in
-    high-dimensional space, the corresponding :math:`k`NN graph is a directed
-    graph with a vertex for every vector in the array, and a directed edge from
-    vertex :math:`i` to vertex :math:`j \\neq i` whenever vector :math:`j` is
-    among the :math:`k` nearest neighbors of vector :math:`i`.
-
-    Parameters
-    ----------
-    n_neighbors : int, optional, default: ``4``
-        Number of neighbors to use. A point is not considered as its own
-        neighbour.
-
-    mode : ``'connectivity'`` | ``'distance'``, optional, \
-        default: ``'connectivity'``
-        Type of returned matrices: ``'connectivity'`` will return the 0-1
-        connectivity matrices, and ``'distance'`` will return the distances
-        between neighbors according to the given metric.
-
-    metric : string or callable, optional, default: ``'euclidean'``
-        The distance metric to use. See the documentation of
-        :class:`sklearn.neighbors.DistanceMetric` for a list of available
-        metrics. If set to ``'precomputed'``, input data is interpreted as a
-        collection of distance matrices.
-
-    p : int, optional, default: ``2``
-        Parameter for the Minkowski (i.e. :math:`\\ell^p`) metric from
-        :func:`sklearn.metrics.pairwise.pairwise_distances`. Only relevant
-        when `metric` is ``'minkowski'``. `p` = 1 is the Manhattan distance,
-        and `p` = 2 reduces to the Euclidean distance.
-
-    metric_params : dict or None, optional, default: ``None``
-        Additional keyword arguments for the metric function.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from gtda.graphs import KNeighborsGraph
-    >>> X = np.array([[[0, 1, 3, 0, 0],
-    ...                [1, 0, 5, 0, 0],
-    ...                [3, 5, 0, 4, 0],
-    ...                [0, 0, 4, 0, 0]]])
-    >>> kng = KNeighborsGraph(n_neighbors=2)
-    >>> Xg = kng.fit_transform(X)
-    >>> print(Xg[0].toarray())
-    [[0. 1. 0. 1.]
-     [1. 0. 0. 1.]
-     [1. 0. 0. 1.]
-     [1. 1. 0. 0.]]
-
-    See also
-    --------
-    TransitionGraph, GraphGeodesicDistance
-
-    Notes
-    -----
-    :func:`sklearn.neighbors.kneighbors_graph` is used to compute the
-    adjacency matrices of kNN graphs.
-
-    """
-
-    def __init__(self, n_neighbors=4, mode='connectivity', metric='euclidean',
-                 p=2, metric_params=None, n_jobs=None):
-        self.n_neighbors = n_neighbors
-        self.mode = mode
-        self.metric = metric
-        self.p = p
-        self.metric_params = metric_params
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : list of length n_samples, or ndarray of shape (n_samples, \
-            n_points, n_dimensions) or (n_samples, n_points, n_points)
-            Input data representing a collection of point clouds. Each entry
-            in `X` is a 2D array of shape ``(n_points, n_dimensions)`` if
-            `metric` is not ``'precomputed'``, or a 2D array or sparse matrix
-            of shape ``(n_points, n_points)`` if `metric` is ``'precomputed'``.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        self._is_precomputed = self.metric == 'precomputed'
-        check_point_clouds(X, accept_sparse=True,
-                           distance_matrices=self._is_precomputed)
-
-        self._is_fitted = True
-        return self
-
-    def transform(self, X, y=None):
-        """Compute kNN graphs and return their adjacency matrices in sparse
-        format.
-
-        Parameters
-        ----------
-        X : list of length n_samples, or ndarray of shape (n_samples, \
-            n_points, n_dimensions) or (n_samples, n_points, n_points)
-            Input data representing a collection of point clouds. Each entry
-            in `X` is a 2D array of shape ``(n_points, n_dimensions)`` if
-            `metric` is not ``'precomputed'``, or a 2D array or sparse matrix
-            of shape ``(n_points, n_points)`` if `metric` is ``'precomputed'``.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : list of length n_samples
-            Adjacency matrices of kNN graphs, in sparse CSR format. The
-            matrices contain ones and zeros if `mode` is ``'connectivity'``,
-            and floats representing distances according to `metric` if `mode`
-            is ``'distance'``.
-
-        """
-        check_is_fitted(self, '_is_fitted')
-        Xt = check_point_clouds(X, accept_sparse=True,
-                                distance_matrices=self._is_precomputed)
-
-        _adjacency_matrix_func = partial(
-            kneighbors_graph, n_neighbors=self.n_neighbors, metric=self.metric,
-            p=self.p, metric_params=self.metric_params, mode=self.mode,
-            include_self=False
-            )
-        Xt = Parallel(n_jobs=self.n_jobs)(delayed(_adjacency_matrix_func)(x)
-                                          for x in Xt)
-
-        return Xt
+"""kNN graphs from point cloud data."""
+# License: GNU AGPLv3
+
+from functools import partial
+
+from joblib import Parallel, delayed
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.neighbors import kneighbors_graph
+from sklearn.utils.validation import check_is_fitted
+
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.validation import check_point_clouds
+
+
+@adapt_fit_transform_docs
+class KNeighborsGraph(BaseEstimator, TransformerMixin):
+    """Adjacency matrices of :math:`k`-nearest neighbor graphs.
+
+    Given a two-dimensional array of row vectors seen as points in
+    high-dimensional space, the corresponding :math:`k`NN graph is a directed
+    graph with a vertex for every vector in the array, and a directed edge from
+    vertex :math:`i` to vertex :math:`j \\neq i` whenever vector :math:`j` is
+    among the :math:`k` nearest neighbors of vector :math:`i`.
+
+    Parameters
+    ----------
+    n_neighbors : int, optional, default: ``4``
+        Number of neighbors to use. A point is not considered as its own
+        neighbour.
+
+    mode : ``'connectivity'`` | ``'distance'``, optional, \
+        default: ``'connectivity'``
+        Type of returned matrices: ``'connectivity'`` will return the 0-1
+        connectivity matrices, and ``'distance'`` will return the distances
+        between neighbors according to the given metric.
+
+    metric : string or callable, optional, default: ``'euclidean'``
+        The distance metric to use. See the documentation of
+        :class:`sklearn.neighbors.DistanceMetric` for a list of available
+        metrics. If set to ``'precomputed'``, input data is interpreted as a
+        collection of distance matrices.
+
+    p : int, optional, default: ``2``
+        Parameter for the Minkowski (i.e. :math:`\\ell^p`) metric from
+        :func:`sklearn.metrics.pairwise.pairwise_distances`. Only relevant
+        when `metric` is ``'minkowski'``. `p` = 1 is the Manhattan distance,
+        and `p` = 2 reduces to the Euclidean distance.
+
+    metric_params : dict or None, optional, default: ``None``
+        Additional keyword arguments for the metric function.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from gtda.graphs import KNeighborsGraph
+    >>> X = np.array([[[0, 1, 3, 0, 0],
+    ...                [1, 0, 5, 0, 0],
+    ...                [3, 5, 0, 4, 0],
+    ...                [0, 0, 4, 0, 0]]])
+    >>> kng = KNeighborsGraph(n_neighbors=2)
+    >>> Xg = kng.fit_transform(X)
+    >>> print(Xg[0].toarray())
+    [[0. 1. 0. 1.]
+     [1. 0. 0. 1.]
+     [1. 0. 0. 1.]
+     [1. 1. 0. 0.]]
+
+    See also
+    --------
+    TransitionGraph, GraphGeodesicDistance
+
+    Notes
+    -----
+    :func:`sklearn.neighbors.kneighbors_graph` is used to compute the
+    adjacency matrices of kNN graphs.
+
+    """
+
+    def __init__(self, n_neighbors=4, mode='connectivity', metric='euclidean',
+                 p=2, metric_params=None, n_jobs=None):
+        self.n_neighbors = n_neighbors
+        self.mode = mode
+        self.metric = metric
+        self.p = p
+        self.metric_params = metric_params
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : list of length n_samples, or ndarray of shape (n_samples, \
+            n_points, n_dimensions) or (n_samples, n_points, n_points)
+            Input data representing a collection of point clouds. Each entry
+            in `X` is a 2D array of shape ``(n_points, n_dimensions)`` if
+            `metric` is not ``'precomputed'``, or a 2D array or sparse matrix
+            of shape ``(n_points, n_points)`` if `metric` is ``'precomputed'``.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        self._is_precomputed = self.metric == 'precomputed'
+        check_point_clouds(X, accept_sparse=True,
+                           distance_matrices=self._is_precomputed)
+
+        self._is_fitted = True
+        return self
+
+    def transform(self, X, y=None):
+        """Compute kNN graphs and return their adjacency matrices in sparse
+        format.
+
+        Parameters
+        ----------
+        X : list of length n_samples, or ndarray of shape (n_samples, \
+            n_points, n_dimensions) or (n_samples, n_points, n_points)
+            Input data representing a collection of point clouds. Each entry
+            in `X` is a 2D array of shape ``(n_points, n_dimensions)`` if
+            `metric` is not ``'precomputed'``, or a 2D array or sparse matrix
+            of shape ``(n_points, n_points)`` if `metric` is ``'precomputed'``.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : list of length n_samples
+            Adjacency matrices of kNN graphs, in sparse CSR format. The
+            matrices contain ones and zeros if `mode` is ``'connectivity'``,
+            and floats representing distances according to `metric` if `mode`
+            is ``'distance'``.
+
+        """
+        check_is_fitted(self, '_is_fitted')
+        Xt = check_point_clouds(X, accept_sparse=True,
+                                distance_matrices=self._is_precomputed)
+
+        _adjacency_matrix_func = partial(
+            kneighbors_graph, n_neighbors=self.n_neighbors, metric=self.metric,
+            p=self.p, metric_params=self.metric_params, mode=self.mode,
+            include_self=False
+            )
+        Xt = Parallel(n_jobs=self.n_jobs)(delayed(_adjacency_matrix_func)(x)
+                                          for x in Xt)
+
+        return Xt
```

## gtda/graphs/transition.py

```diff
@@ -1,197 +1,197 @@
-"""Construct transition graphs from dynamical systems."""
-# License: GNU AGPLv3
-
-from types import FunctionType
-
-import numpy as np
-from joblib import Parallel, delayed
-from scipy.sparse import csr_matrix
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils.validation import check_is_fitted
-
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.validation import validate_params, check_point_clouds
-
-
-def identity(x):
-    """The identity function."""
-    return x
-
-
-@adapt_fit_transform_docs
-class TransitionGraph(BaseEstimator, TransformerMixin):
-    """Undirected transition graphs from arrays of time-evolving states.
-
-    Let A be a two-dimensional array viewed as a time series (along the row
-    axis) of one-dimensional arrays encoding the "state" of a system. The
-    corresponding *undirected transition graph* (or *network*) has as vertex
-    set the set of all unique states (rows) in A, and there is an edge between
-    vertex i and vertex ji if and only if the state corresponding to vertex
-    j immediately follows the one corresponding to vertex i, somewhere in A.
-
-    Given a collection of two-dimensional arrays, this transformer performs two
-    tasks:
-
-        1. Optionally, it preprocesses the arrays by applying a function row by
-           row to them. This can be used e.g. as a "compression" step to reduce
-           the size of the state space.
-        2. It computes the transition graph of each array as a sparse matrix of
-           zeros and ones.
-
-    Parameters
-    ----------
-    func : None or callable, optional, default: ``numpy.argsort``
-        If a callable, it is the function to be applied to each row of each
-        array as a preprocessing step. Allowed callables are functions mapping
-        1D arrays to 1D arrays of constant length, and must be compatible with
-        :func:`numpy.apply_along_axis`. If ``None``, this function is the
-        identity (no preprocessing). The default is ``numpy.argsort``, which
-        makes the final transition graphs *ordinal partition networks*
-        [1]_ [2]_ [3]_.
-
-    func_params : None or dict, optional, default: ``None``
-        Additional keyword arguments for `func`.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    effective_func_params_ : dict
-        A copy of `func_params` if this was not set to ``None``, otherwise an
-        empty dictionary.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from gtda.graphs import TransitionGraph
-    >>> X = np.array([[[1, 0], [2, 3], [5, 4]],
-    ...               [[5, 4], [5, 4], [5, 4]]])
-    >>> X_tg = TransitionGraph().fit_transform(X)
-    >>> print(X_tg[0].toarray())
-    [[0 1]
-     [1 0]]
-    >>> print(X_tg[1].toarray())
-    [[0]]
-
-    See also
-    --------
-    KNeighborsGraph, GraphGeodesicDistance
-
-    Notes
-    -----
-    In general, the shapes of the sparse matrices output by :meth:`transform`
-    will be different across samples, and the same row or column index will
-    refer to different states in different samples.
-
-    References
-    ----------
-    .. [1] M. Small, "Complex networks from time series: Capturing dynamics",
-           *2013 IEEE International Symposium on Circuits and Systems
-           (IS-CAS2013)*, 2013; `DOI: 10.1109/iscas.2013.6572389
-           <http://dx.doi.org/10.1109/iscas.2013.6572389>`_.
-
-    .. [2] M. McCullough, M. Small, T. Stemler, and H. Ho-Ching Iu, "Time
-           lagged ordinal partition networks for capturing dynamics of
-           continuous dynamical systems"; *Chaos: An Interdisciplinary Journal
-           of Nonlinear Science* **25** (5), p. 053101, 2015; `DOI:
-           10.1063/1.4919075 <http://dx.doi.org/10.1063/1.4919075>`_.
-
-    .. [3] A. Myers, E. Munch, and F. A. Khasawneh, "Persistent homology of
-           complex networks for dynamic state detection"; *Phys. Rev. E*
-           **100**, 022314, 2019; `DOI: 10.1103/PhysRevE.100.022314
-           <http://dx.doi.org/10.1109/CVPR.2015.7299106>`_.
-
-    """
-
-    _hyperparameters = {'func': {'type': (FunctionType, type(None))},
-                        'func_params': {'type': (dict, type(None))}}
-
-    def __init__(self, func=np.argsort, func_params=None, n_jobs=None):
-        self.func = func
-        self.func_params = func_params
-        self.n_jobs = n_jobs
-
-    def _make_adjacency_matrix(self, X):
-        Xm = np.apply_along_axis(self._func, 1, X)
-        unique_states, Xm = np.unique(Xm, axis=0, return_inverse=True)
-        n_unique_states = len(unique_states)
-        first = Xm[:-1]
-        second = Xm[1:]
-        non_diag_idx = first != second
-        data = np.full(np.sum(non_diag_idx), 1, dtype=int)
-        first = first[non_diag_idx]
-        second = second[non_diag_idx]
-        Xm = csr_matrix((data, (first, second)),
-                        shape=(n_unique_states, n_unique_states))
-        return Xm
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : list of length n_samples, or ndarray of shape (n_samples, \
-            n_timestamps, n_features)
-            Input data: a collection of 2D arrays of shape
-            ``(n_timestamps, n_features)``.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_point_clouds(X)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        if self.func is None:
-            self._func = identity
-        else:
-            self._func = self.func
-
-        if self.func_params is None:
-            self.effective_func_params_ = {}
-        else:
-            self.effective_func_params_ = self.func_params.copy()
-
-        return self
-
-    def transform(self, X, y=None):
-        """Create transition graphs from the input data and return their
-        adjacency matrices. The graphs are simple and unweighted.
-
-        Parameters
-        ----------
-        X : list of length n_samples, or ndarray of shape (n_samples, \
-            n_timestamps, n_features)
-            Input data: a collection of 2D arrays of shape
-            ``(n_timestamps, n_features)``.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : list of length n_samples
-            Collection of ``n_samples`` transition graphs. Each transition
-            graph is encoded by a sparse CSR matrix of ones and zeros.
-
-        """
-        check_is_fitted(self)
-        Xt = check_point_clouds(X)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._make_adjacency_matrix)(x) for x in Xt
-            )
-        return Xt
+"""Construct transition graphs from dynamical systems."""
+# License: GNU AGPLv3
+
+from typing import Callable
+
+import numpy as np
+from joblib import Parallel, delayed
+from scipy.sparse import csr_matrix
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils.validation import check_is_fitted
+
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.validation import validate_params, check_point_clouds
+
+
+def identity(x):
+    """The identity function."""
+    return x
+
+
+@adapt_fit_transform_docs
+class TransitionGraph(BaseEstimator, TransformerMixin):
+    """Undirected transition graphs from arrays of time-evolving states.
+
+    Let A be a two-dimensional array viewed as a time series (along the row
+    axis) of one-dimensional arrays encoding the "state" of a system. The
+    corresponding *undirected transition graph* (or *network*) has as vertex
+    set the set of all unique states (rows) in A, and there is an edge between
+    vertex i and vertex ji if and only if the state corresponding to vertex
+    j immediately follows the one corresponding to vertex i, somewhere in A.
+
+    Given a collection of two-dimensional arrays, this transformer performs two
+    tasks:
+
+        1. Optionally, it preprocesses the arrays by applying a function row by
+           row to them. This can be used e.g. as a "compression" step to reduce
+           the size of the state space.
+        2. It computes the transition graph of each array as a sparse matrix of
+           zeros and ones.
+
+    Parameters
+    ----------
+    func : None or callable, optional, default: ``numpy.argsort``
+        If a callable, it is the function to be applied to each row of each
+        array as a preprocessing step. Allowed callables are functions mapping
+        1D arrays to 1D arrays of constant length, and must be compatible with
+        :func:`numpy.apply_along_axis`. If ``None``, this function is the
+        identity (no preprocessing). The default is ``numpy.argsort``, which
+        makes the final transition graphs *ordinal partition networks*
+        [1]_ [2]_ [3]_.
+
+    func_params : None or dict, optional, default: ``None``
+        Additional keyword arguments for `func`.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    effective_func_params_ : dict
+        A copy of `func_params` if this was not set to ``None``, otherwise an
+        empty dictionary.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from gtda.graphs import TransitionGraph
+    >>> X = np.array([[[1, 0], [2, 3], [5, 4]],
+    ...               [[5, 4], [5, 4], [5, 4]]])
+    >>> X_tg = TransitionGraph().fit_transform(X)
+    >>> print(X_tg[0].toarray())
+    [[0 1]
+     [1 0]]
+    >>> print(X_tg[1].toarray())
+    [[0]]
+
+    See also
+    --------
+    KNeighborsGraph, GraphGeodesicDistance
+
+    Notes
+    -----
+    In general, the shapes of the sparse matrices output by :meth:`transform`
+    will be different across samples, and the same row or column index will
+    refer to different states in different samples.
+
+    References
+    ----------
+    .. [1] M. Small, "Complex networks from time series: Capturing dynamics",
+           *2013 IEEE International Symposium on Circuits and Systems
+           (IS-CAS2013)*, 2013; `DOI: 10.1109/iscas.2013.6572389
+           <http://dx.doi.org/10.1109/iscas.2013.6572389>`_.
+
+    .. [2] M. McCullough, M. Small, T. Stemler, and H. Ho-Ching Iu, "Time
+           lagged ordinal partition networks for capturing dynamics of
+           continuous dynamical systems"; *Chaos: An Interdisciplinary Journal
+           of Nonlinear Science* **25** (5), p. 053101, 2015; `DOI:
+           10.1063/1.4919075 <http://dx.doi.org/10.1063/1.4919075>`_.
+
+    .. [3] A. Myers, E. Munch, and F. A. Khasawneh, "Persistent homology of
+           complex networks for dynamic state detection"; *Phys. Rev. E*
+           **100**, 022314, 2019; `DOI: 10.1103/PhysRevE.100.022314
+           <http://dx.doi.org/10.1109/CVPR.2015.7299106>`_.
+
+    """
+
+    _hyperparameters = {'func': {'type': (Callable, type(None))},
+                        'func_params': {'type': (dict, type(None))}}
+
+    def __init__(self, func=np.argsort, func_params=None, n_jobs=None):
+        self.func = func
+        self.func_params = func_params
+        self.n_jobs = n_jobs
+
+    def _make_adjacency_matrix(self, X):
+        Xm = np.apply_along_axis(self._func, 1, X)
+        unique_states, Xm = np.unique(Xm, axis=0, return_inverse=True)
+        n_unique_states = len(unique_states)
+        first = Xm[:-1]
+        second = Xm[1:]
+        non_diag_idx = first != second
+        data = np.full(np.sum(non_diag_idx), 1, dtype=int)
+        first = first[non_diag_idx]
+        second = second[non_diag_idx]
+        Xm = csr_matrix((data, (first, second)),
+                        shape=(n_unique_states, n_unique_states))
+        return Xm
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : list of length n_samples, or ndarray of shape (n_samples, \
+            n_timestamps, n_features)
+            Input data: a collection of 2D arrays of shape
+            ``(n_timestamps, n_features)``.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_point_clouds(X)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        if self.func is None:
+            self._func = identity
+        else:
+            self._func = self.func
+
+        if self.func_params is None:
+            self.effective_func_params_ = {}
+        else:
+            self.effective_func_params_ = self.func_params.copy()
+
+        return self
+
+    def transform(self, X, y=None):
+        """Create transition graphs from the input data and return their
+        adjacency matrices. The graphs are simple and unweighted.
+
+        Parameters
+        ----------
+        X : list of length n_samples, or ndarray of shape (n_samples, \
+            n_timestamps, n_features)
+            Input data: a collection of 2D arrays of shape
+            ``(n_timestamps, n_features)``.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : list of length n_samples
+            Collection of ``n_samples`` transition graphs. Each transition
+            graph is encoded by a sparse CSR matrix of ones and zeros.
+
+        """
+        check_is_fitted(self)
+        Xt = check_point_clouds(X)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._make_adjacency_matrix)(x) for x in Xt
+            )
+        return Xt
```

## gtda/graphs/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-"""The module :mod:`gtda.graphs` implements transformers to create graphs or
-extract metric spaces from graphs."""
-
-from .geodesic_distance import GraphGeodesicDistance
-from .kneighbors import KNeighborsGraph
-from .transition import TransitionGraph
-
-
-__all__ = [
-    'TransitionGraph',
-    'KNeighborsGraph',
-    'GraphGeodesicDistance'
-    ]
+"""The module :mod:`gtda.graphs` implements transformers to create graphs or
+extract metric spaces from graphs."""
+
+from .geodesic_distance import GraphGeodesicDistance
+from .kneighbors import KNeighborsGraph
+from .transition import TransitionGraph
+
+
+__all__ = [
+    'TransitionGraph',
+    'KNeighborsGraph',
+    'GraphGeodesicDistance'
+    ]
```

## gtda/homology/cubical.py

 * *Ordering differences only*

```diff
@@ -1,259 +1,259 @@
-"""Persistent homology on grids."""
-# License: GNU AGPLv3
-
-from numbers import Real
-
-import numpy as np
-from joblib import Parallel, delayed
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils.validation import check_is_fitted
-
-from ._utils import _postprocess_diagrams
-from ..base import PlotterMixin
-from ..externals.python import CubicalComplex, PeriodicCubicalComplex
-from ..plotting import plot_diagram
-from ..utils.intervals import Interval
-from ..utils.validation import validate_params, check_collection
-
-
-class CubicalPersistence(BaseEstimator, TransformerMixin, PlotterMixin):
-    """:ref:`Persistence diagrams <persistence_diagram>` resulting from
-    :ref:`filtered cubical complexes <cubical_complex>`.
-
-    Given a :ref:`greyscale image <cubical_chains_and_cubical_homology>`,
-    information about the appearance and disappearance of topological features
-    (technically, :ref:`homology classes <homology_and_cohomology>`) of various
-    dimensions and at different scales is summarised in the corresponding
-    persistence diagram.
-
-    **Important note**:
-        - Persistence diagrams produced by this class must be interpreted with
-          care due to the presence of padding triples which carry no
-          information. See :meth:`transform` for additional information.
-
-    Parameters
-    ----------
-    homology_dimensions : list or tuple, optional, default: ``(0, 1)``
-        Dimensions (non-negative integers) of the topological features to be
-        detected.
-
-    coeff : int prime, optional, default: ``2``
-        Compute homology with coefficients in the prime field
-        :math:`\\mathbb{F}_p = \\{ 0, \\ldots, p - 1 \\}` where :math:`p`
-        equals `coeff`.
-
-    periodic_dimensions : boolean ndarray of shape (n_dimensions,) or None, \
-        optional, default: ``None``
-        Periodicity of the boundaries along each of the axes, where
-        ``n_dimensions`` is the dimension of the images of the collection. The
-        boolean in the `d`th position expresses whether the boundaries along
-        the `d`th axis are periodic. The default ``None`` is equivalent to
-        passing ``numpy.zeros((n_dimensions,), dtype=bool)``, i.e. none of the
-        boundaries are periodic.
-
-    infinity_values : float or None, default: ``None``
-        Which death value to assign to features which are still alive at
-        filtration value ``numpy.inf``. ``None`` assigns the maximum pixel
-        values within all images passed to :meth:`fit`.
-
-    reduced_homology : bool, optional, default: ``True``
-       If ``True``, the earliest-born triple in homology dimension 0 which has
-       infinite death is discarded from each diagram computed in
-       :meth:`transform`.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    periodic_dimensions_ : boolean ndarray of shape (n_dimensions,)
-       Effective periodicity of the boundaries along each of the axes. Set in
-       :meth:`fit`.
-
-    infinity_values_ : float
-       Effective death value to assign to features which have infinite
-       persistence. Set in :meth:`fit`.
-
-    See also
-    --------
-    images.HeightFiltration, images.RadialFiltration, \
-    images.DilationFiltration, images.ErosionFiltration, \
-    images.SignedDistanceFiltration
-
-    Notes
-    -----
-    `GUDHI <https://github.com/GUDHI/gudhi-devel>`_ is used as a C++ backend
-    for computing cubical persistent homology [1]_. Python bindings were
-    modified for performance.
-
-    References
-    ----------
-    .. [1] P. Dlotko, "Cubical complex", 2015; `GUDHI User and Reference
-           Manual <http://gudhi.gforge.inria.fr/doc/latest/group__cubical__\
-           complex.html>`_.
-
-    """
-
-    _hyperparameters = {
-        'homology_dimensions': {
-            'type': (list, tuple),
-            'of': {'type': int, 'in': Interval(0, np.inf, closed='left')}
-            },
-        'coeff': {'type': int, 'in': Interval(2, np.inf, closed='left')},
-        'periodic_dimensions': {'type': (np.ndarray, type(None)),
-                                'of': {'type': np.bool_}},
-        'infinity_values': {'type': (Real, type(None))},
-        'reduced_homology': {'type': bool}
-        }
-
-    def __init__(self, homology_dimensions=(0, 1), coeff=2,
-                 periodic_dimensions=None, infinity_values=None,
-                 reduced_homology=True, n_jobs=None):
-        self.homology_dimensions = homology_dimensions
-        self.coeff = coeff
-        self.periodic_dimensions = periodic_dimensions
-        self.infinity_values = infinity_values
-        self.reduced_homology = reduced_homology
-        self.n_jobs = n_jobs
-
-    def _gudhi_diagram(self, X):
-        cubical_complex = self._filtration(
-            dimensions=X.shape,
-            top_dimensional_cells=X.flatten(order="F"),
-            **self._filtration_kwargs
-            )
-        Xdgm = cubical_complex.persistence(homology_coeff_field=self.coeff,
-                                           min_persistence=0)
-
-        return Xdgm
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_1, ..., n_pixels_d)
-            Input data. Array of d-dimensional images.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_collection(X, force_all_finite=False)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        self._filtration_kwargs = {}
-        if self.periodic_dimensions is None or \
-           np.sum(self.periodic_dimensions) == 0:
-            self._filtration = CubicalComplex
-            self.periodic_dimensions_ = np.zeros(len(X) - 1, dtype=bool)
-        else:
-            self._filtration = PeriodicCubicalComplex
-            self.periodic_dimensions_ = np.array(self.periodic_dimensions,
-                                                 dtype=bool)
-            self._filtration_kwargs['periodic_dimensions'] = \
-                self.periodic_dimensions_
-
-        if self.infinity_values is None:
-            if hasattr(X, 'shape'):
-                self.infinity_values_ = np.max(X)
-            else:
-                self.infinity_values_ = max(map(np.max, X))
-        else:
-            self.infinity_values_ = self.infinity_values
-
-        self._homology_dimensions = sorted(self.homology_dimensions)
-        self._max_homology_dimension = self._homology_dimensions[-1]
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each image in `X`, compute the relevant persistence diagram as
-        an array of triples [b, d, q]. Each triple represents a persistent
-        topological feature in dimension q (belonging to `homology_dimensions`)
-        which is born at b and dies at d. Only triples in which b < d are
-        meaningful. Triples in which b and d are equal ("diagonal elements")
-        may be artificially introduced during the computation for padding
-        purposes, since the number of non-trivial persistent topological
-        features is typically not constant across samples. They carry no
-        information and hence should be effectively ignored by any further
-        computation.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_1, ..., n_pixels_d)
-            Input data. Array of d-dimensional images.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Array of persistence diagrams computed from the feature arrays or
-            distance matrices in `X`. ``n_features`` equals
-            :math:`\\sum_q n_q`, where :math:`n_q` is the maximum number of
-            topological features in dimension :math:`q` across all samples in
-            `X`.
-
-        """
-        check_is_fitted(self)
-        Xt = check_collection(X, force_all_finite=False)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(delayed(self._gudhi_diagram)(x)
-                                          for x in Xt)
-
-        Xt = _postprocess_diagrams(
-            Xt, "gudhi", self._homology_dimensions, self.infinity_values_,
-            self.reduced_homology
-            )
-
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
-        """Plot a sample from a collection of persistence diagrams, with
-        homology in multiple dimensions.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Collection of persistence diagrams, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        homology_dimensions : list, tuple or None, optional, default: ``None``
-            Which homology dimensions to include in the plot. ``None`` means
-            plotting all dimensions present in ``Xt[sample]``.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_diagram(
-            Xt[sample], homology_dimensions=homology_dimensions,
-            plotly_params=plotly_params
-            )
+"""Persistent homology on grids."""
+# License: GNU AGPLv3
+
+from numbers import Real
+
+import numpy as np
+from joblib import Parallel, delayed
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils.validation import check_is_fitted
+
+from ._utils import _postprocess_diagrams
+from ..base import PlotterMixin
+from ..externals.python import CubicalComplex, PeriodicCubicalComplex
+from ..plotting import plot_diagram
+from ..utils.intervals import Interval
+from ..utils.validation import validate_params, check_collection
+
+
+class CubicalPersistence(BaseEstimator, TransformerMixin, PlotterMixin):
+    """:ref:`Persistence diagrams <persistence_diagram>` resulting from
+    :ref:`filtered cubical complexes <cubical_complex>`.
+
+    Given a :ref:`greyscale image <cubical_chains_and_cubical_homology>`,
+    information about the appearance and disappearance of topological features
+    (technically, :ref:`homology classes <homology_and_cohomology>`) of various
+    dimensions and at different scales is summarised in the corresponding
+    persistence diagram.
+
+    **Important note**:
+        - Persistence diagrams produced by this class must be interpreted with
+          care due to the presence of padding triples which carry no
+          information. See :meth:`transform` for additional information.
+
+    Parameters
+    ----------
+    homology_dimensions : list or tuple, optional, default: ``(0, 1)``
+        Dimensions (non-negative integers) of the topological features to be
+        detected.
+
+    coeff : int prime, optional, default: ``2``
+        Compute homology with coefficients in the prime field
+        :math:`\\mathbb{F}_p = \\{ 0, \\ldots, p - 1 \\}` where :math:`p`
+        equals `coeff`.
+
+    periodic_dimensions : boolean ndarray of shape (n_dimensions,) or None, \
+        optional, default: ``None``
+        Periodicity of the boundaries along each of the axes, where
+        ``n_dimensions`` is the dimension of the images of the collection. The
+        boolean in the `d`th position expresses whether the boundaries along
+        the `d`th axis are periodic. The default ``None`` is equivalent to
+        passing ``numpy.zeros((n_dimensions,), dtype=bool)``, i.e. none of the
+        boundaries are periodic.
+
+    infinity_values : float or None, default: ``None``
+        Which death value to assign to features which are still alive at
+        filtration value ``numpy.inf``. ``None`` assigns the maximum pixel
+        values within all images passed to :meth:`fit`.
+
+    reduced_homology : bool, optional, default: ``True``
+       If ``True``, the earliest-born triple in homology dimension 0 which has
+       infinite death is discarded from each diagram computed in
+       :meth:`transform`.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    periodic_dimensions_ : boolean ndarray of shape (n_dimensions,)
+       Effective periodicity of the boundaries along each of the axes. Set in
+       :meth:`fit`.
+
+    infinity_values_ : float
+       Effective death value to assign to features which have infinite
+       persistence. Set in :meth:`fit`.
+
+    See also
+    --------
+    images.HeightFiltration, images.RadialFiltration, \
+    images.DilationFiltration, images.ErosionFiltration, \
+    images.SignedDistanceFiltration
+
+    Notes
+    -----
+    `GUDHI <https://github.com/GUDHI/gudhi-devel>`_ is used as a C++ backend
+    for computing cubical persistent homology [1]_. Python bindings were
+    modified for performance.
+
+    References
+    ----------
+    .. [1] P. Dlotko, "Cubical complex", 2015; `GUDHI User and Reference
+           Manual <http://gudhi.gforge.inria.fr/doc/latest/group__cubical__\
+           complex.html>`_.
+
+    """
+
+    _hyperparameters = {
+        'homology_dimensions': {
+            'type': (list, tuple),
+            'of': {'type': int, 'in': Interval(0, np.inf, closed='left')}
+            },
+        'coeff': {'type': int, 'in': Interval(2, np.inf, closed='left')},
+        'periodic_dimensions': {'type': (np.ndarray, type(None)),
+                                'of': {'type': np.bool_}},
+        'infinity_values': {'type': (Real, type(None))},
+        'reduced_homology': {'type': bool}
+        }
+
+    def __init__(self, homology_dimensions=(0, 1), coeff=2,
+                 periodic_dimensions=None, infinity_values=None,
+                 reduced_homology=True, n_jobs=None):
+        self.homology_dimensions = homology_dimensions
+        self.coeff = coeff
+        self.periodic_dimensions = periodic_dimensions
+        self.infinity_values = infinity_values
+        self.reduced_homology = reduced_homology
+        self.n_jobs = n_jobs
+
+    def _gudhi_diagram(self, X):
+        cubical_complex = self._filtration(
+            dimensions=X.shape,
+            top_dimensional_cells=X.flatten(order="F"),
+            **self._filtration_kwargs
+            )
+        Xdgm = cubical_complex.persistence(homology_coeff_field=self.coeff,
+                                           min_persistence=0)
+
+        return Xdgm
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_1, ..., n_pixels_d)
+            Input data. Array of d-dimensional images.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_collection(X, force_all_finite=False)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        self._filtration_kwargs = {}
+        if self.periodic_dimensions is None or \
+           np.sum(self.periodic_dimensions) == 0:
+            self._filtration = CubicalComplex
+            self.periodic_dimensions_ = np.zeros(len(X) - 1, dtype=bool)
+        else:
+            self._filtration = PeriodicCubicalComplex
+            self.periodic_dimensions_ = np.array(self.periodic_dimensions,
+                                                 dtype=bool)
+            self._filtration_kwargs['periodic_dimensions'] = \
+                self.periodic_dimensions_
+
+        if self.infinity_values is None:
+            if hasattr(X, 'shape'):
+                self.infinity_values_ = np.max(X)
+            else:
+                self.infinity_values_ = max(map(np.max, X))
+        else:
+            self.infinity_values_ = self.infinity_values
+
+        self._homology_dimensions = sorted(self.homology_dimensions)
+        self._max_homology_dimension = self._homology_dimensions[-1]
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each image in `X`, compute the relevant persistence diagram as
+        an array of triples [b, d, q]. Each triple represents a persistent
+        topological feature in dimension q (belonging to `homology_dimensions`)
+        which is born at b and dies at d. Only triples in which b < d are
+        meaningful. Triples in which b and d are equal ("diagonal elements")
+        may be artificially introduced during the computation for padding
+        purposes, since the number of non-trivial persistent topological
+        features is typically not constant across samples. They carry no
+        information and hence should be effectively ignored by any further
+        computation.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_1, ..., n_pixels_d)
+            Input data. Array of d-dimensional images.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Array of persistence diagrams computed from the feature arrays or
+            distance matrices in `X`. ``n_features`` equals
+            :math:`\\sum_q n_q`, where :math:`n_q` is the maximum number of
+            topological features in dimension :math:`q` across all samples in
+            `X`.
+
+        """
+        check_is_fitted(self)
+        Xt = check_collection(X, force_all_finite=False)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(delayed(self._gudhi_diagram)(x)
+                                          for x in Xt)
+
+        Xt = _postprocess_diagrams(
+            Xt, "gudhi", self._homology_dimensions, self.infinity_values_,
+            self.reduced_homology
+            )
+
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
+        """Plot a sample from a collection of persistence diagrams, with
+        homology in multiple dimensions.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Collection of persistence diagrams, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        homology_dimensions : list, tuple or None, optional, default: ``None``
+            Which homology dimensions to include in the plot. ``None`` means
+            plotting all dimensions present in ``Xt[sample]``.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_diagram(
+            Xt[sample], homology_dimensions=homology_dimensions,
+            plotly_params=plotly_params
+            )
```

## gtda/homology/simplicial.py

```diff
@@ -1,1766 +1,1766 @@
-"""Persistent homology on point clouds or finite metric spaces."""
-# License: GNU AGPLv3
-
-from numbers import Real, Integral
-from types import FunctionType
-
-import numpy as np
-from gph import ripser_parallel as ripser
-from joblib import Parallel, delayed
-from pyflagser import flagser_weighted
-from scipy.sparse import coo_matrix
-from scipy.spatial import Delaunay
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.metrics.pairwise import pairwise_distances
-from sklearn.utils.validation import check_is_fitted
-
-from ._utils import _postprocess_diagrams
-from ..base import PlotterMixin
-from ..externals.python import SparseRipsComplex, CechComplex
-from ..plotting import plot_diagram
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.intervals import Interval
-from ..utils.validation import validate_params, check_point_clouds
-
-_AVAILABLE_RIPS_WEIGHTS = {
-    "DTM": {
-        "p": {"type": Real, "in": [1, 2, np.inf]},
-        "r": {"type": Real, "in": Interval(0, np.inf, closed="right")},
-        "n_neighbors": {"type": Integral,
-                        "in": Interval(1, np.inf, closed="left")}
-        },
-    "general": {
-        "p": {"type": Real, "in": [1, 2, np.inf]},
-        }
-    }
-
-
-@adapt_fit_transform_docs
-class VietorisRipsPersistence(BaseEstimator, TransformerMixin, PlotterMixin):
-    """:ref:`Persistence diagrams <persistence_diagram>` resulting from
-    :ref:`VietorisRips filtrations
-    <vietoris-rips_complex_and_vietoris-rips_persistence>`.
-
-    Given a :ref:`point cloud <distance_matrices_and_point_clouds>` in
-    Euclidean space, an abstract :ref:`metric space
-    <distance_matrices_and_point_clouds>` encoded by a distance matrix, or the
-    adjacency matrix of a weighted undirected graph, information about the
-    appearance and disappearance of topological features (technically,
-    :ref:`homology classes <homology_and_cohomology>`) of various dimensions
-    and at different scales is summarised in the corresponding persistence
-    diagram.
-
-    **Important note**:
-
-        - Persistence diagrams produced by this class must be interpreted with
-          care due to the presence of padding triples which carry no
-          information. See :meth:`transform` for additional information.
-
-    Parameters
-    ----------
-    metric : string or callable, optional, default: ``"euclidean"``
-        If set to ``"precomputed"``, input data is to be interpreted as a
-        collection of distance matrices or of adjacency matrices of weighted
-        undirected graphs. Otherwise, input data is to be interpreted as a
-        collection of point clouds (i.e. feature arrays), and `metric`
-        determines a rule with which to calculate distances between pairs of
-        points (i.e. row vectors). If `metric` is a string, it must be one of
-        the options allowed by :func:`scipy.spatial.distance.pdist` for its
-        metric parameter, or a metric listed in
-        :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`, including
-        ``"euclidean"``, ``"manhattan"`` or ``"cosine"``. If `metric` is a
-        callable, it should take pairs of vectors (1D arrays) as input and, for
-        each two vectors in a pair, it should return a scalar indicating the
-        distance/dissimilarity between them.
-
-    metric_params : dict, optional, default: ``{}``
-        Additional parameters to be passed to the distance function.
-
-    homology_dimensions : list or tuple, optional, default: ``(0, 1)``
-        Dimensions (non-negative integers) of the topological features to be
-        detected.
-
-    coeff : int prime, optional, default: ``2``
-        Compute homology with coefficients in the prime field
-        :math:`\\mathbb{F}_p = \\{ 0, \\ldots, p - 1 \\}` where :math:`p`
-        equals `coeff`.
-
-    collapse_edges : bool, optional, default: ``False``
-        Whether to run the edge collapse algorithm in [2]_ prior to the
-        persistent homology computation (see the Notes). Can reduce the runtime
-        dramatically when the data or the maximum homology dimensions are
-        large.
-
-    max_edge_length : float, optional, default: ``numpy.inf``
-        Maximum value of the VietorisRips filtration parameter. Points whose
-        distance is greater than this value will never be connected by an edge,
-        and topological features at scales larger than this value will not be
-        detected.
-
-    infinity_values : float or None, default: ``None``
-        Which death value to assign to features which are still alive at
-        filtration value `max_edge_length`. ``None`` means that this death
-        value is declared to be equal to `max_edge_length`.
-
-    reduced_homology : bool, optional, default: ``True``
-       If ``True``, the earliest-born triple in homology dimension 0 which has
-       infinite death is discarded from each diagram computed in
-       :meth:`transform`.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    infinity_values_ : float
-        Effective death value to assign to features which are still alive at
-        filtration value `max_edge_length`.
-
-    See also
-    --------
-    WeightedRipsPersistence, FlagserPersistence, SparseRipsPersistence,
-    WeakAlphaPersistence, EuclideanCechPersistence, ConsistentRescaling,
-    ConsecutiveRescaling
-
-    Notes
-    -----
-    `giotto-ph <https://github.com/giotto-ai/giotto-ph>`_ [1]_ is used as a C++
-    backend for computing VietorisRips persistent homology and edge collapses.
-
-    References
-    ----------
-    .. [1] J. Burella Prez et al, "giotto-ph: A Python Library for
-           High-Performance Computation of Persistent Homology of VietorisRips
-           Filtrations", 2021; `arXiv:2107.05412
-           <https://arxiv.org/abs/2107.05412>`_.
-
-    .. [2] J.-D. Boissonnat and S. Pritam, "Edge Collapse and Persistence of
-           Flag Complexes"; in *36th International Symposium on Computational
-           Geometry (SoCG 2020)*, pp. 19:119:15,
-           Schloss Dagstuhl-LeibnizZentrum fr Informatik, 2020;
-           `DOI: 10.4230/LIPIcs.SoCG.2020.19
-           <https://doi.org/10.4230/LIPIcs.SoCG.2020.19>`_.
-
-    """
-
-    _hyperparameters = {
-        "metric": {"type": (str, FunctionType)},
-        "metric_params": {"type": dict},
-        "homology_dimensions": {
-            "type": (list, tuple),
-            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
-            },
-        "collapse_edges": {"type": bool},
-        "coeff": {"type": int, "in": Interval(2, np.inf, closed="left")},
-        "max_edge_length": {"type": Real},
-        "infinity_values": {"type": (Real, type(None))},
-        "reduced_homology": {"type": bool}
-        }
-
-    def __init__(self, metric="euclidean", metric_params={},
-                 homology_dimensions=(0, 1), collapse_edges=False, coeff=2,
-                 max_edge_length=np.inf, infinity_values=None,
-                 reduced_homology=True, n_jobs=None):
-        self.metric = metric
-        self.metric_params = metric_params
-        self.homology_dimensions = homology_dimensions
-        self.collapse_edges = collapse_edges
-        self.coeff = coeff
-        self.max_edge_length = max_edge_length
-        self.infinity_values = infinity_values
-        self.reduced_homology = reduced_homology
-        self.n_jobs = n_jobs
-
-    def _ripser_diagram(self, X):
-        Xdgms = ripser(
-            X, maxdim=self._max_homology_dimension,
-            thresh=self.max_edge_length, coeff=self.coeff, metric=self.metric,
-            metric_params=self.metric_params,
-            collapse_edges=self.collapse_edges
-            )["dgms"]
-
-        return Xdgms
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`infinity_values_`. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray or list of length n_samples
-            Input data representing a collection of point clouds if `metric`
-            was not set to ``"precomputed"``, and of distance matrices or
-            adjacency matrices of weighted undirected graphs otherwise. Can be
-            either a 3D ndarray whose zeroth dimension has size ``n_samples``,
-            or a list containing ``n_samples`` 2D ndarrays/sparse matrices.
-            Point cloud arrays have shape ``(n_points, n_dimensions)``, and if
-            `X` is a list these shapes can vary between point clouds. If
-            `metric` was set to ``"precomputed"``, then:
-
-                - Diagonal entries indicate vertex weights, i.e. the filtration
-                  parameters at which vertices appear.
-                - If entries of `X` are dense, only their upper diagonal
-                  portions (including the diagonal) are considered.
-                - If entries of `X` are sparse, they do not need to be upper
-                  diagonal or symmetric. If only one of entry (i, j) and (j, i)
-                  is stored, its value is taken as the weight of the undirected
-                  edge {i, j}. If both are stored, the value in the upper
-                  diagonal is taken. Off-diagonal entries which are not
-                  explicitly stored are treated as infinite, indicating absent
-                  edges.
-                - Entries of `X` should be compatible with a filtration, i.e.
-                  the value at index (i, j) should be no smaller than the
-                  values at diagonal indices (i, i) and (j, j).
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
-
-        self._is_precomputed = self.metric == "precomputed"
-        check_point_clouds(X, accept_sparse=True,
-                           distance_matrices=self._is_precomputed)
-
-        if self.infinity_values is None:
-            self.infinity_values_ = self.max_edge_length
-        else:
-            self.infinity_values_ = self.infinity_values
-
-        self._homology_dimensions = sorted(self.homology_dimensions)
-        self._max_homology_dimension = self._homology_dimensions[-1]
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each point cloud or distance matrix in `X`, compute the
-        relevant persistence diagram as an array of triples [b, d, q]. Each
-        triple represents a persistent topological feature in dimension q
-        (belonging to `homology_dimensions`) which is born at b and dies at d.
-        Only triples in which b < d are meaningful. Triples in which b and d
-        are equal ("diagonal elements") may be artificially introduced during
-        the computation for padding purposes, since the number of non-trivial
-        persistent topological features is typically not constant across
-        samples. They carry no information and hence should be effectively
-        ignored by any further computation.
-
-        Parameters
-        ----------
-        X : ndarray or list of length n_samples
-            Input data representing a collection of point clouds if `metric`
-            was not set to ``"precomputed"``, and of distance matrices or
-            adjacency matrices of weighted undirected graphs otherwise. Can be
-            either a 3D ndarray whose zeroth dimension has size ``n_samples``,
-            or a list containing ``n_samples`` 2D ndarrays/sparse matrices.
-            Point cloud arrays have shape ``(n_points, n_dimensions)``, and if
-            `X` is a list these shapes can vary between point clouds. If
-            `metric` was set to ``"precomputed"``, then:
-
-                - Diagonal entries indicate vertex weights, i.e. the filtration
-                  parameters at which vertices appear.
-                - If entries of `X` are dense, only their upper diagonal
-                  portions (including the diagonal) are considered.
-                - If entries of `X` are sparse, they do not need to be upper
-                  diagonal or symmetric. If only one of entry (i, j) and (j, i)
-                  is stored, its value is taken as the weight of the undirected
-                  edge {i, j}. If both are stored, the value in the upper
-                  diagonal is taken. Off-diagonal entries which are not
-                  explicitly stored are treated as infinite, indicating absent
-                  edges.
-                - Entries of `X` should be compatible with a filtration, i.e.
-                  the value at index (i, j) should be no smaller than the
-                  values at diagonal indices (i, i) and (j, j).
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Array of persistence diagrams computed from the feature arrays or
-            distance matrices in `X`. ``n_features`` equals
-            :math:`\\sum_q n_q`, where :math:`n_q` is the maximum number of
-            topological features in dimension :math:`q` across all samples in
-            `X`.
-
-        """
-        check_is_fitted(self)
-        X = check_point_clouds(X, accept_sparse=True,
-                               distance_matrices=self._is_precomputed)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._ripser_diagram)(x) for x in X)
-
-        Xt = _postprocess_diagrams(
-            Xt, "ripser", self._homology_dimensions, self.infinity_values_,
-            self.reduced_homology
-            )
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
-        """Plot a sample from a collection of persistence diagrams, with
-        homology in multiple dimensions.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Collection of persistence diagrams, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        homology_dimensions : list, tuple or None, optional, default: ``None``
-            Which homology dimensions to include in the plot. ``None`` means
-            plotting all dimensions present in ``Xt[sample]``.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_diagram(
-            Xt[sample], homology_dimensions=homology_dimensions,
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class WeightedRipsPersistence(BaseEstimator, TransformerMixin, PlotterMixin):
-    """:ref:`Persistence diagrams <persistence_diagram>` resulting from
-    :ref:`weighted VietorisRips filtrations <TODO>` as in [3]_.
-
-    Given a :ref:`point cloud <distance_matrices_and_point_clouds>` in
-    Euclidean space, an abstract :ref:`metric space
-    <distance_matrices_and_point_clouds>` encoded by a distance matrix, or the
-    adjacency matrix of a weighted undirected graph, information about the
-    appearance and disappearance of topological features (technically,
-    :ref:`homology classes <homology_and_cohomology>`) of various dimensions
-    and at different scales is summarised in the corresponding persistence
-    diagram.
-
-    Weighted (Vietoris)Rips filtrations can be useful to highlight topological
-    features against outliers and noise. Among them, the distance-to-measure
-    (DTM) filtration is particularly suited to point clouds due to several
-    favourable properties. This implementation follows the general framework
-    described in [3]_. The idea is that, starting from a way to compute vertex
-    weights :math:`\\{w_i\\}_i` from an input point cloud/distance
-    matrix/adjacency matrix, a modified adjacency matrix is determined whose
-    diagonal entries are the :math:`\\{w_i\\}_i`, and whose edge weights are
-
-    .. math:: w_{ij} = \\begin{cases} \\max\\{ w_i, w_j \\} &\\text{if }
-       2\\mathrm{dist}_{ij} \\leq |w_i^p - w_j^p|^{\\frac{1}{p}}, \\\\
-       t &\\text{otherwise} \\end{cases}
-
-    where :math:`t` is the only positive root of
-
-    .. math:: 2 \\mathrm{dist}_{ij} = (t^p - w_i^p)^\\frac{1}{p} +
-       (t^p - w_j^p)^\\frac{1}{p}
-
-    and :math:`p` is a parameter (see `metric_params`). The modified adjacency
-    matrices are then treated exactly as in :class:`VietorisRipsPersistence`.
-
-    **Important notes**:
-
-        - Vertex and edge weights are twice the ones in [3]_ so that the same
-          results as :class:`VietorisRipsPersistence` are obtained when all
-          vertex weights are zero.
-        - Persistence diagrams produced by this class must be interpreted with
-          care due to the presence of padding triples which carry no
-          information. See :meth:`transform` for additional information.
-
-    Parameters
-    ----------
-    metric : string or callable, optional, default: ``"euclidean"``
-        If set to ``"precomputed"``, input data is to be interpreted as a
-        collection of distance matrices or of adjacency matrices of weighted
-        undirected graphs. Otherwise, input data is to be interpreted as a
-        collection of point clouds (i.e. feature arrays), and `metric`
-        determines a rule with which to calculate distances between pairs of
-        points (i.e. row vectors). If `metric` is a string, it must be one of
-        the options allowed by :func:`scipy.spatial.distance.pdist` for its
-        metric parameter, or a metric listed in
-        :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`, including
-        ``"euclidean"``, ``"manhattan"`` or ``"cosine"``. If `metric` is a
-        callable, it should take pairs of vectors (1D arrays) as input and, for
-        each two vectors in a pair, it should return a scalar indicating the
-        distance/dissimilarity between them.
-
-    metric_params : dict, optional, default: ``{}``
-        Additional parameters to be passed to the distance function.
-
-    homology_dimensions : list or tuple, optional, default: ``(0, 1)``
-        Dimensions (non-negative integers) of the topological features to be
-        detected.
-
-    weights : ``"DTM"`` or callable, optional, default: ``"DTM"``
-        Function that will be applied to each input point cloud/distance
-        matrix/adjacency matrix to compute a 1D array of vertex weights for the
-        the modified adjacency matrices. The default ``"DTM"`` denotes the
-        empirical distance-to-measure function defined, following [3]_, by
-
-        .. math:: w(x) = 2\\left(\\frac{1}{n+1} \\sum_{k=1}^n
-           \\mathrm{dist}(x, x_k)^r\\right)^{1/r}.
-
-        Here, :math:`\\mathrm{dist}` is the distance metric used, :math:`x_k`
-        is the :math:`k`-th :math:`\\mathrm{dist}`-nearest neighbour of
-        :math:`x` (:math:`x` is not considered a neighbour of itself),
-        :math:`n` is the number of nearest neighbors to include, and :math:`r`
-        is a parameter (see `weight_params`). If a callable, it must return
-        non-negative 1D arrays.
-
-    weight_params : dict, optional, default: ``{}``
-        Additional parameters for the weighted filtration. ``"p"`` determines
-        the power to be used in computing edge weights from vertex weights. It
-        can be one of ``1``, ``2`` or ``np.inf`` and defaults to ``1``. If
-        `weights` is ``"DTM"``, the additional keys ``"r"`` (default: ``2``)
-        and ``"n_neighbors"`` (default: ``3``) are available (see `weights`,
-        where the latter corresponds to :math:`n`).
-
-    coeff : int prime, optional, default: ``2``
-        Compute homology with coefficients in the prime field
-        :math:`\\mathbb{F}_p = \\{ 0, \\ldots, p - 1 \\}` where :math:`p`
-        equals `coeff`.
-
-    collapse_edges : bool, optional, default: ``False``
-        Whether to run the edge collapse algorithm in [2]_ prior to the
-        persistent homology computation (see the Notes). Can reduce the runtime
-        dramatically when the data or the maximum homology dimensions are
-        large.
-
-    max_edge_weight : float, optional, default: ``numpy.inf``
-        Maximum value of the filtration parameter in the modified adjacency
-        matrix. Edges with weight greater than this value will be considered
-        absent.
-
-    infinity_values : float or None, default: ``None``
-        Which death value to assign to features which are still alive at
-        filtration value `max_edge_weight`. ``None`` means that this death
-        value is declared to be equal to `max_edge_weight`.
-
-    reduced_homology : bool, optional, default: ``True``
-       If ``True``, the earliest-born triple in homology dimension 0 which has
-       infinite death is discarded from each diagram computed in
-       :meth:`transform`.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    infinity_values_ : float
-        Effective death value to assign to features which are still alive at
-        filtration value `max_edge_weight`.
-
-    effective_weight_params_ : dict
-        Effective parameters involved in computing the weighted Rips
-        filtration.
-
-    See also
-    --------
-    VietorisRipsPersistence, SparseRipsPersistence, FlagserPersistence,
-    WeakAlphaPersistence, EuclideanCechPersistence, ConsistentRescaling,
-    ConsecutiveRescaling
-
-    Notes
-    -----
-    `giotto-ph <https://github.com/giotto-ai/giotto-ph>`_ [1]_ is used as a C++
-    backend for computing VietorisRips persistent homology and edge collapses.
-
-    References
-    ----------
-    .. [1] J. Burella Prez et al, "giotto-ph: A Python Library for
-           High-Performance Computation of Persistent Homology of VietorisRips
-           Filtrations", 2021; `arXiv:2107.05412
-           <https://arxiv.org/abs/2107.05412>`_.
-
-    .. [2] J.-D. Boissonnat and S. Pritam, "Edge Collapse and Persistence of
-           Flag Complexes"; in *36th International Symposium on Computational
-           Geometry (SoCG 2020)*, pp. 19:119:15,
-           Schloss Dagstuhl-LeibnizZentrum fr Informatik, 2020;
-           `DOI: 10.4230/LIPIcs.SoCG.2020.19
-           <https://doi.org/10.4230/LIPIcs.SoCG.2020.19>`_.
-
-    .. [3] H. Anai et al, "DTM-Based Filtrations"; in *Topological Data
-           Analysis* (Abel Symposia, vol 15), Springer, 2020;
-           `DOI: 10.1007/978-3-030-43408-3_2
-           <https://doi.org/10.1007/978-3-030-43408-3_2>`_.
-
-    """
-
-    _hyperparameters = {
-        "metric": {"type": (str, FunctionType)},
-        "metric_params": {"type": dict},
-        "homology_dimensions": {
-            "type": (list, tuple),
-            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
-            },
-        "weights": {"type": (str, FunctionType)},
-        "weight_params": {"type": dict},
-        "collapse_edges": {"type": bool},
-        "coeff": {"type": int, "in": Interval(2, np.inf, closed="left")},
-        "max_edge_weight": {"type": Real},
-        "infinity_values": {"type": (Real, type(None))},
-        "reduced_homology": {"type": bool}
-        }
-
-    def __init__(self, metric="euclidean", metric_params={},
-                 homology_dimensions=(0, 1), weights="DTM", weight_params={},
-                 collapse_edges=False, coeff=2, max_edge_weight=np.inf,
-                 infinity_values=None, reduced_homology=True, n_jobs=None):
-        self.metric = metric
-        self.metric_params = metric_params
-        self.homology_dimensions = homology_dimensions
-        self.weights = weights
-        self.weight_params = weight_params
-        self.collapse_edges = collapse_edges
-        self.coeff = coeff
-        self.max_edge_weight = max_edge_weight
-        self.infinity_values = infinity_values
-        self.reduced_homology = reduced_homology
-        self.n_jobs = n_jobs
-
-    def _ripser_diagram(self, X):
-        if isinstance(self.weights, FunctionType):
-            weights = self.weights(X)
-        else:
-            weights = self.weights
-        Xdgms = ripser(
-            X, maxdim=self._max_homology_dimension,
-            thresh=self.max_edge_weight, coeff=self.coeff, metric=self.metric,
-            metric_params=self.metric_params, weights=weights,
-            weight_params=self.effective_weight_params_,
-            collapse_edges=self.collapse_edges
-            )["dgms"]
-
-        return Xdgms
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`infinity_values_`. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray or list of length n_samples
-            Input data representing a collection of point clouds if `metric`
-            was not set to ``"precomputed"``, and of distance matrices or
-            adjacency matrices of weighted undirected graphs otherwise. Can be
-            either a 3D ndarray whose zeroth dimension has size ``n_samples``,
-            or a list containing ``n_samples`` 2D ndarrays/sparse matrices.
-            Point cloud arrays have shape ``(n_points, n_dimensions)``, and if
-            `X` is a list these shapes can vary between point clouds. If
-            `metric` was set to ``"precomputed"``, then:
-
-                - All entries of `X` should not contain infinities or negative
-                  values (contrary to :class:`VietorisRipsPersistence`).
-                - The diagonals of entries of `X` are ignored (after the vertex
-                  weights are computed, when `weights` is a callable).
-                - If entries of `X` are dense, only their upper diagonal
-                  portions are considered.
-                - If entries of `X` are sparse, they do not need to be upper
-                  diagonal or symmetric. If only one of entry (i, j) and (j, i)
-                  is stored, its value is taken as the weight of the undirected
-                  edge {i, j}. If both are stored, the value in the upper
-                  diagonal is taken. Off-diagonal entries which are not
-                  explicitly stored are treated as infinite, indicating absent
-                  edges.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
-        if isinstance(self.weights, str) and self.weights != "DTM":
-            raise ValueError(f"'{self.weights}' passed for `weights` but the "
-                             f"only allowed string is 'DTM'.")
-        self.effective_weight_params_ = {"p": 1}
-        if self.weights == "DTM":
-            key = "DTM"
-            self.effective_weight_params_.update({"n_neighbors": 3, "r": 2})
-        else:
-            key = "general"
-        if self.weight_params:
-            self.effective_weight_params_.update(self.weight_params)
-            validate_params(self.effective_weight_params_,
-                            _AVAILABLE_RIPS_WEIGHTS[key])
-
-        self._is_precomputed = self.metric == "precomputed"
-        check_point_clouds(X, accept_sparse=True, force_all_finite=True,
-                           distance_matrices=self._is_precomputed)
-
-        if self.infinity_values is None:
-            self.infinity_values_ = self.max_edge_weight
-        else:
-            self.infinity_values_ = self.infinity_values
-
-        self._homology_dimensions = sorted(self.homology_dimensions)
-        self._max_homology_dimension = self._homology_dimensions[-1]
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each point cloud or distance matrix in `X`, compute the
-        relevant persistence diagram as an array of triples [b, d, q]. Each
-        triple represents a persistent topological feature in dimension q
-        (belonging to `homology_dimensions`) which is born at b and dies at d.
-        Only triples in which b < d are meaningful. Triples in which b and d
-        are equal ("diagonal elements") may be artificially introduced during
-        the computation for padding purposes, since the number of non-trivial
-        persistent topological features is typically not constant across
-        samples. They carry no information and hence should be effectively
-        ignored by any further computation.
-
-        Parameters
-        ----------
-        X : ndarray or list of length n_samples
-            Input data representing a collection of point clouds if `metric`
-            was not set to ``"precomputed"``, and of distance matrices or
-            adjacency matrices of weighted undirected graphs otherwise. Can be
-            either a 3D ndarray whose zeroth dimension has size ``n_samples``,
-            or a list containing ``n_samples`` 2D ndarrays/sparse matrices.
-            Point cloud arrays have shape ``(n_points, n_dimensions)``, and if
-            `X` is a list these shapes can vary between point clouds. If
-            `metric` was set to ``"precomputed"``, then:
-
-                - All entries of `X` should not contain infinities or negative
-                  values (contrary to :class:`VietorisRipsPersistence`).
-                - The diagonals of entries of `X` are ignored (after the vertex
-                  weights are computed, when `weights` is a callable).
-                - If entries of `X` are dense, only their upper diagonal
-                  portions are considered.
-                - If entries of `X` are sparse, they do not need to be upper
-                  diagonal or symmetric. If only one of entry (i, j) and (j, i)
-                  is stored, its value is taken as the weight of the undirected
-                  edge {i, j}. If both are stored, the value in the upper
-                  diagonal is taken. Off-diagonal entries which are not
-                  explicitly stored are treated as infinite, indicating absent
-                  edges.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Array of persistence diagrams computed from the feature arrays or
-            distance matrices in `X`. ``n_features`` equals
-            :math:`\\sum_q n_q`, where :math:`n_q` is the maximum number of
-            topological features in dimension :math:`q` across all samples in
-            `X`.
-
-        """
-        check_is_fitted(self)
-        X = check_point_clouds(X, accept_sparse=True, force_all_finite=True,
-                               distance_matrices=self._is_precomputed)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._ripser_diagram)(x) for x in X)
-
-        Xt = _postprocess_diagrams(
-            Xt, "ripser", self._homology_dimensions, self.infinity_values_,
-            self.reduced_homology
-            )
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
-        """Plot a sample from a collection of persistence diagrams, with
-        homology in multiple dimensions.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Collection of persistence diagrams, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        homology_dimensions : list, tuple or None, optional, default: ``None``
-            Which homology dimensions to include in the plot. ``None`` means
-            plotting all dimensions present in ``Xt[sample]``.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_diagram(
-            Xt[sample], homology_dimensions=homology_dimensions,
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class SparseRipsPersistence(BaseEstimator, TransformerMixin, PlotterMixin):
-    """:ref:`Persistence diagrams <persistence_diagram>` resulting from
-    :ref:`Sparse VietorisRips filtrations
-    <vietoris-rips_complex_and_vietoris-rips_persistence>`.
-
-    Given a :ref:`point cloud <distance_matrices_and_point_clouds>` in
-    Euclidean space, or an abstract :ref:`metric space
-    <distance_matrices_and_point_clouds>` encoded by a distance matrix,
-    information about the appearance and disappearance of topological features
-    (technically, :ref:`homology classes <homology_and_cohomology>`) of various
-    dimensions and at different scales is summarised in the corresponding
-    persistence diagram.
-
-    **Important note**:
-
-        - Persistence diagrams produced by this class must be interpreted with
-          care due to the presence of padding triples which carry no
-          information. See :meth:`transform` for additional information.
-
-    Parameters
-    ----------
-    metric : string or callable, optional, default: ``"euclidean"``
-        If set to ``"precomputed"``, input data is to be interpreted as a
-        collection of distance matrices. Otherwise, input data is to be
-        interpreted as a collection of point clouds (i.e. feature arrays), and
-        `metric` determines a rule with which to calculate distances between
-        pairs of instances (i.e. rows) in these arrays. If `metric` is a
-        string, it must be one of the options allowed by
-        :func:`scipy.spatial.distance.pdist` for its metric parameter, or a
-        metric listed in :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`,
-        including "euclidean", "manhattan", or "cosine". If `metric` is a
-        callable, it is called on each pair of instances and the resulting
-        value recorded. The callable should take two arrays from the entry in
-        `X` as input, and return a value indicating the distance between them.
-
-    homology_dimensions : list or tuple, optional, default: ``(0, 1)``
-        Dimensions (non-negative integers) of the topological features to be
-        detected.
-
-    coeff : int prime, optional, default: ``2``
-        Compute homology with coefficients in the prime field
-        :math:`\\mathbb{F}_p = \\{ 0, \\ldots, p - 1 \\}` where :math:`p`
-        equals `coeff`.
-
-    epsilon : float between 0. and 1., optional, default: ``0.1``
-        Parameter controlling the approximation to the exact VietorisRips
-        filtration. If set to `0.`, :class:`SparseRipsPersistence` leads to the
-        same results as :class:`VietorisRipsPersistence` but is slower.
-
-    max_edge_length : float, optional, default: ``numpy.inf``
-        Maximum value of the Sparse Rips filtration parameter. Points whose
-        distance is greater than this value will never be connected by an edge,
-        and topological features at scales larger than this value will not be
-        detected.
-
-    infinity_values : float or None, default: ``None``
-        Which death value to assign to features which are still alive at
-        filtration value `max_edge_length`. ``None`` means that this death
-        value is declared to be equal to `max_edge_length`.
-
-    reduced_homology : bool, optional, default: ``True``
-       If ``True``, the earliest-born triple in homology dimension 0 which has
-       infinite death is discarded from each diagram computed in
-       :meth:`transform`.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    infinity_values_ : float
-        Effective death value to assign to features which are still alive at
-        filtration value `max_edge_length`. Set in :meth:`fit`.
-
-    See also
-    --------
-    VietorisRipsPersistence, WeightedRipsPersistence, FlagserPersistence,
-    WeakAlphaPersistence, EuclideanCechPersistence, ConsistentRescaling,
-    ConsecutiveRescaling
-
-    Notes
-    -----
-    `GUDHI <https://github.com/GUDHI/gudhi-devel>`_ is used as a C++ backend
-    for computing sparse VietorisRips persistent homology [1]_. Python
-    bindings were modified for performance.
-
-    References
-    ----------
-    .. [1] C. Maria, "Persistent Cohomology", 2020; `GUDHI User and Reference
-           Manual <http://gudhi.gforge.inria.fr/doc/3.1.0/group__persistent__\
-           cohomology.html>`_.
-
-    """
-
-    _hyperparameters = {
-        "metric": {"type": (str, FunctionType)},
-        "homology_dimensions": {
-            "type": (list, tuple),
-            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
-            },
-        "coeff": {"type": int, "in": Interval(2, np.inf, closed="left")},
-        "epsilon": {"type": Real, "in": Interval(0, 1, closed="both")},
-        "max_edge_length": {"type": Real},
-        "infinity_values": {"type": (Real, type(None))},
-        "reduced_homology": {"type": bool}
-        }
-
-    def __init__(self, metric="euclidean", homology_dimensions=(0, 1),
-                 coeff=2, epsilon=0.1, max_edge_length=np.inf,
-                 infinity_values=None, reduced_homology=True, n_jobs=None):
-        self.metric = metric
-        self.homology_dimensions = homology_dimensions
-        self.coeff = coeff
-        self.epsilon = epsilon
-        self.max_edge_length = max_edge_length
-        self.infinity_values = infinity_values
-        self.reduced_homology = reduced_homology
-        self.n_jobs = n_jobs
-
-    def _gudhi_diagram(self, X):
-        Xdgm = pairwise_distances(X, metric=self.metric)
-        sparse_rips_complex = SparseRipsComplex(
-            distance_matrix=Xdgm, max_edge_length=self.max_edge_length,
-            sparse=self.epsilon
-            )
-        simplex_tree = sparse_rips_complex.create_simplex_tree(
-            max_dimension=max(self._homology_dimensions) + 1
-            )
-        Xdgm = simplex_tree.persistence(
-            homology_coeff_field=self.coeff, min_persistence=0
-            )
-
-        return Xdgm
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`infinity_values_`. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray or list of length n_samples
-            Input data representing a collection of point clouds if `metric`
-            was not set to ``"precomputed"``, and of distance matrices
-            otherwise. Can be either a 3D ndarray whose zeroth dimension has
-            size ``n_samples``, or a list containing ``n_samples`` 2D ndarrays.
-            Point cloud arrays have shape ``(n_points, n_dimensions)``, and if
-            `X` is a list these shapes can vary between point clouds. If
-            `metric` was set to ``"precomputed"``, each entry of `X` should be
-            compatible with a filtration, i.e. the value at index (i, j) should
-            be no smaller than the values at diagonal indices (i, i) and
-            (j, j).
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
-        self._is_precomputed = self.metric == "precomputed"
-        check_point_clouds(X, accept_sparse=True,
-                           distance_matrices=self._is_precomputed)
-
-        if self.infinity_values is None:
-            self.infinity_values_ = self.max_edge_length
-        else:
-            self.infinity_values_ = self.infinity_values
-
-        self._homology_dimensions = sorted(self.homology_dimensions)
-        self._max_homology_dimension = self._homology_dimensions[-1]
-        return self
-
-    def transform(self, X, y=None):
-        """For each point cloud or distance matrix in `X`, compute the
-        relevant persistence diagram as an array of triples [b, d, q]. Each
-        triple represents a persistent topological feature in dimension q
-        (belonging to `homology_dimensions`) which is born at b and dies at d.
-        Only triples in which b < d are meaningful. Triples in which b and d
-        are equal ("diagonal elements") may be artificially introduced during
-        the computation for padding purposes, since the number of non-trivial
-        persistent topological features is typically not constant across
-        samples. They carry no information and hence should be effectively
-        ignored by any further computation.
-
-        Parameters
-        ----------
-        X : ndarray or list of length n_samples
-            Input data representing a collection of point clouds if `metric`
-            was not set to ``"precomputed"``, and of distance matrices
-            otherwise. Can be either a 3D ndarray whose zeroth dimension has
-            size ``n_samples``, or a list containing ``n_samples`` 2D ndarrays.
-            Point cloud arrays have shape ``(n_points, n_dimensions)``, and if
-            `X` is a list these shapes can vary between point clouds. If
-            `metric` was set to ``"precomputed"``, each entry of `X` should be
-            compatible with a filtration, i.e. the value at index (i, j) should
-            be no smaller than the values at diagonal indices (i, i) and
-            (j, j).
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Array of persistence diagrams computed from the feature arrays or
-            distance matrices in `X`. ``n_features`` equals
-            :math:`\\sum_q n_q`, where :math:`n_q` is the maximum number of
-            topological features in dimension :math:`q` across all samples in
-            `X`.
-
-        """
-        check_is_fitted(self)
-        X = check_point_clouds(X, accept_sparse=True,
-                               distance_matrices=self._is_precomputed)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._gudhi_diagram)(x) for x in X)
-
-        Xt = _postprocess_diagrams(
-            Xt, "gudhi", self._homology_dimensions, self.infinity_values_,
-            self.reduced_homology
-            )
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
-        """Plot a sample from a collection of persistence diagrams, with
-        homology in multiple dimensions.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Collection of persistence diagrams, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        homology_dimensions : list, tuple or None, optional, default: ``None``
-            Which homology dimensions to include in the plot. ``None`` means
-            plotting all dimensions present in ``Xt[sample]``.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_diagram(
-            Xt[sample], homology_dimensions=homology_dimensions,
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class WeakAlphaPersistence(BaseEstimator, TransformerMixin, PlotterMixin):
-    """:ref:`Persistence diagrams <persistence_diagram>` resulting from
-    :ref:`weak alpha filtrations <TODO>`.
-
-    Given a :ref:`point cloud <distance_matrices_and_point_clouds>` in
-    Euclidean space, information about the appearance and disappearance of
-    topological features (technically, :ref:`homology classes
-    <homology_and_cohomology>`) of various dimensions and at different scales
-    is summarised in the corresponding persistence diagram.
-
-    The weak alpha filtration of a point cloud is defined to be the
-    :ref:`VietorisRips filtration
-    <vietoris-rips_complex_and_vietoris-rips_persistence>` of the sparse matrix
-    of Euclidean distances between neighbouring vertices in the Delaunay
-    triangulation of the point cloud. In low dimensions, computing the
-    persistent homology of this filtration can be much faster than computing
-    VietorisRips persistent homology via :class:`VietorisRipsPersistence`.
-
-    **Important note**:
-
-        - Persistence diagrams produced by this class must be interpreted with
-          care due to the presence of padding triples which carry no
-          information. See :meth:`transform` for additional information.
-
-    Parameters
-    ----------
-    homology_dimensions : list or tuple, optional, default: ``(0, 1)``
-        Dimensions (non-negative integers) of the topological features to be
-        detected.
-
-    coeff : int prime, optional, default: ``2``
-        Compute homology with coefficients in the prime field
-        :math:`\\mathbb{F}_p = \\{ 0, \\ldots, p - 1 \\}` where :math:`p`
-        equals `coeff`.
-
-    max_edge_length : float, optional, default: ``numpy.inf``
-        Maximum value of the VietorisRips filtration parameter. Points whose
-        distance is greater than this value will never be connected by an edge,
-        and topological features at scales larger than this value will not be
-        detected.
-
-    infinity_values : float or None, default: ``None``
-        Which death value to assign to features which are still alive at
-        filtration value `max_edge_length`. ``None`` means that this death
-        value is declared to be equal to `max_edge_length`.
-
-    reduced_homology : bool, optional, default: ``True``
-       If ``True``, the earliest-born triple in homology dimension 0 which has
-       infinite death is discarded from each diagram computed in
-       :meth:`transform`.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    infinity_values_ : float
-        Effective death value to assign to features which are still alive at
-        filtration value `max_edge_length`.
-
-    See also
-    --------
-    VietorisRipsPersistence, WeightedRipsPersistence, SparseRipsPersistence,
-    FlagserPersistence, EuclideanCechPersistence
-
-    Notes
-    -----
-    Delaunay triangulation are computed by :class:`scipy.spatial.Delaunay`.
-    `giotto-ph <https://github.com/giotto-ai/giotto-ph>`_ [1]_ is used as a C++
-    backend for computing VietorisRips persistent homology.
-
-    References
-    ----------
-    .. [1] J. Burella Prez et al, "giotto-ph: A Python Library for
-           High-Performance Computation of Persistent Homology of VietorisRips
-           Filtrations", 2021; `arXiv:2107.05412
-           <https://arxiv.org/abs/2107.05412>`_.
-
-    """
-
-    _hyperparameters = {
-        "homology_dimensions": {
-            "type": (list, tuple),
-            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
-            },
-        "coeff": {"type": int, "in": Interval(2, np.inf, closed="left")},
-        "max_edge_length": {"type": Real},
-        "infinity_values": {"type": (Real, type(None))},
-        "reduced_homology": {"type": bool}
-        }
-
-    def __init__(self, homology_dimensions=(0, 1), coeff=2,
-                 max_edge_length=np.inf, infinity_values=None,
-                 reduced_homology=True, n_jobs=None):
-        self.homology_dimensions = homology_dimensions
-        self.coeff = coeff
-        self.max_edge_length = max_edge_length
-        self.infinity_values = infinity_values
-        self.reduced_homology = reduced_homology
-        self.n_jobs = n_jobs
-
-    def _weak_alpha_diagram(self, X):
-        # `indices` will serve as the array of column indices
-        indptr, indices = Delaunay(X).vertex_neighbor_vertices
-
-        # Compute the array of row indices
-        row = np.zeros_like(indices)
-        row[indptr[1:-1]] = 1
-        np.cumsum(row, out=row)
-
-        # We only need the upper diagonal
-        mask = indices > row
-        row, col = row[mask], indices[mask]
-        dists = np.linalg.norm(X[row] - X[col], axis=1)
-        # Note: passing the shape explicitly should not be needed in more
-        # recent versions of C++ ripser
-        n_points = len(X)
-        dm = coo_matrix((dists, (row, col)), shape=(n_points, n_points))
-
-        Xdgms = ripser(dm, maxdim=self._max_homology_dimension,
-                       thresh=self.max_edge_length, coeff=self.coeff,
-                       metric="precomputed")["dgms"]
-
-        return Xdgms
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`infinity_values_`. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray or list of length n_samples
-            Input data representing a collection of point clouds. Can be either
-            a 3D ndarray whose zeroth dimension has size ``n_samples``, or a
-            list containing ``n_samples`` 2D ndarrays. Point cloud arrays have
-            shape ``(n_points, n_dimensions)``, and if `X` is a list these
-            shapes can vary between point clouds.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
-        check_point_clouds(X)
-
-        if self.infinity_values is None:
-            self.infinity_values_ = self.max_edge_length
-        else:
-            self.infinity_values_ = self.infinity_values
-
-        self._homology_dimensions = sorted(self.homology_dimensions)
-        self._max_homology_dimension = self._homology_dimensions[-1]
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each point cloud in `X`, compute the relevant persistence
-        diagram as an array of triples [b, d, q]. Each triple represents a
-        persistent topological feature in dimension q (belonging to
-        `homology_dimensions`) which is born at b and dies at d. Only triples
-        in which b < d are meaningful. Triples in which b and d are equal
-        ("diagonal elements") may be artificially introduced during the
-        computation for padding purposes, since the number of non-trivial
-        persistent topological features is typically not constant across
-        samples. They carry no information and hence should be effectively
-        ignored by any further computation.
-
-        Parameters
-        ----------
-        X : ndarray or list of length n_samples
-            Input data representing a collection of point clouds. Can be either
-            a 3D ndarray whose zeroth dimension has size ``n_samples``, or a
-            list containing ``n_samples`` 2D ndarrays. Point cloud arrays have
-            shape ``(n_points, n_dimensions)``, and if `X` is a list these
-            shapes can vary between point clouds.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Array of persistence diagrams computed from the feature arrays or
-            distance matrices in `X`. ``n_features`` equals
-            :math:`\\sum_q n_q`, where :math:`n_q` is the maximum number of
-            topological features in dimension :math:`q` across all samples in
-            `X`.
-
-        """
-        check_is_fitted(self)
-        X = check_point_clouds(X)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._weak_alpha_diagram)(x) for x in X)
-
-        Xt = _postprocess_diagrams(
-            Xt, "ripser", self._homology_dimensions, self.infinity_values_,
-            self.reduced_homology
-            )
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
-        """Plot a sample from a collection of persistence diagrams, with
-        homology in multiple dimensions.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Collection of persistence diagrams, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        homology_dimensions : list, tuple or None, optional, default: ``None``
-            Which homology dimensions to include in the plot. ``None`` means
-            plotting all dimensions present in ``Xt[sample]``.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_diagram(
-            Xt[sample], homology_dimensions=homology_dimensions,
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class EuclideanCechPersistence(BaseEstimator, TransformerMixin, PlotterMixin):
-    """:ref:`Persistence diagrams <persistence_diagram>` resulting from
-    `Cech filtrations <cech_complex_and_cech_persistence>`_.
-
-    Given a :ref:`point cloud <distance_matrices_and_point_clouds>` in
-    Euclidean space, information about the appearance and disappearance of
-    topological features (technically, :ref:`homology classes
-    <homology_and_cohomology>`) of various dimensions and at different scales
-    is summarised in the corresponding persistence diagram.
-
-    **Important note**:
-
-        - Persistence diagrams produced by this class must be interpreted with
-          care due to the presence of padding triples which carry no
-          information. See :meth:`transform` for additional information.
-
-    Parameters
-    ----------
-    homology_dimensions : list or tuple, optional, default: ``(0, 1)``
-        Dimensions (non-negative integers) of the topological features to be
-        detected.
-
-    coeff : int prime, optional, default: ``2``
-        Compute homology with coefficients in the prime field
-        :math:`\\mathbb{F}_p = \\{ 0, \\ldots, p - 1 \\}` where :math:`p`
-        equals `coeff`.
-
-    max_edge_length : float, optional, default: ``numpy.inf``
-        Maximum value of the Cech filtration parameter. Topological features at
-        scales larger than this value will not be detected.
-
-    infinity_values : float or None, default: ``None``
-        Which death value to assign to features which are still alive at
-        filtration value `max_edge_length`. ``None`` means that this death
-        value is declared to be equal to `max_edge_length`.
-
-    reduced_homology : bool, optional, default: ``True``
-       If ``True``, the earliest-born triple in homology dimension 0 which has
-       infinite death is discarded in :meth:`transform`.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    infinity_values_ : float
-        Effective death value to assign to features which are still alive at
-        filtration value `max_edge_length`.
-
-    See also
-    --------
-    VietorisRipsPersistence, FlagserPersistence, SparseRipsPersistence,
-    WeakAlphaPersistence
-
-    Notes
-    -----
-    `GUDHI <https://github.com/GUDHI/gudhi-devel>`_ is used as a C++ backend
-    for computing Cech persistent homology [1]_. Python bindings were modified
-    for performance.
-
-    References
-    ----------
-    .. [1] C. Maria, "Persistent Cohomology", 2020; `GUDHI User and Reference
-           Manual <http://gudhi.gforge.inria.fr/doc/3.1.0/group__persistent__\
-           cohomology.html>`_.
-
-    """
-
-    _hyperparameters = {
-        "homology_dimensions": {
-            "type": (list, tuple),
-            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
-            },
-        "coeff": {"type": int, "in": Interval(2, np.inf, closed="left")},
-        "max_edge_length": {"type": Real,
-                            "in": Interval(0, np.inf, closed="right")},
-        "infinity_values": {"type": (Real, type(None)),
-                            "in": Interval(0, np.inf, closed="neither")},
-        "reduced_homology": {"type": bool}
-        }
-
-    def __init__(self, homology_dimensions=(0, 1), coeff=2,
-                 max_edge_length=np.inf, infinity_values=None,
-                 reduced_homology=True, n_jobs=None):
-        self.homology_dimensions = homology_dimensions
-        self.coeff = coeff
-        self.max_edge_length = max_edge_length
-        self.infinity_values = infinity_values
-        self.reduced_homology = reduced_homology
-        self.n_jobs = n_jobs
-
-    def _gudhi_diagram(self, X):
-        cech_complex = CechComplex(points=X, max_radius=self.max_edge_length)
-        simplex_tree = cech_complex.create_simplex_tree(
-            max_dimension=max(self._homology_dimensions) + 1
-            )
-        Xdgm = simplex_tree.persistence(homology_coeff_field=self.coeff,
-                                        min_persistence=0)
-
-        return Xdgm
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`infinity_values_`. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray or list of length n_samples
-            Input data representing a collection of point clouds. Can be either
-            a 3D ndarray whose zeroth dimension has size ``n_samples``, or a
-            list containing ``n_samples`` 2D ndarrays. Point cloud arrays have
-            shape ``(n_points, n_dimensions)``, and if `X` is a list these
-            shapes can vary between point clouds.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_point_clouds(X)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
-
-        if self.infinity_values is None:
-            self.infinity_values_ = self.max_edge_length
-        else:
-            self.infinity_values_ = self.infinity_values
-
-        self._homology_dimensions = sorted(self.homology_dimensions)
-        self._max_homology_dimension = self._homology_dimensions[-1]
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each point cloud in `X`, compute the relevant persistence
-        diagram as an array of triples [b, d, q]. Each triple represents a
-        persistent topological feature in dimension q (belonging to
-        `homology_dimensions`) which is born at b and dies at d. Only triples
-        in which b < d are meaningful. Triples in which b and d are equal
-        ("diagonal elements") may be artificially introduced during the
-        computation for padding purposes, since the number of non-trivial
-        persistent topological features is typically not constant across
-        samples. They carry no information and hence should be effectively
-        ignored by any further computation.
-
-        Parameters
-        ----------
-        X : ndarray or list of length n_samples
-            Input data representing a collection of point clouds. Can be either
-            a 3D ndarray whose zeroth dimension has size ``n_samples``, or a
-            list containing ``n_samples`` 2D ndarrays. Point cloud arrays have
-            shape ``(n_points, n_dimensions)``, and if `X` is a list these
-            shapes can vary between point clouds.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Array of persistence diagrams computed from the feature arrays in
-            `X`. ``n_features`` equals :math:`\\sum_q n_q`, where :math:`n_q`
-            is the maximum number of topological features in dimension
-            :math:`q` across all samples in `X`.
-
-        """
-        check_is_fitted(self)
-        X = check_point_clouds(X)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(delayed(self._gudhi_diagram)(x)
-                                          for x in X)
-
-        Xt = _postprocess_diagrams(
-            Xt, "gudhi", self._homology_dimensions, self.infinity_values_,
-            self.reduced_homology
-            )
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
-        """Plot a sample from a collection of persistence diagrams, with
-        homology in multiple dimensions.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Collection of persistence diagrams, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        homology_dimensions : list, tuple or None, optional, default: ``None``
-            Which homology dimensions to include in the plot. ``None`` means
-            plotting all dimensions present in ``Xt[sample]``.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_diagram(
-            Xt[sample], homology_dimensions=homology_dimensions,
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class FlagserPersistence(BaseEstimator, TransformerMixin, PlotterMixin):
-    """:ref:`Persistence diagrams <persistence_diagram>` resulting from
-    :ref:`filtrations <filtered_complex>` of :ref:`directed or undirected flag
-    complexes <clique_and_flag_complexes>` [1]_.
-
-    Given a weighted directed or undirected graph, information about the
-    appearance and disappearance of topological features (technically,
-    :ref:`homology classes <homology_and_cohomology>`) of various dimension and
-    at different scales is summarised in the corresponding persistence diagram.
-
-    **Important note**:
-
-        - Persistence diagrams produced by this class must be interpreted with
-          care due to the presence of padding triples which carry no
-          information. See :meth:`transform` for additional information.
-
-    Parameters
-    ----------
-    homology_dimensions : list or tuple, optional, default: ``(0, 1)``
-        Dimensions (non-negative integers) of the topological features to be
-        detected.
-
-    directed : bool, optional, default: ``True``
-        If ``True``, :meth:`transform` computes the persistence diagrams of the
-        filtered directed flag complexes arising from the input collection of
-        weighted directed graphs. If ``False``, :meth:`transform` computes the
-        persistence diagrams of the filtered undirected flag complexes obtained
-        by regarding all input weighted graphs as undirected, and:
-
-        - if `max_edge_weight` is ``numpy.inf``, it is sufficient to pass a
-          collection of (dense or sparse) upper-triangular matrices;
-        - if `max_edge_weight` is finite, it is recommended to pass either a
-          collection of symmetric dense matrices, or a collection of sparse
-          upper-triangular matrices.
-
-    filtration : string, optional, default: ``"max"``
-        Algorithm determining the filtration values of higher order simplices
-        from the weights of the vertices and edges. Possible values are:
-        ["dimension", "zero", "max", "max3", "max_plus_one", "product", "sum",
-        "pmean", "pmoment", "remove_edges", "vertex_degree"]
-
-    coeff : int prime, optional, default: ``2``
-        Compute homology with coefficients in the prime field
-        :math:`\\mathbb{F}_p = \\{ 0, \\ldots, p - 1 \\}` where :math:`p`
-        equals `coeff`.
-
-    max_edge_weight : float, optional, default: ``numpy.inf``
-        Maximum edge weight to be considered in the filtration. All edge
-        weights greater than this value will be considered as absent from the
-        filtration and topological features at scales larger than this value
-        will not be detected.
-
-    infinity_values : float or None, default: ``None``
-        Which death value to assign to features which are still alive at
-        filtration value `max_edge_weight`. ``None`` means that this death
-        value is declared to be equal to `max_edge_weight`.
-
-    reduced_homology : bool, optional, default: ``True``
-       If ``True``, the earliest-born triple in homology dimension 0 which has
-       infinite death is discarded from each diagram computed in
-       :meth:`transform`.
-
-    max_entries : int, optional, default: ``-1``
-        Number controlling the degree of precision in the matrix reductions
-        performed by the the backend. Corresponds to the parameter
-        ``approximation`` in :func:`pyflagser.flagser_weighted` and
-        :func:`pyflagser.flagser_unweighted`. Increase for higher precision,
-        decrease for faster computation. A good value is often ``100000`` in
-        hard problems. A negative value computes highest possible precision.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    infinity_values_ : float
-        Effective death value to assign to features which are still alive at
-        filtration value `max_edge_weight`.
-
-    See also
-    --------
-    VietorisRipsPersistence, WeightedRipsPersistence, SparseRipsPersistence,
-    WeakAlphaPersistence, EuclideanCechPersistence, ConsistentRescaling,
-    ConsecutiveRescaling
-
-    Notes
-    -----
-    The `pyflagser <https://github.com/giotto-ai/pyflagser>`_ Python package
-    is used for binding `Flagser <https://github.com/luetge/flagser>`_, a C++
-    backend for computing the (persistent) homology of (filtered) directed
-    flag complexes. For more details, please refer to the `flagser \
-    documentation <https://github.com/luetge/flagser/blob/master/docs/\
-    documentation_flagser.pdf>`_.
-
-    References
-    ----------
-    .. [1] D. Luetgehetmann, D. Govc, J. P. Smith, and R. Levi, "Computing
-           persistent homology of directed flag complexes", *Algorithms*,
-           13(1), 2020.
-
-    """
-
-    _hyperparameters = {
-        "homology_dimensions": {
-            "type": (list, tuple),
-            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
-            },
-        "directed": {"type": bool},
-        "coeff": {"type": int, "in": Interval(2, np.inf, closed="left")},
-        "max_edge_weight": {"type": Real},
-        "infinity_values": {"type": (Real, type(None))},
-        "reduced_homology": {"type": bool},
-        "max_entries": {"type": int}
-        }
-
-    def __init__(self, homology_dimensions=(0, 1), directed=True,
-                 filtration="max", coeff=2, max_edge_weight=np.inf,
-                 infinity_values=None, reduced_homology=True, max_entries=-1,
-                 n_jobs=None):
-        self.homology_dimensions = homology_dimensions
-        self.directed = directed
-        self.filtration = filtration
-        self.coeff = coeff
-        self.max_edge_weight = max_edge_weight
-        self.infinity_values = infinity_values
-        self.reduced_homology = reduced_homology
-        self.max_entries = max_entries
-        self.n_jobs = n_jobs
-
-    def _flagser_diagram(self, X):
-        Xdgms = [np.empty((0, 2), dtype=float)] * self._min_homology_dimension
-        Xdgms += flagser_weighted(X, max_edge_weight=self.max_edge_weight,
-                                  min_dimension=self._min_homology_dimension,
-                                  max_dimension=self._max_homology_dimension,
-                                  directed=self.directed,
-                                  filtration=self.filtration, coeff=self.coeff,
-                                  approximation=self.max_entries)["dgms"]
-        n_missing_dims = self._max_homology_dimension + 1 - len(Xdgms)
-        if n_missing_dims:
-            Xdgms += [np.empty((0, 2), dtype=float)] * n_missing_dims
-
-        return Xdgms
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`infinity_values_`. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray or list of length n_samples
-            Input collection of adjacency matrices of weighted directed or
-            undirected graphs. Can be either a 3D ndarray whose zeroth
-            dimension has size ``n_samples``, or a list containing
-            ``n_samples`` 2D ndarrays/sparse matrices. In each adjacency
-            matrix, diagonal elements are vertex weights and off-diagonal
-            elements are edge weights. It is assumed that a vertex weight
-            cannot be larger than the weight of the edges it forms. The way
-            zero values are handled depends on the format of the matrix. If the
-            matrix is a dense ``numpy.ndarray``, zero values denote
-            zero-weighted edges. If the matrix is a sparse ``scipy.sparse``
-            matrix, explicitly stored off-diagonal zeros and all diagonal zeros
-            denote zero-weighted edges. Off-diagonal values that have not been
-            explicitly stored are treated by ``scipy.sparse`` as zeros but will
-            be understood as infinitely-valued edges, i.e., edges absent from
-            the filtration.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_point_clouds(X, accept_sparse=True, distance_matrices=True)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=["n_jobs",
-                                                               "filtration"])
-
-        if self.infinity_values is None:
-            self.infinity_values_ = self.max_edge_weight
-        else:
-            self.infinity_values_ = self.infinity_values
-
-        self._homology_dimensions = sorted(self.homology_dimensions)
-        self._min_homology_dimension = self._homology_dimensions[0]
-        self._max_homology_dimension = self._homology_dimensions[-1]
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each adjacency matrix in `X`, compute the relevant persistence
-        diagram as an array of triples [b, d, q]. Each triple represents a
-        persistent topological feature in dimension q (belonging to
-        `homology_dimensions`) which is born at b and dies at d. Only triples
-        in which b < d are meaningful. Triples in which b and d are equal
-        ("diagonal elements") may be artificially introduced during the
-        computation for padding purposes, since the number of non-trivial
-        persistent topological features is typically not constant across
-        samples. They carry no information and hence should be effectively
-        ignored by any further computation.
-
-        Parameters
-        ----------
-        X : ndarray or list of length n_samples
-            Input collection of adjacency matrices of weighted directed or
-            undirected graphs. Can be either a 3D ndarray whose zeroth
-            dimension has size ``n_samples``, or a list containing
-            ``n_samples`` 2D ndarrays/sparse matrices. In each adjacency
-            matrix, diagonal elements are vertex weights and off-diagonal
-            elements are edges weights. It is assumed that a vertex weight
-            cannot be larger than the weight of the edges it forms. The way
-            zero values are handled depends on the format of the matrix. If
-            the matrix is a dense ``numpy.ndarray``, zero values denote
-            zero-weighted edges. If the matrix is a sparse ``scipy.sparse``
-            matrix, explicitly stored off-diagonal zeros and all diagonal zeros
-            denote zero-weighted edges. Off-diagonal values that have not been
-            explicitly stored are treated by ``scipy.sparse`` as zeros but will
-            be understood as infinitely-valued edges, i.e., edges absent from
-            the filtration.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Array of persistence diagrams computed from the feature arrays or
-            distance matrices in `X`. ``n_features`` equals
-            :math:`\\sum_q n_q`, where :math:`n_q` is the maximum number of
-            topological features in dimension :math:`q` across all samples in
-            `X`.
-
-        """
-        check_is_fitted(self)
-        X = check_point_clouds(X, accept_sparse=True, distance_matrices=True)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._flagser_diagram)(x) for x in X)
-
-        Xt = _postprocess_diagrams(
-            Xt, "flagser", self._homology_dimensions, self.infinity_values_,
-            self.reduced_homology
-            )
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
-        """Plot a sample from a collection of persistence diagrams, with
-        homology in multiple dimensions.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Collection of persistence diagrams, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        homology_dimensions : list, tuple or None, optional, default: ``None``
-            Which homology dimensions to include in the plot. ``None`` means
-            plotting all dimensions present in ``Xt[sample]``.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_diagram(
-            Xt[sample], homology_dimensions=homology_dimensions,
-            plotly_params=plotly_params
-            )
+"""Persistent homology on point clouds or finite metric spaces."""
+# License: GNU AGPLv3
+
+from numbers import Real, Integral
+from typing import Callable
+
+import numpy as np
+from gph import ripser_parallel as ripser
+from joblib import Parallel, delayed
+from pyflagser import flagser_weighted
+from scipy.sparse import coo_matrix
+from scipy.spatial import Delaunay
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.metrics.pairwise import pairwise_distances
+from sklearn.utils.validation import check_is_fitted
+
+from ._utils import _postprocess_diagrams
+from ..base import PlotterMixin
+from ..externals.python import SparseRipsComplex, CechComplex
+from ..plotting import plot_diagram
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.intervals import Interval
+from ..utils.validation import validate_params, check_point_clouds
+
+_AVAILABLE_RIPS_WEIGHTS = {
+    "DTM": {
+        "p": {"type": Real, "in": [1, 2, np.inf]},
+        "r": {"type": Real, "in": Interval(0, np.inf, closed="right")},
+        "n_neighbors": {"type": Integral,
+                        "in": Interval(1, np.inf, closed="left")}
+        },
+    "general": {
+        "p": {"type": Real, "in": [1, 2, np.inf]},
+        }
+    }
+
+
+@adapt_fit_transform_docs
+class VietorisRipsPersistence(BaseEstimator, TransformerMixin, PlotterMixin):
+    """:ref:`Persistence diagrams <persistence_diagram>` resulting from
+    :ref:`VietorisRips filtrations
+    <vietoris-rips_complex_and_vietoris-rips_persistence>`.
+
+    Given a :ref:`point cloud <distance_matrices_and_point_clouds>` in
+    Euclidean space, an abstract :ref:`metric space
+    <distance_matrices_and_point_clouds>` encoded by a distance matrix, or the
+    adjacency matrix of a weighted undirected graph, information about the
+    appearance and disappearance of topological features (technically,
+    :ref:`homology classes <homology_and_cohomology>`) of various dimensions
+    and at different scales is summarised in the corresponding persistence
+    diagram.
+
+    **Important note**:
+
+        - Persistence diagrams produced by this class must be interpreted with
+          care due to the presence of padding triples which carry no
+          information. See :meth:`transform` for additional information.
+
+    Parameters
+    ----------
+    metric : string or callable, optional, default: ``"euclidean"``
+        If set to ``"precomputed"``, input data is to be interpreted as a
+        collection of distance matrices or of adjacency matrices of weighted
+        undirected graphs. Otherwise, input data is to be interpreted as a
+        collection of point clouds (i.e. feature arrays), and `metric`
+        determines a rule with which to calculate distances between pairs of
+        points (i.e. row vectors). If `metric` is a string, it must be one of
+        the options allowed by :func:`scipy.spatial.distance.pdist` for its
+        metric parameter, or a metric listed in
+        :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`, including
+        ``"euclidean"``, ``"manhattan"`` or ``"cosine"``. If `metric` is a
+        callable, it should take pairs of vectors (1D arrays) as input and, for
+        each two vectors in a pair, it should return a scalar indicating the
+        distance/dissimilarity between them.
+
+    metric_params : dict, optional, default: ``{}``
+        Additional parameters to be passed to the distance function.
+
+    homology_dimensions : list or tuple, optional, default: ``(0, 1)``
+        Dimensions (non-negative integers) of the topological features to be
+        detected.
+
+    coeff : int prime, optional, default: ``2``
+        Compute homology with coefficients in the prime field
+        :math:`\\mathbb{F}_p = \\{ 0, \\ldots, p - 1 \\}` where :math:`p`
+        equals `coeff`.
+
+    collapse_edges : bool, optional, default: ``False``
+        Whether to run the edge collapse algorithm in [2]_ prior to the
+        persistent homology computation (see the Notes). Can reduce the runtime
+        dramatically when the data or the maximum homology dimensions are
+        large.
+
+    max_edge_length : float, optional, default: ``numpy.inf``
+        Maximum value of the VietorisRips filtration parameter. Points whose
+        distance is greater than this value will never be connected by an edge,
+        and topological features at scales larger than this value will not be
+        detected.
+
+    infinity_values : float or None, default: ``None``
+        Which death value to assign to features which are still alive at
+        filtration value `max_edge_length`. ``None`` means that this death
+        value is declared to be equal to `max_edge_length`.
+
+    reduced_homology : bool, optional, default: ``True``
+       If ``True``, the earliest-born triple in homology dimension 0 which has
+       infinite death is discarded from each diagram computed in
+       :meth:`transform`.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    infinity_values_ : float
+        Effective death value to assign to features which are still alive at
+        filtration value `max_edge_length`.
+
+    See also
+    --------
+    WeightedRipsPersistence, FlagserPersistence, SparseRipsPersistence,
+    WeakAlphaPersistence, EuclideanCechPersistence, ConsistentRescaling,
+    ConsecutiveRescaling
+
+    Notes
+    -----
+    `giotto-ph <https://github.com/giotto-ai/giotto-ph>`_ [1]_ is used as a C++
+    backend for computing VietorisRips persistent homology and edge collapses.
+
+    References
+    ----------
+    .. [1] J. Burella Prez et al, "giotto-ph: A Python Library for
+           High-Performance Computation of Persistent Homology of VietorisRips
+           Filtrations", 2021; `arXiv:2107.05412
+           <https://arxiv.org/abs/2107.05412>`_.
+
+    .. [2] J.-D. Boissonnat and S. Pritam, "Edge Collapse and Persistence of
+           Flag Complexes"; in *36th International Symposium on Computational
+           Geometry (SoCG 2020)*, pp. 19:119:15,
+           Schloss Dagstuhl-LeibnizZentrum fr Informatik, 2020;
+           `DOI: 10.4230/LIPIcs.SoCG.2020.19
+           <https://doi.org/10.4230/LIPIcs.SoCG.2020.19>`_.
+
+    """
+
+    _hyperparameters = {
+        "metric": {"type": (str, Callable)},
+        "metric_params": {"type": dict},
+        "homology_dimensions": {
+            "type": (list, tuple),
+            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
+            },
+        "collapse_edges": {"type": bool},
+        "coeff": {"type": int, "in": Interval(2, np.inf, closed="left")},
+        "max_edge_length": {"type": Real},
+        "infinity_values": {"type": (Real, type(None))},
+        "reduced_homology": {"type": bool}
+        }
+
+    def __init__(self, metric="euclidean", metric_params={},
+                 homology_dimensions=(0, 1), collapse_edges=False, coeff=2,
+                 max_edge_length=np.inf, infinity_values=None,
+                 reduced_homology=True, n_jobs=None):
+        self.metric = metric
+        self.metric_params = metric_params
+        self.homology_dimensions = homology_dimensions
+        self.collapse_edges = collapse_edges
+        self.coeff = coeff
+        self.max_edge_length = max_edge_length
+        self.infinity_values = infinity_values
+        self.reduced_homology = reduced_homology
+        self.n_jobs = n_jobs
+
+    def _ripser_diagram(self, X):
+        Xdgms = ripser(
+            X, maxdim=self._max_homology_dimension,
+            thresh=self.max_edge_length, coeff=self.coeff, metric=self.metric,
+            metric_params=self.metric_params,
+            collapse_edges=self.collapse_edges
+            )["dgms"]
+
+        return Xdgms
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`infinity_values_`. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray or list of length n_samples
+            Input data representing a collection of point clouds if `metric`
+            was not set to ``"precomputed"``, and of distance matrices or
+            adjacency matrices of weighted undirected graphs otherwise. Can be
+            either a 3D ndarray whose zeroth dimension has size ``n_samples``,
+            or a list containing ``n_samples`` 2D ndarrays/sparse matrices.
+            Point cloud arrays have shape ``(n_points, n_dimensions)``, and if
+            `X` is a list these shapes can vary between point clouds. If
+            `metric` was set to ``"precomputed"``, then:
+
+                - Diagonal entries indicate vertex weights, i.e. the filtration
+                  parameters at which vertices appear.
+                - If entries of `X` are dense, only their upper diagonal
+                  portions (including the diagonal) are considered.
+                - If entries of `X` are sparse, they do not need to be upper
+                  diagonal or symmetric. If only one of entry (i, j) and (j, i)
+                  is stored, its value is taken as the weight of the undirected
+                  edge {i, j}. If both are stored, the value in the upper
+                  diagonal is taken. Off-diagonal entries which are not
+                  explicitly stored are treated as infinite, indicating absent
+                  edges.
+                - Entries of `X` should be compatible with a filtration, i.e.
+                  the value at index (i, j) should be no smaller than the
+                  values at diagonal indices (i, i) and (j, j).
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
+
+        self._is_precomputed = self.metric == "precomputed"
+        check_point_clouds(X, accept_sparse=True,
+                           distance_matrices=self._is_precomputed)
+
+        if self.infinity_values is None:
+            self.infinity_values_ = self.max_edge_length
+        else:
+            self.infinity_values_ = self.infinity_values
+
+        self._homology_dimensions = sorted(self.homology_dimensions)
+        self._max_homology_dimension = self._homology_dimensions[-1]
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each point cloud or distance matrix in `X`, compute the
+        relevant persistence diagram as an array of triples [b, d, q]. Each
+        triple represents a persistent topological feature in dimension q
+        (belonging to `homology_dimensions`) which is born at b and dies at d.
+        Only triples in which b < d are meaningful. Triples in which b and d
+        are equal ("diagonal elements") may be artificially introduced during
+        the computation for padding purposes, since the number of non-trivial
+        persistent topological features is typically not constant across
+        samples. They carry no information and hence should be effectively
+        ignored by any further computation.
+
+        Parameters
+        ----------
+        X : ndarray or list of length n_samples
+            Input data representing a collection of point clouds if `metric`
+            was not set to ``"precomputed"``, and of distance matrices or
+            adjacency matrices of weighted undirected graphs otherwise. Can be
+            either a 3D ndarray whose zeroth dimension has size ``n_samples``,
+            or a list containing ``n_samples`` 2D ndarrays/sparse matrices.
+            Point cloud arrays have shape ``(n_points, n_dimensions)``, and if
+            `X` is a list these shapes can vary between point clouds. If
+            `metric` was set to ``"precomputed"``, then:
+
+                - Diagonal entries indicate vertex weights, i.e. the filtration
+                  parameters at which vertices appear.
+                - If entries of `X` are dense, only their upper diagonal
+                  portions (including the diagonal) are considered.
+                - If entries of `X` are sparse, they do not need to be upper
+                  diagonal or symmetric. If only one of entry (i, j) and (j, i)
+                  is stored, its value is taken as the weight of the undirected
+                  edge {i, j}. If both are stored, the value in the upper
+                  diagonal is taken. Off-diagonal entries which are not
+                  explicitly stored are treated as infinite, indicating absent
+                  edges.
+                - Entries of `X` should be compatible with a filtration, i.e.
+                  the value at index (i, j) should be no smaller than the
+                  values at diagonal indices (i, i) and (j, j).
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Array of persistence diagrams computed from the feature arrays or
+            distance matrices in `X`. ``n_features`` equals
+            :math:`\\sum_q n_q`, where :math:`n_q` is the maximum number of
+            topological features in dimension :math:`q` across all samples in
+            `X`.
+
+        """
+        check_is_fitted(self)
+        X = check_point_clouds(X, accept_sparse=True,
+                               distance_matrices=self._is_precomputed)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._ripser_diagram)(x) for x in X)
+
+        Xt = _postprocess_diagrams(
+            Xt, "ripser", self._homology_dimensions, self.infinity_values_,
+            self.reduced_homology
+            )
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
+        """Plot a sample from a collection of persistence diagrams, with
+        homology in multiple dimensions.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Collection of persistence diagrams, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        homology_dimensions : list, tuple or None, optional, default: ``None``
+            Which homology dimensions to include in the plot. ``None`` means
+            plotting all dimensions present in ``Xt[sample]``.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_diagram(
+            Xt[sample], homology_dimensions=homology_dimensions,
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class WeightedRipsPersistence(BaseEstimator, TransformerMixin, PlotterMixin):
+    """:ref:`Persistence diagrams <persistence_diagram>` resulting from
+    :ref:`weighted VietorisRips filtrations <TODO>` as in [3]_.
+
+    Given a :ref:`point cloud <distance_matrices_and_point_clouds>` in
+    Euclidean space, an abstract :ref:`metric space
+    <distance_matrices_and_point_clouds>` encoded by a distance matrix, or the
+    adjacency matrix of a weighted undirected graph, information about the
+    appearance and disappearance of topological features (technically,
+    :ref:`homology classes <homology_and_cohomology>`) of various dimensions
+    and at different scales is summarised in the corresponding persistence
+    diagram.
+
+    Weighted (Vietoris)Rips filtrations can be useful to highlight topological
+    features against outliers and noise. Among them, the distance-to-measure
+    (DTM) filtration is particularly suited to point clouds due to several
+    favourable properties. This implementation follows the general framework
+    described in [3]_. The idea is that, starting from a way to compute vertex
+    weights :math:`\\{w_i\\}_i` from an input point cloud/distance
+    matrix/adjacency matrix, a modified adjacency matrix is determined whose
+    diagonal entries are the :math:`\\{w_i\\}_i`, and whose edge weights are
+
+    .. math:: w_{ij} = \\begin{cases} \\max\\{ w_i, w_j \\} &\\text{if }
+       2\\mathrm{dist}_{ij} \\leq |w_i^p - w_j^p|^{\\frac{1}{p}}, \\\\
+       t &\\text{otherwise} \\end{cases}
+
+    where :math:`t` is the only positive root of
+
+    .. math:: 2 \\mathrm{dist}_{ij} = (t^p - w_i^p)^\\frac{1}{p} +
+       (t^p - w_j^p)^\\frac{1}{p}
+
+    and :math:`p` is a parameter (see `metric_params`). The modified adjacency
+    matrices are then treated exactly as in :class:`VietorisRipsPersistence`.
+
+    **Important notes**:
+
+        - Vertex and edge weights are twice the ones in [3]_ so that the same
+          results as :class:`VietorisRipsPersistence` are obtained when all
+          vertex weights are zero.
+        - Persistence diagrams produced by this class must be interpreted with
+          care due to the presence of padding triples which carry no
+          information. See :meth:`transform` for additional information.
+
+    Parameters
+    ----------
+    metric : string or callable, optional, default: ``"euclidean"``
+        If set to ``"precomputed"``, input data is to be interpreted as a
+        collection of distance matrices or of adjacency matrices of weighted
+        undirected graphs. Otherwise, input data is to be interpreted as a
+        collection of point clouds (i.e. feature arrays), and `metric`
+        determines a rule with which to calculate distances between pairs of
+        points (i.e. row vectors). If `metric` is a string, it must be one of
+        the options allowed by :func:`scipy.spatial.distance.pdist` for its
+        metric parameter, or a metric listed in
+        :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`, including
+        ``"euclidean"``, ``"manhattan"`` or ``"cosine"``. If `metric` is a
+        callable, it should take pairs of vectors (1D arrays) as input and, for
+        each two vectors in a pair, it should return a scalar indicating the
+        distance/dissimilarity between them.
+
+    metric_params : dict, optional, default: ``{}``
+        Additional parameters to be passed to the distance function.
+
+    homology_dimensions : list or tuple, optional, default: ``(0, 1)``
+        Dimensions (non-negative integers) of the topological features to be
+        detected.
+
+    weights : ``"DTM"`` or callable, optional, default: ``"DTM"``
+        Function that will be applied to each input point cloud/distance
+        matrix/adjacency matrix to compute a 1D array of vertex weights for the
+        the modified adjacency matrices. The default ``"DTM"`` denotes the
+        empirical distance-to-measure function defined, following [3]_, by
+
+        .. math:: w(x) = 2\\left(\\frac{1}{n+1} \\sum_{k=1}^n
+           \\mathrm{dist}(x, x_k)^r\\right)^{1/r}.
+
+        Here, :math:`\\mathrm{dist}` is the distance metric used, :math:`x_k`
+        is the :math:`k`-th :math:`\\mathrm{dist}`-nearest neighbour of
+        :math:`x` (:math:`x` is not considered a neighbour of itself),
+        :math:`n` is the number of nearest neighbors to include, and :math:`r`
+        is a parameter (see `weight_params`). If a callable, it must return
+        non-negative 1D arrays.
+
+    weight_params : dict, optional, default: ``{}``
+        Additional parameters for the weighted filtration. ``"p"`` determines
+        the power to be used in computing edge weights from vertex weights. It
+        can be one of ``1``, ``2`` or ``np.inf`` and defaults to ``1``. If
+        `weights` is ``"DTM"``, the additional keys ``"r"`` (default: ``2``)
+        and ``"n_neighbors"`` (default: ``3``) are available (see `weights`,
+        where the latter corresponds to :math:`n`).
+
+    coeff : int prime, optional, default: ``2``
+        Compute homology with coefficients in the prime field
+        :math:`\\mathbb{F}_p = \\{ 0, \\ldots, p - 1 \\}` where :math:`p`
+        equals `coeff`.
+
+    collapse_edges : bool, optional, default: ``False``
+        Whether to run the edge collapse algorithm in [2]_ prior to the
+        persistent homology computation (see the Notes). Can reduce the runtime
+        dramatically when the data or the maximum homology dimensions are
+        large.
+
+    max_edge_weight : float, optional, default: ``numpy.inf``
+        Maximum value of the filtration parameter in the modified adjacency
+        matrix. Edges with weight greater than this value will be considered
+        absent.
+
+    infinity_values : float or None, default: ``None``
+        Which death value to assign to features which are still alive at
+        filtration value `max_edge_weight`. ``None`` means that this death
+        value is declared to be equal to `max_edge_weight`.
+
+    reduced_homology : bool, optional, default: ``True``
+       If ``True``, the earliest-born triple in homology dimension 0 which has
+       infinite death is discarded from each diagram computed in
+       :meth:`transform`.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    infinity_values_ : float
+        Effective death value to assign to features which are still alive at
+        filtration value `max_edge_weight`.
+
+    effective_weight_params_ : dict
+        Effective parameters involved in computing the weighted Rips
+        filtration.
+
+    See also
+    --------
+    VietorisRipsPersistence, SparseRipsPersistence, FlagserPersistence,
+    WeakAlphaPersistence, EuclideanCechPersistence, ConsistentRescaling,
+    ConsecutiveRescaling
+
+    Notes
+    -----
+    `giotto-ph <https://github.com/giotto-ai/giotto-ph>`_ [1]_ is used as a C++
+    backend for computing VietorisRips persistent homology and edge collapses.
+
+    References
+    ----------
+    .. [1] J. Burella Prez et al, "giotto-ph: A Python Library for
+           High-Performance Computation of Persistent Homology of VietorisRips
+           Filtrations", 2021; `arXiv:2107.05412
+           <https://arxiv.org/abs/2107.05412>`_.
+
+    .. [2] J.-D. Boissonnat and S. Pritam, "Edge Collapse and Persistence of
+           Flag Complexes"; in *36th International Symposium on Computational
+           Geometry (SoCG 2020)*, pp. 19:119:15,
+           Schloss Dagstuhl-LeibnizZentrum fr Informatik, 2020;
+           `DOI: 10.4230/LIPIcs.SoCG.2020.19
+           <https://doi.org/10.4230/LIPIcs.SoCG.2020.19>`_.
+
+    .. [3] H. Anai et al, "DTM-Based Filtrations"; in *Topological Data
+           Analysis* (Abel Symposia, vol 15), Springer, 2020;
+           `DOI: 10.1007/978-3-030-43408-3_2
+           <https://doi.org/10.1007/978-3-030-43408-3_2>`_.
+
+    """
+
+    _hyperparameters = {
+        "metric": {"type": (str, Callable)},
+        "metric_params": {"type": dict},
+        "homology_dimensions": {
+            "type": (list, tuple),
+            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
+            },
+        "weights": {"type": (str, Callable)},
+        "weight_params": {"type": dict},
+        "collapse_edges": {"type": bool},
+        "coeff": {"type": int, "in": Interval(2, np.inf, closed="left")},
+        "max_edge_weight": {"type": Real},
+        "infinity_values": {"type": (Real, type(None))},
+        "reduced_homology": {"type": bool}
+        }
+
+    def __init__(self, metric="euclidean", metric_params={},
+                 homology_dimensions=(0, 1), weights="DTM", weight_params={},
+                 collapse_edges=False, coeff=2, max_edge_weight=np.inf,
+                 infinity_values=None, reduced_homology=True, n_jobs=None):
+        self.metric = metric
+        self.metric_params = metric_params
+        self.homology_dimensions = homology_dimensions
+        self.weights = weights
+        self.weight_params = weight_params
+        self.collapse_edges = collapse_edges
+        self.coeff = coeff
+        self.max_edge_weight = max_edge_weight
+        self.infinity_values = infinity_values
+        self.reduced_homology = reduced_homology
+        self.n_jobs = n_jobs
+
+    def _ripser_diagram(self, X):
+        if isinstance(self.weights, Callable):
+            weights = self.weights(X)
+        else:
+            weights = self.weights
+        Xdgms = ripser(
+            X, maxdim=self._max_homology_dimension,
+            thresh=self.max_edge_weight, coeff=self.coeff, metric=self.metric,
+            metric_params=self.metric_params, weights=weights,
+            weight_params=self.effective_weight_params_,
+            collapse_edges=self.collapse_edges
+            )["dgms"]
+
+        return Xdgms
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`infinity_values_`. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray or list of length n_samples
+            Input data representing a collection of point clouds if `metric`
+            was not set to ``"precomputed"``, and of distance matrices or
+            adjacency matrices of weighted undirected graphs otherwise. Can be
+            either a 3D ndarray whose zeroth dimension has size ``n_samples``,
+            or a list containing ``n_samples`` 2D ndarrays/sparse matrices.
+            Point cloud arrays have shape ``(n_points, n_dimensions)``, and if
+            `X` is a list these shapes can vary between point clouds. If
+            `metric` was set to ``"precomputed"``, then:
+
+                - All entries of `X` should not contain infinities or negative
+                  values (contrary to :class:`VietorisRipsPersistence`).
+                - The diagonals of entries of `X` are ignored (after the vertex
+                  weights are computed, when `weights` is a callable).
+                - If entries of `X` are dense, only their upper diagonal
+                  portions are considered.
+                - If entries of `X` are sparse, they do not need to be upper
+                  diagonal or symmetric. If only one of entry (i, j) and (j, i)
+                  is stored, its value is taken as the weight of the undirected
+                  edge {i, j}. If both are stored, the value in the upper
+                  diagonal is taken. Off-diagonal entries which are not
+                  explicitly stored are treated as infinite, indicating absent
+                  edges.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
+        if isinstance(self.weights, str) and self.weights != "DTM":
+            raise ValueError(f"'{self.weights}' passed for `weights` but the "
+                             f"only allowed string is 'DTM'.")
+        self.effective_weight_params_ = {"p": 1}
+        if self.weights == "DTM":
+            key = "DTM"
+            self.effective_weight_params_.update({"n_neighbors": 3, "r": 2})
+        else:
+            key = "general"
+        if self.weight_params:
+            self.effective_weight_params_.update(self.weight_params)
+            validate_params(self.effective_weight_params_,
+                            _AVAILABLE_RIPS_WEIGHTS[key])
+
+        self._is_precomputed = self.metric == "precomputed"
+        check_point_clouds(X, accept_sparse=True, force_all_finite=True,
+                           distance_matrices=self._is_precomputed)
+
+        if self.infinity_values is None:
+            self.infinity_values_ = self.max_edge_weight
+        else:
+            self.infinity_values_ = self.infinity_values
+
+        self._homology_dimensions = sorted(self.homology_dimensions)
+        self._max_homology_dimension = self._homology_dimensions[-1]
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each point cloud or distance matrix in `X`, compute the
+        relevant persistence diagram as an array of triples [b, d, q]. Each
+        triple represents a persistent topological feature in dimension q
+        (belonging to `homology_dimensions`) which is born at b and dies at d.
+        Only triples in which b < d are meaningful. Triples in which b and d
+        are equal ("diagonal elements") may be artificially introduced during
+        the computation for padding purposes, since the number of non-trivial
+        persistent topological features is typically not constant across
+        samples. They carry no information and hence should be effectively
+        ignored by any further computation.
+
+        Parameters
+        ----------
+        X : ndarray or list of length n_samples
+            Input data representing a collection of point clouds if `metric`
+            was not set to ``"precomputed"``, and of distance matrices or
+            adjacency matrices of weighted undirected graphs otherwise. Can be
+            either a 3D ndarray whose zeroth dimension has size ``n_samples``,
+            or a list containing ``n_samples`` 2D ndarrays/sparse matrices.
+            Point cloud arrays have shape ``(n_points, n_dimensions)``, and if
+            `X` is a list these shapes can vary between point clouds. If
+            `metric` was set to ``"precomputed"``, then:
+
+                - All entries of `X` should not contain infinities or negative
+                  values (contrary to :class:`VietorisRipsPersistence`).
+                - The diagonals of entries of `X` are ignored (after the vertex
+                  weights are computed, when `weights` is a callable).
+                - If entries of `X` are dense, only their upper diagonal
+                  portions are considered.
+                - If entries of `X` are sparse, they do not need to be upper
+                  diagonal or symmetric. If only one of entry (i, j) and (j, i)
+                  is stored, its value is taken as the weight of the undirected
+                  edge {i, j}. If both are stored, the value in the upper
+                  diagonal is taken. Off-diagonal entries which are not
+                  explicitly stored are treated as infinite, indicating absent
+                  edges.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Array of persistence diagrams computed from the feature arrays or
+            distance matrices in `X`. ``n_features`` equals
+            :math:`\\sum_q n_q`, where :math:`n_q` is the maximum number of
+            topological features in dimension :math:`q` across all samples in
+            `X`.
+
+        """
+        check_is_fitted(self)
+        X = check_point_clouds(X, accept_sparse=True, force_all_finite=True,
+                               distance_matrices=self._is_precomputed)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._ripser_diagram)(x) for x in X)
+
+        Xt = _postprocess_diagrams(
+            Xt, "ripser", self._homology_dimensions, self.infinity_values_,
+            self.reduced_homology
+            )
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
+        """Plot a sample from a collection of persistence diagrams, with
+        homology in multiple dimensions.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Collection of persistence diagrams, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        homology_dimensions : list, tuple or None, optional, default: ``None``
+            Which homology dimensions to include in the plot. ``None`` means
+            plotting all dimensions present in ``Xt[sample]``.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_diagram(
+            Xt[sample], homology_dimensions=homology_dimensions,
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class SparseRipsPersistence(BaseEstimator, TransformerMixin, PlotterMixin):
+    """:ref:`Persistence diagrams <persistence_diagram>` resulting from
+    :ref:`Sparse VietorisRips filtrations
+    <vietoris-rips_complex_and_vietoris-rips_persistence>`.
+
+    Given a :ref:`point cloud <distance_matrices_and_point_clouds>` in
+    Euclidean space, or an abstract :ref:`metric space
+    <distance_matrices_and_point_clouds>` encoded by a distance matrix,
+    information about the appearance and disappearance of topological features
+    (technically, :ref:`homology classes <homology_and_cohomology>`) of various
+    dimensions and at different scales is summarised in the corresponding
+    persistence diagram.
+
+    **Important note**:
+
+        - Persistence diagrams produced by this class must be interpreted with
+          care due to the presence of padding triples which carry no
+          information. See :meth:`transform` for additional information.
+
+    Parameters
+    ----------
+    metric : string or callable, optional, default: ``"euclidean"``
+        If set to ``"precomputed"``, input data is to be interpreted as a
+        collection of distance matrices. Otherwise, input data is to be
+        interpreted as a collection of point clouds (i.e. feature arrays), and
+        `metric` determines a rule with which to calculate distances between
+        pairs of instances (i.e. rows) in these arrays. If `metric` is a
+        string, it must be one of the options allowed by
+        :func:`scipy.spatial.distance.pdist` for its metric parameter, or a
+        metric listed in :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`,
+        including "euclidean", "manhattan", or "cosine". If `metric` is a
+        callable, it is called on each pair of instances and the resulting
+        value recorded. The callable should take two arrays from the entry in
+        `X` as input, and return a value indicating the distance between them.
+
+    homology_dimensions : list or tuple, optional, default: ``(0, 1)``
+        Dimensions (non-negative integers) of the topological features to be
+        detected.
+
+    coeff : int prime, optional, default: ``2``
+        Compute homology with coefficients in the prime field
+        :math:`\\mathbb{F}_p = \\{ 0, \\ldots, p - 1 \\}` where :math:`p`
+        equals `coeff`.
+
+    epsilon : float between 0. and 1., optional, default: ``0.1``
+        Parameter controlling the approximation to the exact VietorisRips
+        filtration. If set to `0.`, :class:`SparseRipsPersistence` leads to the
+        same results as :class:`VietorisRipsPersistence` but is slower.
+
+    max_edge_length : float, optional, default: ``numpy.inf``
+        Maximum value of the Sparse Rips filtration parameter. Points whose
+        distance is greater than this value will never be connected by an edge,
+        and topological features at scales larger than this value will not be
+        detected.
+
+    infinity_values : float or None, default: ``None``
+        Which death value to assign to features which are still alive at
+        filtration value `max_edge_length`. ``None`` means that this death
+        value is declared to be equal to `max_edge_length`.
+
+    reduced_homology : bool, optional, default: ``True``
+       If ``True``, the earliest-born triple in homology dimension 0 which has
+       infinite death is discarded from each diagram computed in
+       :meth:`transform`.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    infinity_values_ : float
+        Effective death value to assign to features which are still alive at
+        filtration value `max_edge_length`. Set in :meth:`fit`.
+
+    See also
+    --------
+    VietorisRipsPersistence, WeightedRipsPersistence, FlagserPersistence,
+    WeakAlphaPersistence, EuclideanCechPersistence, ConsistentRescaling,
+    ConsecutiveRescaling
+
+    Notes
+    -----
+    `GUDHI <https://github.com/GUDHI/gudhi-devel>`_ is used as a C++ backend
+    for computing sparse VietorisRips persistent homology [1]_. Python
+    bindings were modified for performance.
+
+    References
+    ----------
+    .. [1] C. Maria, "Persistent Cohomology", 2020; `GUDHI User and Reference
+           Manual <http://gudhi.gforge.inria.fr/doc/3.1.0/group__persistent__\
+           cohomology.html>`_.
+
+    """
+
+    _hyperparameters = {
+        "metric": {"type": (str, Callable)},
+        "homology_dimensions": {
+            "type": (list, tuple),
+            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
+            },
+        "coeff": {"type": int, "in": Interval(2, np.inf, closed="left")},
+        "epsilon": {"type": Real, "in": Interval(0, 1, closed="both")},
+        "max_edge_length": {"type": Real},
+        "infinity_values": {"type": (Real, type(None))},
+        "reduced_homology": {"type": bool}
+        }
+
+    def __init__(self, metric="euclidean", homology_dimensions=(0, 1),
+                 coeff=2, epsilon=0.1, max_edge_length=np.inf,
+                 infinity_values=None, reduced_homology=True, n_jobs=None):
+        self.metric = metric
+        self.homology_dimensions = homology_dimensions
+        self.coeff = coeff
+        self.epsilon = epsilon
+        self.max_edge_length = max_edge_length
+        self.infinity_values = infinity_values
+        self.reduced_homology = reduced_homology
+        self.n_jobs = n_jobs
+
+    def _gudhi_diagram(self, X):
+        Xdgm = pairwise_distances(X, metric=self.metric)
+        sparse_rips_complex = SparseRipsComplex(
+            distance_matrix=Xdgm, max_edge_length=self.max_edge_length,
+            sparse=self.epsilon
+            )
+        simplex_tree = sparse_rips_complex.create_simplex_tree(
+            max_dimension=max(self._homology_dimensions) + 1
+            )
+        Xdgm = simplex_tree.persistence(
+            homology_coeff_field=self.coeff, min_persistence=0
+            )
+
+        return Xdgm
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`infinity_values_`. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray or list of length n_samples
+            Input data representing a collection of point clouds if `metric`
+            was not set to ``"precomputed"``, and of distance matrices
+            otherwise. Can be either a 3D ndarray whose zeroth dimension has
+            size ``n_samples``, or a list containing ``n_samples`` 2D ndarrays.
+            Point cloud arrays have shape ``(n_points, n_dimensions)``, and if
+            `X` is a list these shapes can vary between point clouds. If
+            `metric` was set to ``"precomputed"``, each entry of `X` should be
+            compatible with a filtration, i.e. the value at index (i, j) should
+            be no smaller than the values at diagonal indices (i, i) and
+            (j, j).
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
+        self._is_precomputed = self.metric == "precomputed"
+        check_point_clouds(X, accept_sparse=True,
+                           distance_matrices=self._is_precomputed)
+
+        if self.infinity_values is None:
+            self.infinity_values_ = self.max_edge_length
+        else:
+            self.infinity_values_ = self.infinity_values
+
+        self._homology_dimensions = sorted(self.homology_dimensions)
+        self._max_homology_dimension = self._homology_dimensions[-1]
+        return self
+
+    def transform(self, X, y=None):
+        """For each point cloud or distance matrix in `X`, compute the
+        relevant persistence diagram as an array of triples [b, d, q]. Each
+        triple represents a persistent topological feature in dimension q
+        (belonging to `homology_dimensions`) which is born at b and dies at d.
+        Only triples in which b < d are meaningful. Triples in which b and d
+        are equal ("diagonal elements") may be artificially introduced during
+        the computation for padding purposes, since the number of non-trivial
+        persistent topological features is typically not constant across
+        samples. They carry no information and hence should be effectively
+        ignored by any further computation.
+
+        Parameters
+        ----------
+        X : ndarray or list of length n_samples
+            Input data representing a collection of point clouds if `metric`
+            was not set to ``"precomputed"``, and of distance matrices
+            otherwise. Can be either a 3D ndarray whose zeroth dimension has
+            size ``n_samples``, or a list containing ``n_samples`` 2D ndarrays.
+            Point cloud arrays have shape ``(n_points, n_dimensions)``, and if
+            `X` is a list these shapes can vary between point clouds. If
+            `metric` was set to ``"precomputed"``, each entry of `X` should be
+            compatible with a filtration, i.e. the value at index (i, j) should
+            be no smaller than the values at diagonal indices (i, i) and
+            (j, j).
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Array of persistence diagrams computed from the feature arrays or
+            distance matrices in `X`. ``n_features`` equals
+            :math:`\\sum_q n_q`, where :math:`n_q` is the maximum number of
+            topological features in dimension :math:`q` across all samples in
+            `X`.
+
+        """
+        check_is_fitted(self)
+        X = check_point_clouds(X, accept_sparse=True,
+                               distance_matrices=self._is_precomputed)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._gudhi_diagram)(x) for x in X)
+
+        Xt = _postprocess_diagrams(
+            Xt, "gudhi", self._homology_dimensions, self.infinity_values_,
+            self.reduced_homology
+            )
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
+        """Plot a sample from a collection of persistence diagrams, with
+        homology in multiple dimensions.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Collection of persistence diagrams, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        homology_dimensions : list, tuple or None, optional, default: ``None``
+            Which homology dimensions to include in the plot. ``None`` means
+            plotting all dimensions present in ``Xt[sample]``.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_diagram(
+            Xt[sample], homology_dimensions=homology_dimensions,
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class WeakAlphaPersistence(BaseEstimator, TransformerMixin, PlotterMixin):
+    """:ref:`Persistence diagrams <persistence_diagram>` resulting from
+    :ref:`weak alpha filtrations <TODO>`.
+
+    Given a :ref:`point cloud <distance_matrices_and_point_clouds>` in
+    Euclidean space, information about the appearance and disappearance of
+    topological features (technically, :ref:`homology classes
+    <homology_and_cohomology>`) of various dimensions and at different scales
+    is summarised in the corresponding persistence diagram.
+
+    The weak alpha filtration of a point cloud is defined to be the
+    :ref:`VietorisRips filtration
+    <vietoris-rips_complex_and_vietoris-rips_persistence>` of the sparse matrix
+    of Euclidean distances between neighbouring vertices in the Delaunay
+    triangulation of the point cloud. In low dimensions, computing the
+    persistent homology of this filtration can be much faster than computing
+    VietorisRips persistent homology via :class:`VietorisRipsPersistence`.
+
+    **Important note**:
+
+        - Persistence diagrams produced by this class must be interpreted with
+          care due to the presence of padding triples which carry no
+          information. See :meth:`transform` for additional information.
+
+    Parameters
+    ----------
+    homology_dimensions : list or tuple, optional, default: ``(0, 1)``
+        Dimensions (non-negative integers) of the topological features to be
+        detected.
+
+    coeff : int prime, optional, default: ``2``
+        Compute homology with coefficients in the prime field
+        :math:`\\mathbb{F}_p = \\{ 0, \\ldots, p - 1 \\}` where :math:`p`
+        equals `coeff`.
+
+    max_edge_length : float, optional, default: ``numpy.inf``
+        Maximum value of the VietorisRips filtration parameter. Points whose
+        distance is greater than this value will never be connected by an edge,
+        and topological features at scales larger than this value will not be
+        detected.
+
+    infinity_values : float or None, default: ``None``
+        Which death value to assign to features which are still alive at
+        filtration value `max_edge_length`. ``None`` means that this death
+        value is declared to be equal to `max_edge_length`.
+
+    reduced_homology : bool, optional, default: ``True``
+       If ``True``, the earliest-born triple in homology dimension 0 which has
+       infinite death is discarded from each diagram computed in
+       :meth:`transform`.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    infinity_values_ : float
+        Effective death value to assign to features which are still alive at
+        filtration value `max_edge_length`.
+
+    See also
+    --------
+    VietorisRipsPersistence, WeightedRipsPersistence, SparseRipsPersistence,
+    FlagserPersistence, EuclideanCechPersistence
+
+    Notes
+    -----
+    Delaunay triangulation are computed by :class:`scipy.spatial.Delaunay`.
+    `giotto-ph <https://github.com/giotto-ai/giotto-ph>`_ [1]_ is used as a C++
+    backend for computing VietorisRips persistent homology.
+
+    References
+    ----------
+    .. [1] J. Burella Prez et al, "giotto-ph: A Python Library for
+           High-Performance Computation of Persistent Homology of VietorisRips
+           Filtrations", 2021; `arXiv:2107.05412
+           <https://arxiv.org/abs/2107.05412>`_.
+
+    """
+
+    _hyperparameters = {
+        "homology_dimensions": {
+            "type": (list, tuple),
+            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
+            },
+        "coeff": {"type": int, "in": Interval(2, np.inf, closed="left")},
+        "max_edge_length": {"type": Real},
+        "infinity_values": {"type": (Real, type(None))},
+        "reduced_homology": {"type": bool}
+        }
+
+    def __init__(self, homology_dimensions=(0, 1), coeff=2,
+                 max_edge_length=np.inf, infinity_values=None,
+                 reduced_homology=True, n_jobs=None):
+        self.homology_dimensions = homology_dimensions
+        self.coeff = coeff
+        self.max_edge_length = max_edge_length
+        self.infinity_values = infinity_values
+        self.reduced_homology = reduced_homology
+        self.n_jobs = n_jobs
+
+    def _weak_alpha_diagram(self, X):
+        # `indices` will serve as the array of column indices
+        indptr, indices = Delaunay(X).vertex_neighbor_vertices
+
+        # Compute the array of row indices
+        row = np.zeros_like(indices)
+        row[indptr[1:-1]] = 1
+        np.cumsum(row, out=row)
+
+        # We only need the upper diagonal
+        mask = indices > row
+        row, col = row[mask], indices[mask]
+        dists = np.linalg.norm(X[row] - X[col], axis=1)
+        # Note: passing the shape explicitly should not be needed in more
+        # recent versions of C++ ripser
+        n_points = len(X)
+        dm = coo_matrix((dists, (row, col)), shape=(n_points, n_points))
+
+        Xdgms = ripser(dm, maxdim=self._max_homology_dimension,
+                       thresh=self.max_edge_length, coeff=self.coeff,
+                       metric="precomputed")["dgms"]
+
+        return Xdgms
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`infinity_values_`. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray or list of length n_samples
+            Input data representing a collection of point clouds. Can be either
+            a 3D ndarray whose zeroth dimension has size ``n_samples``, or a
+            list containing ``n_samples`` 2D ndarrays. Point cloud arrays have
+            shape ``(n_points, n_dimensions)``, and if `X` is a list these
+            shapes can vary between point clouds.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
+        check_point_clouds(X)
+
+        if self.infinity_values is None:
+            self.infinity_values_ = self.max_edge_length
+        else:
+            self.infinity_values_ = self.infinity_values
+
+        self._homology_dimensions = sorted(self.homology_dimensions)
+        self._max_homology_dimension = self._homology_dimensions[-1]
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each point cloud in `X`, compute the relevant persistence
+        diagram as an array of triples [b, d, q]. Each triple represents a
+        persistent topological feature in dimension q (belonging to
+        `homology_dimensions`) which is born at b and dies at d. Only triples
+        in which b < d are meaningful. Triples in which b and d are equal
+        ("diagonal elements") may be artificially introduced during the
+        computation for padding purposes, since the number of non-trivial
+        persistent topological features is typically not constant across
+        samples. They carry no information and hence should be effectively
+        ignored by any further computation.
+
+        Parameters
+        ----------
+        X : ndarray or list of length n_samples
+            Input data representing a collection of point clouds. Can be either
+            a 3D ndarray whose zeroth dimension has size ``n_samples``, or a
+            list containing ``n_samples`` 2D ndarrays. Point cloud arrays have
+            shape ``(n_points, n_dimensions)``, and if `X` is a list these
+            shapes can vary between point clouds.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Array of persistence diagrams computed from the feature arrays or
+            distance matrices in `X`. ``n_features`` equals
+            :math:`\\sum_q n_q`, where :math:`n_q` is the maximum number of
+            topological features in dimension :math:`q` across all samples in
+            `X`.
+
+        """
+        check_is_fitted(self)
+        X = check_point_clouds(X)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._weak_alpha_diagram)(x) for x in X)
+
+        Xt = _postprocess_diagrams(
+            Xt, "ripser", self._homology_dimensions, self.infinity_values_,
+            self.reduced_homology
+            )
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
+        """Plot a sample from a collection of persistence diagrams, with
+        homology in multiple dimensions.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Collection of persistence diagrams, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        homology_dimensions : list, tuple or None, optional, default: ``None``
+            Which homology dimensions to include in the plot. ``None`` means
+            plotting all dimensions present in ``Xt[sample]``.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_diagram(
+            Xt[sample], homology_dimensions=homology_dimensions,
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class EuclideanCechPersistence(BaseEstimator, TransformerMixin, PlotterMixin):
+    """:ref:`Persistence diagrams <persistence_diagram>` resulting from
+    `Cech filtrations <cech_complex_and_cech_persistence>`_.
+
+    Given a :ref:`point cloud <distance_matrices_and_point_clouds>` in
+    Euclidean space, information about the appearance and disappearance of
+    topological features (technically, :ref:`homology classes
+    <homology_and_cohomology>`) of various dimensions and at different scales
+    is summarised in the corresponding persistence diagram.
+
+    **Important note**:
+
+        - Persistence diagrams produced by this class must be interpreted with
+          care due to the presence of padding triples which carry no
+          information. See :meth:`transform` for additional information.
+
+    Parameters
+    ----------
+    homology_dimensions : list or tuple, optional, default: ``(0, 1)``
+        Dimensions (non-negative integers) of the topological features to be
+        detected.
+
+    coeff : int prime, optional, default: ``2``
+        Compute homology with coefficients in the prime field
+        :math:`\\mathbb{F}_p = \\{ 0, \\ldots, p - 1 \\}` where :math:`p`
+        equals `coeff`.
+
+    max_edge_length : float, optional, default: ``numpy.inf``
+        Maximum value of the Cech filtration parameter. Topological features at
+        scales larger than this value will not be detected.
+
+    infinity_values : float or None, default: ``None``
+        Which death value to assign to features which are still alive at
+        filtration value `max_edge_length`. ``None`` means that this death
+        value is declared to be equal to `max_edge_length`.
+
+    reduced_homology : bool, optional, default: ``True``
+       If ``True``, the earliest-born triple in homology dimension 0 which has
+       infinite death is discarded in :meth:`transform`.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    infinity_values_ : float
+        Effective death value to assign to features which are still alive at
+        filtration value `max_edge_length`.
+
+    See also
+    --------
+    VietorisRipsPersistence, FlagserPersistence, SparseRipsPersistence,
+    WeakAlphaPersistence
+
+    Notes
+    -----
+    `GUDHI <https://github.com/GUDHI/gudhi-devel>`_ is used as a C++ backend
+    for computing Cech persistent homology [1]_. Python bindings were modified
+    for performance.
+
+    References
+    ----------
+    .. [1] C. Maria, "Persistent Cohomology", 2020; `GUDHI User and Reference
+           Manual <http://gudhi.gforge.inria.fr/doc/3.1.0/group__persistent__\
+           cohomology.html>`_.
+
+    """
+
+    _hyperparameters = {
+        "homology_dimensions": {
+            "type": (list, tuple),
+            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
+            },
+        "coeff": {"type": int, "in": Interval(2, np.inf, closed="left")},
+        "max_edge_length": {"type": Real,
+                            "in": Interval(0, np.inf, closed="right")},
+        "infinity_values": {"type": (Real, type(None)),
+                            "in": Interval(0, np.inf, closed="neither")},
+        "reduced_homology": {"type": bool}
+        }
+
+    def __init__(self, homology_dimensions=(0, 1), coeff=2,
+                 max_edge_length=np.inf, infinity_values=None,
+                 reduced_homology=True, n_jobs=None):
+        self.homology_dimensions = homology_dimensions
+        self.coeff = coeff
+        self.max_edge_length = max_edge_length
+        self.infinity_values = infinity_values
+        self.reduced_homology = reduced_homology
+        self.n_jobs = n_jobs
+
+    def _gudhi_diagram(self, X):
+        cech_complex = CechComplex(points=X, max_radius=self.max_edge_length)
+        simplex_tree = cech_complex.create_simplex_tree(
+            max_dimension=max(self._homology_dimensions) + 1
+            )
+        Xdgm = simplex_tree.persistence(homology_coeff_field=self.coeff,
+                                        min_persistence=0)
+
+        return Xdgm
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`infinity_values_`. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray or list of length n_samples
+            Input data representing a collection of point clouds. Can be either
+            a 3D ndarray whose zeroth dimension has size ``n_samples``, or a
+            list containing ``n_samples`` 2D ndarrays. Point cloud arrays have
+            shape ``(n_points, n_dimensions)``, and if `X` is a list these
+            shapes can vary between point clouds.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_point_clouds(X)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
+
+        if self.infinity_values is None:
+            self.infinity_values_ = self.max_edge_length
+        else:
+            self.infinity_values_ = self.infinity_values
+
+        self._homology_dimensions = sorted(self.homology_dimensions)
+        self._max_homology_dimension = self._homology_dimensions[-1]
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each point cloud in `X`, compute the relevant persistence
+        diagram as an array of triples [b, d, q]. Each triple represents a
+        persistent topological feature in dimension q (belonging to
+        `homology_dimensions`) which is born at b and dies at d. Only triples
+        in which b < d are meaningful. Triples in which b and d are equal
+        ("diagonal elements") may be artificially introduced during the
+        computation for padding purposes, since the number of non-trivial
+        persistent topological features is typically not constant across
+        samples. They carry no information and hence should be effectively
+        ignored by any further computation.
+
+        Parameters
+        ----------
+        X : ndarray or list of length n_samples
+            Input data representing a collection of point clouds. Can be either
+            a 3D ndarray whose zeroth dimension has size ``n_samples``, or a
+            list containing ``n_samples`` 2D ndarrays. Point cloud arrays have
+            shape ``(n_points, n_dimensions)``, and if `X` is a list these
+            shapes can vary between point clouds.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Array of persistence diagrams computed from the feature arrays in
+            `X`. ``n_features`` equals :math:`\\sum_q n_q`, where :math:`n_q`
+            is the maximum number of topological features in dimension
+            :math:`q` across all samples in `X`.
+
+        """
+        check_is_fitted(self)
+        X = check_point_clouds(X)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(delayed(self._gudhi_diagram)(x)
+                                          for x in X)
+
+        Xt = _postprocess_diagrams(
+            Xt, "gudhi", self._homology_dimensions, self.infinity_values_,
+            self.reduced_homology
+            )
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
+        """Plot a sample from a collection of persistence diagrams, with
+        homology in multiple dimensions.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Collection of persistence diagrams, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        homology_dimensions : list, tuple or None, optional, default: ``None``
+            Which homology dimensions to include in the plot. ``None`` means
+            plotting all dimensions present in ``Xt[sample]``.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_diagram(
+            Xt[sample], homology_dimensions=homology_dimensions,
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class FlagserPersistence(BaseEstimator, TransformerMixin, PlotterMixin):
+    """:ref:`Persistence diagrams <persistence_diagram>` resulting from
+    :ref:`filtrations <filtered_complex>` of :ref:`directed or undirected flag
+    complexes <clique_and_flag_complexes>` [1]_.
+
+    Given a weighted directed or undirected graph, information about the
+    appearance and disappearance of topological features (technically,
+    :ref:`homology classes <homology_and_cohomology>`) of various dimension and
+    at different scales is summarised in the corresponding persistence diagram.
+
+    **Important note**:
+
+        - Persistence diagrams produced by this class must be interpreted with
+          care due to the presence of padding triples which carry no
+          information. See :meth:`transform` for additional information.
+
+    Parameters
+    ----------
+    homology_dimensions : list or tuple, optional, default: ``(0, 1)``
+        Dimensions (non-negative integers) of the topological features to be
+        detected.
+
+    directed : bool, optional, default: ``True``
+        If ``True``, :meth:`transform` computes the persistence diagrams of the
+        filtered directed flag complexes arising from the input collection of
+        weighted directed graphs. If ``False``, :meth:`transform` computes the
+        persistence diagrams of the filtered undirected flag complexes obtained
+        by regarding all input weighted graphs as undirected, and:
+
+        - if `max_edge_weight` is ``numpy.inf``, it is sufficient to pass a
+          collection of (dense or sparse) upper-triangular matrices;
+        - if `max_edge_weight` is finite, it is recommended to pass either a
+          collection of symmetric dense matrices, or a collection of sparse
+          upper-triangular matrices.
+
+    filtration : string, optional, default: ``"max"``
+        Algorithm determining the filtration values of higher order simplices
+        from the weights of the vertices and edges. Possible values are:
+        ["dimension", "zero", "max", "max3", "max_plus_one", "product", "sum",
+        "pmean", "pmoment", "remove_edges", "vertex_degree"]
+
+    coeff : int prime, optional, default: ``2``
+        Compute homology with coefficients in the prime field
+        :math:`\\mathbb{F}_p = \\{ 0, \\ldots, p - 1 \\}` where :math:`p`
+        equals `coeff`.
+
+    max_edge_weight : float, optional, default: ``numpy.inf``
+        Maximum edge weight to be considered in the filtration. All edge
+        weights greater than this value will be considered as absent from the
+        filtration and topological features at scales larger than this value
+        will not be detected.
+
+    infinity_values : float or None, default: ``None``
+        Which death value to assign to features which are still alive at
+        filtration value `max_edge_weight`. ``None`` means that this death
+        value is declared to be equal to `max_edge_weight`.
+
+    reduced_homology : bool, optional, default: ``True``
+       If ``True``, the earliest-born triple in homology dimension 0 which has
+       infinite death is discarded from each diagram computed in
+       :meth:`transform`.
+
+    max_entries : int, optional, default: ``-1``
+        Number controlling the degree of precision in the matrix reductions
+        performed by the the backend. Corresponds to the parameter
+        ``approximation`` in :func:`pyflagser.flagser_weighted` and
+        :func:`pyflagser.flagser_unweighted`. Increase for higher precision,
+        decrease for faster computation. A good value is often ``100000`` in
+        hard problems. A negative value computes highest possible precision.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    infinity_values_ : float
+        Effective death value to assign to features which are still alive at
+        filtration value `max_edge_weight`.
+
+    See also
+    --------
+    VietorisRipsPersistence, WeightedRipsPersistence, SparseRipsPersistence,
+    WeakAlphaPersistence, EuclideanCechPersistence, ConsistentRescaling,
+    ConsecutiveRescaling
+
+    Notes
+    -----
+    The `pyflagser <https://github.com/giotto-ai/pyflagser>`_ Python package
+    is used for binding `Flagser <https://github.com/luetge/flagser>`_, a C++
+    backend for computing the (persistent) homology of (filtered) directed
+    flag complexes. For more details, please refer to the `flagser \
+    documentation <https://github.com/luetge/flagser/blob/master/docs/\
+    documentation_flagser.pdf>`_.
+
+    References
+    ----------
+    .. [1] D. Luetgehetmann, D. Govc, J. P. Smith, and R. Levi, "Computing
+           persistent homology of directed flag complexes", *Algorithms*,
+           13(1), 2020.
+
+    """
+
+    _hyperparameters = {
+        "homology_dimensions": {
+            "type": (list, tuple),
+            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
+            },
+        "directed": {"type": bool},
+        "coeff": {"type": int, "in": Interval(2, np.inf, closed="left")},
+        "max_edge_weight": {"type": Real},
+        "infinity_values": {"type": (Real, type(None))},
+        "reduced_homology": {"type": bool},
+        "max_entries": {"type": int}
+        }
+
+    def __init__(self, homology_dimensions=(0, 1), directed=True,
+                 filtration="max", coeff=2, max_edge_weight=np.inf,
+                 infinity_values=None, reduced_homology=True, max_entries=-1,
+                 n_jobs=None):
+        self.homology_dimensions = homology_dimensions
+        self.directed = directed
+        self.filtration = filtration
+        self.coeff = coeff
+        self.max_edge_weight = max_edge_weight
+        self.infinity_values = infinity_values
+        self.reduced_homology = reduced_homology
+        self.max_entries = max_entries
+        self.n_jobs = n_jobs
+
+    def _flagser_diagram(self, X):
+        Xdgms = [np.empty((0, 2), dtype=float)] * self._min_homology_dimension
+        Xdgms += flagser_weighted(X, max_edge_weight=self.max_edge_weight,
+                                  min_dimension=self._min_homology_dimension,
+                                  max_dimension=self._max_homology_dimension,
+                                  directed=self.directed,
+                                  filtration=self.filtration, coeff=self.coeff,
+                                  approximation=self.max_entries)["dgms"]
+        n_missing_dims = self._max_homology_dimension + 1 - len(Xdgms)
+        if n_missing_dims:
+            Xdgms += [np.empty((0, 2), dtype=float)] * n_missing_dims
+
+        return Xdgms
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`infinity_values_`. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray or list of length n_samples
+            Input collection of adjacency matrices of weighted directed or
+            undirected graphs. Can be either a 3D ndarray whose zeroth
+            dimension has size ``n_samples``, or a list containing
+            ``n_samples`` 2D ndarrays/sparse matrices. In each adjacency
+            matrix, diagonal elements are vertex weights and off-diagonal
+            elements are edge weights. It is assumed that a vertex weight
+            cannot be larger than the weight of the edges it forms. The way
+            zero values are handled depends on the format of the matrix. If the
+            matrix is a dense ``numpy.ndarray``, zero values denote
+            zero-weighted edges. If the matrix is a sparse ``scipy.sparse``
+            matrix, explicitly stored off-diagonal zeros and all diagonal zeros
+            denote zero-weighted edges. Off-diagonal values that have not been
+            explicitly stored are treated by ``scipy.sparse`` as zeros but will
+            be understood as infinitely-valued edges, i.e., edges absent from
+            the filtration.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_point_clouds(X, accept_sparse=True, distance_matrices=True)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=["n_jobs",
+                                                               "filtration"])
+
+        if self.infinity_values is None:
+            self.infinity_values_ = self.max_edge_weight
+        else:
+            self.infinity_values_ = self.infinity_values
+
+        self._homology_dimensions = sorted(self.homology_dimensions)
+        self._min_homology_dimension = self._homology_dimensions[0]
+        self._max_homology_dimension = self._homology_dimensions[-1]
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each adjacency matrix in `X`, compute the relevant persistence
+        diagram as an array of triples [b, d, q]. Each triple represents a
+        persistent topological feature in dimension q (belonging to
+        `homology_dimensions`) which is born at b and dies at d. Only triples
+        in which b < d are meaningful. Triples in which b and d are equal
+        ("diagonal elements") may be artificially introduced during the
+        computation for padding purposes, since the number of non-trivial
+        persistent topological features is typically not constant across
+        samples. They carry no information and hence should be effectively
+        ignored by any further computation.
+
+        Parameters
+        ----------
+        X : ndarray or list of length n_samples
+            Input collection of adjacency matrices of weighted directed or
+            undirected graphs. Can be either a 3D ndarray whose zeroth
+            dimension has size ``n_samples``, or a list containing
+            ``n_samples`` 2D ndarrays/sparse matrices. In each adjacency
+            matrix, diagonal elements are vertex weights and off-diagonal
+            elements are edges weights. It is assumed that a vertex weight
+            cannot be larger than the weight of the edges it forms. The way
+            zero values are handled depends on the format of the matrix. If
+            the matrix is a dense ``numpy.ndarray``, zero values denote
+            zero-weighted edges. If the matrix is a sparse ``scipy.sparse``
+            matrix, explicitly stored off-diagonal zeros and all diagonal zeros
+            denote zero-weighted edges. Off-diagonal values that have not been
+            explicitly stored are treated by ``scipy.sparse`` as zeros but will
+            be understood as infinitely-valued edges, i.e., edges absent from
+            the filtration.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Array of persistence diagrams computed from the feature arrays or
+            distance matrices in `X`. ``n_features`` equals
+            :math:`\\sum_q n_q`, where :math:`n_q` is the maximum number of
+            topological features in dimension :math:`q` across all samples in
+            `X`.
+
+        """
+        check_is_fitted(self)
+        X = check_point_clouds(X, accept_sparse=True, distance_matrices=True)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._flagser_diagram)(x) for x in X)
+
+        Xt = _postprocess_diagrams(
+            Xt, "flagser", self._homology_dimensions, self.infinity_values_,
+            self.reduced_homology
+            )
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
+        """Plot a sample from a collection of persistence diagrams, with
+        homology in multiple dimensions.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Collection of persistence diagrams, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        homology_dimensions : list, tuple or None, optional, default: ``None``
+            Which homology dimensions to include in the plot. ``None`` means
+            plotting all dimensions present in ``Xt[sample]``.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_diagram(
+            Xt[sample], homology_dimensions=homology_dimensions,
+            plotly_params=plotly_params
+            )
```

## gtda/homology/_utils.py

 * *Ordering differences only*

```diff
@@ -1,65 +1,65 @@
-"""Utility functions for persistent homology."""
-# License: GNU AGPLv3
-
-import numpy as np
-
-
-def _postprocess_diagrams(
-        Xt, format, homology_dimensions, infinity_values, reduced
-        ):
-    # NOTE: `homology_dimensions` must be sorted in ascending order
-    def replace_infinity_values(subdiagram):
-        np.nan_to_num(subdiagram, posinf=infinity_values, copy=False)
-        return subdiagram[subdiagram[:, 0] < subdiagram[:, 1]]
-
-    # Replace np.inf with infinity_values and turn into list of dictionaries
-    # whose keys are the dimensions
-    if format in ["ripser", "flagser"]:  # Input is list of list of subdiagrams
-        # In H0, remove one infinite bar placed at the end by ripser or flagser
-        # only if `reduce` is True
-        slices = {dim: slice(None) if (dim or not reduced) else slice(None, -1)
-                  for dim in homology_dimensions}
-        Xt = [{dim: replace_infinity_values(diagram[dim][slices[dim]])
-               for dim in homology_dimensions} for diagram in Xt]
-    elif format == "gudhi":  # Input is list of list of [dim, (birth, death)]
-        # In H0, remove one infinite bar placed at the beginning by GUDHI only
-        # if `reduce` is True
-        slices = {dim: slice(None) if (dim or not reduced) else slice(1, None)
-                  for dim in homology_dimensions}
-        Xt = [{dim: replace_infinity_values(
-            np.array([pers_info[1] for pers_info in diagram
-                      if pers_info[0] == dim]).reshape(-1, 2)[slices[dim]]
-            )
-            for dim in homology_dimensions} for diagram in Xt]
-    else:
-        raise ValueError(
-            f"Unknown input format {format} for collection of diagrams."
-            )
-
-    # Conversion to array of triples with padding triples
-    start_idx_per_dim = np.cumsum(
-            [0] + [np.max([len(diagram[dim]) for diagram in Xt] + [1])
-                   for dim in homology_dimensions]
-            )
-    min_values = [min([np.min(diagram[dim][:, 0]) if diagram[dim].size
-                       else np.inf for diagram in Xt])
-                  for dim in homology_dimensions]
-    min_values = [min_value if min_value != np.inf else 0
-                  for min_value in min_values]
-    n_features = start_idx_per_dim[-1]
-    Xt_padded = np.empty((len(Xt), n_features, 3), dtype=float)
-
-    for i, dim in enumerate(homology_dimensions):
-        start_idx, end_idx = start_idx_per_dim[i:i + 2]
-        padding_value = min_values[i]
-        # Add dimension as the third elements of each (b, d) tuple globally
-        Xt_padded[:, start_idx:end_idx, 2] = dim
-        for j, diagram in enumerate(Xt):
-            subdiagram = diagram[dim]
-            end_idx_nontrivial = start_idx + len(subdiagram)
-            # Populate nontrivial part of the subdiagram
-            Xt_padded[j, start_idx:end_idx_nontrivial, :2] = subdiagram
-            # Insert padding triples
-            Xt_padded[j, end_idx_nontrivial:end_idx, :2] = [padding_value] * 2
-
-    return Xt_padded
+"""Utility functions for persistent homology."""
+# License: GNU AGPLv3
+
+import numpy as np
+
+
+def _postprocess_diagrams(
+        Xt, format, homology_dimensions, infinity_values, reduced
+        ):
+    # NOTE: `homology_dimensions` must be sorted in ascending order
+    def replace_infinity_values(subdiagram):
+        np.nan_to_num(subdiagram, posinf=infinity_values, copy=False)
+        return subdiagram[subdiagram[:, 0] < subdiagram[:, 1]]
+
+    # Replace np.inf with infinity_values and turn into list of dictionaries
+    # whose keys are the dimensions
+    if format in ["ripser", "flagser"]:  # Input is list of list of subdiagrams
+        # In H0, remove one infinite bar placed at the end by ripser or flagser
+        # only if `reduce` is True
+        slices = {dim: slice(None) if (dim or not reduced) else slice(None, -1)
+                  for dim in homology_dimensions}
+        Xt = [{dim: replace_infinity_values(diagram[dim][slices[dim]])
+               for dim in homology_dimensions} for diagram in Xt]
+    elif format == "gudhi":  # Input is list of list of [dim, (birth, death)]
+        # In H0, remove one infinite bar placed at the beginning by GUDHI only
+        # if `reduce` is True
+        slices = {dim: slice(None) if (dim or not reduced) else slice(1, None)
+                  for dim in homology_dimensions}
+        Xt = [{dim: replace_infinity_values(
+            np.array([pers_info[1] for pers_info in diagram
+                      if pers_info[0] == dim]).reshape(-1, 2)[slices[dim]]
+            )
+            for dim in homology_dimensions} for diagram in Xt]
+    else:
+        raise ValueError(
+            f"Unknown input format {format} for collection of diagrams."
+            )
+
+    # Conversion to array of triples with padding triples
+    start_idx_per_dim = np.cumsum(
+            [0] + [np.max([len(diagram[dim]) for diagram in Xt] + [1])
+                   for dim in homology_dimensions]
+            )
+    min_values = [min([np.min(diagram[dim][:, 0]) if diagram[dim].size
+                       else np.inf for diagram in Xt])
+                  for dim in homology_dimensions]
+    min_values = [min_value if min_value != np.inf else 0
+                  for min_value in min_values]
+    n_features = start_idx_per_dim[-1]
+    Xt_padded = np.empty((len(Xt), n_features, 3), dtype=float)
+
+    for i, dim in enumerate(homology_dimensions):
+        start_idx, end_idx = start_idx_per_dim[i:i + 2]
+        padding_value = min_values[i]
+        # Add dimension as the third elements of each (b, d) tuple globally
+        Xt_padded[:, start_idx:end_idx, 2] = dim
+        for j, diagram in enumerate(Xt):
+            subdiagram = diagram[dim]
+            end_idx_nontrivial = start_idx + len(subdiagram)
+            # Populate nontrivial part of the subdiagram
+            Xt_padded[j, start_idx:end_idx_nontrivial, :2] = subdiagram
+            # Insert padding triples
+            Xt_padded[j, end_idx_nontrivial:end_idx, :2] = [padding_value] * 2
+
+    return Xt_padded
```

## gtda/homology/__init__.py

 * *Ordering differences only*

```diff
@@ -1,18 +1,18 @@
-"""The module :mod:`gtda.homology` implements transformers to generate
-persistence diagrams."""
-# License: GNU AGPLv3
-
-from .simplicial import VietorisRipsPersistence, WeightedRipsPersistence, \
-    SparseRipsPersistence, WeakAlphaPersistence, EuclideanCechPersistence, \
-    FlagserPersistence
-from .cubical import CubicalPersistence
-
-__all__ = [
-    'VietorisRipsPersistence',
-    'WeightedRipsPersistence',
-    'SparseRipsPersistence',
-    'WeakAlphaPersistence',
-    'EuclideanCechPersistence',
-    'FlagserPersistence',
-    'CubicalPersistence',
-    ]
+"""The module :mod:`gtda.homology` implements transformers to generate
+persistence diagrams."""
+# License: GNU AGPLv3
+
+from .simplicial import VietorisRipsPersistence, WeightedRipsPersistence, \
+    SparseRipsPersistence, WeakAlphaPersistence, EuclideanCechPersistence, \
+    FlagserPersistence
+from .cubical import CubicalPersistence
+
+__all__ = [
+    'VietorisRipsPersistence',
+    'WeightedRipsPersistence',
+    'SparseRipsPersistence',
+    'WeakAlphaPersistence',
+    'EuclideanCechPersistence',
+    'FlagserPersistence',
+    'CubicalPersistence',
+    ]
```

## gtda/images/filtrations.py

```diff
@@ -1,1290 +1,1290 @@
-"""Filtrations of 2D/3D binary images."""
-# License: GNU AGPLv3
-
-from numbers import Real, Integral
-from types import FunctionType
-import itertools
-
-import numpy as np
-from joblib import Parallel, delayed, effective_n_jobs
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.metrics import pairwise_distances
-from sklearn.utils import gen_even_slices
-from sklearn.utils.validation import check_array, check_is_fitted
-
-from ._utils import _dilate, _erode
-from .preprocessing import Padder
-from ..base import PlotterMixin
-from ..plotting import plot_heatmap
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.intervals import Interval
-from ..utils.validation import validate_params
-
-
-@adapt_fit_transform_docs
-class HeightFiltration(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Filtrations of 2D/3D binary images based on distances to lines/planes.
-
-    The height filtration assigns to each activated pixel of a binary image a
-    greyscale value equal to the distance between the pixel and the hyperplane
-    defined by a direction vector and the first seen edge of the image
-    following that direction. Deactivated pixels are assigned the value of the
-    maximum distance between any pixel of the image and the hyperplane, plus
-    one.
-
-    Parameters
-    ----------
-    direction : ndarray of shape (n_dimensions,) or None, optional, default: \
-        ``None``
-        Direction vector of the height filtration in
-        ``n_dimensions``-dimensional space, where ``n_dimensions`` is the
-        dimension of the images of the collection (2 or 3). ``None`` is
-        equivalent to passing ``numpy.ones(n_dimensions)``.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    n_dimensions_ : ``2`` or ``3``
-        Dimension of the images. Set in :meth:`fit`.
-
-    direction_ : ndarray of shape (:attr:`n_dimensions_`,)
-        Effective direction of the height filtration. Set in :meth:`fit`.
-
-    mesh_ : ndarray of shape ( n_pixels_x, n_pixels_y [, n_pixels_z])
-        greyscale image corresponding to the height filtration of a binary
-        image where each pixel is activated. Set in :meth:`fit`.
-
-    max_value_ : float
-        Maximum pixel value among all pixels in all images of the collection.
-        Set in :meth:`fit`.
-
-    See also
-    --------
-    RadialFiltration, DilationFiltration, ErosionFiltration, \
-    SignedDistanceFiltration, DensityFiltration, \
-    gtda.homology.CubicalPersistence
-
-    References
-    ----------
-    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
-           Classification of MNIST using TDA"; 19th International IEEE
-           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
-           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
-
-    """
-
-    _hyperparameters = {
-        'direction': {'type': (np.ndarray, type(None)), 'of': {'type': Real}}
-        }
-
-    def __init__(self, direction=None, n_jobs=None):
-        self.direction = direction
-        self.n_jobs = n_jobs
-
-    def _calculate_height(self, X):
-        Xh = np.full(X.shape, self.max_value_)
-
-        for i in range(len(Xh)):
-            Xh[i][np.where(X[i])] = np.dot(self.mesh_[np.where(X[i])],
-                                           self.direction_).reshape((-1,))
-
-        return Xh
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`n_dimensions_`, :attr:`direction_`, :attr:`mesh_`
-        and :attr:`max_value_` from a collection of binary images. Then,
-        return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_array(X, allow_nd=True)
-        self.n_dimensions_ = X.ndim - 1
-        if self.n_dimensions_ > 3:
-            raise ValueError(f"Input of `fit` contains arrays of dimension "
-                             f"{self.n_dimensions_}.")
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        if self.direction is None:
-            self.direction_ = np.ones(self.n_dimensions_,)
-        else:
-            self.direction_ = np.copy(self.direction)
-        self.direction_ = self.direction_ / np.linalg.norm(self.direction_)
-
-        axis_order = [2, 1, 3]
-        mesh_range_list = \
-            [np.arange(X.shape[order]) if self.direction_[i] >= 0
-             else -np.flip(np.arange(X.shape[order])) for i, order
-             in enumerate(axis_order[: self.n_dimensions_])]
-
-        self.mesh_ = np.stack(np.meshgrid(*mesh_range_list, indexing='xy'),
-                              axis=self.n_dimensions_)
-
-        self.max_value_ = 0.
-        self.max_value_ = np.max(self._calculate_height(
-            np.ones((1, *X.shape[1:])))) + 1
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each binary image in the collection `X`, calculate a
-        corresponding greyscale image based on the distance of its pixels to
-        the hyperplane defined by the `direction` vector and the first seen
-        edge of the images following that `direction`. Return the collection
-        of greyscale images.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y \
-            [, n_pixels_z])
-            Transformed collection of images. Each entry along axis 0 is a
-            2D or 3D greyscale image.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, allow_nd=True)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._calculate_height)(X[s])
-            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
-        Xt = np.concatenate(Xt)
-
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, colorscale='greys', origin='upper',
-             plotly_params=None):
-        """Plot a sample from a collection of 2D greyscale images.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
-            Collection of 2D greyscale images, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        colorscale : str, optional, default: ``'greys'``
-            Color scale to be used in the heat map. Can be anything allowed by
-            :class:`plotly.graph_objects.Heatmap`.
-
-        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
-            Position of the [0, 0] pixel of `data`, in the upper left or lower
-            left corner. The convention ``'upper'`` is typically used for
-            matrices and images.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_heatmap(
-            Xt[sample], colorscale=colorscale, origin=origin,
-            title=f"Height filtration of image {sample}",
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class RadialFiltration(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Filtrations of 2D/3D binary images based on distances to a reference
-    pixel.
-
-    The radial filtration assigns to each pixel of a binary image a greyscale
-    value computed as follows in terms of a reference pixel, called the
-    "center", and of a "radius": if the binary pixel is active and lies
-    within a ball defined by this center and this radius, then the assigned
-    value equals this distance. In all other cases, the assigned value equals
-    the maximum distance between any pixel of the image and the center
-    pixel, plus one.
-
-    Parameters
-    ----------
-    center : ndarray of shape (:attr:`n_dimensions_`,) or None, optional,\
-        default: ``None``
-        Coordinates of the center pixel, where ``n_dimensions`` is the
-        dimension of the images of the collection (2 or 3). ``None`` is
-        equivalent to passing ``np.zeros(n_dimensions,)```.
-
-    radius : float or None, default: ``None``
-        The radius of the ball centered in `center` inside which activated
-        pixels are included in the filtration.
-
-    metric : string or callable, optional, default: ``'euclidean'``
-        If set to ``'precomputed'``, each entry in `X` along axis 0 is
-        interpreted to be a distance matrix. Otherwise, entries are
-        interpreted as feature arrays, and `metric` determines a rule with
-        which to calculate distances between pairs of instances (i.e. rows)
-        in these arrays.
-        If `metric` is a string, it must be one of the options allowed by
-        :func:`scipy.spatial.distance.pdist` for its metric parameter, or a
-        metric listed in :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`,
-        including "euclidean", "manhattan" or "cosine".
-        If `metric` is a callable function, it is called on each pair of
-        instances and the resulting value recorded. The callable should take
-        two arrays from the entry in `X` as input, and return a value
-        indicating the distance between them.
-
-    metric_params : dict or None, optional, default: ``{}``
-        Additional keyword arguments for the metric function.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    n_dimensions_ : ``2`` or ``3``
-        Dimension of the images. Set in :meth:`fit`.
-
-    center_ : ndarray of shape (:attr:`n_dimensions_`,)
-        Effective center of the radial filtration. Set in :meth:`fit`.
-
-    mesh_ : ndarray of shape ( n_pixels_x, n_pixels_y [, n_pixels_z])
-        greyscale image corresponding to the radial filtration of a binary
-        image where each pixel is activated. Set in :meth:`fit`.
-
-    max_value_ : float
-        Maximum pixel value among all pixels in all images of the collection.
-        Set in :meth:`fit`.
-
-    See also
-    --------
-    HeightFiltration, DilationFiltration, ErosionFiltration, \
-    SignedDistanceFiltration, DensityFiltration, \
-    gtda.homology.CubicalPersistence
-
-    References
-    ----------
-    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
-           Classification of MNIST using TDA"; 19th International IEEE
-           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
-           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
-
-    """
-
-    _hyperparameters = {
-        'center': {'type': (np.ndarray, type(None)), 'of': {'type': Integral}},
-        'radius': {'type': Real, 'in': Interval(0, np.inf, closed='right')},
-        'metric': {'type': (str, FunctionType)},
-        'metric_params': {'type': dict}
-        }
-
-    def __init__(self, center=None, radius=np.inf, metric='euclidean',
-                 metric_params={}, n_jobs=None):
-        self.center = center
-        self.radius = radius
-        self.metric = metric
-        self.metric_params = metric_params
-        self.n_jobs = n_jobs
-
-    def _calculate_radial(self, X):
-        Xr = np.nan_to_num(self.mesh_ * X, nan=np.inf, posinf=np.inf)
-        Xr = np.nan_to_num(Xr, posinf=-1)
-
-        Xr[X == 0] = self.max_value_
-        Xr[Xr == -1] = self.max_value_
-
-        return Xr
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`center_`, :attr:`n_dimensions_`, :attr:`mesh_` and
-        :attr:`max_value_` from a collection of binary images. Then, return the
-        estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_array(X, allow_nd=True)
-        self.n_dimensions_ = X.ndim - 1
-        if self.n_dimensions_ > 3:
-            raise ValueError(f"Input of `fit` contains arrays of dimension "
-                             f"{self.n_dimensions_}.")
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        if self.center is None:
-            self.center_ = np.zeros(self.n_dimensions_)
-        else:
-            self.center_ = np.copy(self.center)
-        self.center_ = self.center_.reshape((1, -1))
-
-        axis_order = [2, 1, 3]
-        mesh_range_list = [np.arange(0, X.shape[i])
-                           for i in axis_order[:self.n_dimensions_]]
-
-        self.mesh_ = np.stack(
-            np.meshgrid(*mesh_range_list),
-            axis=self.n_dimensions_).reshape((-1, self.n_dimensions_))
-        self.mesh_ = pairwise_distances(
-            self.center_, self.mesh_, metric=self.metric,
-            n_jobs=1, **self.metric_params).reshape(X.shape[1:])
-        self.mesh_[self.mesh_ > self.radius] = np.inf
-
-        self.max_value_ = 0.
-        self.max_value_ = \
-            np.max(self._calculate_radial(np.ones((1, *X.shape[1:])))) + 1
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each binary image in the collection `X`, calculate a
-        corresponding greyscale image based on the distance of its pixels to
-        the center. Return the collection of greyscale images.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_pixels_x,
-            n_pixels_y [, n_pixels_z])
-            Transformed collection of images. Each entry along axis 0 is a
-            2D or 3D greyscale image.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, allow_nd=True)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._calculate_radial)(X[s])
-            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
-        Xt = np.concatenate(Xt)
-
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, colorscale='greys', origin='upper',
-             plotly_params=None):
-        """Plot a sample from a collection of 2D greyscale images.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
-            Collection of 2D greyscale images, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        colorscale : str, optional, default: ``'greys'``
-            Color scale to be used in the heat map. Can be anything allowed by
-            :class:`plotly.graph_objects.Heatmap`.
-
-        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
-            Position of the [0, 0] pixel of `data`, in the upper left or lower
-            left corner. The convention ``'upper'`` is typically used for
-            matrices and images.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_heatmap(
-            Xt[sample], colorscale=colorscale, origin=origin,
-            title=f"Radial filtration of image {sample}",
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class DilationFiltration(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Filtrations of 2D/3D binary images based on the dilation of activated
-    regions.
-
-    Binary dilation is a morphological operator commonly used in
-    image processing and relies on the `scipy.ndimage \
-    <https://docs.scipy.org/doc/scipy/reference/ndimage.html>`_ module.
-
-    This filtration assigns to each pixel in an image a greyscale value
-    calculated as follows. If the minimum Manhattan distance between the
-    pixel and any activated pixel in the image is less than or equal to
-    the parameter `n_iterations`, the assigned value is this distance 
-    in particular, activated pixels are assigned a value of 0.
-    Otherwise, the assigned greyscale value is the sum of the lengths
-    along all axes of the image  equivalently, it is the maximum
-    Manhattan distance between any two pixels in the image. The name of
-    this filtration comes from the fact that these values can be computed
-    by iteratively dilating activated regions, thickening them by a total
-    amount `n_iterations`.
-
-    Parameters
-    ----------
-    n_iterations : int or None, optional, default: ``None``
-        Number of iterations in the dilation process. ``None`` means dilation
-        reaches all deactivated pixels.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    n_dimensions_ : ``2`` or ``3``
-        Dimension of the images. Set in :meth:`fit`.
-
-    n_iterations_ : int
-        Effective number of iterations in the dilation process. Set in
-        :meth:`fit`.
-
-    max_value_ : float
-        Maximum pixel value among all pixels in all images of the collection.
-        Set in :meth:`fit`.
-
-    See also
-    --------
-    HeightFiltration, RadialFiltration, ErosionFiltration, \
-    SignedDistanceFiltration, DensityFiltration, \
-    gtda.homology.CubicalPersistence
-
-    References
-    ----------
-    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
-           Classification of MNIST using TDA"; 19th International IEEE
-           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
-           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
-
-    """
-
-    _hyperparameters = {
-        'n_iterations': {'type': (int, type(None)),
-                         'in': Interval(1, np.inf, closed='left')}
-        }
-
-    def __init__(self, n_iterations=None, n_jobs=None):
-        self.n_iterations = n_iterations
-        self.n_jobs = n_jobs
-
-    def _calculate_dilation(self, X):
-        Xd = _dilate(X, 1, self.n_iterations_, 1, self.max_value_)
-
-        mask_undilated = Xd == 0
-        Xd -= 1
-        Xd[mask_undilated] = self.max_value_
-        return Xd
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`n_dimensions_`, :attr:`n_iterations_` and
-        :attr:`max_value_` from a collection of binary images. Then, return the
-        estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_array(X, allow_nd=True)
-        self.n_dimensions_ = X.ndim - 1
-        if self.n_dimensions_ > 3:
-            raise ValueError(f"Input of `fit` contains arrays of dimension "
-                             f"{self.n_dimensions_}.")
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        self.max_value_ = np.sum(X.shape[1:])
-
-        if self.n_iterations is None:
-            self.n_iterations_ = int(self.max_value_)
-        else:
-            self.n_iterations_ = self.n_iterations
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each binary image in the collection `X`, calculate a
-        corresponding greyscale image based on the distance of its pixels to
-        their closest activated neighboring pixel. Return the collection
-        of greyscale images.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_pixels_x,
-            n_pixels_y [, n_pixels_z])
-            Transformed collection of images. Each entry along axis 0 is a
-            2D or 3D greyscale image.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, allow_nd=True)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._calculate_dilation)(X[s])
-            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
-        Xt = np.concatenate(Xt)
-
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, colorscale='greys', origin='upper',
-             plotly_params=None):
-        """Plot a sample from a collection of 2D greyscale images.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
-            Collection of 2D greyscale images, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        colorscale : str, optional, default: ``'greys'``
-            Color scale to be used in the heat map. Can be anything allowed by
-            :class:`plotly.graph_objects.Heatmap`.
-
-        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
-            Position of the [0, 0] pixel of `data`, in the upper left or lower
-            left corner. The convention ``'upper'`` is typically used for
-            matrices and images.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_heatmap(
-            Xt[sample], colorscale=colorscale, origin=origin,
-            title=f"Dilation filtration of image {sample}",
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class ErosionFiltration(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Filtrations of 2D/3D binary images based on the erosion of activated
-    regions.
-
-    Binary erosion is a morphological operator commonly used in
-    image processing and relies on the `scipy.ndimage \
-    <https://docs.scipy.org/doc/scipy/reference/ndimage.html>`_ module.
-
-    This filtration assigns to each pixel in an image a greyscale value
-    calculated as follows. If the minimum Manhattan distance between the
-    pixel and any deactivated pixel in the image is less than or equal to
-    the parameter `n_iterations`, the assigned value is this distance 
-    in particular, deactivated pixels are assigned a value of 0.
-    Otherwise, the assigned greyscale value is the sum of the lengths
-    along all axes of the image  equivalently, it is the maximum
-    Manhattan distance between any two pixels in the image. The name of
-    this filtration comes from the fact that these values can be computed
-    by iteratively eroding activated regions, shrinking them by a total
-    amount `n_iterations`.
-
-    Parameters
-    ----------
-    n_iterations : int or None, optional, default: ``None``
-        Number of iterations in the erosion process. ``None`` means erosion
-        reaches all activated pixels.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    n_dimensions_ : ``2`` or ``3``
-        Dimension of the images. Set in :meth:`fit`.
-
-    n_iterations_ : int
-        Effective number of iterations in the erosion process. Set in
-        :meth:`fit`.
-
-    max_value_ : float
-        Maximum pixel value among all pixels in all images of the collection.
-        Set in :meth:`fit`.
-
-    See also
-    --------
-    HeightFiltration, RadialFiltration, DilationFiltration, \
-    SignedDistanceFiltration, DensityFiltration, \
-    gtda.homology.CubicalPersistence
-
-    References
-    ----------
-    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
-           Classification of MNIST using TDA"; 19th International IEEE
-           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
-           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
-
-    """
-
-    _hyperparameters = {
-        'n_iterations': {'type': (int, type(None)),
-                         'in': Interval(1, np.inf, closed='left')}
-        }
-
-    def __init__(self, n_iterations=None, n_jobs=None):
-        self.n_iterations = n_iterations
-        self.n_jobs = n_jobs
-
-    def _calculate_erosion(self, X):
-        Xe = _erode(X, 1, self.n_iterations_, 1, self.max_value_)
-
-        mask_uneroded = Xe == 0
-        Xe -= 1
-        Xe[mask_uneroded] = self.max_value_
-        return Xe
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`n_dimensions_`, :attr:`n_iterations_` and
-        :attr:`max_value_` from a collection of binary images. Then, return the
-        estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_array(X, allow_nd=True)
-        self.n_dimensions_ = X.ndim - 1
-        if self.n_dimensions_ > 3:
-            raise ValueError(f"Input of `fit` contains arrays of dimension "
-                             f"{self.n_dimensions_}.")
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        self.max_value_ = np.sum(X.shape[1:])
-
-        if self.n_iterations is None:
-            self.n_iterations_ = int(self.max_value_)
-        else:
-            self.n_iterations_ = self.n_iterations
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each binary image in the collection `X`, calculate a
-        corresponding greyscale image based on the distance of its pixels to
-        their closest activated neighboring pixel. Return the collection
-        of greyscale images.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_pixels_x,
-            n_pixels_y [, n_pixels_z])
-            Transformed collection of images. Each entry along axis 0 is a
-            2D or 3D greyscale image.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, allow_nd=True)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._calculate_erosion)(X[s])
-            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
-        Xt = np.concatenate(Xt)
-
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, colorscale='greys', origin='upper',
-             plotly_params=None):
-        """Plot a sample from a collection of 2D greyscale images.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
-            Collection of 2D greyscale images, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        colorscale : str, optional, default: ``'greys'``
-            Color scale to be used in the heat map. Can be anything allowed by
-            :class:`plotly.graph_objects.Heatmap`.
-
-        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
-            Position of the [0, 0] pixel of `data`, in the upper left or lower
-            left corner. The convention ``'upper'`` is typically used for
-            matrices and images.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_heatmap(
-            Xt[sample], colorscale=colorscale, origin=origin,
-            title=f"Erosion filtration of image {sample}",
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class SignedDistanceFiltration(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Filtrations of 2D/3D binary images based on the dilation and the erosion
-    of activated regions.
-
-    This filtration assigns to each pixel in an image a greyscale value
-    calculated as follows. For activated pixels, if the minimum Manhattan
-    distance between the pixel and any deactivated pixel in the image is less
-    than or equal to the parameter `n_iterations`, the assigned value is
-    this distance minus 1. Otherwise, the assigned greyscale value is the sum
-    of the lengths along all axes of the image  equivalently, it is the
-    maximum Manhattan distance between any two pixels in the image, minus 1.
-    For deactivated pixels, if the minimum Manhattan distance between the pixel
-    and any activated pixel in the image is less than or equal to the parameter
-    `n_iterations`, the assigned value is the opposite of this distance.
-    Otherwise, the assigned greyscale value is the opposite of the maximum
-    Manhattan distance between any two pixels in the image.
-
-    The name of this filtration comes from the fact that it is a a negatively
-    signed dilation plus a positively signed erosion, minus 1 on the activated
-    pixels. Therefore, pixels the activated pixels at the boundary of the
-    activated regions always have a pixel value of 0.
-
-    Parameters
-    ----------
-    n_iterations : int or None, optional, default: ``None``
-        Number of iterations in the dilation process. ``None`` means dilation
-        over the full image.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    n_dimensions_ : ``2`` or ``3``
-        Dimension of the images. Set in :meth:`fit`.
-
-    n_iterations_ : int
-        Effective number of iterations in the dilation process. Set in
-        :meth:`fit`.
-
-    max_value_ : float
-        Maximum pixel value among all pixels in all images of the collection.
-        Set in :meth:`fit`.
-
-    See also
-    --------
-    HeightFiltration, RadialFiltration, DilationFiltration, \
-    ErosionFiltration, DensityFiltration, gtda.homology.CubicalPersistence
-
-    References
-    ----------
-    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
-           Classification of MNIST using TDA"; 19th International IEEE
-           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
-           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
-
-    """
-
-    _hyperparameters = {
-        'n_iterations': {'type': (int, type(None)),
-                         'in': Interval(1, np.inf, closed='left')}
-        }
-
-    def __init__(self, n_iterations=None, n_jobs=None):
-        self.n_iterations = n_iterations
-        self.n_jobs = n_jobs
-
-    def _calculate_signed_distance(self, X):
-        mask = X == 1
-
-        Xd = -_dilate(X, 1, self.n_iterations_, 0, self.max_value_)
-        Xe = _erode(X, 0, self.n_iterations_, 0, self.max_value_)
-
-        mask_e = Xe == 0
-        mask_d = Xd == 0
-        Xe[np.logical_not(mask)] = 0
-        Xe[mask] -= 1
-        Xd[mask] = 0
-        Xd[mask_d] = -self.max_value_
-        Xe[mask_e] = self.max_value_
-        return (Xd + Xe)
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`n_dimensions_`, :attr:`n_iterations_` and
-        :attr:`max_value_` from a collection of binary images. Then, return the
-        estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_array(X, allow_nd=True)
-        self.n_dimensions_ = X.ndim - 1
-        if self.n_dimensions_ > 3:
-            raise ValueError(f"Input of `fit` contains arrays of dimension "
-                             f"{self.n_dimensions_}.")
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        self.max_value_ = np.sum(X.shape[1:])
-
-        if self.n_iterations is None:
-            self.n_iterations_ = int(self.max_value_)
-        else:
-            self.n_iterations_ = self.n_iterations
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each binary image in the collection `X`, calculate a
-        corresponding greyscale image based on the distance of its pixels to
-        their closest activated neighboring pixel. Return the collection
-        of greyscale images.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_pixels_x,
-            n_pixels_y [, n_pixels_z])
-            Transformed collection of images. Each entry along axis 0 is a
-            2D or 3D greyscale image.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, allow_nd=True)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._calculate_signed_distance)(X[s])
-            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
-        Xt = np.concatenate(Xt)
-
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, colorscale='greys', origin='upper',
-             plotly_params=None):
-        """Plot a sample from a collection of 2D greyscale images.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
-            Collection of 2D greyscale images, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        colorscale : str, optional, default: ``'greys'``
-            Color scale to be used in the heat map. Can be anything allowed by
-            :class:`plotly.graph_objects.Heatmap`.
-
-        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
-            Position of the [0, 0] pixel of `data`, in the upper left or lower
-            left corner. The convention ``'upper'`` is typically used for
-            matrices and images.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_heatmap(
-            Xt[sample], colorscale=colorscale, origin=origin,
-            title=f"Signed-distance filtration of image {sample}",
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class DensityFiltration(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Filtrations of 2D/3D binary images based on the number of activated
-    neighboring pixels.
-
-    The density filtration assigns to each pixel of a binary image a greyscale
-    value equal to the number of activated pixels within a ball centered around
-    it.
-
-    Parameters
-    ----------
-    radius : float, optional, default: ``1.``
-        The radius of the ball within which the number of activated pixels is
-        considered.
-
-    metric : string or callable, optional, default: ``'euclidean'``
-        Determines a rule with which to calculate distances between
-        pairs of pixels.
-        If ``metric`` is a string, it must be one of the options allowed by
-        ``scipy.spatial.distance.pdist`` for its metric parameter, or a metric
-        listed in ``sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS``, including
-        "euclidean", "manhattan", or "cosine".
-        If ``metric`` is a callable function, it is called on each pair of
-        instances and the resulting value recorded. The callable should take
-        two arrays from the entry in `X` as input, and return a value
-        indicating the distance between them.
-
-    metric_params : dict, optional, default: ``{}``
-        Additional keyword arguments for the metric function.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    n_dimensions_ : ``2`` or ``3``
-        Dimension of the images. Set in :meth:`fit`.
-
-    mask_ : ndarray of shape (radius, radius [, radius])
-        The mask applied around each pixel to calculate the weighted number of
-        its activated neighbors. Set in :meth:`fit`.
-
-    See also
-    --------
-    HeightFiltration, RadialFiltration, DilationFiltration, \
-    ErosionFiltration, SignedDistanceFiltration, \
-    gtda.homology.CubicalPersistence
-
-    References
-    ----------
-    [1] A. Garin and G. Tauzin, "A topological reading lesson: Classification
-        of MNIST  using  TDA"; 19th International IEEE Conference on Machine
-        Learning and Applications (ICMLA 2020), 2019; arXiv: `1910.08345 \
-        <https://arxiv.org/abs/1910.08345>`_.
-
-    """
-
-    _hyperparameters = {
-        'radius': {'type': Real, 'in': Interval(0, np.inf, closed='right')},
-        'metric': {'type': (str, FunctionType)},
-        'metric_params': {'type': dict},
-        }
-
-    def __init__(self, radius=3, metric='euclidean', metric_params={},
-                 n_jobs=None):
-        self.radius = radius
-        self.metric = metric
-        self.metric_params = metric_params
-        self.n_jobs = n_jobs
-
-    def _calculate_density(self, X):
-        Xd = np.zeros(X.shape)
-
-        # The idea behind this is to sum up pixel values of the image
-        # rolled according to the 3D mask
-        for i, j, k in self._iterator:
-            Xd += np.roll(np.roll(
-                np.roll(X, k, axis=3), j, axis=2), i, axis=1) \
-                * self.mask_[self._size + i, self._size + j,
-                             self._size + k]
-        return Xd
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`n_dimensions_` and :attr:`mask_` from a collection
-        of binary images. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_array(X, allow_nd=True)
-        self.n_dimensions_ = X.ndim - 1
-        if self.n_dimensions_ > 3:
-            raise ValueError(f"Input of `fit` contains arrays of dimension "
-                             f"{self.n_dimensions_}.")
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        # Determine the size of the mask based on the radius and metric
-        self._size = int(np.ceil(
-            pairwise_distances([[0]], [[self.radius]], metric=self.metric,
-                               **self.metric_params)
-            ))
-        # The mask is always 3D but not the iterator.
-        self.mask_ = np.ones(tuple(2 * self._size + 1 for _ in range(3)),
-                             dtype=bool)
-
-        # Create an iterator for applying the mask to every pixel at once
-        iterator_size_list = \
-            [range(-self._size, self._size + 1)] * self.n_dimensions_ + \
-            [[0] for _ in range(3 - self.n_dimensions_)]
-        self._iterator = tuple(itertools.product(*iterator_size_list))
-
-        # We create a mesh so that we have an array with coordinates and we can
-        # calculate the distance of each point to the center
-        mesh_size_list = [np.arange(0, 2 * self._size + 1)] * 3
-        self.mesh_ = np.stack(
-            np.meshgrid(*mesh_size_list), axis=3).reshape((-1, 3))
-
-        # Calculate those distances to the center and use them to set the mask
-        # values so that it corresponds to a ball
-        center = self._size * np.ones((1, 3))
-        self.mask_ = pairwise_distances(
-            center, self.mesh_, metric=self.metric,
-            n_jobs=1, **self.metric_params).reshape(self.mask_.shape)
-
-        self.mask_ = self.mask_ <= self.radius
-
-        # Instantiate a padder to pad all images with 0 so that the rolling of
-        # the mask also works at the boundary of the images
-        padding = np.asarray([*[self._size] * self.n_dimensions_,
-                              *[0] * (3 - self.n_dimensions_)])
-        self._padder = Padder(padding=padding)
-        self._padder.fit(X.reshape((*X.shape[:3], -1)))
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each binary image in the collection `X`, calculate a
-        corresponding greyscale image based on the density of its pixels.
-        Return the collection of greyscale images.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y \
-            [, n_pixels_z])
-            Transformed collection of images. Each entry along axis 0 is a
-            2D or 3D greyscale image.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, allow_nd=True, copy=True)
-
-        # Reshape the images to 3D so that they can be rolled according to the
-        # 3D mask
-        Xt = Xt.reshape((*X.shape[:3], -1))
-        Xt = self._padder.transform(Xt)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._calculate_density)(Xt[s])
-            for s in gen_even_slices(Xt.shape[0],
-                                     effective_n_jobs(self.n_jobs)))
-        Xt = np.concatenate(Xt)
-
-        Xt = Xt[:, self._size: -self._size, self._size: -self._size]
-
-        if self.n_dimensions_ == 3:
-            Xt = Xt[:, :, :, self._size: -self._size]
-
-        Xt = Xt.reshape(X.shape)
-
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, colorscale='greys', origin='upper',
-             plotly_params=None):
-        """Plot a sample from a collection of 2D greyscale images.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
-            Collection of 2D greyscale images, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        colorscale : str, optional, default: ``'greys'``
-            Color scale to be used in the heat map. Can be anything allowed by
-            :class:`plotly.graph_objects.Heatmap`.
-
-        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
-            Position of the [0, 0] pixel of `data`, in the upper left or lower
-            left corner. The convention ``'upper'`` is typically used for
-            matrices and images.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_heatmap(
-            Xt[sample], colorscale=colorscale, origin=origin,
-            title=f"Density filtration of image {sample}",
-            plotly_params=plotly_params
-            )
+"""Filtrations of 2D/3D binary images."""
+# License: GNU AGPLv3
+
+from numbers import Real, Integral
+from typing import Callable
+import itertools
+
+import numpy as np
+from joblib import Parallel, delayed, effective_n_jobs
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.metrics import pairwise_distances
+from sklearn.utils import gen_even_slices
+from sklearn.utils.validation import check_array, check_is_fitted
+
+from ._utils import _dilate, _erode
+from .preprocessing import Padder
+from ..base import PlotterMixin
+from ..plotting import plot_heatmap
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.intervals import Interval
+from ..utils.validation import validate_params
+
+
+@adapt_fit_transform_docs
+class HeightFiltration(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Filtrations of 2D/3D binary images based on distances to lines/planes.
+
+    The height filtration assigns to each activated pixel of a binary image a
+    greyscale value equal to the distance between the pixel and the hyperplane
+    defined by a direction vector and the first seen edge of the image
+    following that direction. Deactivated pixels are assigned the value of the
+    maximum distance between any pixel of the image and the hyperplane, plus
+    one.
+
+    Parameters
+    ----------
+    direction : ndarray of shape (n_dimensions,) or None, optional, default: \
+        ``None``
+        Direction vector of the height filtration in
+        ``n_dimensions``-dimensional space, where ``n_dimensions`` is the
+        dimension of the images of the collection (2 or 3). ``None`` is
+        equivalent to passing ``numpy.ones(n_dimensions)``.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    n_dimensions_ : ``2`` or ``3``
+        Dimension of the images. Set in :meth:`fit`.
+
+    direction_ : ndarray of shape (:attr:`n_dimensions_`,)
+        Effective direction of the height filtration. Set in :meth:`fit`.
+
+    mesh_ : ndarray of shape ( n_pixels_x, n_pixels_y [, n_pixels_z])
+        greyscale image corresponding to the height filtration of a binary
+        image where each pixel is activated. Set in :meth:`fit`.
+
+    max_value_ : float
+        Maximum pixel value among all pixels in all images of the collection.
+        Set in :meth:`fit`.
+
+    See also
+    --------
+    RadialFiltration, DilationFiltration, ErosionFiltration, \
+    SignedDistanceFiltration, DensityFiltration, \
+    gtda.homology.CubicalPersistence
+
+    References
+    ----------
+    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
+           Classification of MNIST using TDA"; 19th International IEEE
+           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
+           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
+
+    """
+
+    _hyperparameters = {
+        'direction': {'type': (np.ndarray, type(None)), 'of': {'type': Real}}
+        }
+
+    def __init__(self, direction=None, n_jobs=None):
+        self.direction = direction
+        self.n_jobs = n_jobs
+
+    def _calculate_height(self, X):
+        Xh = np.full(X.shape, self.max_value_)
+
+        for i in range(len(Xh)):
+            Xh[i][np.where(X[i])] = np.dot(self.mesh_[np.where(X[i])],
+                                           self.direction_).reshape((-1,))
+
+        return Xh
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`n_dimensions_`, :attr:`direction_`, :attr:`mesh_`
+        and :attr:`max_value_` from a collection of binary images. Then,
+        return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_array(X, allow_nd=True)
+        self.n_dimensions_ = X.ndim - 1
+        if self.n_dimensions_ > 3:
+            raise ValueError(f"Input of `fit` contains arrays of dimension "
+                             f"{self.n_dimensions_}.")
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        if self.direction is None:
+            self.direction_ = np.ones(self.n_dimensions_,)
+        else:
+            self.direction_ = np.copy(self.direction)
+        self.direction_ = self.direction_ / np.linalg.norm(self.direction_)
+
+        axis_order = [2, 1, 3]
+        mesh_range_list = \
+            [np.arange(X.shape[order]) if self.direction_[i] >= 0
+             else -np.flip(np.arange(X.shape[order])) for i, order
+             in enumerate(axis_order[: self.n_dimensions_])]
+
+        self.mesh_ = np.stack(np.meshgrid(*mesh_range_list, indexing='xy'),
+                              axis=self.n_dimensions_)
+
+        self.max_value_ = 0.
+        self.max_value_ = np.max(self._calculate_height(
+            np.ones((1, *X.shape[1:])))) + 1
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each binary image in the collection `X`, calculate a
+        corresponding greyscale image based on the distance of its pixels to
+        the hyperplane defined by the `direction` vector and the first seen
+        edge of the images following that `direction`. Return the collection
+        of greyscale images.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y \
+            [, n_pixels_z])
+            Transformed collection of images. Each entry along axis 0 is a
+            2D or 3D greyscale image.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, allow_nd=True)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._calculate_height)(X[s])
+            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
+        Xt = np.concatenate(Xt)
+
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, colorscale='greys', origin='upper',
+             plotly_params=None):
+        """Plot a sample from a collection of 2D greyscale images.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
+            Collection of 2D greyscale images, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        colorscale : str, optional, default: ``'greys'``
+            Color scale to be used in the heat map. Can be anything allowed by
+            :class:`plotly.graph_objects.Heatmap`.
+
+        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
+            Position of the [0, 0] pixel of `data`, in the upper left or lower
+            left corner. The convention ``'upper'`` is typically used for
+            matrices and images.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_heatmap(
+            Xt[sample], colorscale=colorscale, origin=origin,
+            title=f"Height filtration of image {sample}",
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class RadialFiltration(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Filtrations of 2D/3D binary images based on distances to a reference
+    pixel.
+
+    The radial filtration assigns to each pixel of a binary image a greyscale
+    value computed as follows in terms of a reference pixel, called the
+    "center", and of a "radius": if the binary pixel is active and lies
+    within a ball defined by this center and this radius, then the assigned
+    value equals this distance. In all other cases, the assigned value equals
+    the maximum distance between any pixel of the image and the center
+    pixel, plus one.
+
+    Parameters
+    ----------
+    center : ndarray of shape (:attr:`n_dimensions_`,) or None, optional,\
+        default: ``None``
+        Coordinates of the center pixel, where ``n_dimensions`` is the
+        dimension of the images of the collection (2 or 3). ``None`` is
+        equivalent to passing ``np.zeros(n_dimensions,)```.
+
+    radius : float or None, default: ``None``
+        The radius of the ball centered in `center` inside which activated
+        pixels are included in the filtration.
+
+    metric : string or callable, optional, default: ``'euclidean'``
+        If set to ``'precomputed'``, each entry in `X` along axis 0 is
+        interpreted to be a distance matrix. Otherwise, entries are
+        interpreted as feature arrays, and `metric` determines a rule with
+        which to calculate distances between pairs of instances (i.e. rows)
+        in these arrays.
+        If `metric` is a string, it must be one of the options allowed by
+        :func:`scipy.spatial.distance.pdist` for its metric parameter, or a
+        metric listed in :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`,
+        including "euclidean", "manhattan" or "cosine".
+        If `metric` is a callable function, it is called on each pair of
+        instances and the resulting value recorded. The callable should take
+        two arrays from the entry in `X` as input, and return a value
+        indicating the distance between them.
+
+    metric_params : dict or None, optional, default: ``{}``
+        Additional keyword arguments for the metric function.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    n_dimensions_ : ``2`` or ``3``
+        Dimension of the images. Set in :meth:`fit`.
+
+    center_ : ndarray of shape (:attr:`n_dimensions_`,)
+        Effective center of the radial filtration. Set in :meth:`fit`.
+
+    mesh_ : ndarray of shape ( n_pixels_x, n_pixels_y [, n_pixels_z])
+        greyscale image corresponding to the radial filtration of a binary
+        image where each pixel is activated. Set in :meth:`fit`.
+
+    max_value_ : float
+        Maximum pixel value among all pixels in all images of the collection.
+        Set in :meth:`fit`.
+
+    See also
+    --------
+    HeightFiltration, DilationFiltration, ErosionFiltration, \
+    SignedDistanceFiltration, DensityFiltration, \
+    gtda.homology.CubicalPersistence
+
+    References
+    ----------
+    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
+           Classification of MNIST using TDA"; 19th International IEEE
+           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
+           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
+
+    """
+
+    _hyperparameters = {
+        'center': {'type': (np.ndarray, type(None)), 'of': {'type': Integral}},
+        'radius': {'type': Real, 'in': Interval(0, np.inf, closed='right')},
+        'metric': {'type': (str, Callable)},
+        'metric_params': {'type': dict}
+        }
+
+    def __init__(self, center=None, radius=np.inf, metric='euclidean',
+                 metric_params={}, n_jobs=None):
+        self.center = center
+        self.radius = radius
+        self.metric = metric
+        self.metric_params = metric_params
+        self.n_jobs = n_jobs
+
+    def _calculate_radial(self, X):
+        Xr = np.nan_to_num(self.mesh_ * X, nan=np.inf, posinf=np.inf)
+        Xr = np.nan_to_num(Xr, posinf=-1)
+
+        Xr[X == 0] = self.max_value_
+        Xr[Xr == -1] = self.max_value_
+
+        return Xr
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`center_`, :attr:`n_dimensions_`, :attr:`mesh_` and
+        :attr:`max_value_` from a collection of binary images. Then, return the
+        estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_array(X, allow_nd=True)
+        self.n_dimensions_ = X.ndim - 1
+        if self.n_dimensions_ > 3:
+            raise ValueError(f"Input of `fit` contains arrays of dimension "
+                             f"{self.n_dimensions_}.")
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        if self.center is None:
+            self.center_ = np.zeros(self.n_dimensions_)
+        else:
+            self.center_ = np.copy(self.center)
+        self.center_ = self.center_.reshape((1, -1))
+
+        axis_order = [2, 1, 3]
+        mesh_range_list = [np.arange(0, X.shape[i])
+                           for i in axis_order[:self.n_dimensions_]]
+
+        self.mesh_ = np.stack(
+            np.meshgrid(*mesh_range_list),
+            axis=self.n_dimensions_).reshape((-1, self.n_dimensions_))
+        self.mesh_ = pairwise_distances(
+            self.center_, self.mesh_, metric=self.metric,
+            n_jobs=1, **self.metric_params).reshape(X.shape[1:])
+        self.mesh_[self.mesh_ > self.radius] = np.inf
+
+        self.max_value_ = 0.
+        self.max_value_ = \
+            np.max(self._calculate_radial(np.ones((1, *X.shape[1:])))) + 1
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each binary image in the collection `X`, calculate a
+        corresponding greyscale image based on the distance of its pixels to
+        the center. Return the collection of greyscale images.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_pixels_x,
+            n_pixels_y [, n_pixels_z])
+            Transformed collection of images. Each entry along axis 0 is a
+            2D or 3D greyscale image.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, allow_nd=True)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._calculate_radial)(X[s])
+            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
+        Xt = np.concatenate(Xt)
+
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, colorscale='greys', origin='upper',
+             plotly_params=None):
+        """Plot a sample from a collection of 2D greyscale images.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
+            Collection of 2D greyscale images, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        colorscale : str, optional, default: ``'greys'``
+            Color scale to be used in the heat map. Can be anything allowed by
+            :class:`plotly.graph_objects.Heatmap`.
+
+        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
+            Position of the [0, 0] pixel of `data`, in the upper left or lower
+            left corner. The convention ``'upper'`` is typically used for
+            matrices and images.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_heatmap(
+            Xt[sample], colorscale=colorscale, origin=origin,
+            title=f"Radial filtration of image {sample}",
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class DilationFiltration(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Filtrations of 2D/3D binary images based on the dilation of activated
+    regions.
+
+    Binary dilation is a morphological operator commonly used in
+    image processing and relies on the `scipy.ndimage \
+    <https://docs.scipy.org/doc/scipy/reference/ndimage.html>`_ module.
+
+    This filtration assigns to each pixel in an image a greyscale value
+    calculated as follows. If the minimum Manhattan distance between the
+    pixel and any activated pixel in the image is less than or equal to
+    the parameter `n_iterations`, the assigned value is this distance 
+    in particular, activated pixels are assigned a value of 0.
+    Otherwise, the assigned greyscale value is the sum of the lengths
+    along all axes of the image  equivalently, it is the maximum
+    Manhattan distance between any two pixels in the image. The name of
+    this filtration comes from the fact that these values can be computed
+    by iteratively dilating activated regions, thickening them by a total
+    amount `n_iterations`.
+
+    Parameters
+    ----------
+    n_iterations : int or None, optional, default: ``None``
+        Number of iterations in the dilation process. ``None`` means dilation
+        reaches all deactivated pixels.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    n_dimensions_ : ``2`` or ``3``
+        Dimension of the images. Set in :meth:`fit`.
+
+    n_iterations_ : int
+        Effective number of iterations in the dilation process. Set in
+        :meth:`fit`.
+
+    max_value_ : float
+        Maximum pixel value among all pixels in all images of the collection.
+        Set in :meth:`fit`.
+
+    See also
+    --------
+    HeightFiltration, RadialFiltration, ErosionFiltration, \
+    SignedDistanceFiltration, DensityFiltration, \
+    gtda.homology.CubicalPersistence
+
+    References
+    ----------
+    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
+           Classification of MNIST using TDA"; 19th International IEEE
+           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
+           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
+
+    """
+
+    _hyperparameters = {
+        'n_iterations': {'type': (int, type(None)),
+                         'in': Interval(1, np.inf, closed='left')}
+        }
+
+    def __init__(self, n_iterations=None, n_jobs=None):
+        self.n_iterations = n_iterations
+        self.n_jobs = n_jobs
+
+    def _calculate_dilation(self, X):
+        Xd = _dilate(X, 1, self.n_iterations_, 1, self.max_value_)
+
+        mask_undilated = Xd == 0
+        Xd -= 1
+        Xd[mask_undilated] = self.max_value_
+        return Xd
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`n_dimensions_`, :attr:`n_iterations_` and
+        :attr:`max_value_` from a collection of binary images. Then, return the
+        estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_array(X, allow_nd=True)
+        self.n_dimensions_ = X.ndim - 1
+        if self.n_dimensions_ > 3:
+            raise ValueError(f"Input of `fit` contains arrays of dimension "
+                             f"{self.n_dimensions_}.")
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        self.max_value_ = np.sum(X.shape[1:])
+
+        if self.n_iterations is None:
+            self.n_iterations_ = int(self.max_value_)
+        else:
+            self.n_iterations_ = self.n_iterations
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each binary image in the collection `X`, calculate a
+        corresponding greyscale image based on the distance of its pixels to
+        their closest activated neighboring pixel. Return the collection
+        of greyscale images.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_pixels_x,
+            n_pixels_y [, n_pixels_z])
+            Transformed collection of images. Each entry along axis 0 is a
+            2D or 3D greyscale image.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, allow_nd=True)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._calculate_dilation)(X[s])
+            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
+        Xt = np.concatenate(Xt)
+
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, colorscale='greys', origin='upper',
+             plotly_params=None):
+        """Plot a sample from a collection of 2D greyscale images.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
+            Collection of 2D greyscale images, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        colorscale : str, optional, default: ``'greys'``
+            Color scale to be used in the heat map. Can be anything allowed by
+            :class:`plotly.graph_objects.Heatmap`.
+
+        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
+            Position of the [0, 0] pixel of `data`, in the upper left or lower
+            left corner. The convention ``'upper'`` is typically used for
+            matrices and images.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_heatmap(
+            Xt[sample], colorscale=colorscale, origin=origin,
+            title=f"Dilation filtration of image {sample}",
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class ErosionFiltration(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Filtrations of 2D/3D binary images based on the erosion of activated
+    regions.
+
+    Binary erosion is a morphological operator commonly used in
+    image processing and relies on the `scipy.ndimage \
+    <https://docs.scipy.org/doc/scipy/reference/ndimage.html>`_ module.
+
+    This filtration assigns to each pixel in an image a greyscale value
+    calculated as follows. If the minimum Manhattan distance between the
+    pixel and any deactivated pixel in the image is less than or equal to
+    the parameter `n_iterations`, the assigned value is this distance 
+    in particular, deactivated pixels are assigned a value of 0.
+    Otherwise, the assigned greyscale value is the sum of the lengths
+    along all axes of the image  equivalently, it is the maximum
+    Manhattan distance between any two pixels in the image. The name of
+    this filtration comes from the fact that these values can be computed
+    by iteratively eroding activated regions, shrinking them by a total
+    amount `n_iterations`.
+
+    Parameters
+    ----------
+    n_iterations : int or None, optional, default: ``None``
+        Number of iterations in the erosion process. ``None`` means erosion
+        reaches all activated pixels.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    n_dimensions_ : ``2`` or ``3``
+        Dimension of the images. Set in :meth:`fit`.
+
+    n_iterations_ : int
+        Effective number of iterations in the erosion process. Set in
+        :meth:`fit`.
+
+    max_value_ : float
+        Maximum pixel value among all pixels in all images of the collection.
+        Set in :meth:`fit`.
+
+    See also
+    --------
+    HeightFiltration, RadialFiltration, DilationFiltration, \
+    SignedDistanceFiltration, DensityFiltration, \
+    gtda.homology.CubicalPersistence
+
+    References
+    ----------
+    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
+           Classification of MNIST using TDA"; 19th International IEEE
+           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
+           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
+
+    """
+
+    _hyperparameters = {
+        'n_iterations': {'type': (int, type(None)),
+                         'in': Interval(1, np.inf, closed='left')}
+        }
+
+    def __init__(self, n_iterations=None, n_jobs=None):
+        self.n_iterations = n_iterations
+        self.n_jobs = n_jobs
+
+    def _calculate_erosion(self, X):
+        Xe = _erode(X, 1, self.n_iterations_, 1, self.max_value_)
+
+        mask_uneroded = Xe == 0
+        Xe -= 1
+        Xe[mask_uneroded] = self.max_value_
+        return Xe
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`n_dimensions_`, :attr:`n_iterations_` and
+        :attr:`max_value_` from a collection of binary images. Then, return the
+        estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_array(X, allow_nd=True)
+        self.n_dimensions_ = X.ndim - 1
+        if self.n_dimensions_ > 3:
+            raise ValueError(f"Input of `fit` contains arrays of dimension "
+                             f"{self.n_dimensions_}.")
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        self.max_value_ = np.sum(X.shape[1:])
+
+        if self.n_iterations is None:
+            self.n_iterations_ = int(self.max_value_)
+        else:
+            self.n_iterations_ = self.n_iterations
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each binary image in the collection `X`, calculate a
+        corresponding greyscale image based on the distance of its pixels to
+        their closest activated neighboring pixel. Return the collection
+        of greyscale images.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_pixels_x,
+            n_pixels_y [, n_pixels_z])
+            Transformed collection of images. Each entry along axis 0 is a
+            2D or 3D greyscale image.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, allow_nd=True)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._calculate_erosion)(X[s])
+            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
+        Xt = np.concatenate(Xt)
+
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, colorscale='greys', origin='upper',
+             plotly_params=None):
+        """Plot a sample from a collection of 2D greyscale images.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
+            Collection of 2D greyscale images, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        colorscale : str, optional, default: ``'greys'``
+            Color scale to be used in the heat map. Can be anything allowed by
+            :class:`plotly.graph_objects.Heatmap`.
+
+        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
+            Position of the [0, 0] pixel of `data`, in the upper left or lower
+            left corner. The convention ``'upper'`` is typically used for
+            matrices and images.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_heatmap(
+            Xt[sample], colorscale=colorscale, origin=origin,
+            title=f"Erosion filtration of image {sample}",
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class SignedDistanceFiltration(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Filtrations of 2D/3D binary images based on the dilation and the erosion
+    of activated regions.
+
+    This filtration assigns to each pixel in an image a greyscale value
+    calculated as follows. For activated pixels, if the minimum Manhattan
+    distance between the pixel and any deactivated pixel in the image is less
+    than or equal to the parameter `n_iterations`, the assigned value is
+    this distance minus 1. Otherwise, the assigned greyscale value is the sum
+    of the lengths along all axes of the image  equivalently, it is the
+    maximum Manhattan distance between any two pixels in the image, minus 1.
+    For deactivated pixels, if the minimum Manhattan distance between the pixel
+    and any activated pixel in the image is less than or equal to the parameter
+    `n_iterations`, the assigned value is the opposite of this distance.
+    Otherwise, the assigned greyscale value is the opposite of the maximum
+    Manhattan distance between any two pixels in the image.
+
+    The name of this filtration comes from the fact that it is a a negatively
+    signed dilation plus a positively signed erosion, minus 1 on the activated
+    pixels. Therefore, pixels the activated pixels at the boundary of the
+    activated regions always have a pixel value of 0.
+
+    Parameters
+    ----------
+    n_iterations : int or None, optional, default: ``None``
+        Number of iterations in the dilation process. ``None`` means dilation
+        over the full image.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    n_dimensions_ : ``2`` or ``3``
+        Dimension of the images. Set in :meth:`fit`.
+
+    n_iterations_ : int
+        Effective number of iterations in the dilation process. Set in
+        :meth:`fit`.
+
+    max_value_ : float
+        Maximum pixel value among all pixels in all images of the collection.
+        Set in :meth:`fit`.
+
+    See also
+    --------
+    HeightFiltration, RadialFiltration, DilationFiltration, \
+    ErosionFiltration, DensityFiltration, gtda.homology.CubicalPersistence
+
+    References
+    ----------
+    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
+           Classification of MNIST using TDA"; 19th International IEEE
+           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
+           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
+
+    """
+
+    _hyperparameters = {
+        'n_iterations': {'type': (int, type(None)),
+                         'in': Interval(1, np.inf, closed='left')}
+        }
+
+    def __init__(self, n_iterations=None, n_jobs=None):
+        self.n_iterations = n_iterations
+        self.n_jobs = n_jobs
+
+    def _calculate_signed_distance(self, X):
+        mask = X == 1
+
+        Xd = -_dilate(X, 1, self.n_iterations_, 0, self.max_value_)
+        Xe = _erode(X, 0, self.n_iterations_, 0, self.max_value_)
+
+        mask_e = Xe == 0
+        mask_d = Xd == 0
+        Xe[np.logical_not(mask)] = 0
+        Xe[mask] -= 1
+        Xd[mask] = 0
+        Xd[mask_d] = -self.max_value_
+        Xe[mask_e] = self.max_value_
+        return (Xd + Xe)
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`n_dimensions_`, :attr:`n_iterations_` and
+        :attr:`max_value_` from a collection of binary images. Then, return the
+        estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_array(X, allow_nd=True)
+        self.n_dimensions_ = X.ndim - 1
+        if self.n_dimensions_ > 3:
+            raise ValueError(f"Input of `fit` contains arrays of dimension "
+                             f"{self.n_dimensions_}.")
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        self.max_value_ = np.sum(X.shape[1:])
+
+        if self.n_iterations is None:
+            self.n_iterations_ = int(self.max_value_)
+        else:
+            self.n_iterations_ = self.n_iterations
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each binary image in the collection `X`, calculate a
+        corresponding greyscale image based on the distance of its pixels to
+        their closest activated neighboring pixel. Return the collection
+        of greyscale images.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_pixels_x,
+            n_pixels_y [, n_pixels_z])
+            Transformed collection of images. Each entry along axis 0 is a
+            2D or 3D greyscale image.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, allow_nd=True)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._calculate_signed_distance)(X[s])
+            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
+        Xt = np.concatenate(Xt)
+
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, colorscale='greys', origin='upper',
+             plotly_params=None):
+        """Plot a sample from a collection of 2D greyscale images.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
+            Collection of 2D greyscale images, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        colorscale : str, optional, default: ``'greys'``
+            Color scale to be used in the heat map. Can be anything allowed by
+            :class:`plotly.graph_objects.Heatmap`.
+
+        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
+            Position of the [0, 0] pixel of `data`, in the upper left or lower
+            left corner. The convention ``'upper'`` is typically used for
+            matrices and images.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_heatmap(
+            Xt[sample], colorscale=colorscale, origin=origin,
+            title=f"Signed-distance filtration of image {sample}",
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class DensityFiltration(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Filtrations of 2D/3D binary images based on the number of activated
+    neighboring pixels.
+
+    The density filtration assigns to each pixel of a binary image a greyscale
+    value equal to the number of activated pixels within a ball centered around
+    it.
+
+    Parameters
+    ----------
+    radius : float, optional, default: ``1.``
+        The radius of the ball within which the number of activated pixels is
+        considered.
+
+    metric : string or callable, optional, default: ``'euclidean'``
+        Determines a rule with which to calculate distances between
+        pairs of pixels.
+        If ``metric`` is a string, it must be one of the options allowed by
+        ``scipy.spatial.distance.pdist`` for its metric parameter, or a metric
+        listed in ``sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS``, including
+        "euclidean", "manhattan", or "cosine".
+        If ``metric`` is a callable function, it is called on each pair of
+        instances and the resulting value recorded. The callable should take
+        two arrays from the entry in `X` as input, and return a value
+        indicating the distance between them.
+
+    metric_params : dict, optional, default: ``{}``
+        Additional keyword arguments for the metric function.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    n_dimensions_ : ``2`` or ``3``
+        Dimension of the images. Set in :meth:`fit`.
+
+    mask_ : ndarray of shape (radius, radius [, radius])
+        The mask applied around each pixel to calculate the weighted number of
+        its activated neighbors. Set in :meth:`fit`.
+
+    See also
+    --------
+    HeightFiltration, RadialFiltration, DilationFiltration, \
+    ErosionFiltration, SignedDistanceFiltration, \
+    gtda.homology.CubicalPersistence
+
+    References
+    ----------
+    [1] A. Garin and G. Tauzin, "A topological reading lesson: Classification
+        of MNIST  using  TDA"; 19th International IEEE Conference on Machine
+        Learning and Applications (ICMLA 2020), 2019; arXiv: `1910.08345 \
+        <https://arxiv.org/abs/1910.08345>`_.
+
+    """
+
+    _hyperparameters = {
+        'radius': {'type': Real, 'in': Interval(0, np.inf, closed='right')},
+        'metric': {'type': (str, Callable)},
+        'metric_params': {'type': dict},
+        }
+
+    def __init__(self, radius=3, metric='euclidean', metric_params={},
+                 n_jobs=None):
+        self.radius = radius
+        self.metric = metric
+        self.metric_params = metric_params
+        self.n_jobs = n_jobs
+
+    def _calculate_density(self, X):
+        Xd = np.zeros(X.shape)
+
+        # The idea behind this is to sum up pixel values of the image
+        # rolled according to the 3D mask
+        for i, j, k in self._iterator:
+            Xd += np.roll(np.roll(
+                np.roll(X, k, axis=3), j, axis=2), i, axis=1) \
+                * self.mask_[self._size + i, self._size + j,
+                             self._size + k]
+        return Xd
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`n_dimensions_` and :attr:`mask_` from a collection
+        of binary images. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_array(X, allow_nd=True)
+        self.n_dimensions_ = X.ndim - 1
+        if self.n_dimensions_ > 3:
+            raise ValueError(f"Input of `fit` contains arrays of dimension "
+                             f"{self.n_dimensions_}.")
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        # Determine the size of the mask based on the radius and metric
+        self._size = int(np.ceil(
+            pairwise_distances([[0]], [[self.radius]], metric=self.metric,
+                               **self.metric_params)
+            ))
+        # The mask is always 3D but not the iterator.
+        self.mask_ = np.ones(tuple(2 * self._size + 1 for _ in range(3)),
+                             dtype=bool)
+
+        # Create an iterator for applying the mask to every pixel at once
+        iterator_size_list = \
+            [range(-self._size, self._size + 1)] * self.n_dimensions_ + \
+            [[0] for _ in range(3 - self.n_dimensions_)]
+        self._iterator = tuple(itertools.product(*iterator_size_list))
+
+        # We create a mesh so that we have an array with coordinates and we can
+        # calculate the distance of each point to the center
+        mesh_size_list = [np.arange(0, 2 * self._size + 1)] * 3
+        self.mesh_ = np.stack(
+            np.meshgrid(*mesh_size_list), axis=3).reshape((-1, 3))
+
+        # Calculate those distances to the center and use them to set the mask
+        # values so that it corresponds to a ball
+        center = self._size * np.ones((1, 3))
+        self.mask_ = pairwise_distances(
+            center, self.mesh_, metric=self.metric,
+            n_jobs=1, **self.metric_params).reshape(self.mask_.shape)
+
+        self.mask_ = self.mask_ <= self.radius
+
+        # Instantiate a padder to pad all images with 0 so that the rolling of
+        # the mask also works at the boundary of the images
+        padding = np.asarray([*[self._size] * self.n_dimensions_,
+                              *[0] * (3 - self.n_dimensions_)])
+        self._padder = Padder(padding=padding)
+        self._padder.fit(X.reshape((*X.shape[:3], -1)))
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each binary image in the collection `X`, calculate a
+        corresponding greyscale image based on the density of its pixels.
+        Return the collection of greyscale images.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y \
+            [, n_pixels_z])
+            Transformed collection of images. Each entry along axis 0 is a
+            2D or 3D greyscale image.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, allow_nd=True, copy=True)
+
+        # Reshape the images to 3D so that they can be rolled according to the
+        # 3D mask
+        Xt = Xt.reshape((*X.shape[:3], -1))
+        Xt = self._padder.transform(Xt)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._calculate_density)(Xt[s])
+            for s in gen_even_slices(Xt.shape[0],
+                                     effective_n_jobs(self.n_jobs)))
+        Xt = np.concatenate(Xt)
+
+        Xt = Xt[:, self._size: -self._size, self._size: -self._size]
+
+        if self.n_dimensions_ == 3:
+            Xt = Xt[:, :, :, self._size: -self._size]
+
+        Xt = Xt.reshape(X.shape)
+
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, colorscale='greys', origin='upper',
+             plotly_params=None):
+        """Plot a sample from a collection of 2D greyscale images.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
+            Collection of 2D greyscale images, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        colorscale : str, optional, default: ``'greys'``
+            Color scale to be used in the heat map. Can be anything allowed by
+            :class:`plotly.graph_objects.Heatmap`.
+
+        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
+            Position of the [0, 0] pixel of `data`, in the upper left or lower
+            left corner. The convention ``'upper'`` is typically used for
+            matrices and images.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_heatmap(
+            Xt[sample], colorscale=colorscale, origin=origin,
+            title=f"Density filtration of image {sample}",
+            plotly_params=plotly_params
+            )
```

## gtda/images/preprocessing.py

 * *Ordering differences only*

```diff
@@ -1,660 +1,660 @@
-"""Image preprocessing module."""
-# License: GNU AGPLv3
-
-from functools import reduce
-from operator import iconcat
-from numbers import Real, Integral
-
-import numpy as np
-from joblib import Parallel, delayed, effective_n_jobs
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils import gen_even_slices
-from sklearn.utils.validation import check_array, check_is_fitted
-
-from ..base import PlotterMixin
-from ..plotting import plot_point_cloud, plot_heatmap
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.intervals import Interval
-from ..utils.validation import validate_params
-
-
-@adapt_fit_transform_docs
-class Binarizer(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Binarize all 2D/3D greyscale images in a collection.
-
-    Parameters
-    ----------
-    threshold : float, default: 0.5
-        Fraction of the maximum pixel value `max_value_` from which to
-        binarize.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    n_dimensions_ : ``2`` or ``3``
-        Dimension of the images. Set in meth:`fit`.
-
-    max_value_ : float
-        Maximum pixel value among all pixels in all images of the collection.
-        Set in meth:`fit`.
-
-    See also
-    --------
-    gtda.homology.CubicalPersistence
-
-    References
-    ----------
-    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
-           Classification of MNIST using TDA"; 19th International IEEE
-           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
-           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
-
-    """
-
-    _hyperparameters = {
-        'threshold': {'type': Real, 'in': Interval(0, 1, closed='right')}
-        }
-
-    def __init__(self, threshold=0.5, n_jobs=None):
-        self.threshold = threshold
-        self.n_jobs = n_jobs
-
-    def _binarize(self, X):
-        Xbin = X / self.max_value_ > self.threshold
-
-        return Xbin
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`n_dimensions_` and :attr:`max_value_` from the
-        collection of greyscale images. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y \
-            [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            greyscale image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_array(X, allow_nd=True)
-        self.n_dimensions_ = X.ndim - 1
-        if self.n_dimensions_ > 3:
-            raise ValueError(f"Input of `fit` contains arrays of dimension "
-                             f"{self.n_dimensions_}.")
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        self.max_value_ = np.max(X)
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each greyscale image in the collection `X`, calculate a
-        corresponding binary image by applying the `threshold`. Return the
-        collection of binary images.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            greyscale image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y \
-            [, n_pixels_z])
-            Transformed collection of images. Each entry along axis 0 is a
-            2D or 3D binary image.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, allow_nd=True)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(delayed(
-            self._binarize)(Xt[s])
-            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
-        Xt = np.concatenate(Xt)
-
-        if self.n_dimensions_ == 2:
-            Xt = Xt.reshape(X.shape)
-
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, colorscale='greys', origin='upper',
-             plotly_params=None):
-        """Plot a sample from a collection of 2D binary images.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
-            Collection of 2D binary images, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        colorscale : str, optional, default: ``'greys'``
-            Color scale to be used in the heat map. Can be anything allowed by
-            :class:`plotly.graph_objects.Heatmap`.
-
-        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
-            Position of the [0, 0] pixel of `data`, in the upper left or lower
-            left corner. The convention ``'upper'`` is typically used for
-            matrices and images.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_heatmap(
-            Xt[sample] * 1, colorscale=colorscale, origin=origin,
-            title=f"Binarization of image {sample}",
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class Inverter(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Invert all 2D/3D images in a collection.
-
-    Applies an inversion function to the value of all pixels of all images in
-    the input collection. If the images are binary, the inversion function is
-    defined as the logical NOT function. Otherwise, it is the function
-    :math:`f(x) = M - x`, where `x` is a pixel value and `M` is
-    :attr:`max_value_`.
-
-    Parameters
-    ----------
-    max_value : bool, int, float or None, optional, default: ``None``
-        Maximum possible pixel value in the images. It should be a boolean if
-        input images are binary and an int or a float if they are greyscale.
-        If ``None``, it is calculated from the collection of images passed in
-        :meth:`fit`.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    n_dimensions_ : ``2`` or ``3``
-        Dimension of the images. Set in :meth:`fit`.
-
-    max_value_ : int, float or bool
-       Effective maximum value of the images' pixels. Set in :meth:`fit`.
-
-    References
-    ----------
-    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
-           Classification of MNIST using TDA"; 19th International IEEE
-           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
-           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
-
-    """
-
-    _hyperparameters = {
-        'max_value': {'type': (bool, Real, type(None))}
-        }
-
-    def __init__(self, max_value=None, n_jobs=None):
-        self.max_value = max_value
-        self.n_jobs = n_jobs
-
-    def _invert(self, X):
-        if self.max_value_ is True:
-            return np.logical_not(X)
-        else:
-            return self.max_value_ - X
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`n_dimensions_` and :attr:`max_value_` from the
-        collection of images. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_array(X, allow_nd=True)
-        self.n_dimensions_ = X.ndim - 1
-        if self.n_dimensions_ > 3:
-            raise ValueError(f"Input of `fit` contains arrays of dimension "
-                             f"{self.n_dimensions_}.")
-        validate_params(self.get_params(), self._hyperparameters,
-                        exclude=['n_jobs'])
-
-        if self.max_value is None:
-            if np.issubdtype(X.dtype, bool):
-                self.max_value_ = True
-            else:
-                self.max_value_ = np.max(X)
-        else:
-            self.max_value_ = self.max_value
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each binary image in the collection `X`, calculate its negation.
-        Return the collection of negated binary images.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y \
-            [, n_pixels_z])
-            Transformed collection of images. Each entry along axis 0 is a
-            2D or 3D binary image.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, allow_nd=True)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(delayed(
-            self._invert)(Xt[s])
-            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
-        Xt = np.concatenate(Xt)
-
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, colorscale='greys', origin='upper',
-             plotly_params=None):
-        """Plot a sample from a collection of 2D binary images.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
-            Collection of 2D binary images, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        colorscale : str, optional, default: ``'greys'``
-            Color scale to be used in the heat map. Can be anything allowed by
-            :class:`plotly.graph_objects.Heatmap`.
-
-        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
-            Position of the [0, 0] pixel of `data`, in the upper left or lower
-            left corner. The convention ``'upper'`` is typically used for
-            matrices and images.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_heatmap(
-            Xt[sample] * 1, colorscale=colorscale, origin=origin,
-            title=f"Inversion of image {sample}",
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class Padder(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Pad all 2D/3D images in a collection.
-
-    Parameters
-    ----------
-    padding : int ndarray of shape (padding_x, padding_y [, padding_z]) or \
-        None, optional, default: ``None``
-        Number of pixels to pad the images along each axis and on both side of
-        the images. By default, a frame of a single pixel width is added
-        around the image (``1 = padding_x = padding_y [= padding_z]``).
-
-    value : bool, int, or float, optional, default: ``0``
-        Value given to the padded pixels. It should be a boolean if the input
-        images are binary and an int or float if they are greyscale.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    n_dimensions_ : ``2`` or ``3``
-        Dimension of the images. Set in :meth:`fit`.
-
-    padding_ : int ndarray of shape (padding_x, padding_y [, padding_z])
-       Effective padding along each of the axes. Set in :meth:`fit`.
-
-    References
-    ----------
-    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
-           Classification of MNIST using TDA"; 19th International IEEE
-           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
-           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
-
-    """
-
-    _hyperparameters = {
-        'padding': {'type': (np.ndarray, type(None)),
-                    'of': {'type': Integral}},
-        'value': {'type': (bool, Real)}
-        }
-
-    def __init__(self, padding=None, value=False, n_jobs=None):
-        self.padding = padding
-        self.value = value
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`n_dimensions_` and :attr:`padding_` from a
-        collection of images. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_array(X, allow_nd=True)
-        self.n_dimensions_ = X.ndim - 1
-        if self.n_dimensions_ > 3:
-            raise ValueError(f"Input of `fit` contains arrays of dimension "
-                             f"{self.n_dimensions_}.")
-        validate_params(self.get_params(), self._hyperparameters,
-                        exclude=['value', 'n_jobs'])
-
-        if self.padding is None:
-            self.padding_ = np.ones((self.n_dimensions_,), dtype=int)
-        elif len(self.padding) != self.n_dimensions_:
-            raise ValueError(
-                f"`padding` has length {self.padding} while the input "
-                f"data requires it to have length equal to "
-                f"{self.n_dimensions_}.")
-        else:
-            self.padding_ = self.padding
-
-        self._pad_width = ((0, 0),
-                           *[(self.padding_[axis], self.padding_[axis])
-                             for axis in range(self.n_dimensions_)])
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each binary image in the collection `X`, adds a padding.
-        Return the collection of padded binary images.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_pixels_x + 2 * padding_x, \
-            n_pixels_y + 2 * padding_y [, n_pixels_z + 2 * padding_z])
-            Transformed collection of images. Each entry along axis 0 is a
-            2D or 3D binary image.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, allow_nd=True)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(delayed(
-            np.pad)(Xt[s], pad_width=self._pad_width,
-                    constant_values=self.value)
-            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
-        Xt = np.concatenate(Xt)
-
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, colorscale='greys', origin='upper',
-             plotly_params=None):
-        """Plot a sample from a collection of 2D binary images.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
-            Collection of 2D binary images, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        colorscale : str, optional, default: ``'greys'``
-            Color scale to be used in the heat map. Can be anything allowed by
-            :class:`plotly.graph_objects.Heatmap`.
-
-        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
-            Position of the [0, 0] pixel of `data`, in the upper left or lower
-            left corner. The convention ``'upper'`` is typically used for
-            matrices and images.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_heatmap(
-            Xt[sample] * 1, colorscale=colorscale, origin=origin,
-            title=f"Padded version of image {sample}",
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class ImageToPointCloud(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Represent active pixels in 2D/3D binary images as points in 2D/3D space.
-
-    The coordinates of each point is calculated as follows. For each activated
-    pixel, assign coordinates that are the pixel index on this image, after
-    flipping the rows and then swapping between rows and columns.
-
-    This transformer is meant to transform a collection of images to a
-    collection of point clouds so that persistent homology calculations can be
-    performed.
-
-    Parameters
-    ----------
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    n_dimensions_ : ``2`` or ``3``
-        Dimension of the images. Set in :meth:`fit`.
-
-    See also
-    --------
-    gtda.homology.VietorisRipsPersistence, gtda.homology.SparseRipsPersistence,
-    gtda.homology.EuclideanCechPersistence
-
-    References
-    ----------
-    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
-           Classification of MNIST using TDA"; 19th International IEEE
-           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
-           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
-
-    """
-
-    def __init__(self, n_jobs=None):
-        self.n_jobs = n_jobs
-
-    @staticmethod
-    def _embed(X):
-        return [np.argwhere(x) for x in X]
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`n_dimensions_` from a collection of binary images.
-        Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_array(X, allow_nd=True)
-        self.n_dimensions_ = X.ndim - 1
-        if self.n_dimensions_ > 3:
-            raise ValueError(f"Input of `fit` contains arrays of dimension "
-                             f"{self.n_dimensions_}.")
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each collection of binary images, calculate the corresponding
-        collection of point clouds based on the coordinates of activated
-        pixels.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
-            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
-            binary image.
-
-        y : None
-            There is no need of a target in a transformer, yet the pipeline API
-            requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_pixels_x * n_pixels_y [* \
-            n_pixels_z], n_dimensions)
-            Transformed collection of images. Each entry along axis 0 is a
-            point cloud in ``n_dimensions``-dimensional space.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, allow_nd=True)
-
-        Xt = np.swapaxes(np.flip(Xt, axis=1), 1, 2)
-        Xt = Parallel(n_jobs=self.n_jobs)(delayed(
-            self._embed)(Xt[s])
-            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
-        Xt = reduce(iconcat, Xt, [])
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, plotly_params=None):
-        """Plot a sample from a collection of point clouds. If the point cloud
-        is in more than three dimensions, only the first three are plotted.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_points, n_dimensions)
-            Collection of point clouds in ``n_dimension``-dimensional space,
-            such as returned by :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_point_cloud(Xt[sample], plotly_params=plotly_params)
+"""Image preprocessing module."""
+# License: GNU AGPLv3
+
+from functools import reduce
+from operator import iconcat
+from numbers import Real, Integral
+
+import numpy as np
+from joblib import Parallel, delayed, effective_n_jobs
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils import gen_even_slices
+from sklearn.utils.validation import check_array, check_is_fitted
+
+from ..base import PlotterMixin
+from ..plotting import plot_point_cloud, plot_heatmap
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.intervals import Interval
+from ..utils.validation import validate_params
+
+
+@adapt_fit_transform_docs
+class Binarizer(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Binarize all 2D/3D greyscale images in a collection.
+
+    Parameters
+    ----------
+    threshold : float, default: 0.5
+        Fraction of the maximum pixel value `max_value_` from which to
+        binarize.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    n_dimensions_ : ``2`` or ``3``
+        Dimension of the images. Set in meth:`fit`.
+
+    max_value_ : float
+        Maximum pixel value among all pixels in all images of the collection.
+        Set in meth:`fit`.
+
+    See also
+    --------
+    gtda.homology.CubicalPersistence
+
+    References
+    ----------
+    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
+           Classification of MNIST using TDA"; 19th International IEEE
+           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
+           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
+
+    """
+
+    _hyperparameters = {
+        'threshold': {'type': Real, 'in': Interval(0, 1, closed='right')}
+        }
+
+    def __init__(self, threshold=0.5, n_jobs=None):
+        self.threshold = threshold
+        self.n_jobs = n_jobs
+
+    def _binarize(self, X):
+        Xbin = X / self.max_value_ > self.threshold
+
+        return Xbin
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`n_dimensions_` and :attr:`max_value_` from the
+        collection of greyscale images. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y \
+            [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            greyscale image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_array(X, allow_nd=True)
+        self.n_dimensions_ = X.ndim - 1
+        if self.n_dimensions_ > 3:
+            raise ValueError(f"Input of `fit` contains arrays of dimension "
+                             f"{self.n_dimensions_}.")
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        self.max_value_ = np.max(X)
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each greyscale image in the collection `X`, calculate a
+        corresponding binary image by applying the `threshold`. Return the
+        collection of binary images.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            greyscale image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y \
+            [, n_pixels_z])
+            Transformed collection of images. Each entry along axis 0 is a
+            2D or 3D binary image.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, allow_nd=True)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(delayed(
+            self._binarize)(Xt[s])
+            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
+        Xt = np.concatenate(Xt)
+
+        if self.n_dimensions_ == 2:
+            Xt = Xt.reshape(X.shape)
+
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, colorscale='greys', origin='upper',
+             plotly_params=None):
+        """Plot a sample from a collection of 2D binary images.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
+            Collection of 2D binary images, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        colorscale : str, optional, default: ``'greys'``
+            Color scale to be used in the heat map. Can be anything allowed by
+            :class:`plotly.graph_objects.Heatmap`.
+
+        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
+            Position of the [0, 0] pixel of `data`, in the upper left or lower
+            left corner. The convention ``'upper'`` is typically used for
+            matrices and images.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_heatmap(
+            Xt[sample] * 1, colorscale=colorscale, origin=origin,
+            title=f"Binarization of image {sample}",
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class Inverter(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Invert all 2D/3D images in a collection.
+
+    Applies an inversion function to the value of all pixels of all images in
+    the input collection. If the images are binary, the inversion function is
+    defined as the logical NOT function. Otherwise, it is the function
+    :math:`f(x) = M - x`, where `x` is a pixel value and `M` is
+    :attr:`max_value_`.
+
+    Parameters
+    ----------
+    max_value : bool, int, float or None, optional, default: ``None``
+        Maximum possible pixel value in the images. It should be a boolean if
+        input images are binary and an int or a float if they are greyscale.
+        If ``None``, it is calculated from the collection of images passed in
+        :meth:`fit`.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    n_dimensions_ : ``2`` or ``3``
+        Dimension of the images. Set in :meth:`fit`.
+
+    max_value_ : int, float or bool
+       Effective maximum value of the images' pixels. Set in :meth:`fit`.
+
+    References
+    ----------
+    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
+           Classification of MNIST using TDA"; 19th International IEEE
+           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
+           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
+
+    """
+
+    _hyperparameters = {
+        'max_value': {'type': (bool, Real, type(None))}
+        }
+
+    def __init__(self, max_value=None, n_jobs=None):
+        self.max_value = max_value
+        self.n_jobs = n_jobs
+
+    def _invert(self, X):
+        if self.max_value_ is True:
+            return np.logical_not(X)
+        else:
+            return self.max_value_ - X
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`n_dimensions_` and :attr:`max_value_` from the
+        collection of images. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_array(X, allow_nd=True)
+        self.n_dimensions_ = X.ndim - 1
+        if self.n_dimensions_ > 3:
+            raise ValueError(f"Input of `fit` contains arrays of dimension "
+                             f"{self.n_dimensions_}.")
+        validate_params(self.get_params(), self._hyperparameters,
+                        exclude=['n_jobs'])
+
+        if self.max_value is None:
+            if np.issubdtype(X.dtype, bool):
+                self.max_value_ = True
+            else:
+                self.max_value_ = np.max(X)
+        else:
+            self.max_value_ = self.max_value
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each binary image in the collection `X`, calculate its negation.
+        Return the collection of negated binary images.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y \
+            [, n_pixels_z])
+            Transformed collection of images. Each entry along axis 0 is a
+            2D or 3D binary image.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, allow_nd=True)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(delayed(
+            self._invert)(Xt[s])
+            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
+        Xt = np.concatenate(Xt)
+
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, colorscale='greys', origin='upper',
+             plotly_params=None):
+        """Plot a sample from a collection of 2D binary images.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
+            Collection of 2D binary images, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        colorscale : str, optional, default: ``'greys'``
+            Color scale to be used in the heat map. Can be anything allowed by
+            :class:`plotly.graph_objects.Heatmap`.
+
+        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
+            Position of the [0, 0] pixel of `data`, in the upper left or lower
+            left corner. The convention ``'upper'`` is typically used for
+            matrices and images.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_heatmap(
+            Xt[sample] * 1, colorscale=colorscale, origin=origin,
+            title=f"Inversion of image {sample}",
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class Padder(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Pad all 2D/3D images in a collection.
+
+    Parameters
+    ----------
+    padding : int ndarray of shape (padding_x, padding_y [, padding_z]) or \
+        None, optional, default: ``None``
+        Number of pixels to pad the images along each axis and on both side of
+        the images. By default, a frame of a single pixel width is added
+        around the image (``1 = padding_x = padding_y [= padding_z]``).
+
+    value : bool, int, or float, optional, default: ``0``
+        Value given to the padded pixels. It should be a boolean if the input
+        images are binary and an int or float if they are greyscale.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    n_dimensions_ : ``2`` or ``3``
+        Dimension of the images. Set in :meth:`fit`.
+
+    padding_ : int ndarray of shape (padding_x, padding_y [, padding_z])
+       Effective padding along each of the axes. Set in :meth:`fit`.
+
+    References
+    ----------
+    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
+           Classification of MNIST using TDA"; 19th International IEEE
+           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
+           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
+
+    """
+
+    _hyperparameters = {
+        'padding': {'type': (np.ndarray, type(None)),
+                    'of': {'type': Integral}},
+        'value': {'type': (bool, Real)}
+        }
+
+    def __init__(self, padding=None, value=False, n_jobs=None):
+        self.padding = padding
+        self.value = value
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`n_dimensions_` and :attr:`padding_` from a
+        collection of images. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_array(X, allow_nd=True)
+        self.n_dimensions_ = X.ndim - 1
+        if self.n_dimensions_ > 3:
+            raise ValueError(f"Input of `fit` contains arrays of dimension "
+                             f"{self.n_dimensions_}.")
+        validate_params(self.get_params(), self._hyperparameters,
+                        exclude=['value', 'n_jobs'])
+
+        if self.padding is None:
+            self.padding_ = np.ones((self.n_dimensions_,), dtype=int)
+        elif len(self.padding) != self.n_dimensions_:
+            raise ValueError(
+                f"`padding` has length {self.padding} while the input "
+                f"data requires it to have length equal to "
+                f"{self.n_dimensions_}.")
+        else:
+            self.padding_ = self.padding
+
+        self._pad_width = ((0, 0),
+                           *[(self.padding_[axis], self.padding_[axis])
+                             for axis in range(self.n_dimensions_)])
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each binary image in the collection `X`, adds a padding.
+        Return the collection of padded binary images.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_pixels_x + 2 * padding_x, \
+            n_pixels_y + 2 * padding_y [, n_pixels_z + 2 * padding_z])
+            Transformed collection of images. Each entry along axis 0 is a
+            2D or 3D binary image.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, allow_nd=True)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(delayed(
+            np.pad)(Xt[s], pad_width=self._pad_width,
+                    constant_values=self.value)
+            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
+        Xt = np.concatenate(Xt)
+
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, colorscale='greys', origin='upper',
+             plotly_params=None):
+        """Plot a sample from a collection of 2D binary images.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_pixels_x, n_pixels_y)
+            Collection of 2D binary images, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        colorscale : str, optional, default: ``'greys'``
+            Color scale to be used in the heat map. Can be anything allowed by
+            :class:`plotly.graph_objects.Heatmap`.
+
+        origin : ``'upper'`` | ``'lower'``, optional, default: ``'upper'``
+            Position of the [0, 0] pixel of `data`, in the upper left or lower
+            left corner. The convention ``'upper'`` is typically used for
+            matrices and images.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_heatmap(
+            Xt[sample] * 1, colorscale=colorscale, origin=origin,
+            title=f"Padded version of image {sample}",
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class ImageToPointCloud(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Represent active pixels in 2D/3D binary images as points in 2D/3D space.
+
+    The coordinates of each point is calculated as follows. For each activated
+    pixel, assign coordinates that are the pixel index on this image, after
+    flipping the rows and then swapping between rows and columns.
+
+    This transformer is meant to transform a collection of images to a
+    collection of point clouds so that persistent homology calculations can be
+    performed.
+
+    Parameters
+    ----------
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    n_dimensions_ : ``2`` or ``3``
+        Dimension of the images. Set in :meth:`fit`.
+
+    See also
+    --------
+    gtda.homology.VietorisRipsPersistence, gtda.homology.SparseRipsPersistence,
+    gtda.homology.EuclideanCechPersistence
+
+    References
+    ----------
+    .. [1] A. Garin and G. Tauzin, "A topological reading lesson:
+           Classification of MNIST using TDA"; 19th International IEEE
+           Conference on Machine Learning and Applications (ICMLA 2020), 2019;
+           `arXiv:1910.08345 <https://arxiv.org/abs/1910.08345>`_.
+
+    """
+
+    def __init__(self, n_jobs=None):
+        self.n_jobs = n_jobs
+
+    @staticmethod
+    def _embed(X):
+        return [np.argwhere(x) for x in X]
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`n_dimensions_` from a collection of binary images.
+        Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_array(X, allow_nd=True)
+        self.n_dimensions_ = X.ndim - 1
+        if self.n_dimensions_ > 3:
+            raise ValueError(f"Input of `fit` contains arrays of dimension "
+                             f"{self.n_dimensions_}.")
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each collection of binary images, calculate the corresponding
+        collection of point clouds based on the coordinates of activated
+        pixels.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_pixels_x, n_pixels_y [, n_pixels_z])
+            Input data. Each entry along axis 0 is interpreted as a 2D or 3D
+            binary image.
+
+        y : None
+            There is no need of a target in a transformer, yet the pipeline API
+            requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_pixels_x * n_pixels_y [* \
+            n_pixels_z], n_dimensions)
+            Transformed collection of images. Each entry along axis 0 is a
+            point cloud in ``n_dimensions``-dimensional space.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, allow_nd=True)
+
+        Xt = np.swapaxes(np.flip(Xt, axis=1), 1, 2)
+        Xt = Parallel(n_jobs=self.n_jobs)(delayed(
+            self._embed)(Xt[s])
+            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
+        Xt = reduce(iconcat, Xt, [])
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, plotly_params=None):
+        """Plot a sample from a collection of point clouds. If the point cloud
+        is in more than three dimensions, only the first three are plotted.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_points, n_dimensions)
+            Collection of point clouds in ``n_dimension``-dimensional space,
+            such as returned by :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_point_cloud(Xt[sample], plotly_params=plotly_params)
```

## gtda/images/_utils.py

 * *Ordering differences only*

```diff
@@ -1,23 +1,23 @@
-"""Helper functions for image processing."""
-# License: GNU AGPLv3
-
-import numpy as np
-from scipy import ndimage as ndi
-
-
-def _dilate(X, min_iteration, max_iteration, min_value, max_value):
-    X = X * 1.
-    for iteration in range(min_iteration, min(max_iteration, max_value) + 1):
-        Xtemp = np.asarray([ndi.binary_dilation(x) for x in X])
-        Xnew = (X + Xtemp) == 1
-        if np.any(Xnew):
-            X[Xnew] = iteration + min_value
-        else:
-            break
-
-    return X
-
-
-def _erode(X, min_iteration, max_iteration, min_value, max_value):
-    return _dilate(np.logical_not(X), min_iteration, max_iteration,
-                   min_value, max_value)
+"""Helper functions for image processing."""
+# License: GNU AGPLv3
+
+import numpy as np
+from scipy import ndimage as ndi
+
+
+def _dilate(X, min_iteration, max_iteration, min_value, max_value):
+    X = X * 1.
+    for iteration in range(min_iteration, min(max_iteration, max_value) + 1):
+        Xtemp = np.asarray([ndi.binary_dilation(x) for x in X])
+        Xnew = (X + Xtemp) == 1
+        if np.any(Xnew):
+            X[Xnew] = iteration + min_value
+        else:
+            break
+
+    return X
+
+
+def _erode(X, min_iteration, max_iteration, min_value, max_value):
+    return _dilate(np.logical_not(X), min_iteration, max_iteration,
+                   min_value, max_value)
```

## gtda/images/__init__.py

 * *Ordering differences only*

```diff
@@ -1,21 +1,21 @@
-"""The module :mod:`gtda.images` implements techniques that can be used to
-apply Topological Data Analysis to images."""
-# License: GNU AGPLv3
-
-from .preprocessing import Binarizer, Inverter, Padder, ImageToPointCloud
-from .filtrations import HeightFiltration, RadialFiltration, \
-    DilationFiltration, ErosionFiltration, SignedDistanceFiltration, \
-    DensityFiltration
-
-__all__ = [
-    'Binarizer',
-    'Inverter',
-    'Padder',
-    'ImageToPointCloud',
-    'HeightFiltration',
-    'RadialFiltration',
-    'DilationFiltration',
-    'ErosionFiltration',
-    'SignedDistanceFiltration',
-    'DensityFiltration'
-    ]
+"""The module :mod:`gtda.images` implements techniques that can be used to
+apply Topological Data Analysis to images."""
+# License: GNU AGPLv3
+
+from .preprocessing import Binarizer, Inverter, Padder, ImageToPointCloud
+from .filtrations import HeightFiltration, RadialFiltration, \
+    DilationFiltration, ErosionFiltration, SignedDistanceFiltration, \
+    DensityFiltration
+
+__all__ = [
+    'Binarizer',
+    'Inverter',
+    'Padder',
+    'ImageToPointCloud',
+    'HeightFiltration',
+    'RadialFiltration',
+    'DilationFiltration',
+    'ErosionFiltration',
+    'SignedDistanceFiltration',
+    'DensityFiltration'
+    ]
```

## gtda/local_homology/simplicial.py

```diff
@@ -1,455 +1,455 @@
-from numbers import Real
-from types import FunctionType
-
-import numpy as np
-import warnings
-
-from scipy.spatial.distance import pdist, squareform
-from sklearn.neighbors import KNeighborsTransformer, RadiusNeighborsTransformer
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils import check_array
-from sklearn.utils.validation import check_is_fitted
-
-from gtda.homology import VietorisRipsPersistence
-from gtda.utils.intervals import Interval
-from gtda.utils.validation import validate_params
-from gtda.plotting import plot_diagram
-from gtda.base import PlotterMixin
-from gtda.utils._docs import adapt_fit_transform_docs
-
-
-class LocalVietorisRipsBase(BaseEstimator,
-                            TransformerMixin,
-                            PlotterMixin):
-    """Base class for KNeighboursLocalVietorisRips and RadiusLocalVietorisRips.
-    LocalVietorisRipsBase is not meant to be used. Please see documentation
-    for KNeighboursLocalVietorisRips and RadiusLocalVietorisRips.
-
-    """
-
-    def __init__(self, metric="euclidean", homology_dimensions=(1, 2),
-                 neighborhood_params=(1, 2), collapse_edges=False,
-                 n_jobs=None):
-        """Initializes the base class by setting the basic parameters.
-        For more specific description, see specific children classes."""
-        # metric for the point cloud
-        self.metric = metric
-
-        # topological dimension of features to be computed
-        self.homology_dimensions = homology_dimensions
-
-        # Tuple of parameters defining "neighborhoods" of points. These
-        # parameters are input in the Transformer objects determining what
-        # points lie in the "neighborhoods" of each points. The points outside
-        # the "neighborhood" defined by the largest entry are discarded, and
-        # the points between the smaller and largest "neighborhoods" are "coned
-        # off". See more in the corresponding fit methods.
-        self.neighborhood_params = neighborhood_params
-
-        # parameter to feed into the homology transformer
-        self.collapse_edges = collapse_edges
-
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Initializes the object used for computing persistence homology,
-        checks that the parameters were initialized correctly.
-
-        """
-        # object used to compute persistence diagrams
-        self.homology = VietorisRipsPersistence(
-            metric="precomputed",
-            collapse_edges=self.collapse_edges,
-            homology_dimensions=self.homology_dimensions,
-            n_jobs=self.n_jobs
-            )
-        # make sure the neighborhood_params has been set correctly.
-        if self.neighborhood_params[0] > self.neighborhood_params[1]:
-            warnings.warn("First `neighborhood_params` is larger than second. "
-                          "The values are permuted. ")
-            self.neighborhood_params = (self.neighborhood_params[1],
-                                        self.neighborhood_params[0])
-        if self.neighborhood_params[1] == 0:
-            warnings.warn("Second `neighborhood_params` is less than 0. "
-                          "Second radius set to 1. ")
-            self.radii = (self.radii[0], 1)
-        if self.neighborhood_params[0] == self.neighborhood_params[1]:
-            warnings.warn("For meaningful features, first "
-                          "`neighborhood_params` should be strictly smaller "
-                          "than second.")
-        return self
-
-    def transform(self, X, y=None):
-        """Computes the local persistence diagrams at each element of X, and
-        returns a list of persistence diagrams, indexed as the points of X.
-        This is done in several steps:
-            - First compute the nearest neighbors in the point cloud that
-            was fitted on, for both values in n_neighbors.
-            - For each point, compute the relevant points (corresponding to
-            the larger neighborhood_params value), the close points
-            (corresponding to the smaller neighborhood_params value), and the
-            annulus to cone off (relevant points, but not close points).
-            Compute the distance matrix of the relevant points, and add an
-            additional row and column corresponding to the coning off point.
-            - Finally compute the persistence diagrams of each coned matrices.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_points, dimension)
-             Input data representing  point cloud:
-             an array of shape ``(n_points, n_dimensions)``.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Array of persistence diagrams computed from the feature arrays.
-            ``n_features`` equals :math:`\\sum_q n_q`, where :math:`n_q`
-            is the maximum number of topological features in dimension
-            :math:`q` across all samples in `X`.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, accept_sparse=False)
-
-        # sparse binary matrices where rows indicate the indices of points
-        # which are nearest neighbors to the row's index point.
-        Xt_close = self.close_neighbors_.transform(Xt)
-        Xt_relevant = self.relevant_neighbors_.transform(Xt)
-
-        coned_mats = []
-        for i in range(len(Xt)):
-            # get indices of points close to point at index i
-            close_indices = Xt_close.getrow(i).indices
-            # get indices of points in second "neighborhood"
-            relevant_indices = Xt_relevant.getrow(i).indices
-            annulus_indices = list(set(relevant_indices) - set(close_indices))
-            # Order them such that the last ones are the ones to cone off
-            reordered_relevant_indices = np.concatenate((close_indices,
-                                                         annulus_indices))
-            if len(close_indices) == 0:
-                # The coned off space retracts to the cone point
-                coned_mat = np.zeros((1, 1))
-            else:
-                # Fetch the coordinates
-                relevant_points = [self.relevant_neighbors_._fit_X[int(y)]
-                                   for y in reordered_relevant_indices]
-                # Dense distance matrix between all relevant points
-                local_mat = squareform(pdist(relevant_points,
-                                             metric=self.metric))
-                # Now add the cone point:
-                new_row = np.concatenate((np.ones(len(close_indices))*np.inf,
-                                          np.zeros(len(annulus_indices))))
-                new_col = np.concatenate((new_row, [0]))
-                pre_cone = np.concatenate((local_mat, [new_row]))
-                coned_mat = np.concatenate(
-                                           (pre_cone, np.array([new_col],
-                                                               dtype=float).T),
-                                           axis=1)
-            coned_mats += [coned_mat]
-        # Compute the Vietoris Rips Persistence diagrams
-        Xt = self.homology.fit_transform(coned_mats)
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
-        """Plot a sample from a collection of persistence diagrams, with
-        homology in multiple dimensions.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_features, 3)
-            Collection of persistence diagrams, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        homology_dimensions : list, tuple or None, optional, default: ``None``
-            Which homology dimensions to include in the plot. ``None`` means
-            plotting all dimensions present in ``Xt[sample]``.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"traces"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_diagram(
-            Xt[sample], homology_dimensions=homology_dimensions,
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class KNeighborsLocalVietorisRips(LocalVietorisRipsBase):
-    """Given a :ref:`point cloud <finite_metric_spaces_and_point_clouds>` in
-    Euclidean space, or an abstract :ref:`metric space
-    <finite_metric_spaces_and_point_clouds>` encoded by a distance matrix,
-    information about the local topology around each point is summarized in a
-    collection of persistence diagrams.
-
-    This is done by first isolating appropriate neighborhoods around each point
-    using a nearest neighbor transformer, then "coning off" points in an
-    annulus around each point, and finally computing the corresponding
-    associated persistence diagram. The output can then be used to explore the
-    point cloud, or fed into a vectorizer to obtain features.
-
-    Parameters
-    ----------
-    metric : string or callable, optional, default: ``"euclidean"``
-        Input data is to be interpreted as a point cloud (i.e. feature arrays),
-        and `metric`determines a rule with which to calculate distances between
-        pairs of points (i.e. row vectors). If `metric` is a string, it must be
-        one of the options allowed by :func:`scipy.spatial.distance.pdist`
-        for its metric parameter, or a metric listed in
-        :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`, including
-        ``"euclidean"``, ``"manhattan"`` or ``"cosine"``. If `metric` is a
-        callable, it should take pairs of vectors (1D arrays) as input and, for
-        each two vectors in a pair, it should return a scalar indicating the
-        distance/dissimilarity between them.
-
-    n_neighbors: tuple, optional, default: ``(10, 50)``, has to
-        consist of two non-negative integers. This defines the number of points
-        in the first and second neighborhoods considered.
-
-    homology_dimensions: tuple, optional, default: ``(1, 2)``. Dimensions
-        (non-negative integers) of the topological features to be detected.
-
-    collapse_edges : bool, optional, default: ``False``
-        Whether to run the edge collapse algorithm in [1]_ prior to the
-        persistent homology computation (see the Notes). Can reduce the runtime
-        dramatically when the data or the maximum homology dimensions are
-        large.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    References
-    ----------
-
-    .. [1] J.-D. Boissonnat and S. Pritam, "Edge Collapse and Persistence of
-           Flag Complexes"; in *36th International Symposium on Computational
-           Geometry (SoCG 2020)*, pp. 19:119:15,
-           Schloss Dagstuhl-LeibnizZentrum fr Informatik, 2020;
-           `DOI: 10.4230/LIPIcs.SoCG.2020.19
-           <https://doi.org/10.4230/LIPIcs.SoCG.2020.19>`_.
-
-    """
-
-    _hyperparameters = {
-        "metric": {"type": (str, FunctionType)},
-        "n_neighbors": {"type": (tuple, list),
-                        "of": {type: int,
-                               "in": Interval(1, np.inf, closed="left")}
-                        },
-        "homology_dimensions": {
-            "type": (tuple, list),
-            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
-            },
-        "collapse_edges": {"type": bool}
-        }
-
-    def __init__(self, metric="euclidean", homology_dimensions=(1, 2),
-                 n_neighbors=(1, 2), collapse_edges=False, n_jobs=None):
-        self.n_neighbors = n_neighbors
-        super().__init__(metric=metric,
-                         homology_dimensions=homology_dimensions,
-                         neighborhood_params=self.n_neighbors,
-                         collapse_edges=collapse_edges,
-                         n_jobs=n_jobs)
-
-    def fit(self, X, y=None):
-        """Initiates and fits the transformers that efficiently computes the
-        nearest neighbors of given points.
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_points, dimension)
-            Input data representing  point cloud. Can be either
-            a point cloud: an array of shape ``(n_points, n_dimensions)``.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        super().fit(X)
-
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
-        check_array(X, accept_sparse=False)
-
-        # make sure that the parameters are set correctly
-        self.size_ = len(X)
-        if self.size_ <= self.neighborhood_params[0]:
-            warnings.warn("First n_neighbors is too large to be relevant. "
-                          "Consider reducing it.")
-            self.neighborhood_params = (self.size_-1, self.size_)
-        if self.size_ < self.neighborhood_params[1]:
-            warnings.warn("Second `n_neighbors` is too large to be "
-                          "relevant. Consider reducing it. ")
-            self.neighborhood_params = (self.neighborhood_params[0],
-                                        self.size_)
-
-        # Objects used for finding nearest neighbors
-        self.close_neighbors_ = KNeighborsTransformer(
-            mode="connectivity",
-            n_neighbors=self.neighborhood_params[0],
-            metric=self.metric,
-            n_jobs=self.n_jobs
-            )
-
-        self.relevant_neighbors_ = KNeighborsTransformer(
-            mode="connectivity",
-            n_neighbors=self.neighborhood_params[1],
-            metric=self.metric,
-            n_jobs=self.n_jobs
-            )
-
-        self.close_neighbors_.fit(X)
-        self.relevant_neighbors_.fit(X)
-        return self
-
-
-@adapt_fit_transform_docs
-class RadiusLocalVietorisRips(LocalVietorisRipsBase):
-    """Given a :ref:`point cloud <finite_metric_spaces_and_point_clouds>` in
-    Euclidean space, or an abstract :ref:`metric space
-    <finite_metric_spaces_and_point_clouds>` encoded by a distance matrix,
-    information about the local topology around each point is summarized in a
-    collection of persistence diagrams.
-
-    This is done by first isolating appropriate neighborhoods around each point
-    using a radius neighbor transformer, then "coning off" points in an annulus
-    around each point, and finally computing the corresponding associated
-    persistence diagram. The output can then be used to explore the point
-    cloud, or fed into a vectorizer to obtain features.
-
-    Parameters
-    ----------
-    metric : string or callable, optional, default: ``"euclidean"``
-        Input data is to be interpreted as a point cloud (i.e. feature arrays),
-        and `metric`determines a rule with which to calculate distances between
-        pairs of points (i.e. row vectors). If `metric` is a string, it must be
-        one of the options allowed by :func:`scipy.spatial.distance.pdist` for
-        its `metric` parameter, or a metric listed in
-        :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`, including
-        ``"euclidean"``, ``"manhattan"`` or ``"cosine"``. If `metric` is a
-        callable, it should take pairs of vectors (1D arrays) as input and, for
-        each two vectors in a pair, it should return a scalar indicating the
-        distance/dissimilarity between them.
-
-    radii: tuple, optional, default: ``(0.0, 1.0)`` has to consist of two
-    non-negative floats. This determines the radius of the first and second
-    neighborhood around points considered.
-
-    homology_dimensions: tuple, optional, default: ``(1, 2)``. Dimensions
-        (non-negative integers) of the topological features to be detected.
-
-    collapse_edges : bool, optional, default: ``False``
-        Whether to run the edge collapse algorithm in [1]_ prior to the
-        persistent homology computation (see the Notes). Can reduce the runtime
-        dramatically when the data or the maximum homology dimensions are
-        large.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    References
-    ----------
-    .. [1] J.-D. Boissonnat and S. Pritam, "Edge Collapse and Persistence of
-           Flag Complexes"; in *36th International Symposium on Computational
-           Geometry (SoCG 2020)*, pp. 19:119:15,
-           Schloss Dagstuhl-LeibnizZentrum fr Informatik, 2020;
-           `DOI: 10.4230/LIPIcs.SoCG.2020.19
-           <https://doi.org/10.4230/LIPIcs.SoCG.2020.19>`_.
-
-    """
-
-    _hyperparameters = {
-        "metric": {"type": (str, FunctionType)},
-        "radii": {"type": (tuple, list),
-                  "of": {type: Real, "in": Interval(0, np.inf, closed="left")}
-                  },
-        "homology_dimensions": {
-            "type": (tuple, list),
-            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
-            },
-        "collapse_edges": {"type": bool}
-        }
-
-    def __init__(self, metric="euclidean", homology_dimensions=(1, 2),
-                 radii=(1, 2), collapse_edges=False, n_jobs=None):
-        self.radii = radii
-        super().__init__(metric=metric,
-                         homology_dimensions=homology_dimensions,
-                         neighborhood_params=self.radii,
-                         collapse_edges=collapse_edges,
-                         n_jobs=n_jobs)
-
-    def fit(self, X, y=None):
-        """Initiates and fits the transformers that efficiently computes the
-        nearest neighbors of given points.
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_points, dimension)
-            Input data representing  point cloud. Can be either
-            a point cloud: an array of shape ``(n_points, n_dimensions)``.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        super().fit(X)
-
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
-        check_array(X, accept_sparse=False)
-
-        # Objects used for finding nearest neighbors
-        self.close_neighbors_ = RadiusNeighborsTransformer(
-            mode="connectivity",
-            radius=self.neighborhood_params[0],
-            metric=self.metric,
-            n_jobs=self.n_jobs
-            )
-
-        self.relevant_neighbors_ = RadiusNeighborsTransformer(
-            mode="connectivity",
-            radius=self.neighborhood_params[1],
-            metric=self.metric,
-            n_jobs=self.n_jobs
-            )
-
-        self.close_neighbors_.fit(X)
-        self.relevant_neighbors_.fit(X)
-        return self
+from numbers import Real
+from typing import Callable
+
+import numpy as np
+import warnings
+
+from scipy.spatial.distance import pdist, squareform
+from sklearn.neighbors import KNeighborsTransformer, RadiusNeighborsTransformer
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils import check_array
+from sklearn.utils.validation import check_is_fitted
+
+from gtda.homology import VietorisRipsPersistence
+from gtda.utils.intervals import Interval
+from gtda.utils.validation import validate_params
+from gtda.plotting import plot_diagram
+from gtda.base import PlotterMixin
+from gtda.utils._docs import adapt_fit_transform_docs
+
+
+class LocalVietorisRipsBase(BaseEstimator,
+                            TransformerMixin,
+                            PlotterMixin):
+    """Base class for KNeighboursLocalVietorisRips and RadiusLocalVietorisRips.
+    LocalVietorisRipsBase is not meant to be used. Please see documentation
+    for KNeighboursLocalVietorisRips and RadiusLocalVietorisRips.
+
+    """
+
+    def __init__(self, metric="euclidean", homology_dimensions=(1, 2),
+                 neighborhood_params=(1, 2), collapse_edges=False,
+                 n_jobs=None):
+        """Initializes the base class by setting the basic parameters.
+        For more specific description, see specific children classes."""
+        # metric for the point cloud
+        self.metric = metric
+
+        # topological dimension of features to be computed
+        self.homology_dimensions = homology_dimensions
+
+        # Tuple of parameters defining "neighborhoods" of points. These
+        # parameters are input in the Transformer objects determining what
+        # points lie in the "neighborhoods" of each points. The points outside
+        # the "neighborhood" defined by the largest entry are discarded, and
+        # the points between the smaller and largest "neighborhoods" are "coned
+        # off". See more in the corresponding fit methods.
+        self.neighborhood_params = neighborhood_params
+
+        # parameter to feed into the homology transformer
+        self.collapse_edges = collapse_edges
+
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Initializes the object used for computing persistence homology,
+        checks that the parameters were initialized correctly.
+
+        """
+        # object used to compute persistence diagrams
+        self.homology = VietorisRipsPersistence(
+            metric="precomputed",
+            collapse_edges=self.collapse_edges,
+            homology_dimensions=self.homology_dimensions,
+            n_jobs=self.n_jobs
+            )
+        # make sure the neighborhood_params has been set correctly.
+        if self.neighborhood_params[0] > self.neighborhood_params[1]:
+            warnings.warn("First `neighborhood_params` is larger than second. "
+                          "The values are permuted. ")
+            self.neighborhood_params = (self.neighborhood_params[1],
+                                        self.neighborhood_params[0])
+        if self.neighborhood_params[1] == 0:
+            warnings.warn("Second `neighborhood_params` is less than 0. "
+                          "Second radius set to 1. ")
+            self.radii = (self.radii[0], 1)
+        if self.neighborhood_params[0] == self.neighborhood_params[1]:
+            warnings.warn("For meaningful features, first "
+                          "`neighborhood_params` should be strictly smaller "
+                          "than second.")
+        return self
+
+    def transform(self, X, y=None):
+        """Computes the local persistence diagrams at each element of X, and
+        returns a list of persistence diagrams, indexed as the points of X.
+        This is done in several steps:
+            - First compute the nearest neighbors in the point cloud that
+            was fitted on, for both values in n_neighbors.
+            - For each point, compute the relevant points (corresponding to
+            the larger neighborhood_params value), the close points
+            (corresponding to the smaller neighborhood_params value), and the
+            annulus to cone off (relevant points, but not close points).
+            Compute the distance matrix of the relevant points, and add an
+            additional row and column corresponding to the coning off point.
+            - Finally compute the persistence diagrams of each coned matrices.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_points, dimension)
+             Input data representing  point cloud:
+             an array of shape ``(n_points, n_dimensions)``.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Array of persistence diagrams computed from the feature arrays.
+            ``n_features`` equals :math:`\\sum_q n_q`, where :math:`n_q`
+            is the maximum number of topological features in dimension
+            :math:`q` across all samples in `X`.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, accept_sparse=False)
+
+        # sparse binary matrices where rows indicate the indices of points
+        # which are nearest neighbors to the row's index point.
+        Xt_close = self.close_neighbors_.transform(Xt)
+        Xt_relevant = self.relevant_neighbors_.transform(Xt)
+
+        coned_mats = []
+        for i in range(len(Xt)):
+            # get indices of points close to point at index i
+            close_indices = Xt_close.getrow(i).indices
+            # get indices of points in second "neighborhood"
+            relevant_indices = Xt_relevant.getrow(i).indices
+            annulus_indices = list(set(relevant_indices) - set(close_indices))
+            # Order them such that the last ones are the ones to cone off
+            reordered_relevant_indices = np.concatenate((close_indices,
+                                                         annulus_indices))
+            if len(close_indices) == 0:
+                # The coned off space retracts to the cone point
+                coned_mat = np.zeros((1, 1))
+            else:
+                # Fetch the coordinates
+                relevant_points = [self.relevant_neighbors_._fit_X[int(y)]
+                                   for y in reordered_relevant_indices]
+                # Dense distance matrix between all relevant points
+                local_mat = squareform(pdist(relevant_points,
+                                             metric=self.metric))
+                # Now add the cone point:
+                new_row = np.concatenate((np.ones(len(close_indices))*np.inf,
+                                          np.zeros(len(annulus_indices))))
+                new_col = np.concatenate((new_row, [0]))
+                pre_cone = np.concatenate((local_mat, [new_row]))
+                coned_mat = np.concatenate(
+                                           (pre_cone, np.array([new_col],
+                                                               dtype=float).T),
+                                           axis=1)
+            coned_mats += [coned_mat]
+        # Compute the Vietoris Rips Persistence diagrams
+        Xt = self.homology.fit_transform(coned_mats)
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, homology_dimensions=None, plotly_params=None):
+        """Plot a sample from a collection of persistence diagrams, with
+        homology in multiple dimensions.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_features, 3)
+            Collection of persistence diagrams, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        homology_dimensions : list, tuple or None, optional, default: ``None``
+            Which homology dimensions to include in the plot. ``None`` means
+            plotting all dimensions present in ``Xt[sample]``.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"traces"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_diagram(
+            Xt[sample], homology_dimensions=homology_dimensions,
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class KNeighborsLocalVietorisRips(LocalVietorisRipsBase):
+    """Given a :ref:`point cloud <finite_metric_spaces_and_point_clouds>` in
+    Euclidean space, or an abstract :ref:`metric space
+    <finite_metric_spaces_and_point_clouds>` encoded by a distance matrix,
+    information about the local topology around each point is summarized in a
+    collection of persistence diagrams.
+
+    This is done by first isolating appropriate neighborhoods around each point
+    using a nearest neighbor transformer, then "coning off" points in an
+    annulus around each point, and finally computing the corresponding
+    associated persistence diagram. The output can then be used to explore the
+    point cloud, or fed into a vectorizer to obtain features.
+
+    Parameters
+    ----------
+    metric : string or callable, optional, default: ``"euclidean"``
+        Input data is to be interpreted as a point cloud (i.e. feature arrays),
+        and `metric`determines a rule with which to calculate distances between
+        pairs of points (i.e. row vectors). If `metric` is a string, it must be
+        one of the options allowed by :func:`scipy.spatial.distance.pdist`
+        for its metric parameter, or a metric listed in
+        :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`, including
+        ``"euclidean"``, ``"manhattan"`` or ``"cosine"``. If `metric` is a
+        callable, it should take pairs of vectors (1D arrays) as input and, for
+        each two vectors in a pair, it should return a scalar indicating the
+        distance/dissimilarity between them.
+
+    n_neighbors: tuple, optional, default: ``(10, 50)``, has to
+        consist of two non-negative integers. This defines the number of points
+        in the first and second neighborhoods considered.
+
+    homology_dimensions: tuple, optional, default: ``(1, 2)``. Dimensions
+        (non-negative integers) of the topological features to be detected.
+
+    collapse_edges : bool, optional, default: ``False``
+        Whether to run the edge collapse algorithm in [1]_ prior to the
+        persistent homology computation (see the Notes). Can reduce the runtime
+        dramatically when the data or the maximum homology dimensions are
+        large.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    References
+    ----------
+
+    .. [1] J.-D. Boissonnat and S. Pritam, "Edge Collapse and Persistence of
+           Flag Complexes"; in *36th International Symposium on Computational
+           Geometry (SoCG 2020)*, pp. 19:119:15,
+           Schloss Dagstuhl-LeibnizZentrum fr Informatik, 2020;
+           `DOI: 10.4230/LIPIcs.SoCG.2020.19
+           <https://doi.org/10.4230/LIPIcs.SoCG.2020.19>`_.
+
+    """
+
+    _hyperparameters = {
+        "metric": {"type": (str, Callable)},
+        "n_neighbors": {"type": (tuple, list),
+                        "of": {type: int,
+                               "in": Interval(1, np.inf, closed="left")}
+                        },
+        "homology_dimensions": {
+            "type": (tuple, list),
+            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
+            },
+        "collapse_edges": {"type": bool}
+        }
+
+    def __init__(self, metric="euclidean", homology_dimensions=(1, 2),
+                 n_neighbors=(1, 2), collapse_edges=False, n_jobs=None):
+        self.n_neighbors = n_neighbors
+        super().__init__(metric=metric,
+                         homology_dimensions=homology_dimensions,
+                         neighborhood_params=self.n_neighbors,
+                         collapse_edges=collapse_edges,
+                         n_jobs=n_jobs)
+
+    def fit(self, X, y=None):
+        """Initiates and fits the transformers that efficiently computes the
+        nearest neighbors of given points.
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_points, dimension)
+            Input data representing  point cloud. Can be either
+            a point cloud: an array of shape ``(n_points, n_dimensions)``.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        super().fit(X)
+
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
+        check_array(X, accept_sparse=False)
+
+        # make sure that the parameters are set correctly
+        self.size_ = len(X)
+        if self.size_ <= self.neighborhood_params[0]:
+            warnings.warn("First n_neighbors is too large to be relevant. "
+                          "Consider reducing it.")
+            self.neighborhood_params = (self.size_-1, self.size_)
+        if self.size_ < self.neighborhood_params[1]:
+            warnings.warn("Second `n_neighbors` is too large to be "
+                          "relevant. Consider reducing it. ")
+            self.neighborhood_params = (self.neighborhood_params[0],
+                                        self.size_)
+
+        # Objects used for finding nearest neighbors
+        self.close_neighbors_ = KNeighborsTransformer(
+            mode="connectivity",
+            n_neighbors=self.neighborhood_params[0],
+            metric=self.metric,
+            n_jobs=self.n_jobs
+            )
+
+        self.relevant_neighbors_ = KNeighborsTransformer(
+            mode="connectivity",
+            n_neighbors=self.neighborhood_params[1],
+            metric=self.metric,
+            n_jobs=self.n_jobs
+            )
+
+        self.close_neighbors_.fit(X)
+        self.relevant_neighbors_.fit(X)
+        return self
+
+
+@adapt_fit_transform_docs
+class RadiusLocalVietorisRips(LocalVietorisRipsBase):
+    """Given a :ref:`point cloud <finite_metric_spaces_and_point_clouds>` in
+    Euclidean space, or an abstract :ref:`metric space
+    <finite_metric_spaces_and_point_clouds>` encoded by a distance matrix,
+    information about the local topology around each point is summarized in a
+    collection of persistence diagrams.
+
+    This is done by first isolating appropriate neighborhoods around each point
+    using a radius neighbor transformer, then "coning off" points in an annulus
+    around each point, and finally computing the corresponding associated
+    persistence diagram. The output can then be used to explore the point
+    cloud, or fed into a vectorizer to obtain features.
+
+    Parameters
+    ----------
+    metric : string or callable, optional, default: ``"euclidean"``
+        Input data is to be interpreted as a point cloud (i.e. feature arrays),
+        and `metric`determines a rule with which to calculate distances between
+        pairs of points (i.e. row vectors). If `metric` is a string, it must be
+        one of the options allowed by :func:`scipy.spatial.distance.pdist` for
+        its `metric` parameter, or a metric listed in
+        :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`, including
+        ``"euclidean"``, ``"manhattan"`` or ``"cosine"``. If `metric` is a
+        callable, it should take pairs of vectors (1D arrays) as input and, for
+        each two vectors in a pair, it should return a scalar indicating the
+        distance/dissimilarity between them.
+
+    radii: tuple, optional, default: ``(0.0, 1.0)`` has to consist of two
+    non-negative floats. This determines the radius of the first and second
+    neighborhood around points considered.
+
+    homology_dimensions: tuple, optional, default: ``(1, 2)``. Dimensions
+        (non-negative integers) of the topological features to be detected.
+
+    collapse_edges : bool, optional, default: ``False``
+        Whether to run the edge collapse algorithm in [1]_ prior to the
+        persistent homology computation (see the Notes). Can reduce the runtime
+        dramatically when the data or the maximum homology dimensions are
+        large.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    References
+    ----------
+    .. [1] J.-D. Boissonnat and S. Pritam, "Edge Collapse and Persistence of
+           Flag Complexes"; in *36th International Symposium on Computational
+           Geometry (SoCG 2020)*, pp. 19:119:15,
+           Schloss Dagstuhl-LeibnizZentrum fr Informatik, 2020;
+           `DOI: 10.4230/LIPIcs.SoCG.2020.19
+           <https://doi.org/10.4230/LIPIcs.SoCG.2020.19>`_.
+
+    """
+
+    _hyperparameters = {
+        "metric": {"type": (str, Callable)},
+        "radii": {"type": (tuple, list),
+                  "of": {type: Real, "in": Interval(0, np.inf, closed="left")}
+                  },
+        "homology_dimensions": {
+            "type": (tuple, list),
+            "of": {"type": int, "in": Interval(0, np.inf, closed="left")}
+            },
+        "collapse_edges": {"type": bool}
+        }
+
+    def __init__(self, metric="euclidean", homology_dimensions=(1, 2),
+                 radii=(1, 2), collapse_edges=False, n_jobs=None):
+        self.radii = radii
+        super().__init__(metric=metric,
+                         homology_dimensions=homology_dimensions,
+                         neighborhood_params=self.radii,
+                         collapse_edges=collapse_edges,
+                         n_jobs=n_jobs)
+
+    def fit(self, X, y=None):
+        """Initiates and fits the transformers that efficiently computes the
+        nearest neighbors of given points.
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_points, dimension)
+            Input data representing  point cloud. Can be either
+            a point cloud: an array of shape ``(n_points, n_dimensions)``.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        super().fit(X)
+
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=["n_jobs"])
+        check_array(X, accept_sparse=False)
+
+        # Objects used for finding nearest neighbors
+        self.close_neighbors_ = RadiusNeighborsTransformer(
+            mode="connectivity",
+            radius=self.neighborhood_params[0],
+            metric=self.metric,
+            n_jobs=self.n_jobs
+            )
+
+        self.relevant_neighbors_ = RadiusNeighborsTransformer(
+            mode="connectivity",
+            radius=self.neighborhood_params[1],
+            metric=self.metric,
+            n_jobs=self.n_jobs
+            )
+
+        self.close_neighbors_.fit(X)
+        self.relevant_neighbors_.fit(X)
+        return self
```

## gtda/local_homology/__init__.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-"""The module :mod:`gtda.local_homology` implements transformers
-to generate local persistence diagrams."""
-
-
-from .simplicial import KNeighborsLocalVietorisRips, \
-    RadiusLocalVietorisRips
-
-__all__ = [
-    'KNeighborsLocalVietorisRips',
-    'RadiusLocalVietorisRips',
-    ]
+"""The module :mod:`gtda.local_homology` implements transformers
+to generate local persistence diagrams."""
+
+
+from .simplicial import KNeighborsLocalVietorisRips, \
+    RadiusLocalVietorisRips
+
+__all__ = [
+    'KNeighborsLocalVietorisRips',
+    'RadiusLocalVietorisRips',
+    ]
```

## gtda/local_homology/tests/test_simplicial.py

 * *Ordering differences only*

```diff
@@ -1,109 +1,109 @@
-from hypothesis import given, settings
-from hypothesis.strategies import floats, integers, composite, lists
-
-from gtda.local_homology.simplicial import KNeighborsLocalVietorisRips, \
-      RadiusLocalVietorisRips
-
-
-@composite
-def gen_3d_point_cloud(draw):
-    """ Generates point clouds as lists of floats in the unit cube of
-        size between 2 and 20."""
-    three_floats = lists(floats(min_value=0, max_value=1), min_size=3,
-                         max_size=3, unique=True)
-    return draw(lists(three_floats, min_size=2, max_size=20))
-
-
-@composite
-def gen_dimensions(draw):
-    """ Generates dimension as tuples of integers."""
-    return tuple(draw(lists(integers(min_value=1, max_value=10),
-                      min_size=1, max_size=5, unique=True)))
-
-
-@composite
-def gen_epsilon(draw):
-    """ Generates radii as floats. """
-    epsilon1 = draw(floats(min_value=0, max_value=1))
-    # max to in min_value to avoid warning
-    epsilon2 = draw(floats(min_value=max(epsilon1 + 1e-3, 1e-3),
-                           max_value=1 + 2e-3))
-    return (epsilon1, epsilon2)
-
-
-@composite
-def gen_n_neighbors(draw):
-    """Generates number of neighbors as integers. """
-    n_neighbor1 = draw(integers(min_value=1, max_value=20))
-    n_neighbor2 = draw(integers(min_value=n_neighbor1+1, max_value=30))
-    return (n_neighbor1, n_neighbor2)
-
-
-@settings(deadline=None)
-@given(point_cloud=gen_3d_point_cloud(),
-       point_cloud2=gen_3d_point_cloud(),
-       dims=gen_dimensions(),
-       n_neighbors=gen_n_neighbors())
-def test_KNeighborsLocalVietoris(point_cloud, point_cloud2, dims,
-                                 n_neighbors):
-    # fit transform on same point cloud:
-    X = point_cloud
-
-    # 'min' below to avoid warnings
-    n_neighbors = (min(len(X)-1, n_neighbors[0]),
-                   min(len(X), n_neighbors[1]))
-
-    lh_kn = KNeighborsLocalVietorisRips(metric='euclidean',
-                                        n_neighbors=n_neighbors,
-                                        homology_dimensions=dims,
-                                        n_jobs=-1)
-    lh_kn.fit(X)
-    lh_kn.transform(X)
-    lh_kn = KNeighborsLocalVietorisRips(metric='euclidean',
-                                        n_neighbors=n_neighbors,
-                                        homology_dimensions=dims,
-                                        collapse_edges=True,
-                                        n_jobs=-1)
-    lh_kn.fit_transform(X)
-
-    # fit and transform on different point clouds:
-    Y = point_cloud2
-    lh_kn = KNeighborsLocalVietorisRips(metric='euclidean',
-                                        n_neighbors=n_neighbors,
-                                        homology_dimensions=dims,
-                                        n_jobs=-1)
-    lh_kn.fit(X)
-    lh_kn.transform(Y)
-
-
-@settings(deadline=None)
-@given(point_cloud=gen_3d_point_cloud(),
-       point_cloud2=gen_3d_point_cloud(),
-       dims=gen_dimensions(),
-       radii=gen_epsilon())
-def test_RadiusLocalVietoris(point_cloud, point_cloud2, dims, radii):
-    # fit transform on same point cloud:
-    X = point_cloud
-
-    lh_rad = RadiusLocalVietorisRips(metric='euclidean',
-                                     radii=radii,
-                                     homology_dimensions=dims,
-                                     collapse_edges=True,
-                                     n_jobs=-1)
-    lh_rad.fit(X)
-    lh_rad.transform(X)
-    lh_rad = RadiusLocalVietorisRips(metric='euclidean',
-                                     radii=radii,
-                                     homology_dimensions=dims,
-                                     n_jobs=-1)
-    lh_rad.fit_transform(X)
-
-    # fit and transform on different point clouds:
-    Y = point_cloud2
-    lh_rad = RadiusLocalVietorisRips(metric='euclidean',
-                                     radii=radii,
-                                     homology_dimensions=dims,
-                                     collapse_edges=True,
-                                     n_jobs=-1)
-    lh_rad.fit(X)
-    lh_rad.transform(Y)
+from hypothesis import given, settings
+from hypothesis.strategies import floats, integers, composite, lists
+
+from gtda.local_homology.simplicial import KNeighborsLocalVietorisRips, \
+      RadiusLocalVietorisRips
+
+
+@composite
+def gen_3d_point_cloud(draw):
+    """ Generates point clouds as lists of floats in the unit cube of
+        size between 2 and 20."""
+    three_floats = lists(floats(min_value=0, max_value=1), min_size=3,
+                         max_size=3, unique=True)
+    return draw(lists(three_floats, min_size=2, max_size=20))
+
+
+@composite
+def gen_dimensions(draw):
+    """ Generates dimension as tuples of integers."""
+    return tuple(draw(lists(integers(min_value=1, max_value=10),
+                      min_size=1, max_size=5, unique=True)))
+
+
+@composite
+def gen_epsilon(draw):
+    """ Generates radii as floats. """
+    epsilon1 = draw(floats(min_value=0, max_value=1))
+    # max to in min_value to avoid warning
+    epsilon2 = draw(floats(min_value=max(epsilon1 + 1e-3, 1e-3),
+                           max_value=1 + 2e-3))
+    return (epsilon1, epsilon2)
+
+
+@composite
+def gen_n_neighbors(draw):
+    """Generates number of neighbors as integers. """
+    n_neighbor1 = draw(integers(min_value=1, max_value=20))
+    n_neighbor2 = draw(integers(min_value=n_neighbor1+1, max_value=30))
+    return (n_neighbor1, n_neighbor2)
+
+
+@settings(deadline=None)
+@given(point_cloud=gen_3d_point_cloud(),
+       point_cloud2=gen_3d_point_cloud(),
+       dims=gen_dimensions(),
+       n_neighbors=gen_n_neighbors())
+def test_KNeighborsLocalVietoris(point_cloud, point_cloud2, dims,
+                                 n_neighbors):
+    # fit transform on same point cloud:
+    X = point_cloud
+
+    # 'min' below to avoid warnings
+    n_neighbors = (min(len(X)-1, n_neighbors[0]),
+                   min(len(X), n_neighbors[1]))
+
+    lh_kn = KNeighborsLocalVietorisRips(metric='euclidean',
+                                        n_neighbors=n_neighbors,
+                                        homology_dimensions=dims,
+                                        n_jobs=-1)
+    lh_kn.fit(X)
+    lh_kn.transform(X)
+    lh_kn = KNeighborsLocalVietorisRips(metric='euclidean',
+                                        n_neighbors=n_neighbors,
+                                        homology_dimensions=dims,
+                                        collapse_edges=True,
+                                        n_jobs=-1)
+    lh_kn.fit_transform(X)
+
+    # fit and transform on different point clouds:
+    Y = point_cloud2
+    lh_kn = KNeighborsLocalVietorisRips(metric='euclidean',
+                                        n_neighbors=n_neighbors,
+                                        homology_dimensions=dims,
+                                        n_jobs=-1)
+    lh_kn.fit(X)
+    lh_kn.transform(Y)
+
+
+@settings(deadline=None)
+@given(point_cloud=gen_3d_point_cloud(),
+       point_cloud2=gen_3d_point_cloud(),
+       dims=gen_dimensions(),
+       radii=gen_epsilon())
+def test_RadiusLocalVietoris(point_cloud, point_cloud2, dims, radii):
+    # fit transform on same point cloud:
+    X = point_cloud
+
+    lh_rad = RadiusLocalVietorisRips(metric='euclidean',
+                                     radii=radii,
+                                     homology_dimensions=dims,
+                                     collapse_edges=True,
+                                     n_jobs=-1)
+    lh_rad.fit(X)
+    lh_rad.transform(X)
+    lh_rad = RadiusLocalVietorisRips(metric='euclidean',
+                                     radii=radii,
+                                     homology_dimensions=dims,
+                                     n_jobs=-1)
+    lh_rad.fit_transform(X)
+
+    # fit and transform on different point clouds:
+    Y = point_cloud2
+    lh_rad = RadiusLocalVietorisRips(metric='euclidean',
+                                     radii=radii,
+                                     homology_dimensions=dims,
+                                     collapse_edges=True,
+                                     n_jobs=-1)
+    lh_rad.fit(X)
+    lh_rad.transform(Y)
```

## gtda/mapper/cluster.py

```diff
@@ -1,612 +1,612 @@
-"""Clustering methods and classes for parallelised clustering."""
-# License: GNU AGPLv3
-
-from inspect import signature
-from numbers import Real
-
-import numpy as np
-from joblib import Parallel, delayed
-from sklearn.base import BaseEstimator, ClusterMixin, clone
-from sklearn.cluster._agglomerative import _TREE_BUILDERS, _hc_cut
-from sklearn.utils import check_array
-from sklearn.utils.validation import check_memory
-
-from .utils._cluster import _num_clusters_histogram, _num_clusters_simple
-from ..utils.intervals import Interval
-from ..utils.validation import validate_params
-
-
-def _sample_weight_computer(rel_indices, sample_weight):
-    return {"sample_weight": sample_weight[rel_indices]}
-
-
-def _empty_dict(*args):
-    return {}
-
-
-def _indices_computer_precomputed(rel_indices):
-    return np.ix_(rel_indices, rel_indices)
-
-
-def _indices_computer_not_precomputed(rel_indices):
-    return rel_indices
-
-
-class ParallelClustering(BaseEstimator):
-    """Employ joblib parallelism to cluster different portions of a dataset.
-
-    An arbitrary clustering class which stores a ``labels_`` attribute in
-    ``fit`` can be passed to the constructor. Examples are most classes in
-    ``sklearn.cluster``. The input of :meth:`fit` is of the form ``[X_tot,
-    masks]`` where ``X_tot`` is the full dataset, and ``masks`` is a 2D boolean
-    array, each column of which indicates the location of a portion of
-    ``X_tot`` to cluster separately. Parallelism is achieved over the columns
-    of ``masks``.
-
-    Parameters
-    ----------
-    clusterer : object
-        Clustering object derived from :class:`sklearn.base.ClusterMixin`.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    parallel_backend_prefer : ``"processes"`` | ``"threads"`` | ``None``, \
-        optional, default: ``None``
-        Soft hint for the selection of the default joblib backend. The default
-        process-based backend is 'loky' and the default thread-based backend is
-        'threading'. See [1]_.
-
-    Attributes
-    ----------
-    labels_ : ndarray of shape (n_samples,)
-       For each point in the dataset passed to :meth:`fit`, a tuple of pairs
-       of the form ``(i, partial_label)`` where ``i`` is the index of a boolean
-       mask which selects that point and ``partial_label`` is the cluster label
-       assigned to the point when clustering the subset of the data selected by
-       mask ``i``.
-
-    References
-    ----------
-    .. [1] "Thread-based parallelism vs process-based parallelism", in
-       `joblib documentation
-       <https://joblib.readthedocs.io/en/latest/parallel.html>`_.
-
-    """
-
-    def __init__(self, clusterer, n_jobs=None, parallel_backend_prefer=None):
-        self.clusterer = clusterer
-        self.n_jobs = n_jobs
-        self.parallel_backend_prefer = parallel_backend_prefer
-
-    def _validate_clusterer(self):
-        """Set :attr:`clusterer_` depending on the value of `clusterer`.
-
-        Also verify whether calculations are to be based on precomputed
-        metric/affinity information or not.
-
-        """
-        if not isinstance(self.clusterer, ClusterMixin):
-            raise TypeError("`clusterer` must be an instance of "
-                            "sklearn.base.ClusterMixin.")
-        params = [param for param in ['metric', 'affinity']
-                  if param in signature(self.clusterer.__init__).parameters]
-        precomputed = [(getattr(self.clusterer, param) == 'precomputed')
-                       for param in params]
-        if not precomputed:
-            self._precomputed = False
-        elif len(precomputed) == 1:
-            self._precomputed = precomputed[0]
-        else:
-            raise NotImplementedError("Behaviour when metric and affinity "
-                                      "are both set to 'precomputed' not yet "
-                                      "implemented by ParallelClustering.")
-
-    def fit(self, X, y=None, sample_weight=None):
-        """Fit the clusterer on each portion of the data.
-
-        :attr:`clusterers_` and :attr:`clusters_` are computed and stored.
-
-        Parameters
-        ----------
-        X : list-like of form ``[X_tot, masks]``
-            Input data as a list of length 2. ``X_tot`` is an ndarray of shape
-            (n_samples, n_features) or (n_samples, n_samples) specifying the
-            full data. ``masks`` is a boolean ndarray of shape
-            (n_samples, n_portions) whose columns are boolean masks
-            on ``X_tot``, specifying the portions of ``X_tot`` to be
-            independently clustered.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        sample_weight : array-like or None, optional, default: ``None``
-            The weights for each observation in the full data. If ``None``,
-            all observations are assigned equal weight. Otherwise, it has
-            shape (n_samples,).
-
-        Returns
-        -------
-        self : object
-
-        """
-        X_tot, masks = X
-        check_array(X_tot, ensure_2d=True)
-        check_array(masks, ensure_2d=True)
-        if not np.issubdtype(masks.dtype, bool):
-            raise TypeError("`masks` must be a boolean array.")
-        if len(X_tot) != len(masks):
-            raise ValueError("`X_tot` and `masks` must have the same number "
-                             "of rows.")
-        self._validate_clusterer()
-
-        fit_params = signature(self.clusterer.fit).parameters
-        if sample_weight is not None and "sample_weight" in fit_params:
-            self._sample_weight_computer = _sample_weight_computer
-        else:
-            self._sample_weight_computer = _empty_dict
-
-        if self._precomputed:
-            self._indices_computer = _indices_computer_precomputed
-        else:
-            self._indices_computer = _indices_computer_not_precomputed
-
-        # This seems necessary to avoid large overheads when running fit a
-        # second time. Probably due to refcounts. NOTE: Only works if done
-        # before assigning labels_single. TODO: Investigate
-        self.labels_ = None
-
-        labels_single = Parallel(n_jobs=self.n_jobs,
-                                 prefer=self.parallel_backend_prefer)(
-            delayed(self._labels_single)(
-                X_tot[self._indices_computer(rel_indices)],
-                rel_indices,
-                sample_weight
-                )
-            for rel_indices in map(np.flatnonzero, masks.T)
-            )
-
-        self.labels_ = np.empty(len(X_tot), dtype=object)
-        self.labels_[:] = [tuple([])] * len(X_tot)
-        for i, (rel_indices, partial_labels) in enumerate(labels_single):
-            n_labels = len(partial_labels)
-            labels_i = np.empty(n_labels, dtype=object)
-            labels_i[:] = [((i, partial_label),)
-                           for partial_label in partial_labels]
-            self.labels_[rel_indices] += labels_i
-
-        return self
-
-    def _labels_single(self, X, rel_indices, sample_weight):
-        cloned_clusterer = clone(self.clusterer)
-        kwargs = self._sample_weight_computer(rel_indices, sample_weight)
-
-        return rel_indices, cloned_clusterer.fit(X, **kwargs).labels_
-
-    def fit_predict(self, X, y=None, sample_weight=None):
-        """Fit to the data, and return the found clusters.
-
-        Parameters
-        ----------
-        X : list-like of form ``[X_tot, masks]``
-            Input data as a list of length 2. ``X_tot`` is an ndarray of shape
-            (n_samples, n_features) or (n_samples, n_samples) specifying the
-            full data. ``masks`` is a boolean ndarray of shape
-            (n_samples, n_portions) whose columns are boolean masks
-            on ``X_tot``, specifying the portions of ``X_tot`` to be
-            independently clustered.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        sample_weight : array-like or None, optional, default: ``None``
-            The weights for each observation in the full data. If ``None``,
-            all observations are assigned equal weight. Otherwise, it has
-            shape (n_samples,).
-
-        Returns
-        -------
-        labels : ndarray of shape (n_samples,)
-            See :attr:`labels_`.
-
-        """
-        self.fit(X, sample_weight=sample_weight)
-        return self.labels_
-
-    def transform(self, X, y=None):
-        """Not implemented.
-
-        Only present so that the class is a valid step in a scikit-learn
-        pipeline.
-
-        Parameters
-        ----------
-        X : Ignored
-            Ignored.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        """
-        raise NotImplementedError(
-            "Transforming new data with a fitted ParallelClustering object "
-            "not yet implemented, use fit_transform instead."
-            )
-
-    def fit_transform(self, X, y=None, **fit_params):
-        """Alias for :meth:`fit_predict`.
-
-        Allows for this class to be used as an intermediate step in a
-        scikit-learn pipeline.
-
-        Parameters
-        ----------
-        X : list-like of form ``[X_tot, masks]``
-            Input data as a list of length 2. ``X_tot`` is an ndarray of shape
-            (n_samples, n_features) or (n_samples, n_samples) specifying the
-            full data. ``masks`` is a boolean ndarray of shape
-            (n_samples, n_portions) whose columns are boolean masks
-            on ``X_tot``, specifying the portions of ``X_tot`` to be
-            independently clustered.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples,)
-            See :attr:`labels_`.
-
-        """
-        Xt = self.fit_predict(X, y, **fit_params)
-        return Xt
-
-
-class Agglomerative:
-    """Base class for agglomerative clustering.
-
-    Implements scikit-learn's tree building algorithms for linkage-based
-    clustering. Inheriting classes may implement stopping rules for determining
-    the number of clusters.
-
-    Attributes
-    ----------
-    children_ : ndarray of shape (n_nodes - 1, 2)
-        The children of each non-leaf node. Values less than ``n_samples``
-        correspond to leaves of the tree which are the original samples.
-        A node ``i`` greater than or equal to ``n_samples`` is a non-leaf
-        node and has children ``children_[i - n_samples]``. Alternatively
-        at the ``i``-th iteration, ``children[i][0]`` and ``children[i][1]``
-        are merged to form node ``n_samples + i``.
-
-    n_leaves_ : int
-        Number of leaves in the hierarchical tree.
-
-    distances_ : ndarray of shape (n_nodes - 1,)
-        Distances between nodes in the corresponding place in
-        :attr:`children_`.
-
-    """
-
-    def _build_tree(self, X):
-        memory = check_memory(self.memory)
-
-        if self.linkage == "ward" and self.affinity != "euclidean":
-            raise ValueError(f"{self.affinity} was provided as affinity. "
-                             f"Ward can only work with Euclidean distances.")
-        if self.linkage not in _TREE_BUILDERS:
-            raise ValueError(f"Unknown linkage type {self.linkage}. Valid "
-                             f"options are {_TREE_BUILDERS.keys()}")
-        tree_builder = _TREE_BUILDERS[self.linkage]
-
-        # Construct the tree
-        kwargs = {}
-        if self.linkage != 'ward':
-            kwargs['linkage'] = self.linkage
-            kwargs['affinity'] = self.affinity
-
-        out = memory.cache(tree_builder)(
-            X, n_clusters=None, return_distance=True, **kwargs)
-
-        # Scikit-learn's tree_builder returns a tuple (children,
-        # n_connected_components, n_leaves, parent, distances)
-        self.children_, _, self.n_leaves_, _, self.distances_ = out
-
-
-class FirstSimpleGap(ClusterMixin, BaseEstimator, Agglomerative):
-    """Agglomerative clustering cutting the dendrogram at the first instance
-    of a sufficiently large gap.
-
-    A simple threshold is determined as a fraction of the largest linkage
-    value in the full dendrogram. If possible, the dendrogram is cut at the
-    first occurrence of a gap, between the linkage values of successive merges,
-    which exceeds this threshold. Otherwise, a single cluster is returned. The
-    algorithm can be partially overridden to ensure that the final number of
-    clusters does not exceed a certain threshold, by passing a parameter
-    `max_fraction`.
-
-    Parameters
-    ----------
-    linkage : ``'ward'`` | ``'complete'`` | ``'average'`` | ``'single'``, \
-        optional, default: ``'single'``
-        Which linkage criterion to use. The linkage criterion determines which
-        distance to use between sets of observation. The algorithm will merge
-        the pairs of cluster that minimize this criterion.
-
-        - ``'ward'`` minimizes the variance of the clusters being merged.
-        - ``'average'`` uses the average of the distances of each observation
-          of the two sets.
-        - ``'complete'`` linkage uses the maximum distances between
-          all observations of the two sets.
-        - ``'single'`` uses the minimum of the distances between all
-          observations of the two sets.
-
-    affinity : str, optional, default: ``'euclidean'``
-        Metric used to compute the linkage. Can be ``'euclidean'``, ``'l1'``,
-        ``'l2'``, ``'manhattan'``, ``'cosine'``, or ``'precomputed'``.
-        If linkage is ``'ward'``, only ``'euclidean'`` is accepted.
-        If ``'precomputed'``, a distance matrix (instead of a similarity
-        matrix) is needed as input for :meth:`fit`.
-
-    relative_gap_size : float, optional, default: ``0.3``
-        The fraction of the largest linkage in the dendrogram to be used as
-        a threshold for determining a large enough gap.
-
-    max_fraction : float, optional, default: ``1.``
-        When not ``None``, the algorithm is constrained to produce no more
-        than ``max_fraction * n_samples`` clusters, even if a candidate gap is
-        observed in the iterative process which would produce a greater number
-        of clusters.
-
-    memory : None, str or object with the joblib.Memory interface, \
-        optional, default: ``None``
-        Used to cache the output of the computation of the tree. By default, no
-        caching is performed. If a string is given, it is the path to the
-        caching directory.
-
-    Attributes
-    ----------
-    n_clusters_ : int
-        The number of clusters found by the algorithm.
-
-    labels_ : ndarray of shape (n_samples,)
-        Cluster labels for each sample.
-
-    children_ : ndarray of shape (n_nodes - 1, 2)
-        The children of each non-leaf node. Values less than ``n_samples``
-        correspond to leaves of the tree which are the original samples.
-        A node ``i`` greater than or equal to ``n_samples`` is a non-leaf
-        node and has children ``children_[i - n_samples]``. Alternatively
-        at the ``i``-th iteration, ``children[i][0]`` and ``children[i][1]``
-        are merged to form node ``n_samples + i``.
-
-    n_leaves_ : int
-        Number of leaves in the hierarchical tree.
-
-    distances_ : ndarray of shape (n_nodes - 1,)
-        Distances between nodes in the corresponding place in
-        :attr:`children_`.
-
-    See also
-    --------
-    FirstHistogramGap
-
-    """
-
-    _hyperparameters = {
-        'linkage': {'type': str},
-        'affinity': {'type': str},
-        'relative_gap_size': {'type': Real,
-                              'in': Interval(0, 1, closed='right')},
-        'max_fraction': {'type': Real, 'in': Interval(0, 1, closed='right')}
-        }
-
-    def __init__(self, linkage='single', affinity='euclidean',
-                 relative_gap_size=0.3, max_fraction=1., memory=None):
-        self.linkage = linkage
-        self.affinity = affinity
-        self.relative_gap_size = relative_gap_size
-        self.max_fraction = max_fraction
-        self.memory = memory
-
-    def fit(self, X, y=None):
-        """Fit the agglomerative clustering from features or distance matrix.
-
-        The stopping rule is used to determine :attr:`n_clusters_`, and the
-        full dendrogram is cut there to compute :attr:`labels_`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features) or (n_samples, n_samples)
-            Training instances to cluster, or distances between instances if
-            ``affinity='precomputed'``.
-
-        y : ignored
-            Not used, present here for API consistency by convention.
-
-        Returns
-        -------
-        self
-
-        """
-        X = check_array(X)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['memory'])
-
-        if X.shape[0] == 1:
-            self.labels_ = np.array([0])
-            self.n_clusters_ = 1
-            return self
-
-        self._build_tree(X)
-
-        min_gap_size = self.relative_gap_size * self.distances_[-1]
-        self.n_clusters_ = _num_clusters_simple(
-            self.distances_, min_gap_size, self.max_fraction)
-
-        # Cut the tree to find labels
-        # TODO: Verify whether Daniel Mullner's implementation of this step
-        #  offers any advantage
-        self.labels_ = _hc_cut(self.n_clusters_, self.children_,
-                               self.n_leaves_)
-        return self
-
-
-class FirstHistogramGap(ClusterMixin, BaseEstimator, Agglomerative):
-    """Agglomerative clustering with stopping rule given by a histogram-based
-    version of the first gap method, introduced in [1]_.
-
-    Given a frequency threshold f and an initial integer k: 1) create a
-    histogram of k equally spaced bins of the number of merges in the
-    dendrogram, as a function of the linkage parameter; 2) the value of linkage
-    at which the tree is to be cut is the first one after which a bin of height
-    no greater than f (i.e. a "gap") is observed; 3) if no gap is observed,
-    increase k and repeat 1) and 2) until termination. The algorithm can be
-    partially overridden to ensure that the final number of clusters does not
-    exceed a certain threshold, by passing a parameter `max_fraction`.
-
-    Parameters
-    ----------
-    linkage : ``'ward'`` | ``'complete'`` | ``'average'`` | ``'single'``, \
-        optional, default: ``'single'``
-        Which linkage criterion to use. The linkage criterion determines which
-        distance to use between sets of observation. The algorithm will merge
-        the pairs of cluster that minimize this criterion.
-
-        - ``'ward'`` minimizes the variance of the clusters being merged.
-        - ``'average'`` uses the average of the distances of each observation
-          of the two sets.
-        - ``'complete'`` linkage uses the maximum distances between all
-          observations of the two sets.
-        - ``'single'`` uses the minimum of the distances between all
-          observations of the two sets.
-
-    affinity : str, optional, default: ``'euclidean'``
-        Metric used to compute the linkage. Can be ``'euclidean'``, ``'l1'``,
-        ``'l2'``, ``'manhattan'``, ``'cosine'``, or ``'precomputed'``.
-        If linkage is ``'ward'``, only ``'euclidean'`` is accepted.
-        If ``'precomputed'``, a distance matrix (instead of a similarity
-        matrix) is needed as input for :meth:`fit`.
-
-    freq_threshold : int, optional, default: ``0``
-        The frequency threshold for declaring that a gap in the histogram of
-        merges is present.
-
-    max_fraction : float, optional, default: ``1.``
-        When not ``None``, the algorithm is constrained to produce no more
-        than ``max_fraction * n_samples`` clusters, even if a candidate gap is
-        observed in the iterative process which would produce a greater number
-        of clusters.
-
-    n_bins_start : int, optional, default: ``5``
-        The initial number of bins in the iterative process for finding a gap
-        in the histogram of merges.
-
-    memory : None, str or object with the joblib.Memory interface, \
-        optional, default: ``None``
-        Used to cache the output of the computation of the tree. By default, no
-        caching is performed. If a string is given, it is the path to the
-        caching directory.
-
-    Attributes
-    ----------
-    n_clusters_ : int
-        The number of clusters found by the algorithm.
-
-    labels_ : ndarray of shape (n_samples,)
-        Cluster labels for each sample.
-
-    children_ : ndarray of shape (n_nodes - 1, 2)
-        The children of each non-leaf node. Values less than ``n_samples``
-        correspond to leaves of the tree which are the original samples.
-        A node ``i`` greater than or equal to ``n_samples`` is a non-leaf
-        node and has children ``children_[i - n_samples]``. Alternatively
-        at the ``i``-th iteration, ``children[i][0]`` and ``children[i][1]``
-        are merged to form node ``n_samples + i``.
-
-    n_leaves_ : int
-        Number of leaves in the hierarchical tree.
-
-    distances_ : ndarray of shape (n_nodes - 1,)
-        Distances between nodes in the corresponding place in
-        :attr:`children_`.
-
-    See also
-    --------
-    FirstSimpleGap
-
-    References
-    ----------
-    .. [1] G. Singh, F. Mmoli, and G. Carlsson, "Topological methods for the
-           analysis of high dimensional data sets and 3D object recognition";
-           in *SPBG*, pp. 91--100, 2007.
-
-    """
-
-    _hyperparameters = {
-        'linkage': {'type': str},
-        'affinity': {'type': str},
-        'freq_threshold': {'type': int,
-                           'in': Interval(0, np.inf, closed='left')},
-        'max_fraction': {'type': Real, 'in': Interval(0, 1, closed='right')},
-        'n_bins_start': {'type': int,
-                         'in': Interval(1, np.inf, closed='left')},
-        }
-
-    def __init__(self, linkage='single', affinity='euclidean',
-                 freq_threshold=0, max_fraction=1., n_bins_start=5,
-                 memory=None):
-        self.linkage = linkage
-        self.affinity = affinity
-        self.freq_threshold = freq_threshold
-        self.max_fraction = max_fraction
-        self.n_bins_start = n_bins_start
-        self.memory = memory
-
-    def fit(self, X, y=None):
-        """Fit the agglomerative clustering from features or distance matrix.
-
-        The stopping rule is used to determine :attr:`n_clusters_`, and the
-        full dendrogram is cut there to compute :attr:`labels_`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features) or (n_samples, n_samples)
-            Training instances to cluster, or distances between instances if
-            ``affinity='precomputed'``.
-
-        y : ignored
-            Not used, present here for API consistency by convention.
-
-        Returns
-        -------
-        self
-
-        """
-        X = check_array(X)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['memory'])
-
-        if X.shape[0] == 1:
-            self.labels_ = np.array([0])
-            self.n_clusters_ = 1
-            return self
-
-        self._build_tree(X)
-
-        self.n_clusters_ = _num_clusters_histogram(
-            self.distances_, self.freq_threshold, self.n_bins_start,
-            self.max_fraction)
-
-        # Cut the tree to find labels
-        # TODO: Verify whether Daniel Mullner's implementation of this step
-        #  offers any advantage
-        self.labels_ = _hc_cut(self.n_clusters_, self.children_,
-                               self.n_leaves_)
-        return self
+"""Clustering methods and classes for parallelised clustering."""
+# License: GNU AGPLv3
+
+from inspect import signature
+from numbers import Real
+
+import numpy as np
+from joblib import Parallel, delayed
+from sklearn.base import BaseEstimator, ClusterMixin, clone
+from sklearn.cluster._agglomerative import _TREE_BUILDERS, _hc_cut
+from sklearn.utils import check_array
+from sklearn.utils.validation import check_memory
+
+from .utils._cluster import _num_clusters_histogram, _num_clusters_simple
+from ..utils.intervals import Interval
+from ..utils.validation import validate_params
+
+
+def _sample_weight_computer(rel_indices, sample_weight):
+    return {"sample_weight": sample_weight[rel_indices]}
+
+
+def _empty_dict(*args):
+    return {}
+
+
+def _indices_computer_precomputed(rel_indices):
+    return np.ix_(rel_indices, rel_indices)
+
+
+def _indices_computer_not_precomputed(rel_indices):
+    return rel_indices
+
+
+class ParallelClustering(BaseEstimator):
+    """Employ joblib parallelism to cluster different portions of a dataset.
+
+    An arbitrary clustering class which stores a ``labels_`` attribute in
+    ``fit`` can be passed to the constructor. Examples are most classes in
+    ``sklearn.cluster``. The input of :meth:`fit` is of the form ``[X_tot,
+    masks]`` where ``X_tot`` is the full dataset, and ``masks`` is a 2D boolean
+    array, each column of which indicates the location of a portion of
+    ``X_tot`` to cluster separately. Parallelism is achieved over the columns
+    of ``masks``.
+
+    Parameters
+    ----------
+    clusterer : object
+        Clustering object derived from :class:`sklearn.base.ClusterMixin`.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    parallel_backend_prefer : ``"processes"`` | ``"threads"`` | ``None``, \
+        optional, default: ``None``
+        Soft hint for the selection of the default joblib backend. The default
+        process-based backend is 'loky' and the default thread-based backend is
+        'threading'. See [1]_.
+
+    Attributes
+    ----------
+    labels_ : ndarray of shape (n_samples,)
+       For each point in the dataset passed to :meth:`fit`, a tuple of pairs
+       of the form ``(i, partial_label)`` where ``i`` is the index of a boolean
+       mask which selects that point and ``partial_label`` is the cluster label
+       assigned to the point when clustering the subset of the data selected by
+       mask ``i``.
+
+    References
+    ----------
+    .. [1] "Thread-based parallelism vs process-based parallelism", in
+       `joblib documentation
+       <https://joblib.readthedocs.io/en/latest/parallel.html>`_.
+
+    """
+
+    def __init__(self, clusterer, n_jobs=None, parallel_backend_prefer=None):
+        self.clusterer = clusterer
+        self.n_jobs = n_jobs
+        self.parallel_backend_prefer = parallel_backend_prefer
+
+    def _validate_clusterer(self):
+        """Set :attr:`clusterer_` depending on the value of `clusterer`.
+
+        Also verify whether calculations are to be based on precomputed
+        metric/affinity information or not.
+
+        """
+        if not isinstance(self.clusterer, ClusterMixin):
+            raise TypeError("`clusterer` must be an instance of "
+                            "sklearn.base.ClusterMixin.")
+        params = [param for param in ['metric', 'affinity']
+                  if param in signature(self.clusterer.__init__).parameters]
+        precomputed = [param for param in params
+                       if (getattr(self.clusterer, param) == 'precomputed')]
+        if not precomputed:
+            self._precomputed = False
+        elif len(precomputed) == 1:
+            self._precomputed = precomputed[0]
+        else:
+            raise NotImplementedError("Behaviour when metric and affinity "
+                                      "are both set to 'precomputed' not yet "
+                                      "implemented by ParallelClustering.")
+
+    def fit(self, X, y=None, sample_weight=None):
+        """Fit the clusterer on each portion of the data.
+
+        :attr:`clusterers_` and :attr:`clusters_` are computed and stored.
+
+        Parameters
+        ----------
+        X : list-like of form ``[X_tot, masks]``
+            Input data as a list of length 2. ``X_tot`` is an ndarray of shape
+            (n_samples, n_features) or (n_samples, n_samples) specifying the
+            full data. ``masks`` is a boolean ndarray of shape
+            (n_samples, n_portions) whose columns are boolean masks
+            on ``X_tot``, specifying the portions of ``X_tot`` to be
+            independently clustered.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        sample_weight : array-like or None, optional, default: ``None``
+            The weights for each observation in the full data. If ``None``,
+            all observations are assigned equal weight. Otherwise, it has
+            shape (n_samples,).
+
+        Returns
+        -------
+        self : object
+
+        """
+        X_tot, masks = X
+        check_array(X_tot, ensure_2d=True)
+        check_array(masks, ensure_2d=True)
+        if not np.issubdtype(masks.dtype, bool):
+            raise TypeError("`masks` must be a boolean array.")
+        if len(X_tot) != len(masks):
+            raise ValueError("`X_tot` and `masks` must have the same number "
+                             "of rows.")
+        self._validate_clusterer()
+
+        fit_params = signature(self.clusterer.fit).parameters
+        if sample_weight is not None and "sample_weight" in fit_params:
+            self._sample_weight_computer = _sample_weight_computer
+        else:
+            self._sample_weight_computer = _empty_dict
+
+        if self._precomputed:
+            self._indices_computer = _indices_computer_precomputed
+        else:
+            self._indices_computer = _indices_computer_not_precomputed
+
+        # This seems necessary to avoid large overheads when running fit a
+        # second time. Probably due to refcounts. NOTE: Only works if done
+        # before assigning labels_single. TODO: Investigate
+        self.labels_ = None
+
+        labels_single = Parallel(n_jobs=self.n_jobs,
+                                 prefer=self.parallel_backend_prefer)(
+            delayed(self._labels_single)(
+                X_tot[self._indices_computer(rel_indices)],
+                rel_indices,
+                sample_weight
+                )
+            for rel_indices in map(np.flatnonzero, masks.T)
+            )
+
+        self.labels_ = np.empty(len(X_tot), dtype=object)
+        self.labels_[:] = [tuple([])] * len(X_tot)
+        for i, (rel_indices, partial_labels) in enumerate(labels_single):
+            n_labels = len(partial_labels)
+            labels_i = np.empty(n_labels, dtype=object)
+            labels_i[:] = [((i, partial_label),)
+                           for partial_label in partial_labels]
+            self.labels_[rel_indices] += labels_i
+
+        return self
+
+    def _labels_single(self, X, rel_indices, sample_weight):
+        cloned_clusterer = clone(self.clusterer)
+        kwargs = self._sample_weight_computer(rel_indices, sample_weight)
+
+        return rel_indices, cloned_clusterer.fit(X, **kwargs).labels_
+
+    def fit_predict(self, X, y=None, sample_weight=None):
+        """Fit to the data, and return the found clusters.
+
+        Parameters
+        ----------
+        X : list-like of form ``[X_tot, masks]``
+            Input data as a list of length 2. ``X_tot`` is an ndarray of shape
+            (n_samples, n_features) or (n_samples, n_samples) specifying the
+            full data. ``masks`` is a boolean ndarray of shape
+            (n_samples, n_portions) whose columns are boolean masks
+            on ``X_tot``, specifying the portions of ``X_tot`` to be
+            independently clustered.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        sample_weight : array-like or None, optional, default: ``None``
+            The weights for each observation in the full data. If ``None``,
+            all observations are assigned equal weight. Otherwise, it has
+            shape (n_samples,).
+
+        Returns
+        -------
+        labels : ndarray of shape (n_samples,)
+            See :attr:`labels_`.
+
+        """
+        self.fit(X, sample_weight=sample_weight)
+        return self.labels_
+
+    def transform(self, X, y=None):
+        """Not implemented.
+
+        Only present so that the class is a valid step in a scikit-learn
+        pipeline.
+
+        Parameters
+        ----------
+        X : Ignored
+            Ignored.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        """
+        raise NotImplementedError(
+            "Transforming new data with a fitted ParallelClustering object "
+            "not yet implemented, use fit_transform instead."
+            )
+
+    def fit_transform(self, X, y=None, **fit_params):
+        """Alias for :meth:`fit_predict`.
+
+        Allows for this class to be used as an intermediate step in a
+        scikit-learn pipeline.
+
+        Parameters
+        ----------
+        X : list-like of form ``[X_tot, masks]``
+            Input data as a list of length 2. ``X_tot`` is an ndarray of shape
+            (n_samples, n_features) or (n_samples, n_samples) specifying the
+            full data. ``masks`` is a boolean ndarray of shape
+            (n_samples, n_portions) whose columns are boolean masks
+            on ``X_tot``, specifying the portions of ``X_tot`` to be
+            independently clustered.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples,)
+            See :attr:`labels_`.
+
+        """
+        Xt = self.fit_predict(X, y, **fit_params)
+        return Xt
+
+
+class Agglomerative:
+    """Base class for agglomerative clustering.
+
+    Implements scikit-learn's tree building algorithms for linkage-based
+    clustering. Inheriting classes may implement stopping rules for determining
+    the number of clusters.
+
+    Attributes
+    ----------
+    children_ : ndarray of shape (n_nodes - 1, 2)
+        The children of each non-leaf node. Values less than ``n_samples``
+        correspond to leaves of the tree which are the original samples.
+        A node ``i`` greater than or equal to ``n_samples`` is a non-leaf
+        node and has children ``children_[i - n_samples]``. Alternatively
+        at the ``i``-th iteration, ``children[i][0]`` and ``children[i][1]``
+        are merged to form node ``n_samples + i``.
+
+    n_leaves_ : int
+        Number of leaves in the hierarchical tree.
+
+    distances_ : ndarray of shape (n_nodes - 1,)
+        Distances between nodes in the corresponding place in
+        :attr:`children_`.
+
+    """
+
+    def _build_tree(self, X):
+        memory = check_memory(self.memory)
+
+        if self.linkage == "ward" and self.affinity != "euclidean":
+            raise ValueError(f"{self.affinity} was provided as affinity. "
+                             f"Ward can only work with Euclidean distances.")
+        if self.linkage not in _TREE_BUILDERS:
+            raise ValueError(f"Unknown linkage type {self.linkage}. Valid "
+                             f"options are {_TREE_BUILDERS.keys()}")
+        tree_builder = _TREE_BUILDERS[self.linkage]
+
+        # Construct the tree
+        kwargs = {}
+        if self.linkage != 'ward':
+            kwargs['linkage'] = self.linkage
+            kwargs['affinity'] = self.affinity
+
+        out = memory.cache(tree_builder)(
+            X, n_clusters=None, return_distance=True, **kwargs)
+
+        # Scikit-learn's tree_builder returns a tuple (children,
+        # n_connected_components, n_leaves, parent, distances)
+        self.children_, _, self.n_leaves_, _, self.distances_ = out
+
+
+class FirstSimpleGap(ClusterMixin, BaseEstimator, Agglomerative):
+    """Agglomerative clustering cutting the dendrogram at the first instance
+    of a sufficiently large gap.
+
+    A simple threshold is determined as a fraction of the largest linkage
+    value in the full dendrogram. If possible, the dendrogram is cut at the
+    first occurrence of a gap, between the linkage values of successive merges,
+    which exceeds this threshold. Otherwise, a single cluster is returned. The
+    algorithm can be partially overridden to ensure that the final number of
+    clusters does not exceed a certain threshold, by passing a parameter
+    `max_fraction`.
+
+    Parameters
+    ----------
+    linkage : ``'ward'`` | ``'complete'`` | ``'average'`` | ``'single'``, \
+        optional, default: ``'single'``
+        Which linkage criterion to use. The linkage criterion determines which
+        distance to use between sets of observation. The algorithm will merge
+        the pairs of cluster that minimize this criterion.
+
+        - ``'ward'`` minimizes the variance of the clusters being merged.
+        - ``'average'`` uses the average of the distances of each observation
+          of the two sets.
+        - ``'complete'`` linkage uses the maximum distances between
+          all observations of the two sets.
+        - ``'single'`` uses the minimum of the distances between all
+          observations of the two sets.
+
+    affinity : str, optional, default: ``'euclidean'``
+        Metric used to compute the linkage. Can be ``'euclidean'``, ``'l1'``,
+        ``'l2'``, ``'manhattan'``, ``'cosine'``, or ``'precomputed'``.
+        If linkage is ``'ward'``, only ``'euclidean'`` is accepted.
+        If ``'precomputed'``, a distance matrix (instead of a similarity
+        matrix) is needed as input for :meth:`fit`.
+
+    relative_gap_size : float, optional, default: ``0.3``
+        The fraction of the largest linkage in the dendrogram to be used as
+        a threshold for determining a large enough gap.
+
+    max_fraction : float, optional, default: ``1.``
+        When not ``None``, the algorithm is constrained to produce no more
+        than ``max_fraction * n_samples`` clusters, even if a candidate gap is
+        observed in the iterative process which would produce a greater number
+        of clusters.
+
+    memory : None, str or object with the joblib.Memory interface, \
+        optional, default: ``None``
+        Used to cache the output of the computation of the tree. By default, no
+        caching is performed. If a string is given, it is the path to the
+        caching directory.
+
+    Attributes
+    ----------
+    n_clusters_ : int
+        The number of clusters found by the algorithm.
+
+    labels_ : ndarray of shape (n_samples,)
+        Cluster labels for each sample.
+
+    children_ : ndarray of shape (n_nodes - 1, 2)
+        The children of each non-leaf node. Values less than ``n_samples``
+        correspond to leaves of the tree which are the original samples.
+        A node ``i`` greater than or equal to ``n_samples`` is a non-leaf
+        node and has children ``children_[i - n_samples]``. Alternatively
+        at the ``i``-th iteration, ``children[i][0]`` and ``children[i][1]``
+        are merged to form node ``n_samples + i``.
+
+    n_leaves_ : int
+        Number of leaves in the hierarchical tree.
+
+    distances_ : ndarray of shape (n_nodes - 1,)
+        Distances between nodes in the corresponding place in
+        :attr:`children_`.
+
+    See also
+    --------
+    FirstHistogramGap
+
+    """
+
+    _hyperparameters = {
+        'linkage': {'type': str},
+        'affinity': {'type': str},
+        'relative_gap_size': {'type': Real,
+                              'in': Interval(0, 1, closed='right')},
+        'max_fraction': {'type': Real, 'in': Interval(0, 1, closed='right')}
+        }
+
+    def __init__(self, linkage='single', affinity='euclidean',
+                 relative_gap_size=0.3, max_fraction=1., memory=None):
+        self.linkage = linkage
+        self.affinity = affinity
+        self.relative_gap_size = relative_gap_size
+        self.max_fraction = max_fraction
+        self.memory = memory
+
+    def fit(self, X, y=None):
+        """Fit the agglomerative clustering from features or distance matrix.
+
+        The stopping rule is used to determine :attr:`n_clusters_`, and the
+        full dendrogram is cut there to compute :attr:`labels_`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features) or (n_samples, n_samples)
+            Training instances to cluster, or distances between instances if
+            ``affinity='precomputed'``.
+
+        y : ignored
+            Not used, present here for API consistency by convention.
+
+        Returns
+        -------
+        self
+
+        """
+        X = check_array(X)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['memory'])
+
+        if X.shape[0] == 1:
+            self.labels_ = np.array([0])
+            self.n_clusters_ = 1
+            return self
+
+        self._build_tree(X)
+
+        min_gap_size = self.relative_gap_size * self.distances_[-1]
+        self.n_clusters_ = _num_clusters_simple(
+            self.distances_, min_gap_size, self.max_fraction)
+
+        # Cut the tree to find labels
+        # TODO: Verify whether Daniel Mullner's implementation of this step
+        #  offers any advantage
+        self.labels_ = _hc_cut(self.n_clusters_, self.children_,
+                               self.n_leaves_)
+        return self
+
+
+class FirstHistogramGap(ClusterMixin, BaseEstimator, Agglomerative):
+    """Agglomerative clustering with stopping rule given by a histogram-based
+    version of the first gap method, introduced in [1]_.
+
+    Given a frequency threshold f and an initial integer k: 1) create a
+    histogram of k equally spaced bins of the number of merges in the
+    dendrogram, as a function of the linkage parameter; 2) the value of linkage
+    at which the tree is to be cut is the first one after which a bin of height
+    no greater than f (i.e. a "gap") is observed; 3) if no gap is observed,
+    increase k and repeat 1) and 2) until termination. The algorithm can be
+    partially overridden to ensure that the final number of clusters does not
+    exceed a certain threshold, by passing a parameter `max_fraction`.
+
+    Parameters
+    ----------
+    linkage : ``'ward'`` | ``'complete'`` | ``'average'`` | ``'single'``, \
+        optional, default: ``'single'``
+        Which linkage criterion to use. The linkage criterion determines which
+        distance to use between sets of observation. The algorithm will merge
+        the pairs of cluster that minimize this criterion.
+
+        - ``'ward'`` minimizes the variance of the clusters being merged.
+        - ``'average'`` uses the average of the distances of each observation
+          of the two sets.
+        - ``'complete'`` linkage uses the maximum distances between all
+          observations of the two sets.
+        - ``'single'`` uses the minimum of the distances between all
+          observations of the two sets.
+
+    affinity : str, optional, default: ``'euclidean'``
+        Metric used to compute the linkage. Can be ``'euclidean'``, ``'l1'``,
+        ``'l2'``, ``'manhattan'``, ``'cosine'``, or ``'precomputed'``.
+        If linkage is ``'ward'``, only ``'euclidean'`` is accepted.
+        If ``'precomputed'``, a distance matrix (instead of a similarity
+        matrix) is needed as input for :meth:`fit`.
+
+    freq_threshold : int, optional, default: ``0``
+        The frequency threshold for declaring that a gap in the histogram of
+        merges is present.
+
+    max_fraction : float, optional, default: ``1.``
+        When not ``None``, the algorithm is constrained to produce no more
+        than ``max_fraction * n_samples`` clusters, even if a candidate gap is
+        observed in the iterative process which would produce a greater number
+        of clusters.
+
+    n_bins_start : int, optional, default: ``5``
+        The initial number of bins in the iterative process for finding a gap
+        in the histogram of merges.
+
+    memory : None, str or object with the joblib.Memory interface, \
+        optional, default: ``None``
+        Used to cache the output of the computation of the tree. By default, no
+        caching is performed. If a string is given, it is the path to the
+        caching directory.
+
+    Attributes
+    ----------
+    n_clusters_ : int
+        The number of clusters found by the algorithm.
+
+    labels_ : ndarray of shape (n_samples,)
+        Cluster labels for each sample.
+
+    children_ : ndarray of shape (n_nodes - 1, 2)
+        The children of each non-leaf node. Values less than ``n_samples``
+        correspond to leaves of the tree which are the original samples.
+        A node ``i`` greater than or equal to ``n_samples`` is a non-leaf
+        node and has children ``children_[i - n_samples]``. Alternatively
+        at the ``i``-th iteration, ``children[i][0]`` and ``children[i][1]``
+        are merged to form node ``n_samples + i``.
+
+    n_leaves_ : int
+        Number of leaves in the hierarchical tree.
+
+    distances_ : ndarray of shape (n_nodes - 1,)
+        Distances between nodes in the corresponding place in
+        :attr:`children_`.
+
+    See also
+    --------
+    FirstSimpleGap
+
+    References
+    ----------
+    .. [1] G. Singh, F. Mmoli, and G. Carlsson, "Topological methods for the
+           analysis of high dimensional data sets and 3D object recognition";
+           in *SPBG*, pp. 91--100, 2007.
+
+    """
+
+    _hyperparameters = {
+        'linkage': {'type': str},
+        'affinity': {'type': str},
+        'freq_threshold': {'type': int,
+                           'in': Interval(0, np.inf, closed='left')},
+        'max_fraction': {'type': Real, 'in': Interval(0, 1, closed='right')},
+        'n_bins_start': {'type': int,
+                         'in': Interval(1, np.inf, closed='left')},
+        }
+
+    def __init__(self, linkage='single', affinity='euclidean',
+                 freq_threshold=0, max_fraction=1., n_bins_start=5,
+                 memory=None):
+        self.linkage = linkage
+        self.affinity = affinity
+        self.freq_threshold = freq_threshold
+        self.max_fraction = max_fraction
+        self.n_bins_start = n_bins_start
+        self.memory = memory
+
+    def fit(self, X, y=None):
+        """Fit the agglomerative clustering from features or distance matrix.
+
+        The stopping rule is used to determine :attr:`n_clusters_`, and the
+        full dendrogram is cut there to compute :attr:`labels_`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features) or (n_samples, n_samples)
+            Training instances to cluster, or distances between instances if
+            ``affinity='precomputed'``.
+
+        y : ignored
+            Not used, present here for API consistency by convention.
+
+        Returns
+        -------
+        self
+
+        """
+        X = check_array(X)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['memory'])
+
+        if X.shape[0] == 1:
+            self.labels_ = np.array([0])
+            self.n_clusters_ = 1
+            return self
+
+        self._build_tree(X)
+
+        self.n_clusters_ = _num_clusters_histogram(
+            self.distances_, self.freq_threshold, self.n_bins_start,
+            self.max_fraction)
+
+        # Cut the tree to find labels
+        # TODO: Verify whether Daniel Mullner's implementation of this step
+        #  offers any advantage
+        self.labels_ = _hc_cut(self.n_clusters_, self.children_,
+                               self.n_leaves_)
+        return self
```

## gtda/mapper/cover.py

 * *Ordering differences only*

```diff
@@ -1,561 +1,561 @@
-"""Covering schemes for one or several dimensions."""
-# License: GNU AGPLv3
-
-import warnings
-from functools import partial
-from itertools import product
-
-import numpy as np
-from scipy.stats import rankdata
-from sklearn.base import BaseEstimator, TransformerMixin, clone
-from sklearn.exceptions import DataDimensionalityWarning, NotFittedError
-from sklearn.utils import check_array
-from sklearn.utils.validation import check_is_fitted
-
-from .utils._cover import _check_has_one_column, \
-    _remove_empty_and_duplicate_intervals
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.intervals import Interval
-from ..utils.validation import validate_params
-
-
-@adapt_fit_transform_docs
-class OneDimensionalCover(BaseEstimator, TransformerMixin):
-    """Cover of one-dimensional data coming from open overlapping intervals.
-
-    In :meth:`fit`, given a training array `X` representing a collection of
-    real numbers, a cover of the real line by open intervals
-    :math:`I_k = (a_k, b_k)` (:math:`k = 1, \\ldots, n`,
-    :math:`a_k < a_{k+1}`, :math:`b_k < b_{k+1}`) is constructed
-    based on the distribution of values in `X`. In :meth:`transform`,
-    the cover is applied to a new array `X'` to yield a cover of `X'`.
-
-    All covers constructed in :meth:`fit` have :math:`a_1 = -\\infty`
-    and :math:`b_n = + \\infty``. Two kinds of cover are currently available:
-    "uniform" and "balanced". A uniform cover is such that
-    :math:`b_1 - m = b_2 - a_2 = \\cdots = M - a_n` where :math:`m` and
-    :math:`M` are the minimum and maximum values in `X` respectively. A
-    balanced cover is such that approximately the same number of unique
-    values from `X` is contained in each cover interval.
-
-    Parameters
-    ----------
-    kind : ``'uniform'`` | ``'balanced'``, optional, default: ``'uniform'``
-        The kind of cover to use.
-
-    n_intervals : int, optional, default: ``10``
-        The number of intervals in the cover calculated in :meth:`fit`.
-
-    overlap_frac : float, optional, default: ``0.1``
-        If the cover is uniform, this is the ratio between the length of the
-        intersection between consecutive intervals and the length of each
-        interval. If the cover is balanced, this is the analogous fractional
-        overlap for a uniform cover of the closed interval
-        :math:`(0.5, N + 0.5)` where :math:`N` is the number of unique
-        values in the training array (see the Notes).
-
-    Attributes
-    ----------
-    left_limits_ : ndarray of shape (n_intervals,)
-        Left limits of the cover intervals computed in :meth:`fit`. See the
-        Notes.
-
-    right_limits_ : ndarray of shape (n_intervals,)
-        Right limits of the cover intervals computed in :meth:`fit`. See the
-        Notes.
-
-    Notes
-    -----
-    In the case of a balanced cover, :meth:`left_limits_` and
-    :meth:`right_limits_` are computed as follows given a training array `X`:
-    first, entries in `X` are ranked in ascending order, starting at 1 and
-    with the same rank repeated in the case of equal values; then, the closed
-    interval :math:`(0.5, N + 0.5)`, where :math:`N` is the maximum
-    rank observed, is covered uniformly with parameters `n_intervals` and
-    `overlap_frac`, yielding intervals :math:`(\\alpha_k, \\beta_k)`;
-    the final cover is made of intervals :math:`(a_k, b_k)` where, for
-    :math:`k > 1` (resp. :math:`k < ` `n_intervals`), :math:`a_k` (resp.
-    :math:`b_k`) is the value of any entry in `X` ranked as the floor (
-    resp. ceiling) of :math:`\\alpha_k` (resp. :math:`\\beta_k`).
-
-    See also
-    --------
-    CubicalCover
-
-    """
-
-    _hyperparameters = {
-        'kind': {'type': str, 'in': ['uniform', 'balanced']},
-        'n_intervals': {'type': int, 'in': Interval(1, np.inf, closed='left')},
-        'overlap_frac': {'type': float, 'in': Interval(0, 1, closed='neither')}
-        }
-
-    def __init__(self, kind='uniform', n_intervals=10, overlap_frac=0.1):
-        self.kind = kind
-        self.n_intervals = n_intervals
-        self.overlap_frac = overlap_frac
-
-    def _fit_uniform(self, X):
-        self.left_limits_, self.right_limits_ = self._find_interval_limits(
-            X, self.n_intervals, self.overlap_frac, is_uniform=True)
-        return self
-
-    def _fit_balanced(self, X):
-        X_rank = rankdata(X, method='dense') - 1
-        left_limits, right_limits = self._find_interval_limits(
-            X_rank, self.n_intervals, self.overlap_frac, is_uniform=False)
-        left_limits_int = left_limits.astype(int)
-        left_ranks = np.where(left_limits >= 0, left_limits_int, -1)
-        right_limits_int = right_limits.astype(int)
-        right_ranks = np.where(right_limits_int == right_limits,
-                               right_limits_int,
-                               right_limits_int + 1)
-        self.left_limits_, self.right_limits_ = self._limits_from_ranks(
-            X_rank, X.flatten(), left_ranks, right_ranks)
-        return self
-
-    def fit(self, X, y=None):
-        """Compute all cover interval limits according to `X` and store them
-        in :attr:`left_limits_` and :attr:`right_limits_`. Then, return the
-        estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples,) or (n_samples, 1)
-            Input data.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_array(X, ensure_2d=False)
-        validate_params(self.get_params(), self._hyperparameters)
-        if self.overlap_frac <= 1e-8:
-            warnings.warn("`overlap_frac` is close to zero, "
-                          "which might cause numerical issues and errors.",
-                          RuntimeWarning)
-
-        if X.ndim == 2:
-            _check_has_one_column(X)
-
-        is_uniform = self.kind == 'uniform'
-        fitter = self._fit_uniform if is_uniform else self._fit_balanced
-        return fitter(X)
-
-    def _transform(self, X):
-        return np.logical_and(X > self.left_limits_, X < self.right_limits_)
-
-    def transform(self, X, y=None):
-        """Compute a cover of `X` according to the cover of the real line
-        computed in :meth:`fit`, and return it as a two-dimensional boolean
-        array. Each column indicates the location of entries in `X`
-        belonging to a common cover interval.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples,) or (n_samples, 1)
-            Input data.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_cover_sets)
-            Encoding of the cover of `X` as a boolean array. In general,
-            ``n_cover_sets`` is less than or equal to `n_intervals` as empty
-            or duplicated cover sets are removed.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, ensure_2d=False)
-
-        if Xt.ndim == 2:
-            _check_has_one_column(Xt)
-        else:
-            Xt = Xt[:, None]
-
-        if self.kind == 'balanced':
-            # Test whether self.left_limits_ and self.right_limits_ have
-            # been created -- to catch cases in which transform is run after
-            # fit_transform but not after fit.
-            self._check_limit_attrs()
-
-        Xt = self._transform(Xt)
-        Xt = _remove_empty_and_duplicate_intervals(Xt)
-        return Xt
-
-    def _fit_transform_balanced(self, X):
-        """Shortcut in the case of a balanced cover, avoiding overhead
-        from calculation of self.left_limits_ and self.right_limits_.
-
-        Stores hidden attributes _left_limits and _right_limits which refer
-        to a cover of the interval (-0.5, n_unique - 0.5) where n_unique is
-        the number of unique points in X.
-
-        """
-        X_rank = rankdata(X, method='dense') - 1
-        self._left_limits, self._right_limits = self._find_interval_limits(
-            X_rank, self.n_intervals, self.overlap_frac, is_uniform=False)
-        X_rank = np.broadcast_to(X_rank[:, None],
-                                 (X.shape[0], self.n_intervals))
-        Xt = np.logical_and(X_rank > self._left_limits,
-                            X_rank < self._right_limits)
-        return Xt
-
-    def _fit_transform(self, X):
-        if self.kind == 'uniform':
-            Xt = self._fit_uniform(X)._transform(X)
-        else:
-            Xt = self._fit_transform_balanced(X)
-        return Xt
-
-    def fit_transform(self, X, y=None, **fit_params):
-        """Fit to the data, then transform it.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples,) or (n_samples, 1)
-            Input data.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_cover_sets)
-            Encoding of the cover of `X` as a boolean array. In general,
-            ``n_cover_sets`` is less than or equal to `n_intervals` as empty
-            or duplicated cover sets are removed.
-
-        """
-        Xt = check_array(X, ensure_2d=False)
-        validate_params(self.get_params(), self._hyperparameters)
-
-        if Xt.ndim == 2:
-            _check_has_one_column(Xt)
-        else:
-            Xt = Xt[:, None]
-
-        Xt = self._fit_transform(Xt)
-        Xt = _remove_empty_and_duplicate_intervals(Xt)
-        return Xt
-
-    def get_fitted_intervals(self):
-        """Returns the open intervals computed in :meth:`fit`, as a list of
-        tuples (a, b) where a < b.
-
-        """
-        check_is_fitted(self)
-        if self.kind == 'balanced':
-            # Test whether self.left_limits_ and self.right_limits_ have
-            # been created
-            self._check_limit_attrs()
-        return list(zip(self.left_limits_, self.right_limits_))
-
-    def _check_limit_attrs(self):
-        limit_attrs = ['left_limits_', 'right_limits_']
-        has_limits = all([hasattr(self, attr) for attr in limit_attrs])
-        if not has_limits:
-            raise NotFittedError(
-                "When the cover is balanced and n_intervals > 1, the left "
-                "and right limits of the cover intervals are not "
-                "explicitly calculated during 'fit_transform'. Please "
-                "call 'fit' explicitly on the same data before using this "
-                "method.")
-
-    def _find_interval_limits(self, X, n_intervals, overlap_frac,
-                              is_uniform=True):
-        if is_uniform:
-            min_val, max_val = np.min(X), np.max(X)
-            only_one_pt = (min_val == max_val)
-        else:
-            # Assume X is the result of a call to scipy.stats.rankdata
-            min_val, max_val = -0.5, np.max(X) + 0.5
-            only_one_pt = (min_val == max_val - 1)
-
-        # Allow X to have one unique sample only if one interval is required,
-        # in which case the fitted interval will be (-np.inf, np.inf).
-        if only_one_pt and n_intervals > 1:
-            raise ValueError(
-                f"Only one unique filter value found, cannot fit "
-                f"{n_intervals} > 1 intervals.")
-
-        left_limits, right_limits = \
-            self._cover_limits(min_val, max_val, n_intervals, overlap_frac)
-        if is_uniform:
-            left_limits[0], right_limits[-1] = -np.inf, np.inf
-        return left_limits, right_limits
-
-    def _limits_from_ranks(self, X_rank, X, left_ranks, right_ranks):
-        n_intervals = self.n_intervals
-        X_rank = np.broadcast_to(X_rank[:, None],
-                                 (X_rank.shape[0], n_intervals))
-        left_mask = (X_rank == left_ranks)
-        right_mask = (X_rank == right_ranks)
-        left_indices = (np.flatnonzero(left_mask[:, i])
-                        for i in range(n_intervals))
-        right_indices = (np.flatnonzero(right_mask[:, i])
-                         for i in range(n_intervals))
-        left_limits = np.array([
-            X[nonzero_indices[0]] if nonzero_indices.size else -np.inf
-            for nonzero_indices in left_indices
-            ])
-        right_limits = np.array([
-            X[nonzero_indices[0]] if nonzero_indices.size else np.inf
-            for nonzero_indices in right_indices
-            ])
-        left_limits[0] = -np.inf
-        right_limits[-1] = np.inf
-        return left_limits, right_limits
-
-    @staticmethod
-    def _cover_limits(min_val, max_val, n_intervals, overlap_frac):
-        # Construct a uniform cover of the interval [min_val, max_val].
-        # Let the length of each interval be l. The equation to solve for l is
-        # (n_intervals - 1) * l * (1 - overlap_frac) + l = max_val - min_val.
-        # The maximum left endpoint is at min_val + (n_intervals - 1) * (1 -
-        # overlap_frac) * l
-        total_len = max_val - min_val
-        interval_len = total_len / \
-            (n_intervals - (n_intervals - 1) * overlap_frac)
-
-        last = min_val + (n_intervals - 1) * (1 - overlap_frac) * interval_len
-        left_limits = np.linspace(min_val, last, num=n_intervals,
-                                  endpoint=True)
-        right_limits = left_limits + interval_len
-        return left_limits, right_limits
-
-
-@adapt_fit_transform_docs
-class CubicalCover(BaseEstimator, TransformerMixin):
-    """Cover of multi-dimensional data coming from overlapping hypercubes
-    (technically, parallelopipeds) given by taking products of one-dimensional
-    intervals.
-
-    In :meth:`fit`, :class:`OneDimensionalCover` objects are fitted
-    independently on each column of the input array, according to the same
-    parameters passed to the constructor. For example, if the
-    :class:`CubicalCover` object is instantiated with ``kind='uniform'``,
-    ``n_intervals=10`` and ``overlap_frac=0.1``, then each column of the
-    input array is used to construct a cover of the real line by 10
-    equal-length intervals with fractional overlap of 0.1. Each element of the
-    resulting multi-dimensional cover of Euclidean space is of the form
-    :math:`I_{i, \\ldots, k} = I^{(0)}_i \\times \\cdots \\times
-    I^{(d-1)}_k` where :math:`d` is the number of columns in the input
-    array, and :math:`I^{(l)}_j` is the :math:`j`th cover interval
-    constructed for feature dimension :math:`l`. In :meth:`transform`,
-    the cover is applied to a new array `X'` to yield a cover of `X'`.
-
-    Parameters
-    ----------
-    kind : ``'uniform'`` | ``'balanced'``, optional, default: ``'uniform'``
-        The kind of cover to use.
-
-    n_intervals : int, optional, default: ``10``
-        The number of intervals in the covers of each feature dimension
-        calculated in :meth:`fit`.
-
-    overlap_frac : float, optional, default: ``0.1``
-        The fractional overlap between consecutive intervals in the covers of
-        each feature dimension calculated in :meth:`fit`.
-
-    See also
-    --------
-    OneDimensionalCover
-
-    """
-
-    _hyperparameters = {
-        'kind': {'type': str, 'in': ['uniform', 'balanced']},
-        'n_intervals': {'type': int, 'in': Interval(1, np.inf, closed='left')},
-        'overlap_frac': {'type': float, 'in': Interval(0, 1, closed='neither')}
-        }
-
-    def __init__(self, kind='uniform', n_intervals=10, overlap_frac=0.1):
-        self.kind = kind
-        self.n_intervals = n_intervals
-        self.overlap_frac = overlap_frac
-
-    def _clone_and_apply_to_column(self, X, coverer, method_name, i):
-        # method is either a fit-type or a fit_transform-type method
-        try:
-            return getattr(clone(coverer), method_name)(X[:, [i]])
-        except ValueError as ve:
-            if ve.args[0] == f"Only one unique filter value found, cannot " \
-                             f"fit {self.n_intervals} > 1 intervals.":
-                raise ValueError(
-                    f"Only one unique filter value found along feature "
-                    f"dimension {i}, cannot fit {self.n_intervals} > 1 "
-                    f"intervals there.")
-            else:
-                raise ve
-
-    def _fit(self, X):
-        coverer = OneDimensionalCover(kind=self.kind,
-                                      n_intervals=self.n_intervals,
-                                      overlap_frac=self.overlap_frac)
-        is_uniform = self.kind == 'uniform'
-        fitter = '_fit_uniform' if is_uniform else '_fit_balanced'
-        self._coverers = [
-            partial(self._clone_and_apply_to_column, X, coverer, fitter)(i)
-            for i in range(X.shape[1])
-            ]
-        self._n_features_fit = X.shape[1]
-        return self
-
-    def fit(self, X, y=None):
-        """Compute all open cover parallelopipeds according to `X`,
-        as products of one-dimensional intervals covering each feature
-        dimension separately. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features)
-            Input data.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = check_array(X, ensure_2d=False)
-        validate_params(self.get_params(), self._hyperparameters)
-
-        # Reshape filter function values derived from FunctionTransformer
-        if X.ndim == 1:
-            X = X[:, None]
-
-        return self._fit(X)
-
-    def _transform(self, X):
-        # Calculate 1D cover for each column
-        covers = [coverer._transform(X[:, [i]])
-                  for i, coverer in enumerate(self._coverers)]
-
-        Xt = self._combine_one_dim_covers(covers)
-        return Xt
-
-    def transform(self, X, y=None):
-        """Compute a cover of `X` according to the cover of Euclidean space
-        computed in :meth:`fit`, and return it as a two-dimensional boolean
-        array whose each column indicates the location of entries in `X`
-        belonging to a common cover interval.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features)
-            Input data.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_cover_sets)
-            Encoding of the cover of `X` as a boolean array. In general,
-            ``n_cover_sets`` is less than or equal to n_intervals *
-            n_features` as empty or duplicated cover sets are removed.
-
-        """
-        check_is_fitted(self, '_coverers')
-        Xt = check_array(X, ensure_2d=False)
-
-        # Reshape filter function values derived from FunctionTransformer
-        if Xt.ndim == 1:
-            Xt = Xt[:, None]
-
-        n_features_fit = self._n_features_fit
-        n_features = Xt.shape[1]
-        if n_features != n_features_fit:
-            raise DataDimensionalityWarning(
-                f"Different number of columns between `fit` ({n_features_fit})"
-                f" and `transform` ({n_features}).")
-
-        if self.kind == 'balanced':
-            # Test on the first coverer whether the left_limits_ and
-            # right_limits_ attributes are present
-            self._coverers[0]._check_limit_attrs()
-
-        Xt = self._transform(Xt)
-        return Xt
-
-    def fit_transform(self, X, y=None, **fit_params):
-        """Fit to the data, then transform it.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_features)
-            Input data.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_cover_sets)
-            Encoding of the cover of `X` as a boolean array. In general,
-            ``n_cover_sets`` is less than or equal to `n_intervals *
-            n_features` as empty or duplicated cover sets are removed.
-
-        """
-        Xt = check_array(X, ensure_2d=False)
-        validate_params(self.get_params(), self._hyperparameters)
-
-        # Reshape filter function values derived from FunctionTransformer
-        if Xt.ndim == 1:
-            Xt = Xt[:, None]
-
-        if self.kind == 'uniform':
-            Xt = self._fit(Xt)._transform(Xt)
-            return Xt
-
-        # Calculate 1D cover for each column
-        coverer = OneDimensionalCover(kind=self.kind,
-                                      n_intervals=self.n_intervals,
-                                      overlap_frac=self.overlap_frac)
-        coverers = [clone(coverer) for _ in range(Xt.shape[1])]
-        fit_transformer = '_fit_transform_balanced'
-        covers = [
-            partial(self._clone_and_apply_to_column,
-                    Xt, coverer, fit_transformer)(i)
-            for i, coverer in enumerate(coverers)
-            ]
-        # Only store attributes if above succeeds
-        self._coverers = coverers
-        self._n_features_fit = Xt.shape[1]
-        Xt = self._combine_one_dim_covers(covers)
-        return Xt
-
-    @staticmethod
-    def _combine_one_dim_covers(covers):
-        # Stack intervals for each cover
-        intervals = (
-            [cover[:, i] for i in range(cover.shape[1])] for cover in covers
-            )
-
-        # Calculate masks for pullback cover
-        Xt = np.array([np.logical_and.reduce(t)
-                       for t in product(*intervals)]).T
-
-        Xt = _remove_empty_and_duplicate_intervals(Xt)
-        return Xt
+"""Covering schemes for one or several dimensions."""
+# License: GNU AGPLv3
+
+import warnings
+from functools import partial
+from itertools import product
+
+import numpy as np
+from scipy.stats import rankdata
+from sklearn.base import BaseEstimator, TransformerMixin, clone
+from sklearn.exceptions import DataDimensionalityWarning, NotFittedError
+from sklearn.utils import check_array
+from sklearn.utils.validation import check_is_fitted
+
+from .utils._cover import _check_has_one_column, \
+    _remove_empty_and_duplicate_intervals
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.intervals import Interval
+from ..utils.validation import validate_params
+
+
+@adapt_fit_transform_docs
+class OneDimensionalCover(BaseEstimator, TransformerMixin):
+    """Cover of one-dimensional data coming from open overlapping intervals.
+
+    In :meth:`fit`, given a training array `X` representing a collection of
+    real numbers, a cover of the real line by open intervals
+    :math:`I_k = (a_k, b_k)` (:math:`k = 1, \\ldots, n`,
+    :math:`a_k < a_{k+1}`, :math:`b_k < b_{k+1}`) is constructed
+    based on the distribution of values in `X`. In :meth:`transform`,
+    the cover is applied to a new array `X'` to yield a cover of `X'`.
+
+    All covers constructed in :meth:`fit` have :math:`a_1 = -\\infty`
+    and :math:`b_n = + \\infty``. Two kinds of cover are currently available:
+    "uniform" and "balanced". A uniform cover is such that
+    :math:`b_1 - m = b_2 - a_2 = \\cdots = M - a_n` where :math:`m` and
+    :math:`M` are the minimum and maximum values in `X` respectively. A
+    balanced cover is such that approximately the same number of unique
+    values from `X` is contained in each cover interval.
+
+    Parameters
+    ----------
+    kind : ``'uniform'`` | ``'balanced'``, optional, default: ``'uniform'``
+        The kind of cover to use.
+
+    n_intervals : int, optional, default: ``10``
+        The number of intervals in the cover calculated in :meth:`fit`.
+
+    overlap_frac : float, optional, default: ``0.1``
+        If the cover is uniform, this is the ratio between the length of the
+        intersection between consecutive intervals and the length of each
+        interval. If the cover is balanced, this is the analogous fractional
+        overlap for a uniform cover of the closed interval
+        :math:`(0.5, N + 0.5)` where :math:`N` is the number of unique
+        values in the training array (see the Notes).
+
+    Attributes
+    ----------
+    left_limits_ : ndarray of shape (n_intervals,)
+        Left limits of the cover intervals computed in :meth:`fit`. See the
+        Notes.
+
+    right_limits_ : ndarray of shape (n_intervals,)
+        Right limits of the cover intervals computed in :meth:`fit`. See the
+        Notes.
+
+    Notes
+    -----
+    In the case of a balanced cover, :meth:`left_limits_` and
+    :meth:`right_limits_` are computed as follows given a training array `X`:
+    first, entries in `X` are ranked in ascending order, starting at 1 and
+    with the same rank repeated in the case of equal values; then, the closed
+    interval :math:`(0.5, N + 0.5)`, where :math:`N` is the maximum
+    rank observed, is covered uniformly with parameters `n_intervals` and
+    `overlap_frac`, yielding intervals :math:`(\\alpha_k, \\beta_k)`;
+    the final cover is made of intervals :math:`(a_k, b_k)` where, for
+    :math:`k > 1` (resp. :math:`k < ` `n_intervals`), :math:`a_k` (resp.
+    :math:`b_k`) is the value of any entry in `X` ranked as the floor (
+    resp. ceiling) of :math:`\\alpha_k` (resp. :math:`\\beta_k`).
+
+    See also
+    --------
+    CubicalCover
+
+    """
+
+    _hyperparameters = {
+        'kind': {'type': str, 'in': ['uniform', 'balanced']},
+        'n_intervals': {'type': int, 'in': Interval(1, np.inf, closed='left')},
+        'overlap_frac': {'type': float, 'in': Interval(0, 1, closed='neither')}
+        }
+
+    def __init__(self, kind='uniform', n_intervals=10, overlap_frac=0.1):
+        self.kind = kind
+        self.n_intervals = n_intervals
+        self.overlap_frac = overlap_frac
+
+    def _fit_uniform(self, X):
+        self.left_limits_, self.right_limits_ = self._find_interval_limits(
+            X, self.n_intervals, self.overlap_frac, is_uniform=True)
+        return self
+
+    def _fit_balanced(self, X):
+        X_rank = rankdata(X, method='dense') - 1
+        left_limits, right_limits = self._find_interval_limits(
+            X_rank, self.n_intervals, self.overlap_frac, is_uniform=False)
+        left_limits_int = left_limits.astype(int)
+        left_ranks = np.where(left_limits >= 0, left_limits_int, -1)
+        right_limits_int = right_limits.astype(int)
+        right_ranks = np.where(right_limits_int == right_limits,
+                               right_limits_int,
+                               right_limits_int + 1)
+        self.left_limits_, self.right_limits_ = self._limits_from_ranks(
+            X_rank, X.flatten(), left_ranks, right_ranks)
+        return self
+
+    def fit(self, X, y=None):
+        """Compute all cover interval limits according to `X` and store them
+        in :attr:`left_limits_` and :attr:`right_limits_`. Then, return the
+        estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples,) or (n_samples, 1)
+            Input data.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_array(X, ensure_2d=False)
+        validate_params(self.get_params(), self._hyperparameters)
+        if self.overlap_frac <= 1e-8:
+            warnings.warn("`overlap_frac` is close to zero, "
+                          "which might cause numerical issues and errors.",
+                          RuntimeWarning)
+
+        if X.ndim == 2:
+            _check_has_one_column(X)
+
+        is_uniform = self.kind == 'uniform'
+        fitter = self._fit_uniform if is_uniform else self._fit_balanced
+        return fitter(X)
+
+    def _transform(self, X):
+        return np.logical_and(X > self.left_limits_, X < self.right_limits_)
+
+    def transform(self, X, y=None):
+        """Compute a cover of `X` according to the cover of the real line
+        computed in :meth:`fit`, and return it as a two-dimensional boolean
+        array. Each column indicates the location of entries in `X`
+        belonging to a common cover interval.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples,) or (n_samples, 1)
+            Input data.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_cover_sets)
+            Encoding of the cover of `X` as a boolean array. In general,
+            ``n_cover_sets`` is less than or equal to `n_intervals` as empty
+            or duplicated cover sets are removed.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, ensure_2d=False)
+
+        if Xt.ndim == 2:
+            _check_has_one_column(Xt)
+        else:
+            Xt = Xt[:, None]
+
+        if self.kind == 'balanced':
+            # Test whether self.left_limits_ and self.right_limits_ have
+            # been created -- to catch cases in which transform is run after
+            # fit_transform but not after fit.
+            self._check_limit_attrs()
+
+        Xt = self._transform(Xt)
+        Xt = _remove_empty_and_duplicate_intervals(Xt)
+        return Xt
+
+    def _fit_transform_balanced(self, X):
+        """Shortcut in the case of a balanced cover, avoiding overhead
+        from calculation of self.left_limits_ and self.right_limits_.
+
+        Stores hidden attributes _left_limits and _right_limits which refer
+        to a cover of the interval (-0.5, n_unique - 0.5) where n_unique is
+        the number of unique points in X.
+
+        """
+        X_rank = rankdata(X, method='dense') - 1
+        self._left_limits, self._right_limits = self._find_interval_limits(
+            X_rank, self.n_intervals, self.overlap_frac, is_uniform=False)
+        X_rank = np.broadcast_to(X_rank[:, None],
+                                 (X.shape[0], self.n_intervals))
+        Xt = np.logical_and(X_rank > self._left_limits,
+                            X_rank < self._right_limits)
+        return Xt
+
+    def _fit_transform(self, X):
+        if self.kind == 'uniform':
+            Xt = self._fit_uniform(X)._transform(X)
+        else:
+            Xt = self._fit_transform_balanced(X)
+        return Xt
+
+    def fit_transform(self, X, y=None, **fit_params):
+        """Fit to the data, then transform it.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples,) or (n_samples, 1)
+            Input data.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_cover_sets)
+            Encoding of the cover of `X` as a boolean array. In general,
+            ``n_cover_sets`` is less than or equal to `n_intervals` as empty
+            or duplicated cover sets are removed.
+
+        """
+        Xt = check_array(X, ensure_2d=False)
+        validate_params(self.get_params(), self._hyperparameters)
+
+        if Xt.ndim == 2:
+            _check_has_one_column(Xt)
+        else:
+            Xt = Xt[:, None]
+
+        Xt = self._fit_transform(Xt)
+        Xt = _remove_empty_and_duplicate_intervals(Xt)
+        return Xt
+
+    def get_fitted_intervals(self):
+        """Returns the open intervals computed in :meth:`fit`, as a list of
+        tuples (a, b) where a < b.
+
+        """
+        check_is_fitted(self)
+        if self.kind == 'balanced':
+            # Test whether self.left_limits_ and self.right_limits_ have
+            # been created
+            self._check_limit_attrs()
+        return list(zip(self.left_limits_, self.right_limits_))
+
+    def _check_limit_attrs(self):
+        limit_attrs = ['left_limits_', 'right_limits_']
+        has_limits = all([hasattr(self, attr) for attr in limit_attrs])
+        if not has_limits:
+            raise NotFittedError(
+                "When the cover is balanced and n_intervals > 1, the left "
+                "and right limits of the cover intervals are not "
+                "explicitly calculated during 'fit_transform'. Please "
+                "call 'fit' explicitly on the same data before using this "
+                "method.")
+
+    def _find_interval_limits(self, X, n_intervals, overlap_frac,
+                              is_uniform=True):
+        if is_uniform:
+            min_val, max_val = np.min(X), np.max(X)
+            only_one_pt = (min_val == max_val)
+        else:
+            # Assume X is the result of a call to scipy.stats.rankdata
+            min_val, max_val = -0.5, np.max(X) + 0.5
+            only_one_pt = (min_val == max_val - 1)
+
+        # Allow X to have one unique sample only if one interval is required,
+        # in which case the fitted interval will be (-np.inf, np.inf).
+        if only_one_pt and n_intervals > 1:
+            raise ValueError(
+                f"Only one unique filter value found, cannot fit "
+                f"{n_intervals} > 1 intervals.")
+
+        left_limits, right_limits = \
+            self._cover_limits(min_val, max_val, n_intervals, overlap_frac)
+        if is_uniform:
+            left_limits[0], right_limits[-1] = -np.inf, np.inf
+        return left_limits, right_limits
+
+    def _limits_from_ranks(self, X_rank, X, left_ranks, right_ranks):
+        n_intervals = self.n_intervals
+        X_rank = np.broadcast_to(X_rank[:, None],
+                                 (X_rank.shape[0], n_intervals))
+        left_mask = (X_rank == left_ranks)
+        right_mask = (X_rank == right_ranks)
+        left_indices = (np.flatnonzero(left_mask[:, i])
+                        for i in range(n_intervals))
+        right_indices = (np.flatnonzero(right_mask[:, i])
+                         for i in range(n_intervals))
+        left_limits = np.array([
+            X[nonzero_indices[0]] if nonzero_indices.size else -np.inf
+            for nonzero_indices in left_indices
+            ])
+        right_limits = np.array([
+            X[nonzero_indices[0]] if nonzero_indices.size else np.inf
+            for nonzero_indices in right_indices
+            ])
+        left_limits[0] = -np.inf
+        right_limits[-1] = np.inf
+        return left_limits, right_limits
+
+    @staticmethod
+    def _cover_limits(min_val, max_val, n_intervals, overlap_frac):
+        # Construct a uniform cover of the interval [min_val, max_val].
+        # Let the length of each interval be l. The equation to solve for l is
+        # (n_intervals - 1) * l * (1 - overlap_frac) + l = max_val - min_val.
+        # The maximum left endpoint is at min_val + (n_intervals - 1) * (1 -
+        # overlap_frac) * l
+        total_len = max_val - min_val
+        interval_len = total_len / \
+            (n_intervals - (n_intervals - 1) * overlap_frac)
+
+        last = min_val + (n_intervals - 1) * (1 - overlap_frac) * interval_len
+        left_limits = np.linspace(min_val, last, num=n_intervals,
+                                  endpoint=True)
+        right_limits = left_limits + interval_len
+        return left_limits, right_limits
+
+
+@adapt_fit_transform_docs
+class CubicalCover(BaseEstimator, TransformerMixin):
+    """Cover of multi-dimensional data coming from overlapping hypercubes
+    (technically, parallelopipeds) given by taking products of one-dimensional
+    intervals.
+
+    In :meth:`fit`, :class:`OneDimensionalCover` objects are fitted
+    independently on each column of the input array, according to the same
+    parameters passed to the constructor. For example, if the
+    :class:`CubicalCover` object is instantiated with ``kind='uniform'``,
+    ``n_intervals=10`` and ``overlap_frac=0.1``, then each column of the
+    input array is used to construct a cover of the real line by 10
+    equal-length intervals with fractional overlap of 0.1. Each element of the
+    resulting multi-dimensional cover of Euclidean space is of the form
+    :math:`I_{i, \\ldots, k} = I^{(0)}_i \\times \\cdots \\times
+    I^{(d-1)}_k` where :math:`d` is the number of columns in the input
+    array, and :math:`I^{(l)}_j` is the :math:`j`th cover interval
+    constructed for feature dimension :math:`l`. In :meth:`transform`,
+    the cover is applied to a new array `X'` to yield a cover of `X'`.
+
+    Parameters
+    ----------
+    kind : ``'uniform'`` | ``'balanced'``, optional, default: ``'uniform'``
+        The kind of cover to use.
+
+    n_intervals : int, optional, default: ``10``
+        The number of intervals in the covers of each feature dimension
+        calculated in :meth:`fit`.
+
+    overlap_frac : float, optional, default: ``0.1``
+        The fractional overlap between consecutive intervals in the covers of
+        each feature dimension calculated in :meth:`fit`.
+
+    See also
+    --------
+    OneDimensionalCover
+
+    """
+
+    _hyperparameters = {
+        'kind': {'type': str, 'in': ['uniform', 'balanced']},
+        'n_intervals': {'type': int, 'in': Interval(1, np.inf, closed='left')},
+        'overlap_frac': {'type': float, 'in': Interval(0, 1, closed='neither')}
+        }
+
+    def __init__(self, kind='uniform', n_intervals=10, overlap_frac=0.1):
+        self.kind = kind
+        self.n_intervals = n_intervals
+        self.overlap_frac = overlap_frac
+
+    def _clone_and_apply_to_column(self, X, coverer, method_name, i):
+        # method is either a fit-type or a fit_transform-type method
+        try:
+            return getattr(clone(coverer), method_name)(X[:, [i]])
+        except ValueError as ve:
+            if ve.args[0] == f"Only one unique filter value found, cannot " \
+                             f"fit {self.n_intervals} > 1 intervals.":
+                raise ValueError(
+                    f"Only one unique filter value found along feature "
+                    f"dimension {i}, cannot fit {self.n_intervals} > 1 "
+                    f"intervals there.")
+            else:
+                raise ve
+
+    def _fit(self, X):
+        coverer = OneDimensionalCover(kind=self.kind,
+                                      n_intervals=self.n_intervals,
+                                      overlap_frac=self.overlap_frac)
+        is_uniform = self.kind == 'uniform'
+        fitter = '_fit_uniform' if is_uniform else '_fit_balanced'
+        self._coverers = [
+            partial(self._clone_and_apply_to_column, X, coverer, fitter)(i)
+            for i in range(X.shape[1])
+            ]
+        self._n_features_fit = X.shape[1]
+        return self
+
+    def fit(self, X, y=None):
+        """Compute all open cover parallelopipeds according to `X`,
+        as products of one-dimensional intervals covering each feature
+        dimension separately. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features)
+            Input data.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = check_array(X, ensure_2d=False)
+        validate_params(self.get_params(), self._hyperparameters)
+
+        # Reshape filter function values derived from FunctionTransformer
+        if X.ndim == 1:
+            X = X[:, None]
+
+        return self._fit(X)
+
+    def _transform(self, X):
+        # Calculate 1D cover for each column
+        covers = [coverer._transform(X[:, [i]])
+                  for i, coverer in enumerate(self._coverers)]
+
+        Xt = self._combine_one_dim_covers(covers)
+        return Xt
+
+    def transform(self, X, y=None):
+        """Compute a cover of `X` according to the cover of Euclidean space
+        computed in :meth:`fit`, and return it as a two-dimensional boolean
+        array whose each column indicates the location of entries in `X`
+        belonging to a common cover interval.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features)
+            Input data.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_cover_sets)
+            Encoding of the cover of `X` as a boolean array. In general,
+            ``n_cover_sets`` is less than or equal to n_intervals *
+            n_features` as empty or duplicated cover sets are removed.
+
+        """
+        check_is_fitted(self, '_coverers')
+        Xt = check_array(X, ensure_2d=False)
+
+        # Reshape filter function values derived from FunctionTransformer
+        if Xt.ndim == 1:
+            Xt = Xt[:, None]
+
+        n_features_fit = self._n_features_fit
+        n_features = Xt.shape[1]
+        if n_features != n_features_fit:
+            raise DataDimensionalityWarning(
+                f"Different number of columns between `fit` ({n_features_fit})"
+                f" and `transform` ({n_features}).")
+
+        if self.kind == 'balanced':
+            # Test on the first coverer whether the left_limits_ and
+            # right_limits_ attributes are present
+            self._coverers[0]._check_limit_attrs()
+
+        Xt = self._transform(Xt)
+        return Xt
+
+    def fit_transform(self, X, y=None, **fit_params):
+        """Fit to the data, then transform it.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_features)
+            Input data.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_cover_sets)
+            Encoding of the cover of `X` as a boolean array. In general,
+            ``n_cover_sets`` is less than or equal to `n_intervals *
+            n_features` as empty or duplicated cover sets are removed.
+
+        """
+        Xt = check_array(X, ensure_2d=False)
+        validate_params(self.get_params(), self._hyperparameters)
+
+        # Reshape filter function values derived from FunctionTransformer
+        if Xt.ndim == 1:
+            Xt = Xt[:, None]
+
+        if self.kind == 'uniform':
+            Xt = self._fit(Xt)._transform(Xt)
+            return Xt
+
+        # Calculate 1D cover for each column
+        coverer = OneDimensionalCover(kind=self.kind,
+                                      n_intervals=self.n_intervals,
+                                      overlap_frac=self.overlap_frac)
+        coverers = [clone(coverer) for _ in range(Xt.shape[1])]
+        fit_transformer = '_fit_transform_balanced'
+        covers = [
+            partial(self._clone_and_apply_to_column,
+                    Xt, coverer, fit_transformer)(i)
+            for i, coverer in enumerate(coverers)
+            ]
+        # Only store attributes if above succeeds
+        self._coverers = coverers
+        self._n_features_fit = Xt.shape[1]
+        Xt = self._combine_one_dim_covers(covers)
+        return Xt
+
+    @staticmethod
+    def _combine_one_dim_covers(covers):
+        # Stack intervals for each cover
+        intervals = (
+            [cover[:, i] for i in range(cover.shape[1])] for cover in covers
+            )
+
+        # Calculate masks for pullback cover
+        Xt = np.array([np.logical_and.reduce(t)
+                       for t in product(*intervals)]).T
+
+        Xt = _remove_empty_and_duplicate_intervals(Xt)
+        return Xt
```

## gtda/mapper/filter.py

 * *Ordering differences only*

```diff
@@ -1,248 +1,248 @@
-"""Filter functions commonly used with Mapper."""
-# License: GNU AGPLv3
-
-import warnings
-
-import numpy as np
-from scipy.spatial.distance import pdist, squareform
-from scipy.stats import entropy
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils.validation import check_array, check_is_fitted
-
-from ..utils._docs import adapt_fit_transform_docs
-
-
-@adapt_fit_transform_docs
-class Eccentricity(BaseEstimator, TransformerMixin):
-    """Eccentricities of points in a point cloud or abstract metric space.
-
-    Let `D` be a square matrix representing distances between points in a
-    point cloud, or directly defining an abstract metric (or metric-like)
-    space. The eccentricity of point `i` in the point cloud or abstract
-    metric space is the `p`-norm (for some `p`) of row `i` in `D`.
-
-    Parameters
-    ----------
-    exponent : int or float, optional, default: ``2``
-        `p`-norm exponent used to calculate eccentricities from the distance
-        matrix.
-
-    metric : str or function, optional, default: ``'euclidean'``
-        Metric to use to compute the distance matrix if point cloud data is
-        passed as input, or ``'precomputed'`` to specify that the input is
-        already a distance matrix. If not ``'precomputed'``, it may be
-        anything allowed by :func:`scipy.spatial.distance.pdist`.
-
-    metric_params : dict, optional, default: ``{}``
-        Additional keyword arguments for the metric function.
-
-    """
-
-    def __init__(self, exponent=2, metric='euclidean', metric_params={}):
-        self.exponent = exponent
-        self.metric = metric
-        self.metric_params = metric_params
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method exists to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : array-like of shape (n_samples, n_features) or (n_samples, \
-            n_samples)
-            Input data.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        # TODO: Consider making this transformer stateful so that the
-        #  eccentricities of new points relative to the data seen in fit
-        #  may be computed. May be useful for supervised tasks with Mapper?
-        #  Evaluate performance impact of doing this.
-        check_array(X)
-
-        self._is_fitted = True
-        return self
-
-    def transform(self, X, y=None):
-        """Compute the eccentricities of points (i.e. rows) in  `X`.
-
-        Parameters
-        ----------
-        X : array-like of shape (n_samples, n_features) or (n_samples, \
-            n_samples)
-            Input data.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, 1)
-            Column vector of eccentricities of points in `X`.
-
-        """
-        check_is_fitted(self, '_is_fitted')
-        Xt = check_array(X)
-
-        if self.metric != 'precomputed':
-            Xt = squareform(
-                pdist(Xt, metric=self.metric, **self.metric_params)
-                )
-
-        Xt = np.linalg.norm(Xt, axis=1, ord=self.exponent, keepdims=True)
-        return Xt
-
-
-@adapt_fit_transform_docs
-class Entropy(BaseEstimator, TransformerMixin):
-    """Entropy of rows in a two-dimensional array.
-
-    The rows of the array are interpreted as probability vectors, after taking
-    absolute values if necessary and normalizing. Then, their (base 2) Shannon
-    entropies are computed and returned.
-
-    """
-
-    def __init__(self):
-        pass
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method exists to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : array-like of shape (n_samples, n_features)
-            Input data.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_array(X)
-
-        self._is_fitted = True
-        return self
-
-    def transform(self, X, y=None):
-        """For each row in the array, take absolute values of any negative
-        entry, normalise, and compute the Shannon entropy.
-
-        Parameters
-        ----------
-        X : array-like of shape (n_samples, n_features)
-            Input data.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, 1)
-            Array of Shannon entropies.
-
-        """
-        # TODO: The following is a crude method to ensure each row vector
-        #  consists of "probabilities" that sum to one. Consider normalisation
-        #  in terms of bin counts?
-        check_is_fitted(self, '_is_fitted')
-        Xt = check_array(X)
-
-        if np.any(Xt < 0):
-            warnings.warn("Negative values detected in X! Taking absolute "
-                          "value to calculate probabilities.")
-            Xt = np.abs(Xt)
-
-        Xt = entropy(Xt, base=2, axis=1)[:, None]
-        return Xt
-
-
-@adapt_fit_transform_docs
-class Projection(BaseEstimator, TransformerMixin):
-    """Projection onto specified columns.
-
-    In practice, this simply means returning a selection of columns of the
-    data.
-
-    Parameters
-    ----------
-    columns : int or list of int, optional, default: ``0``
-        The column indices of the array to project onto.
-
-    """
-
-    def __init__(self, columns=0):
-        self.columns = columns
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method exists to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : array-like of shape (n_samples, n_features)
-            Input data.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_array(X)
-
-        self._is_fitted = True
-        return self
-
-    def transform(self, X, y=None):
-        """Return selected columns of the data.
-
-        Parameters
-        ----------
-        X : array-like of shape (n_samples, n_features)
-            Input data.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_columns)
-            Output array, where ``n_columns = len(columns)``.
-
-        """
-        check_is_fitted(self, '_is_fitted')
-        # Simple duck typing to handle case of pandas dataframe input
-        if hasattr(X, 'columns'):
-            # NB in this case we do not check the health of other columns
-            Xt = check_array(X[self.columns], ensure_2d=False, copy=True)
-        else:
-            Xt = check_array(X, copy=True)
-            Xt = Xt[:, self.columns]
-        Xt = Xt.reshape(len(Xt), -1)
-        return Xt
+"""Filter functions commonly used with Mapper."""
+# License: GNU AGPLv3
+
+import warnings
+
+import numpy as np
+from scipy.spatial.distance import pdist, squareform
+from scipy.stats import entropy
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils.validation import check_array, check_is_fitted
+
+from ..utils._docs import adapt_fit_transform_docs
+
+
+@adapt_fit_transform_docs
+class Eccentricity(BaseEstimator, TransformerMixin):
+    """Eccentricities of points in a point cloud or abstract metric space.
+
+    Let `D` be a square matrix representing distances between points in a
+    point cloud, or directly defining an abstract metric (or metric-like)
+    space. The eccentricity of point `i` in the point cloud or abstract
+    metric space is the `p`-norm (for some `p`) of row `i` in `D`.
+
+    Parameters
+    ----------
+    exponent : int or float, optional, default: ``2``
+        `p`-norm exponent used to calculate eccentricities from the distance
+        matrix.
+
+    metric : str or function, optional, default: ``'euclidean'``
+        Metric to use to compute the distance matrix if point cloud data is
+        passed as input, or ``'precomputed'`` to specify that the input is
+        already a distance matrix. If not ``'precomputed'``, it may be
+        anything allowed by :func:`scipy.spatial.distance.pdist`.
+
+    metric_params : dict, optional, default: ``{}``
+        Additional keyword arguments for the metric function.
+
+    """
+
+    def __init__(self, exponent=2, metric='euclidean', metric_params={}):
+        self.exponent = exponent
+        self.metric = metric
+        self.metric_params = metric_params
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method exists to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features) or (n_samples, \
+            n_samples)
+            Input data.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        # TODO: Consider making this transformer stateful so that the
+        #  eccentricities of new points relative to the data seen in fit
+        #  may be computed. May be useful for supervised tasks with Mapper?
+        #  Evaluate performance impact of doing this.
+        check_array(X)
+
+        self._is_fitted = True
+        return self
+
+    def transform(self, X, y=None):
+        """Compute the eccentricities of points (i.e. rows) in  `X`.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features) or (n_samples, \
+            n_samples)
+            Input data.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, 1)
+            Column vector of eccentricities of points in `X`.
+
+        """
+        check_is_fitted(self, '_is_fitted')
+        Xt = check_array(X)
+
+        if self.metric != 'precomputed':
+            Xt = squareform(
+                pdist(Xt, metric=self.metric, **self.metric_params)
+                )
+
+        Xt = np.linalg.norm(Xt, axis=1, ord=self.exponent, keepdims=True)
+        return Xt
+
+
+@adapt_fit_transform_docs
+class Entropy(BaseEstimator, TransformerMixin):
+    """Entropy of rows in a two-dimensional array.
+
+    The rows of the array are interpreted as probability vectors, after taking
+    absolute values if necessary and normalizing. Then, their (base 2) Shannon
+    entropies are computed and returned.
+
+    """
+
+    def __init__(self):
+        pass
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method exists to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            Input data.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_array(X)
+
+        self._is_fitted = True
+        return self
+
+    def transform(self, X, y=None):
+        """For each row in the array, take absolute values of any negative
+        entry, normalise, and compute the Shannon entropy.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            Input data.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, 1)
+            Array of Shannon entropies.
+
+        """
+        # TODO: The following is a crude method to ensure each row vector
+        #  consists of "probabilities" that sum to one. Consider normalisation
+        #  in terms of bin counts?
+        check_is_fitted(self, '_is_fitted')
+        Xt = check_array(X)
+
+        if np.any(Xt < 0):
+            warnings.warn("Negative values detected in X! Taking absolute "
+                          "value to calculate probabilities.")
+            Xt = np.abs(Xt)
+
+        Xt = entropy(Xt, base=2, axis=1)[:, None]
+        return Xt
+
+
+@adapt_fit_transform_docs
+class Projection(BaseEstimator, TransformerMixin):
+    """Projection onto specified columns.
+
+    In practice, this simply means returning a selection of columns of the
+    data.
+
+    Parameters
+    ----------
+    columns : int or list of int, optional, default: ``0``
+        The column indices of the array to project onto.
+
+    """
+
+    def __init__(self, columns=0):
+        self.columns = columns
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method exists to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            Input data.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_array(X)
+
+        self._is_fitted = True
+        return self
+
+    def transform(self, X, y=None):
+        """Return selected columns of the data.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            Input data.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_columns)
+            Output array, where ``n_columns = len(columns)``.
+
+        """
+        check_is_fitted(self, '_is_fitted')
+        # Simple duck typing to handle case of pandas dataframe input
+        if hasattr(X, 'columns'):
+            # NB in this case we do not check the health of other columns
+            Xt = check_array(X[self.columns], ensure_2d=False, copy=True)
+        else:
+            Xt = check_array(X, copy=True)
+            Xt = Xt[:, self.columns]
+        Xt = Xt.reshape(len(Xt), -1)
+        return Xt
```

## gtda/mapper/nerve.py

 * *Ordering differences only*

```diff
@@ -1,244 +1,244 @@
-"""Construct the nerve of a refined Mapper cover."""
-# License: GNU AGPLv3
-
-from collections import defaultdict
-from itertools import combinations, filterfalse
-
-import numpy as np
-from igraph import Graph
-from sklearn.base import BaseEstimator, TransformerMixin
-
-
-def _limit_mapping(mapping):
-    """Given a 1D array interpreted as a function
-    :math:`f : \\{0, \\ldots, n - 1\\}} \to \\{0, \\ldots, n - 1\\}}`, such
-    that :math:`f^{(k)} = f^{(k + 1)}` for some :math:`k`, find the 1D array
-    corresponding to :math:`f^{(k)}`."""
-    terminal_states = np.empty_like(mapping)
-    for i, initial_target_idx in enumerate(mapping):
-        temp_target_idx = i
-        next_target_idx = initial_target_idx
-        while temp_target_idx != next_target_idx:
-            temp_target_idx = mapping[temp_target_idx]
-            next_target_idx = mapping[mapping[temp_target_idx]]
-        terminal_states[i] = temp_target_idx
-
-    return terminal_states
-
-
-class Nerve(BaseEstimator, TransformerMixin):
-    """1-skeleton of the nerve of a refined Mapper cover, i.e. the Mapper
-    graph.
-
-    This transformer is the final step in the
-    :class:`gtda.mapper.pipeline.MapperPipeline` objects created
-    by :func:`gtda.mapper.make_mapper_pipeline`. It corresponds the last two
-    arrows in `this diagram <../../../../_images/mapper_pipeline.svg>`_.
-
-    Parameters
-    ----------
-    min_intersection : int, optional, default: ``1``
-        Minimum size of the intersection, between data subsets associated to
-        any two Mapper nodes, required to create an edge between the nodes in
-        the Mapper graph. Must be positive.
-
-    store_edge_elements : bool, optional, default: ``False``
-        Whether the indices of data elements associated to Mapper edges (i.e.
-        in the intersections allowed by `min_intersection`) should be stored in
-        the :class:`igraph.Graph` object output by :meth:`fit_transform`. When
-        ``True``, might lead to a large :class:`igraph.Graph` object.
-
-    contract_nodes : bool, optional, default: ``False``
-        If ``True``, any node representing a cluster which is a strict subset
-        of the cluster corresponding to another node is eliminated, and only
-        one maximal node is kept.
-
-    Attributes
-    ----------
-    graph_ : :class:`igraph.Graph` object
-        Mapper graph obtained from the input data. Created when :meth:`fit` is
-        called.
-
-    """
-
-    def __init__(self, min_intersection=1, store_edge_elements=False,
-                 contract_nodes=False):
-        self.min_intersection = min_intersection
-        self.store_edge_elements = store_edge_elements
-        self.contract_nodes = contract_nodes
-
-    def fit(self, X, y=None):
-        """Compute the Mapper graph as in :meth:`fit_transform`, but store the
-        graph as :attr:`graph_` and return the estimator.
-
-        Parameters
-        ----------
-        X : list of list of tuple
-            See :meth:`fit_transform`.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        self.graph_ = self.fit_transform(X, y=y)
-        return self
-
-    def fit_transform(self, X, y=None):
-        """Construct a Mapper graph from a refined Mapper cover.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples,)
-            Cluster labels describing a refined cover of a dataset produced by
-            the clustering step of a :class:`gtda.mapper.MapperPipeline`,
-            as depicted in
-            `this diagram <../../../../_images/mapper_pipeline.svg>`_. Each
-            entry in `X` is a tuple of pairs of the form
-            ``(pullback_set_label, partial_cluster_label)`` where
-            ``partial_cluster_label`` is a cluster label within the pullback
-            cover set identified by ``pullback_set_label``. The unique pairs
-            correspond to nodes in the output graph.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        graph : :class:`igraph.Graph` object
-            Undirected Mapper graph according to `X` and `min_intersection`.
-            Each node is an :class:`igraph.Vertex` object with attributes
-            ``"pullback_set_label"``, ``"partial_cluster_label"`` and
-            ``"node_elements"``. Each edge is an :class:`igraph.Edge` object
-            with a ``"weight"`` attribute which is equal to the size of the
-            intersection between the data subsets associated to its two nodes.
-            If `store_edge_elements` is ``True`` each edge also has an
-            additional attribute ``"edge_elements"``.
-
-        """
-        # TODO: Include a validation step for X
-        # Graph construction -- vertices with their metadata
-        labels_to_indices = defaultdict(list)
-        for i, sample in enumerate(X):
-            for node_id_pair in sample:
-                labels_to_indices[node_id_pair].append(i)
-        labels_to_indices = {key: np.array(value)
-                             for key, value in labels_to_indices.items()}
-        n_nodes = len(labels_to_indices)
-        graph = Graph(n_nodes)
-
-        # labels_to_indices is a dictionary of, say, N key-value pairs of the
-        # form (pullback_set_label, partial_cluster_label): node_elements.
-        # Hence, zip(*labels_to_indices) generates two tuples of length N, each
-        # corresponding to a type of node attribute in the final graph.
-        node_attributes = zip(*labels_to_indices)
-        graph.vs["pullback_set_label"] = next(node_attributes)
-        graph.vs["partial_cluster_label"] = next(node_attributes)
-        graph.vs["node_elements"] = [*labels_to_indices.values()]
-
-        # Graph construction -- edges with weights given by intersection sizes.
-        # In general, we need all information in `nodes` to narrow down the set
-        # of combinations to check, especially when `contract_nodes` is True.
-        nodes = zip(*zip(*labels_to_indices), labels_to_indices.values())
-        node_index_pairs, weights, intersections, mapping = \
-            self._generate_edge_data(nodes, n_nodes)
-        graph.es["weight"] = 1
-        graph.add_edges(node_index_pairs)
-        graph.es["weight"] = weights
-        if self.store_edge_elements:
-            graph.es["edge_elements"] = intersections
-        if self.contract_nodes:
-            # Due to the order in which itertools.combinations produces pairs,
-            # and to the preference given to node 1 in the if-elif-else clause
-            # in `_subset_check_metadata_append`, `mapping` is guaranteed to
-            # send everything to one of its fixed points after sufficiently
-            # many repeated applications and, by construction, no two pairs of
-            # indices in `_limit_mapping(mapping)` can correspond to data
-            # subsets which are in a subset relation. Thus the nodes are
-            # correctly contracted by `_limit_mapping(mapping)`.
-            limit_mapping = _limit_mapping(mapping)
-            graph.contract_vertices(limit_mapping,
-                                    combine_attrs="first")
-            graph.delete_vertices([i for i in graph.vs.indices
-                                   if i != limit_mapping[i]])
-
-        return graph
-
-    def _generate_edge_data(self, nodes, n_nodes):
-        def _in_same_pullback_set(_node_tuple):
-            return _node_tuple[0][1][0] == _node_tuple[1][1][0]
-
-        def _do_nothing(*args):
-            pass
-
-        def _intersections_append(_intersection):
-            return intersections.append(_intersection)
-
-        def _metadata_append(
-                _node_1_idx, _node_2_idx, _intersection_size, _intersection,
-                *args
-                ):
-            if _intersection_size >= self.min_intersection:
-                # Add edge (as a node tuple) to list of node index pairs
-                node_index_pairs.append((_node_1_idx, _node_2_idx))
-                weights.append(_intersection_size)
-                intersection_behavior(_intersection)
-
-        def _subset_check_metadata_append(
-                _node_1_idx, _node_2_idx, _intersection_size, _intersection,
-                _node_1_elements, _node_2_elements
-                ):
-            if _intersection_size == len(_node_2_elements):
-                # Node 2 is contained in node 1 and we remove it in favour of
-                # node 1.
-                mapping[_node_2_idx] = _node_1_idx
-            elif _intersection_size == len(_node_1_elements):
-                # Node 1 is strictly contained in node 2 and we remove it in
-                # favour of node 2.
-                mapping[_node_1_idx] = _node_2_idx
-            else:
-                # Edge exists provided `_intersection_size` is large enough
-                _metadata_append(_node_1_idx, _node_2_idx, _intersection_size,
-                                 _intersection)
-
-        node_tuples = combinations(enumerate(nodes), 2)
-
-        node_index_pairs = []
-        weights = []
-        intersections = []
-
-        # Choose whether intersections are stored or not.
-        # `intersection_behavior` is in scope for `_metadata_append` and
-        # `_subset_check_metadata_append`.
-        if self.store_edge_elements:
-            intersection_behavior = _intersections_append
-        else:
-            intersection_behavior = _do_nothing
-
-        if self.contract_nodes:
-            mapping = np.arange(n_nodes)
-            behavior = _subset_check_metadata_append
-        else:
-            mapping = None
-            behavior = _metadata_append
-
-        # No need to check for intersections within each pullback set as the
-        # input is assumed to be a refined Mapper cover
-        for node_tuple in filterfalse(_in_same_pullback_set, node_tuples):
-            ((node_1_idx, (_, _, node_1_elements)),
-             (node_2_idx, (_, _, node_2_elements))) = node_tuple
-            intersection = np.intersect1d(node_1_elements, node_2_elements)
-            intersection_size = len(intersection)
-
-            if intersection_size:
-                behavior(node_1_idx, node_2_idx, intersection_size,
-                         intersection, node_1_elements, node_2_elements)
-            else:
-                continue
-
-        return node_index_pairs, weights, intersections, mapping
+"""Construct the nerve of a refined Mapper cover."""
+# License: GNU AGPLv3
+
+from collections import defaultdict
+from itertools import combinations, filterfalse
+
+import numpy as np
+from igraph import Graph
+from sklearn.base import BaseEstimator, TransformerMixin
+
+
+def _limit_mapping(mapping):
+    """Given a 1D array interpreted as a function
+    :math:`f : \\{0, \\ldots, n - 1\\}} \to \\{0, \\ldots, n - 1\\}}`, such
+    that :math:`f^{(k)} = f^{(k + 1)}` for some :math:`k`, find the 1D array
+    corresponding to :math:`f^{(k)}`."""
+    terminal_states = np.empty_like(mapping)
+    for i, initial_target_idx in enumerate(mapping):
+        temp_target_idx = i
+        next_target_idx = initial_target_idx
+        while temp_target_idx != next_target_idx:
+            temp_target_idx = mapping[temp_target_idx]
+            next_target_idx = mapping[mapping[temp_target_idx]]
+        terminal_states[i] = temp_target_idx
+
+    return terminal_states
+
+
+class Nerve(BaseEstimator, TransformerMixin):
+    """1-skeleton of the nerve of a refined Mapper cover, i.e. the Mapper
+    graph.
+
+    This transformer is the final step in the
+    :class:`gtda.mapper.pipeline.MapperPipeline` objects created
+    by :func:`gtda.mapper.make_mapper_pipeline`. It corresponds the last two
+    arrows in `this diagram <../../../../_images/mapper_pipeline.svg>`_.
+
+    Parameters
+    ----------
+    min_intersection : int, optional, default: ``1``
+        Minimum size of the intersection, between data subsets associated to
+        any two Mapper nodes, required to create an edge between the nodes in
+        the Mapper graph. Must be positive.
+
+    store_edge_elements : bool, optional, default: ``False``
+        Whether the indices of data elements associated to Mapper edges (i.e.
+        in the intersections allowed by `min_intersection`) should be stored in
+        the :class:`igraph.Graph` object output by :meth:`fit_transform`. When
+        ``True``, might lead to a large :class:`igraph.Graph` object.
+
+    contract_nodes : bool, optional, default: ``False``
+        If ``True``, any node representing a cluster which is a strict subset
+        of the cluster corresponding to another node is eliminated, and only
+        one maximal node is kept.
+
+    Attributes
+    ----------
+    graph_ : :class:`igraph.Graph` object
+        Mapper graph obtained from the input data. Created when :meth:`fit` is
+        called.
+
+    """
+
+    def __init__(self, min_intersection=1, store_edge_elements=False,
+                 contract_nodes=False):
+        self.min_intersection = min_intersection
+        self.store_edge_elements = store_edge_elements
+        self.contract_nodes = contract_nodes
+
+    def fit(self, X, y=None):
+        """Compute the Mapper graph as in :meth:`fit_transform`, but store the
+        graph as :attr:`graph_` and return the estimator.
+
+        Parameters
+        ----------
+        X : list of list of tuple
+            See :meth:`fit_transform`.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        self.graph_ = self.fit_transform(X, y=y)
+        return self
+
+    def fit_transform(self, X, y=None):
+        """Construct a Mapper graph from a refined Mapper cover.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples,)
+            Cluster labels describing a refined cover of a dataset produced by
+            the clustering step of a :class:`gtda.mapper.MapperPipeline`,
+            as depicted in
+            `this diagram <../../../../_images/mapper_pipeline.svg>`_. Each
+            entry in `X` is a tuple of pairs of the form
+            ``(pullback_set_label, partial_cluster_label)`` where
+            ``partial_cluster_label`` is a cluster label within the pullback
+            cover set identified by ``pullback_set_label``. The unique pairs
+            correspond to nodes in the output graph.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        graph : :class:`igraph.Graph` object
+            Undirected Mapper graph according to `X` and `min_intersection`.
+            Each node is an :class:`igraph.Vertex` object with attributes
+            ``"pullback_set_label"``, ``"partial_cluster_label"`` and
+            ``"node_elements"``. Each edge is an :class:`igraph.Edge` object
+            with a ``"weight"`` attribute which is equal to the size of the
+            intersection between the data subsets associated to its two nodes.
+            If `store_edge_elements` is ``True`` each edge also has an
+            additional attribute ``"edge_elements"``.
+
+        """
+        # TODO: Include a validation step for X
+        # Graph construction -- vertices with their metadata
+        labels_to_indices = defaultdict(list)
+        for i, sample in enumerate(X):
+            for node_id_pair in sample:
+                labels_to_indices[node_id_pair].append(i)
+        labels_to_indices = {key: np.array(value)
+                             for key, value in labels_to_indices.items()}
+        n_nodes = len(labels_to_indices)
+        graph = Graph(n_nodes)
+
+        # labels_to_indices is a dictionary of, say, N key-value pairs of the
+        # form (pullback_set_label, partial_cluster_label): node_elements.
+        # Hence, zip(*labels_to_indices) generates two tuples of length N, each
+        # corresponding to a type of node attribute in the final graph.
+        node_attributes = zip(*labels_to_indices)
+        graph.vs["pullback_set_label"] = next(node_attributes)
+        graph.vs["partial_cluster_label"] = next(node_attributes)
+        graph.vs["node_elements"] = [*labels_to_indices.values()]
+
+        # Graph construction -- edges with weights given by intersection sizes.
+        # In general, we need all information in `nodes` to narrow down the set
+        # of combinations to check, especially when `contract_nodes` is True.
+        nodes = zip(*zip(*labels_to_indices), labels_to_indices.values())
+        node_index_pairs, weights, intersections, mapping = \
+            self._generate_edge_data(nodes, n_nodes)
+        graph.es["weight"] = 1
+        graph.add_edges(node_index_pairs)
+        graph.es["weight"] = weights
+        if self.store_edge_elements:
+            graph.es["edge_elements"] = intersections
+        if self.contract_nodes:
+            # Due to the order in which itertools.combinations produces pairs,
+            # and to the preference given to node 1 in the if-elif-else clause
+            # in `_subset_check_metadata_append`, `mapping` is guaranteed to
+            # send everything to one of its fixed points after sufficiently
+            # many repeated applications and, by construction, no two pairs of
+            # indices in `_limit_mapping(mapping)` can correspond to data
+            # subsets which are in a subset relation. Thus the nodes are
+            # correctly contracted by `_limit_mapping(mapping)`.
+            limit_mapping = _limit_mapping(mapping)
+            graph.contract_vertices(limit_mapping,
+                                    combine_attrs="first")
+            graph.delete_vertices([i for i in graph.vs.indices
+                                   if i != limit_mapping[i]])
+
+        return graph
+
+    def _generate_edge_data(self, nodes, n_nodes):
+        def _in_same_pullback_set(_node_tuple):
+            return _node_tuple[0][1][0] == _node_tuple[1][1][0]
+
+        def _do_nothing(*args):
+            pass
+
+        def _intersections_append(_intersection):
+            return intersections.append(_intersection)
+
+        def _metadata_append(
+                _node_1_idx, _node_2_idx, _intersection_size, _intersection,
+                *args
+                ):
+            if _intersection_size >= self.min_intersection:
+                # Add edge (as a node tuple) to list of node index pairs
+                node_index_pairs.append((_node_1_idx, _node_2_idx))
+                weights.append(_intersection_size)
+                intersection_behavior(_intersection)
+
+        def _subset_check_metadata_append(
+                _node_1_idx, _node_2_idx, _intersection_size, _intersection,
+                _node_1_elements, _node_2_elements
+                ):
+            if _intersection_size == len(_node_2_elements):
+                # Node 2 is contained in node 1 and we remove it in favour of
+                # node 1.
+                mapping[_node_2_idx] = _node_1_idx
+            elif _intersection_size == len(_node_1_elements):
+                # Node 1 is strictly contained in node 2 and we remove it in
+                # favour of node 2.
+                mapping[_node_1_idx] = _node_2_idx
+            else:
+                # Edge exists provided `_intersection_size` is large enough
+                _metadata_append(_node_1_idx, _node_2_idx, _intersection_size,
+                                 _intersection)
+
+        node_tuples = combinations(enumerate(nodes), 2)
+
+        node_index_pairs = []
+        weights = []
+        intersections = []
+
+        # Choose whether intersections are stored or not.
+        # `intersection_behavior` is in scope for `_metadata_append` and
+        # `_subset_check_metadata_append`.
+        if self.store_edge_elements:
+            intersection_behavior = _intersections_append
+        else:
+            intersection_behavior = _do_nothing
+
+        if self.contract_nodes:
+            mapping = np.arange(n_nodes)
+            behavior = _subset_check_metadata_append
+        else:
+            mapping = None
+            behavior = _metadata_append
+
+        # No need to check for intersections within each pullback set as the
+        # input is assumed to be a refined Mapper cover
+        for node_tuple in filterfalse(_in_same_pullback_set, node_tuples):
+            ((node_1_idx, (_, _, node_1_elements)),
+             (node_2_idx, (_, _, node_2_elements))) = node_tuple
+            intersection = np.intersect1d(node_1_elements, node_2_elements)
+            intersection_size = len(intersection)
+
+            if intersection_size:
+                behavior(node_1_idx, node_2_idx, intersection_size,
+                         intersection, node_1_elements, node_2_elements)
+            else:
+                continue
+
+        return node_index_pairs, weights, intersections, mapping
```

## gtda/mapper/pipeline.py

 * *Ordering differences only*

```diff
@@ -1,429 +1,429 @@
-"""Construct and handle Mapper pipelines."""
-# License: GNU AGPLv3
-
-from sklearn.pipeline import Pipeline
-
-from .cluster import ParallelClustering
-from .nerve import Nerve
-from .utils._list_feature_union import ListFeatureUnion
-from .utils.pipeline import transformer_from_callable_on_rows, identity
-
-global_pipeline_params = ("memory", "verbose")
-nodes_params = ("scaler", "filter_func", "cover")
-clust_prepr_params = ("clustering_preprocessing",)
-clust_params = ("clusterer", "n_jobs",
-                "parallel_backend_prefer")
-nerve_params = ("min_intersection", "store_edge_elements", "contract_nodes")
-clust_prepr_params_prefix = "pullback_cover__"
-nodes_params_prefix = "pullback_cover__map_and_cover__"
-clust_params_prefix = "clustering__"
-nerve_params_prefix = "nerve__"
-
-
-class MapperPipeline(Pipeline):
-    """Subclass of :class:`sklearn.pipeline.Pipeline` to deal with
-    pipelines generated by :func:`~gtda.mapper.pipeline.make_mapper_pipeline`.
-
-    The :meth:`set_params` method is modified from the corresponding method in
-    :class:`sklearn.pipeline.Pipeline` to allow for simple access to the
-    parameters involved in the definition of the Mapper algorithm, without the
-    need to interface with the nested structure of the Pipeline objects
-    generated by :func:`~gtda.mapper.pipeline.make_mapper_pipeline`. The
-    convenience method :meth:`get_mapper_params` shows which parameters can
-    be set. See the Examples below.
-
-    Examples
-    --------
-    >>> from sklearn.cluster import DBSCAN
-    >>> from sklearn.decomposition import PCA
-    >>> from gtda.mapper import make_mapper_pipeline, CubicalCover
-    >>> filter_func = PCA(n_components=2)
-    >>> cover = CubicalCover()
-    >>> clusterer = DBSCAN()
-    >>> pipe = make_mapper_pipeline(filter_func=filter_func,
-    ...                             cover=cover,
-    ...                             clusterer=clusterer)
-    >>> print(pipe.get_mapper_params()["clusterer__eps"])
-    0.5
-    >>> pipe.set_params(clusterer___eps=0.1)
-    >>> print(pipe.get_mapper_params()["clusterer__eps"])
-    0.1
-
-    See also
-    --------
-    make_mapper_pipeline
-
-    """
-
-    # TODO: Abstract away common logic into a more generalisable implementation
-    def get_mapper_params(self, deep=True):
-        """Get all Mapper parameters for this estimator.
-
-        Parameters
-        ----------
-        deep : boolean, optional, default: ``True``
-            If ``True``, will return the parameters for this estimator and
-            contained subobjects that are estimators.
-
-        Returns
-        -------
-        params : mapping of string to any
-            Parameter names mapped to their values.
-
-        """
-        pipeline_params = super().get_params(deep=deep)
-        return {**{param: pipeline_params[param]
-                   for param in global_pipeline_params},
-                **self._clean_dict_keys(pipeline_params, nodes_params_prefix),
-                **self._clean_dict_keys(
-                    pipeline_params, clust_prepr_params_prefix),
-                **self._clean_dict_keys(pipeline_params, clust_params_prefix),
-                **self._clean_dict_keys(pipeline_params, nerve_params_prefix)}
-
-    def set_params(self, **kwargs):
-        """Set the Mapper parameters.
-
-        Valid parameter keys can be listed with :meth:`get_mapper_params()`.
-
-        Returns
-        -------
-        self
-
-        """
-        mapper_nodes_kwargs = self._subset_kwargs(kwargs, nodes_params)
-        mapper_clust_prepr_kwargs = \
-            self._subset_kwargs(kwargs, clust_prepr_params)
-        mapper_clust_kwargs = self._subset_kwargs(kwargs, clust_params)
-        mapper_nerve_kwargs = self._subset_kwargs(kwargs, nerve_params)
-        if mapper_nodes_kwargs:
-            super().set_params(
-                **{nodes_params_prefix + key: mapper_nodes_kwargs[key]
-                   for key in mapper_nodes_kwargs})
-            [kwargs.pop(key) for key in mapper_nodes_kwargs]
-        if mapper_clust_prepr_kwargs:
-            super().set_params(
-                **{clust_prepr_params_prefix + key:
-                    mapper_clust_prepr_kwargs[key] for key in
-                   mapper_clust_prepr_kwargs})
-            [kwargs.pop(key) for key in mapper_clust_prepr_kwargs]
-        if mapper_clust_kwargs:
-            super().set_params(
-                **{clust_params_prefix + key: mapper_clust_kwargs[key]
-                   for key in mapper_clust_kwargs})
-            [kwargs.pop(key) for key in mapper_clust_kwargs]
-        if mapper_nerve_kwargs:
-            super().set_params(
-                **{nerve_params_prefix + key: mapper_nerve_kwargs[key]
-                   for key in mapper_nerve_kwargs})
-            [kwargs.pop(key) for key in mapper_nerve_kwargs]
-        super().set_params(**kwargs)
-        return self
-
-    @staticmethod
-    def _subset_kwargs(kwargs, param_strings):
-        return {key: value for key, value in kwargs.items()
-                if key.startswith(param_strings)}
-
-    @staticmethod
-    def _clean_dict_keys(kwargs, prefix):
-        return {
-            key[len(prefix):]: kwargs[key]
-            for key in kwargs
-            if (key.startswith(prefix)
-                and not key.startswith(prefix + "steps")
-                and not key.startswith(prefix + "memory")
-                and not key.startswith(prefix + "verbose")
-                and not key.startswith(prefix + "transformer_list")
-                and not key.startswith(prefix + "n_jobs")
-                and not key.startswith(prefix + "transformer_weights")
-                and not key.startswith(prefix + "map_and_cover"))
-            }
-
-
-def make_mapper_pipeline(scaler=None,
-                         filter_func=None,
-                         cover=None,
-                         clustering_preprocessing=None,
-                         clusterer=None,
-                         n_jobs=None,
-                         parallel_backend_prefer=None,
-                         graph_step=True,
-                         min_intersection=1,
-                         store_edge_elements=False,
-                         contract_nodes=False,
-                         memory=None,
-                         verbose=False):
-    """Construct a MapperPipeline object according to the specified Mapper
-    steps [1]_.
-
-    The role of this function's main parameters is illustrated in `this diagram
-    <../../../../_images/mapper_pipeline.svg>`_. All computational steps may
-    be scikit-learn estimators, including Pipeline objects.
-
-    Parameters
-    ----------
-    scaler : object or None, optional, default: ``None``
-        If ``None``, no scaling is performed. Otherwise, it must be an
-        object with a ``fit_transform`` method.
-
-    filter_func : object, callable or None, optional, default: ``None``
-        If ``None``, PCA (:class:`sklearn.decomposition.PCA`) with 2
-        components and default parameters is used as a default filter
-        function. Otherwise, it may be an object with a ``fit_transform``
-        method, or a callable acting on one-dimensional arrays -- in which
-        case the callable is applied independently to each row of the
-        (scaled) data.
-
-    cover : object or None, optional, default: ``None``
-        Covering transformer, e.g. an instance of
-        :class:`~gtda.mapper.OneDimensionalCover` or of
-        :class:`~gtda.mapper.CubicalCover`. ``None`` is equivalent to passing
-        an instance of :class:`~gtda.mapper.CubicalCover` with its default
-        parameters.
-
-    clustering_preprocessing : object or None, optional, default: ``None``
-        If not ``None``, it is a transformer which is applied to the
-        data independently to the `scaler` -> `filter_func` -> `cover`
-        pipeline. Clustering is then performed on portions (determined by
-        the `scaler` -> `filter_func` -> `cover` pipeline) of the transformed
-        data.
-
-    clusterer : object or None, optional, default: ``None``
-        Clustering object with a ``fit`` method which stores cluster labels.
-        ``None`` is equivalent to passing an instance of
-        :class:`sklearn.cluster.DBSCAN` with its default parameters.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use in a joblib-parallel application of the
-        clustering step across pullback cover sets. To be used in
-        conjunction with `parallel_backend_prefer`. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    parallel_backend_prefer : ``"processes"`` | ``"threads"`` | ``None``, \
-        optional, default: ``None``
-        Soft hint for the default joblib backend to use in a joblib-parallel
-        application of the clustering step across pullback cover sets. To be
-        used in conjunction with `n_jobs`. The default process-based backend is
-        "loky" and the default thread-based backend is "threading". See [2]_.
-
-    graph_step : bool, optional, default: ``True``
-        Whether the resulting pipeline should stop at the calculation of the
-        (refined) Mapper cover, or include the construction of the Mapper
-        graph.
-
-    min_intersection : int, optional, default: ``1``
-        Minimum size of the intersection between clusters required for creating
-        an edge in the Mapper graph. Ignored if `graph_step` is set to
-        ``False``.
-
-    store_edge_elements : bool, optional, default: ``False``
-        Whether the indices of data elements associated to Mapper edges (i.e.
-        in the intersections allowed by `min_intersection`) should be stored in
-        the :class:`igraph.Graph` object output by the pipeline's
-        :meth:`fit_transform`. When ``True``, might lead to large
-        :class:`igraph.Graph` objects.
-
-    contract_nodes : bool, optional, default: ``False``
-        If ``True``, any node representing a cluster which is a strict subset
-        of the cluster corresponding to another node is eliminated, and only
-        one maximal node is kept.
-
-    memory : None, str or object with the joblib.Memory interface, \
-        optional, default: ``None``
-        Used to cache the fitted transformers which make up the pipeline. This
-        is advantageous when the fitting of early steps is time consuming and
-        only later steps in the pipeline are modified (e.g. using
-        :meth:`set_params`) before refitting on the same data. To be used
-        exactly as for :func:`sklearn.pipeline.make_pipeline`. By default, no
-        no caching is performed. If a string is given, it is the path to the
-        caching directory. See [3]_.
-
-    verbose : bool, optional, default: ``False``
-        If True, the time elapsed while fitting each step will be printed as it
-        is completed.
-
-    Returns
-    -------
-    mapper_pipeline : :class:`~gtda.mapper.pipeline.MapperPipeline` object
-        Output Mapper pipeline. The output of `mapper_pipeline`'s
-        :meth:`fit_transform` is: a) an :class:`igraph.Graph` object as per the
-        output of :class:`~gtda.mapper.nerve.Nerve`, when `graph_step` is
-        ``True``; b) a list of lists of tuples as per the output of
-        :class:`~gtda.mapper.ParallelClustering` (or input of
-        :class:`~gtda.mapper.Nerve`), otherwise.
-
-    Examples
-    --------
-    Basic usage with default parameters
-
-    >>> import numpy as np
-    >>> from gtda.mapper import make_mapper_pipeline
-    >>> mapper = make_mapper_pipeline()
-    >>> print(mapper.__class__)
-    <class 'gtda.mapper.pipeline.MapperPipeline'>
-    >>> mapper_params = mapper.get_mapper_params()
-    >>> print(mapper_params["filter_func"].__class__)
-    <class 'sklearn.decomposition._pca.PCA'>
-    >>> print(mapper_params["cover"].__class__)
-    <class 'gtda.mapper.cover.CubicalCover'>
-    >>> print(mapper_params["clusterer"].__class__)
-    <class 'sklearn.cluster._dbscan.DBSCAN'>
-    >>> X = np.random.random((10000, 4))  # 10000 points in 4-dimensional space
-    >>> mapper_graph = mapper.fit_transform(X)  # Create the mapper graph
-    >>> print(type(mapper_graph))
-    igraph.Graph
-    >>> # Node metadata stored as vertex attributes in graph object
-    >>> print(mapper_graph.vs.attributes())
-    ['pullback_set_label', 'partial_cluster_label', 'node_elements']
-    >>> # Find which points belong to first node of graph
-    >>> node_id = 0
-    >>> node_elements = mapper_graph.vs["node_elements"]
-    >>> print(f"Node ID: {node_id}, Node elements: {node_elements[node_id]}, "
-    ...       f"Data points: {X[node_elements[node_id]")
-    Node Id: 0,
-    Node elements: [8768],
-    Data points: [[0.01838998 0.76928754 0.98199244 0.0074299 ]]
-
-    Using a scaler from scikit-learn, a filter function from
-    ``gtda.mapper.filter``, and a clusterer from ``gtda.mapper.cluster``
-
-    >>> from sklearn.preprocessing import MinMaxScaler
-    >>> from gtda.mapper import Projection, FirstHistogramGap
-    >>> scaler = MinMaxScaler()
-    >>> filter_func = Projection(columns=[0, 1])
-    >>> clusterer = FirstHistogramGap()
-    >>> mapper = make_mapper_pipeline(scaler=scaler,
-    ...                               filter_func=filter_func,
-    ...                               clusterer=clusterer)
-
-    Using a callable acting on each row of X separately
-
-    >>> import numpy as np
-    >>> from gtda.mapper import OneDimensionalCover
-    >>> cover = OneDimensionalCover()
-    >>> mapper.set_params(scaler=None, filter_func=np.sum, cover=cover)
-
-    Setting the memory parameter to cache each step and avoid recomputation
-    of early steps
-
-    >>> from tempfile import mkdtemp
-    >>> from shutil import rmtree
-    >>> cachedir = mkdtemp()
-    >>> mapper.set_params(memory=cachedir, verbose=True)
-    >>> mapper_graph = mapper.fit_transform(X)
-    [Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.0s
-    [Pipeline] ....... (step 2 of 3) Processing filter_func, total=   0.0s
-    [Pipeline] ............. (step 3 of 3) Processing cover, total=   0.0s
-    [Pipeline] .... (step 1 of 3) Processing pullback_cover, total=   0.0s
-    [Pipeline] ........ (step 2 of 3) Processing clustering, total=   0.3s
-    [Pipeline] ............. (step 3 of 3) Processing nerve, total=   0.0s
-    >>> mapper.set_params(min_intersection=3)
-    >>> mapper_graph = mapper.fit_transform(X)
-    [Pipeline] ............. (step 3 of 3) Processing nerve, total=   0.0s
-    >>> # Clear the cache directory when you don't need it anymore
-    >>> rmtree(cachedir)
-
-    Using a large dataset for which parallelism in clustering across
-    the pullback cover sets can be beneficial
-
-    >>> from sklearn.cluster import DBSCAN
-    >>> mapper = make_mapper_pipeline(clusterer=DBSCAN(),
-    ...                               n_jobs=6,
-    ...                               memory=mkdtemp(),
-    ...                               verbose=True)
-    >>> X = np.random.random((100000, 4))
-    >>> mapper.fit_transform(X)
-    [Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.0s
-    [Pipeline] ....... (step 2 of 3) Processing filter_func, total=   0.1s
-    [Pipeline] ............. (step 3 of 3) Processing cover, total=   0.6s
-    [Pipeline] .... (step 1 of 3) Processing pullback_cover, total=   0.7s
-    [Pipeline] ........ (step 2 of 3) Processing clustering, total=   1.9s
-    [Pipeline] ............. (step 3 of 3) Processing nerve, total=   0.3s
-    >>> mapper.set_params(n_jobs=1)
-    >>> mapper.fit_transform(X)
-    [Pipeline] ........ (step 2 of 3) Processing clustering, total=   5.3s
-    [Pipeline] ............. (step 3 of 3) Processing nerve, total=   0.3s
-
-    See also
-    --------
-    MapperPipeline, method_to_transform
-
-    References
-    ----------
-    .. [1] G. Singh, F. Mmoli, and G. Carlsson, "Topological methods for the
-           analysis of high dimensional data sets and 3D object recognition";
-           in *SPBG*, pp. 91--100, 2007.
-
-    .. [2] "Thread-based parallelism vs process-based parallelism", in
-           `joblib documentation
-           <https://joblib.readthedocs.io/en/latest/parallel.html>`_.
-
-    .. [3] "Caching transformers: avoid repeated computation", in
-            `scikit-learn documentation \
-            <https://scikit-learn.org/stable/modules/compose.html>`_.
-
-    """
-
-    # TODO: Implement parameter validation
-
-    if scaler is None:
-        _scaler = identity(validate=False)
-    else:
-        _scaler = scaler
-
-    # If filter_func is not a scikit-learn transformer, assume it is a callable
-    # to be applied on each row separately. Then attempt to create a
-    # FunctionTransformer object to implement this behaviour.
-    if filter_func is None:
-        from sklearn.decomposition import PCA
-        _filter_func = PCA(n_components=2)
-    elif not hasattr(filter_func, "fit_transform"):
-        _filter_func = transformer_from_callable_on_rows(filter_func)
-    else:
-        _filter_func = filter_func
-
-    if cover is None:
-        from .cover import CubicalCover
-        _cover = CubicalCover()
-    else:
-        _cover = cover
-
-    if clustering_preprocessing is None:
-        _clustering_preprocessing = identity(validate=True)
-    else:
-        _clustering_preprocessing = clustering_preprocessing
-
-    if clusterer is None:
-        from sklearn.cluster import DBSCAN
-        _clusterer = DBSCAN()
-    else:
-        _clusterer = clusterer
-
-    map_and_cover = Pipeline(
-        steps=[("scaler", _scaler),
-               ("filter_func", _filter_func),
-               ("cover", _cover)],
-        verbose=verbose)
-
-    all_steps = [
-        ("pullback_cover", ListFeatureUnion(
-            [("clustering_preprocessing", _clustering_preprocessing),
-             ("map_and_cover", map_and_cover)])),
-        ("clustering", ParallelClustering(
-            _clusterer,
-            n_jobs=n_jobs,
-            parallel_backend_prefer=parallel_backend_prefer))
-        ]
-
-    if graph_step:
-        all_steps.append(
-            ("nerve", Nerve(min_intersection=min_intersection,
-                            store_edge_elements=store_edge_elements,
-                            contract_nodes=contract_nodes))
-            )
-
-    mapper_pipeline = MapperPipeline(
-        steps=all_steps, memory=memory, verbose=verbose)
-
-    return mapper_pipeline
+"""Construct and handle Mapper pipelines."""
+# License: GNU AGPLv3
+
+from sklearn.pipeline import Pipeline
+
+from .cluster import ParallelClustering
+from .nerve import Nerve
+from .utils._list_feature_union import ListFeatureUnion
+from .utils.pipeline import transformer_from_callable_on_rows, identity
+
+global_pipeline_params = ("memory", "verbose")
+nodes_params = ("scaler", "filter_func", "cover")
+clust_prepr_params = ("clustering_preprocessing",)
+clust_params = ("clusterer", "n_jobs",
+                "parallel_backend_prefer")
+nerve_params = ("min_intersection", "store_edge_elements", "contract_nodes")
+clust_prepr_params_prefix = "pullback_cover__"
+nodes_params_prefix = "pullback_cover__map_and_cover__"
+clust_params_prefix = "clustering__"
+nerve_params_prefix = "nerve__"
+
+
+class MapperPipeline(Pipeline):
+    """Subclass of :class:`sklearn.pipeline.Pipeline` to deal with
+    pipelines generated by :func:`~gtda.mapper.pipeline.make_mapper_pipeline`.
+
+    The :meth:`set_params` method is modified from the corresponding method in
+    :class:`sklearn.pipeline.Pipeline` to allow for simple access to the
+    parameters involved in the definition of the Mapper algorithm, without the
+    need to interface with the nested structure of the Pipeline objects
+    generated by :func:`~gtda.mapper.pipeline.make_mapper_pipeline`. The
+    convenience method :meth:`get_mapper_params` shows which parameters can
+    be set. See the Examples below.
+
+    Examples
+    --------
+    >>> from sklearn.cluster import DBSCAN
+    >>> from sklearn.decomposition import PCA
+    >>> from gtda.mapper import make_mapper_pipeline, CubicalCover
+    >>> filter_func = PCA(n_components=2)
+    >>> cover = CubicalCover()
+    >>> clusterer = DBSCAN()
+    >>> pipe = make_mapper_pipeline(filter_func=filter_func,
+    ...                             cover=cover,
+    ...                             clusterer=clusterer)
+    >>> print(pipe.get_mapper_params()["clusterer__eps"])
+    0.5
+    >>> pipe.set_params(clusterer___eps=0.1)
+    >>> print(pipe.get_mapper_params()["clusterer__eps"])
+    0.1
+
+    See also
+    --------
+    make_mapper_pipeline
+
+    """
+
+    # TODO: Abstract away common logic into a more generalisable implementation
+    def get_mapper_params(self, deep=True):
+        """Get all Mapper parameters for this estimator.
+
+        Parameters
+        ----------
+        deep : boolean, optional, default: ``True``
+            If ``True``, will return the parameters for this estimator and
+            contained subobjects that are estimators.
+
+        Returns
+        -------
+        params : mapping of string to any
+            Parameter names mapped to their values.
+
+        """
+        pipeline_params = super().get_params(deep=deep)
+        return {**{param: pipeline_params[param]
+                   for param in global_pipeline_params},
+                **self._clean_dict_keys(pipeline_params, nodes_params_prefix),
+                **self._clean_dict_keys(
+                    pipeline_params, clust_prepr_params_prefix),
+                **self._clean_dict_keys(pipeline_params, clust_params_prefix),
+                **self._clean_dict_keys(pipeline_params, nerve_params_prefix)}
+
+    def set_params(self, **kwargs):
+        """Set the Mapper parameters.
+
+        Valid parameter keys can be listed with :meth:`get_mapper_params()`.
+
+        Returns
+        -------
+        self
+
+        """
+        mapper_nodes_kwargs = self._subset_kwargs(kwargs, nodes_params)
+        mapper_clust_prepr_kwargs = \
+            self._subset_kwargs(kwargs, clust_prepr_params)
+        mapper_clust_kwargs = self._subset_kwargs(kwargs, clust_params)
+        mapper_nerve_kwargs = self._subset_kwargs(kwargs, nerve_params)
+        if mapper_nodes_kwargs:
+            super().set_params(
+                **{nodes_params_prefix + key: mapper_nodes_kwargs[key]
+                   for key in mapper_nodes_kwargs})
+            [kwargs.pop(key) for key in mapper_nodes_kwargs]
+        if mapper_clust_prepr_kwargs:
+            super().set_params(
+                **{clust_prepr_params_prefix + key:
+                    mapper_clust_prepr_kwargs[key] for key in
+                   mapper_clust_prepr_kwargs})
+            [kwargs.pop(key) for key in mapper_clust_prepr_kwargs]
+        if mapper_clust_kwargs:
+            super().set_params(
+                **{clust_params_prefix + key: mapper_clust_kwargs[key]
+                   for key in mapper_clust_kwargs})
+            [kwargs.pop(key) for key in mapper_clust_kwargs]
+        if mapper_nerve_kwargs:
+            super().set_params(
+                **{nerve_params_prefix + key: mapper_nerve_kwargs[key]
+                   for key in mapper_nerve_kwargs})
+            [kwargs.pop(key) for key in mapper_nerve_kwargs]
+        super().set_params(**kwargs)
+        return self
+
+    @staticmethod
+    def _subset_kwargs(kwargs, param_strings):
+        return {key: value for key, value in kwargs.items()
+                if key.startswith(param_strings)}
+
+    @staticmethod
+    def _clean_dict_keys(kwargs, prefix):
+        return {
+            key[len(prefix):]: kwargs[key]
+            for key in kwargs
+            if (key.startswith(prefix)
+                and not key.startswith(prefix + "steps")
+                and not key.startswith(prefix + "memory")
+                and not key.startswith(prefix + "verbose")
+                and not key.startswith(prefix + "transformer_list")
+                and not key.startswith(prefix + "n_jobs")
+                and not key.startswith(prefix + "transformer_weights")
+                and not key.startswith(prefix + "map_and_cover"))
+            }
+
+
+def make_mapper_pipeline(scaler=None,
+                         filter_func=None,
+                         cover=None,
+                         clustering_preprocessing=None,
+                         clusterer=None,
+                         n_jobs=None,
+                         parallel_backend_prefer=None,
+                         graph_step=True,
+                         min_intersection=1,
+                         store_edge_elements=False,
+                         contract_nodes=False,
+                         memory=None,
+                         verbose=False):
+    """Construct a MapperPipeline object according to the specified Mapper
+    steps [1]_.
+
+    The role of this function's main parameters is illustrated in `this diagram
+    <../../../../_images/mapper_pipeline.svg>`_. All computational steps may
+    be scikit-learn estimators, including Pipeline objects.
+
+    Parameters
+    ----------
+    scaler : object or None, optional, default: ``None``
+        If ``None``, no scaling is performed. Otherwise, it must be an
+        object with a ``fit_transform`` method.
+
+    filter_func : object, callable or None, optional, default: ``None``
+        If ``None``, PCA (:class:`sklearn.decomposition.PCA`) with 2
+        components and default parameters is used as a default filter
+        function. Otherwise, it may be an object with a ``fit_transform``
+        method, or a callable acting on one-dimensional arrays -- in which
+        case the callable is applied independently to each row of the
+        (scaled) data.
+
+    cover : object or None, optional, default: ``None``
+        Covering transformer, e.g. an instance of
+        :class:`~gtda.mapper.OneDimensionalCover` or of
+        :class:`~gtda.mapper.CubicalCover`. ``None`` is equivalent to passing
+        an instance of :class:`~gtda.mapper.CubicalCover` with its default
+        parameters.
+
+    clustering_preprocessing : object or None, optional, default: ``None``
+        If not ``None``, it is a transformer which is applied to the
+        data independently to the `scaler` -> `filter_func` -> `cover`
+        pipeline. Clustering is then performed on portions (determined by
+        the `scaler` -> `filter_func` -> `cover` pipeline) of the transformed
+        data.
+
+    clusterer : object or None, optional, default: ``None``
+        Clustering object with a ``fit`` method which stores cluster labels.
+        ``None`` is equivalent to passing an instance of
+        :class:`sklearn.cluster.DBSCAN` with its default parameters.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use in a joblib-parallel application of the
+        clustering step across pullback cover sets. To be used in
+        conjunction with `parallel_backend_prefer`. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    parallel_backend_prefer : ``"processes"`` | ``"threads"`` | ``None``, \
+        optional, default: ``None``
+        Soft hint for the default joblib backend to use in a joblib-parallel
+        application of the clustering step across pullback cover sets. To be
+        used in conjunction with `n_jobs`. The default process-based backend is
+        "loky" and the default thread-based backend is "threading". See [2]_.
+
+    graph_step : bool, optional, default: ``True``
+        Whether the resulting pipeline should stop at the calculation of the
+        (refined) Mapper cover, or include the construction of the Mapper
+        graph.
+
+    min_intersection : int, optional, default: ``1``
+        Minimum size of the intersection between clusters required for creating
+        an edge in the Mapper graph. Ignored if `graph_step` is set to
+        ``False``.
+
+    store_edge_elements : bool, optional, default: ``False``
+        Whether the indices of data elements associated to Mapper edges (i.e.
+        in the intersections allowed by `min_intersection`) should be stored in
+        the :class:`igraph.Graph` object output by the pipeline's
+        :meth:`fit_transform`. When ``True``, might lead to large
+        :class:`igraph.Graph` objects.
+
+    contract_nodes : bool, optional, default: ``False``
+        If ``True``, any node representing a cluster which is a strict subset
+        of the cluster corresponding to another node is eliminated, and only
+        one maximal node is kept.
+
+    memory : None, str or object with the joblib.Memory interface, \
+        optional, default: ``None``
+        Used to cache the fitted transformers which make up the pipeline. This
+        is advantageous when the fitting of early steps is time consuming and
+        only later steps in the pipeline are modified (e.g. using
+        :meth:`set_params`) before refitting on the same data. To be used
+        exactly as for :func:`sklearn.pipeline.make_pipeline`. By default, no
+        no caching is performed. If a string is given, it is the path to the
+        caching directory. See [3]_.
+
+    verbose : bool, optional, default: ``False``
+        If True, the time elapsed while fitting each step will be printed as it
+        is completed.
+
+    Returns
+    -------
+    mapper_pipeline : :class:`~gtda.mapper.pipeline.MapperPipeline` object
+        Output Mapper pipeline. The output of `mapper_pipeline`'s
+        :meth:`fit_transform` is: a) an :class:`igraph.Graph` object as per the
+        output of :class:`~gtda.mapper.nerve.Nerve`, when `graph_step` is
+        ``True``; b) a list of lists of tuples as per the output of
+        :class:`~gtda.mapper.ParallelClustering` (or input of
+        :class:`~gtda.mapper.Nerve`), otherwise.
+
+    Examples
+    --------
+    Basic usage with default parameters
+
+    >>> import numpy as np
+    >>> from gtda.mapper import make_mapper_pipeline
+    >>> mapper = make_mapper_pipeline()
+    >>> print(mapper.__class__)
+    <class 'gtda.mapper.pipeline.MapperPipeline'>
+    >>> mapper_params = mapper.get_mapper_params()
+    >>> print(mapper_params["filter_func"].__class__)
+    <class 'sklearn.decomposition._pca.PCA'>
+    >>> print(mapper_params["cover"].__class__)
+    <class 'gtda.mapper.cover.CubicalCover'>
+    >>> print(mapper_params["clusterer"].__class__)
+    <class 'sklearn.cluster._dbscan.DBSCAN'>
+    >>> X = np.random.random((10000, 4))  # 10000 points in 4-dimensional space
+    >>> mapper_graph = mapper.fit_transform(X)  # Create the mapper graph
+    >>> print(type(mapper_graph))
+    igraph.Graph
+    >>> # Node metadata stored as vertex attributes in graph object
+    >>> print(mapper_graph.vs.attributes())
+    ['pullback_set_label', 'partial_cluster_label', 'node_elements']
+    >>> # Find which points belong to first node of graph
+    >>> node_id = 0
+    >>> node_elements = mapper_graph.vs["node_elements"]
+    >>> print(f"Node ID: {node_id}, Node elements: {node_elements[node_id]}, "
+    ...       f"Data points: {X[node_elements[node_id]")
+    Node Id: 0,
+    Node elements: [8768],
+    Data points: [[0.01838998 0.76928754 0.98199244 0.0074299 ]]
+
+    Using a scaler from scikit-learn, a filter function from
+    ``gtda.mapper.filter``, and a clusterer from ``gtda.mapper.cluster``
+
+    >>> from sklearn.preprocessing import MinMaxScaler
+    >>> from gtda.mapper import Projection, FirstHistogramGap
+    >>> scaler = MinMaxScaler()
+    >>> filter_func = Projection(columns=[0, 1])
+    >>> clusterer = FirstHistogramGap()
+    >>> mapper = make_mapper_pipeline(scaler=scaler,
+    ...                               filter_func=filter_func,
+    ...                               clusterer=clusterer)
+
+    Using a callable acting on each row of X separately
+
+    >>> import numpy as np
+    >>> from gtda.mapper import OneDimensionalCover
+    >>> cover = OneDimensionalCover()
+    >>> mapper.set_params(scaler=None, filter_func=np.sum, cover=cover)
+
+    Setting the memory parameter to cache each step and avoid recomputation
+    of early steps
+
+    >>> from tempfile import mkdtemp
+    >>> from shutil import rmtree
+    >>> cachedir = mkdtemp()
+    >>> mapper.set_params(memory=cachedir, verbose=True)
+    >>> mapper_graph = mapper.fit_transform(X)
+    [Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.0s
+    [Pipeline] ....... (step 2 of 3) Processing filter_func, total=   0.0s
+    [Pipeline] ............. (step 3 of 3) Processing cover, total=   0.0s
+    [Pipeline] .... (step 1 of 3) Processing pullback_cover, total=   0.0s
+    [Pipeline] ........ (step 2 of 3) Processing clustering, total=   0.3s
+    [Pipeline] ............. (step 3 of 3) Processing nerve, total=   0.0s
+    >>> mapper.set_params(min_intersection=3)
+    >>> mapper_graph = mapper.fit_transform(X)
+    [Pipeline] ............. (step 3 of 3) Processing nerve, total=   0.0s
+    >>> # Clear the cache directory when you don't need it anymore
+    >>> rmtree(cachedir)
+
+    Using a large dataset for which parallelism in clustering across
+    the pullback cover sets can be beneficial
+
+    >>> from sklearn.cluster import DBSCAN
+    >>> mapper = make_mapper_pipeline(clusterer=DBSCAN(),
+    ...                               n_jobs=6,
+    ...                               memory=mkdtemp(),
+    ...                               verbose=True)
+    >>> X = np.random.random((100000, 4))
+    >>> mapper.fit_transform(X)
+    [Pipeline] ............ (step 1 of 3) Processing scaler, total=   0.0s
+    [Pipeline] ....... (step 2 of 3) Processing filter_func, total=   0.1s
+    [Pipeline] ............. (step 3 of 3) Processing cover, total=   0.6s
+    [Pipeline] .... (step 1 of 3) Processing pullback_cover, total=   0.7s
+    [Pipeline] ........ (step 2 of 3) Processing clustering, total=   1.9s
+    [Pipeline] ............. (step 3 of 3) Processing nerve, total=   0.3s
+    >>> mapper.set_params(n_jobs=1)
+    >>> mapper.fit_transform(X)
+    [Pipeline] ........ (step 2 of 3) Processing clustering, total=   5.3s
+    [Pipeline] ............. (step 3 of 3) Processing nerve, total=   0.3s
+
+    See also
+    --------
+    MapperPipeline, method_to_transform
+
+    References
+    ----------
+    .. [1] G. Singh, F. Mmoli, and G. Carlsson, "Topological methods for the
+           analysis of high dimensional data sets and 3D object recognition";
+           in *SPBG*, pp. 91--100, 2007.
+
+    .. [2] "Thread-based parallelism vs process-based parallelism", in
+           `joblib documentation
+           <https://joblib.readthedocs.io/en/latest/parallel.html>`_.
+
+    .. [3] "Caching transformers: avoid repeated computation", in
+            `scikit-learn documentation \
+            <https://scikit-learn.org/stable/modules/compose.html>`_.
+
+    """
+
+    # TODO: Implement parameter validation
+
+    if scaler is None:
+        _scaler = identity(validate=False)
+    else:
+        _scaler = scaler
+
+    # If filter_func is not a scikit-learn transformer, assume it is a callable
+    # to be applied on each row separately. Then attempt to create a
+    # FunctionTransformer object to implement this behaviour.
+    if filter_func is None:
+        from sklearn.decomposition import PCA
+        _filter_func = PCA(n_components=2)
+    elif not hasattr(filter_func, "fit_transform"):
+        _filter_func = transformer_from_callable_on_rows(filter_func)
+    else:
+        _filter_func = filter_func
+
+    if cover is None:
+        from .cover import CubicalCover
+        _cover = CubicalCover()
+    else:
+        _cover = cover
+
+    if clustering_preprocessing is None:
+        _clustering_preprocessing = identity(validate=True)
+    else:
+        _clustering_preprocessing = clustering_preprocessing
+
+    if clusterer is None:
+        from sklearn.cluster import DBSCAN
+        _clusterer = DBSCAN()
+    else:
+        _clusterer = clusterer
+
+    map_and_cover = Pipeline(
+        steps=[("scaler", _scaler),
+               ("filter_func", _filter_func),
+               ("cover", _cover)],
+        verbose=verbose)
+
+    all_steps = [
+        ("pullback_cover", ListFeatureUnion(
+            [("clustering_preprocessing", _clustering_preprocessing),
+             ("map_and_cover", map_and_cover)])),
+        ("clustering", ParallelClustering(
+            _clusterer,
+            n_jobs=n_jobs,
+            parallel_backend_prefer=parallel_backend_prefer))
+        ]
+
+    if graph_step:
+        all_steps.append(
+            ("nerve", Nerve(min_intersection=min_intersection,
+                            store_edge_elements=store_edge_elements,
+                            contract_nodes=contract_nodes))
+            )
+
+    mapper_pipeline = MapperPipeline(
+        steps=all_steps, memory=memory, verbose=verbose)
+
+    return mapper_pipeline
```

## gtda/mapper/visualization.py

 * *Ordering differences only*

```diff
@@ -1,735 +1,735 @@
-"""Static and interactive visualisation functions for Mapper graphs."""
-# License: GNU AGPLv3
-
-import logging
-import traceback
-
-import numpy as np
-import plotly.graph_objects as go
-from ipywidgets import widgets, Layout, HTML
-from sklearn.base import clone
-
-from .utils._logging import OutputWidgetHandler
-from .utils._visualization import (
-    _validate_color_kwargs,
-    _calculate_graph_data,
-    _produce_static_figure,
-    _get_column_color_buttons,
-    _get_colors_for_vals,
-)
-
-
-def plot_static_mapper_graph(
-        pipeline, data, color_data=None, color_features=None,
-        node_color_statistic=None, layout="kamada_kawai", layout_dim=2,
-        clone_pipeline=True, n_sig_figs=3, node_scale=12, plotly_params=None
-        ):
-    """Plot Mapper graphs without interactivity on pipeline parameters.
-
-    The output graph is a rendition of the :class:`igraph.Graph` object
-    computed by calling the :meth:`fit_transform` method of the
-    :class:`~gtda.mapper.pipeline.MapperPipeline` instance `pipeline` on the
-    input `data`. The graph's nodes correspond to subsets of elements (rows) in
-    `data`; these subsets are clusters in larger portions of `data` called
-    "pullback (cover) sets", which are computed by means of the `pipeline`'s
-    "filter function" and "cover" and correspond to the differently-colored
-    portions in `this diagram <../../../../_images/mapper_pipeline.svg>`_.
-    Two clusters from different pullback cover sets can overlap; if they do, an
-    edge between the corresponding nodes in the graph may be drawn.
-
-    Nodes are colored according to `color_features` and `node_color_statistic`
-    and are sized according to the number of elements they represent. The
-    hovertext on each node displays, in this order:
-
-        - a globally unique ID for the node, which can be used to retrieve
-          node information from the :class:`igraph.Graph` object, see
-          :class:`~gtda.mapper.nerve.Nerve`;
-        - the label of the pullback (cover) set which the node's elements
-          form a cluster in;
-        - a label identifying the node as a cluster within that pullback set;
-        - the number of elements of `data` associated with the node;
-        - the value of the summary statistic which determines the node's color.
-
-    Parameters
-    ----------
-    pipeline : :class:`~gtda.mapper.pipeline.MapperPipeline` object
-        Mapper pipeline to act onto data.
-
-    data : array-like of shape (n_samples, n_features)
-        Data used to generate the Mapper graph. Can be a pandas dataframe.
-
-    color_data : array-like of length n_samples, or None, optional, \
-        default: ``None``
-        Data to be used to construct node colors in the Mapper graph (according
-        to `color_features` and `node_color_statistic`). Must have the same
-        length as `data`. ``None`` is the same as passing
-        ``numpy.arange(len(data))``.
-
-    color_features : object or None, optional, default: ``None``
-        Specifies one or more feature of interest from `color_data` to be used,
-        together with `node_color_statistic`, to determine node colors. Ignored
-        if `node_color_statistic` is a numpy array.
-
-            1. ``None`` is equivalent to passing `color_data`.
-            2. If an object implementing :meth:`transform` or
-               :meth:`fit_transform`, or a callable, it is applied to
-               `color_data` to generate the features of interest.
-            3. If an index or string, or list of indices/strings, it is
-               equivalent to selecting a column or subset of columns from
-               `color_data`.
-
-    node_color_statistic : None, callable, or ndarray of shape (n_nodes,) or \
-        (n_nodes, 1), optional, default: ``None``
-        If a callable, node colors will be computed as summary statistics from
-        the feature array ``y`` determined by `color_data` and
-        `color_features`. Let ``y`` have ``n`` columns (note: 1d feature arrays
-        are converted to column vectors). Then, for a node representing a list
-        ``I`` of row indices, there will be ``n`` colors, each computed as
-        ``node_color_statistic(y[I, i])`` for ``i`` between ``0`` and ``n``.
-        ``None`` is equivalent to passing :func:`numpy.mean`. If a numpy array,
-        it must have the same length as the number of nodes in the Mapper graph
-        and its values are used directly as node colors (`color_features` is
-        ignored).
-
-    layout : None, str or callable, optional, default: ``"kamada-kawai"``
-        Layout algorithm for the graph. Can be any accepted value for the
-        ``layout`` parameter in the :meth:`layout` method of
-        :class:`igraph.Graph` [1]_.
-
-    layout_dim : int, default: ``2``
-        The number of dimensions for the layout. Can be 2 or 3.
-
-    clone_pipeline : bool, optional, default: ``True``
-        If ``True``, the input `pipeline` is cloned before computing the
-        Mapper graph to prevent unexpected side effects from in-place
-        parameter updates.
-
-    n_sig_figs : int or None, optional, default: ``3``
-       If not ``None``, number of significant figures to which to round node
-       summary statistics. If ``None``, no rounding is performed.
-
-    node_scale : int or float, optional, default: ``12``
-        Sets the scale factor used to determine the rendered size of the
-        nodes. Increase for larger nodes. Implements a formula in the
-        `Plotly documentation \
-        <https://plotly.com/python/bubble-charts/#scaling-the-size-of-bubble\
-        -charts>`_.
-
-    plotly_params : dict or None, optional, default: ``None``
-        Custom parameters to configure the plotly figure. Allowed keys are
-        ``"node_trace"``, ``"edge_trace"`` and ``"layout"``, and the
-        corresponding values should be dictionaries containing keyword
-        arguments as would be fed to the :meth:`update_traces` and
-        :meth:`update_layout` methods of :class:`plotly.graph_objects.Figure`.
-
-    Returns
-    -------
-    fig : :class:`plotly.graph_objects.FigureWidget` object
-        Figure representing the Mapper graph with appropriate node colouring
-        and size.
-
-    Examples
-    --------
-    Setting a colorscale different from the default one:
-
-    >>> import numpy as np
-    >>> np.random.seed(1)
-    >>> from gtda.mapper import make_mapper_pipeline, plot_static_mapper_graph
-    >>> pipeline = make_mapper_pipeline()
-    >>> data = np.random.random((100, 3))
-    >>> plotly_params = {"node_trace": {"marker_colorscale": "Blues"}}
-    >>> fig = plot_static_mapper_graph(pipeline, data,
-    ...                                plotly_params=plotly_params)
-
-    Inspect the composition of a node with "Node ID" displayed as 0 in the
-    hovertext:
-
-    >>> graph = pipeline.fit_transform(data)
-    >>> graph.vs[0]["node_elements"]
-    array([70])
-
-    Write the figure to a file using Plotly:
-    >>> fname = "current_figure"
-    >>> fig.write_html(fname + ".html")
-    >>> fig.write_image(fname + ".svg")  # Requires psutil
-
-    See also
-    --------
-    MapperInteractivePlotter, plot_interactive_mapper_graph, \
-    gtda.mapper.make_mapper_pipeline
-
-    References
-    ----------
-    .. [1] `igraph.Graph.layout
-            <https://igraph.org/python/doc/igraph.Graph-class.html#layout>`_
-            documentation.
-
-    """
-
-    # Compute the graph and fetch the indices of points in each node
-    _pipeline = clone(pipeline) if clone_pipeline else pipeline
-
-    graph = _pipeline.fit_transform(data)
-    (color_data_transformed, column_names_dropdown,
-     node_color_statistic) = \
-        _validate_color_kwargs(graph, data, color_data, color_features,
-                               node_color_statistic, interactive=False)
-    edge_trace, node_trace, node_colors_color_features = \
-        _calculate_graph_data(
-            graph, color_data_transformed, node_color_statistic, layout,
-            layout_dim, n_sig_figs, node_scale
-            )
-
-    figure = _produce_static_figure(
-        edge_trace, node_trace, node_colors_color_features,
-        column_names_dropdown, layout_dim, n_sig_figs, plotly_params
-        )
-
-    return figure
-
-
-def plot_interactive_mapper_graph(
-        pipeline, data, color_data=None, color_features=None,
-        node_color_statistic=None, layout="kamada_kawai", layout_dim=2,
-        clone_pipeline=True, n_sig_figs=3, node_scale=12, plotly_params=None
-        ):
-    """*As of version 0.5.0, we recommend using the object-oriented interface
-    provided by :class:`MapperInteractivePlotter` instead of this function.*
-
-    Plot Mapper graphs in a Jupyter session, with interactivity on pipeline
-    parameters.
-
-    Extends :func:`~gtda.mapper.visualization.plot_static_mapper_graph` by
-    providing functionality to interactively update parameters from the cover,
-    clustering and graph construction steps defined in `pipeline`.
-
-    Parameters
-    ----------
-    pipeline : :class:`~gtda.mapper.pipeline.MapperPipeline` object
-        Mapper pipeline to act on to data.
-
-    data : array-like of shape (n_samples, n_features)
-        Data used to generate the Mapper graph. Can be a pandas dataframe.
-
-    color_data : array-like of length n_samples, or None, optional, \
-        default: ``None``
-        Data to be used to construct node colors in the Mapper graph (according
-        to `color_features` and `node_color_statistic`). Must have the same
-        length as `data`. ``None`` is the same as passing
-        ``numpy.arange(len(data))``.
-
-    color_features : object or None, optional, default: ``None``
-        Specifies one or more feature of interest from `color_data` to be used,
-        together with `node_color_statistic`, to determine node colors.
-
-            1. ``None`` is equivalent to passing `color_data`.
-            2. If an object implementing :meth:`transform` or
-               :meth:`fit_transform`, or a callable, it is applied to
-               `color_data` to generate the features of interest.
-            3. If an index or string, or list of indices/strings, it is
-               equivalent to selecting a column or subset of columns from
-               `color_data`.
-
-    node_color_statistic : None or callable, optional, default: ``None``
-        If a callable, node colors will be computed as summary statistics from
-        the feature array ``y`` determined by `color_data` and
-        `color_features`. Let ``y`` have ``n`` columns (note: 1d feature arrays
-        are converted to column vectors). Then, for a node representing a list
-        ``I`` of row indices, there will be ``n`` colors, each computed as
-        ``node_color_statistic(y[I, i])`` for ``i`` between ``0`` and ``n``.
-        ``None`` is equivalent to passing :func:`numpy.mean`.
-
-    layout : None, str or callable, optional, default: ``"kamada-kawai"``
-        Layout algorithm for the graph. Can be any accepted value for the
-        ``layout`` parameter in the :meth:`layout` method of
-        :class:`igraph.Graph` [1]_.
-
-    layout_dim : int, default: ``2``
-        The number of dimensions for the layout. Can be 2 or 3.
-
-    clone_pipeline : bool, optional, default: ``True``
-        If ``True``, the input `pipeline` is cloned before computing the
-        Mapper graph to prevent unexpected side effects from in-place
-        parameter updates.
-
-    n_sig_figs : int or None, optional, default: ``3``
-       If not ``None``, number of significant figures to which to round node
-       summary statistics. If ``None``, no rounding is performed.
-
-    node_scale : int or float, optional, default: ``12``
-        Sets the scale factor used to determine the rendered size of the
-        nodes. Increase for larger nodes. Implements a formula in the
-        `Plotly documentation \
-        <plotly.com/python/bubble-charts/#scaling-the-size-of-bubble-charts>`_.
-
-    plotly_params : dict or None, optional, default: ``None``
-        Custom parameters to configure the plotly figure. Allowed keys are
-        ``"node_trace"``, ``"edge_trace"`` and ``"layout"``, and the
-        corresponding values should be dictionaries containing keyword
-        arguments as would be fed to the :meth:`update_traces` and
-        :meth:`update_layout` methods of :class:`plotly.graph_objects.Figure`.
-
-    Returns
-    -------
-    box : :class:`ipywidgets.VBox` object
-        A box containing the following widgets: parameters of the clustering
-        algorithm, parameters for the covering scheme, a Mapper graph arising
-        from those parameters, a validation box, and logs.
-
-    See also
-    --------
-    MapperInteractivePlotter, plot_static_mapper_graph, \
-    gtda.mapper.pipeline.make_mapper_pipeline
-
-    References
-    ----------
-    .. [1] `igraph.Graph.layout
-            <https://igraph.org/python/doc/igraph.Graph-class.html#layout>`_
-            documentation.
-
-    """
-
-    plotter = MapperInteractivePlotter(pipeline, data, clone_pipeline)
-
-    return plotter.plot(
-        color_data=color_data, color_features=color_features,
-        node_color_statistic=node_color_statistic, layout=layout,
-        layout_dim=layout_dim, n_sig_figs=n_sig_figs, node_scale=node_scale,
-        plotly_params=plotly_params
-        )
-
-
-class MapperInteractivePlotter:
-    """Plot Mapper graphs in a Jupyter session, with interactivity on pipeline
-    parameters.
-
-    Provides functionality to interactively update parameters from the cover,
-    clustering and graph construction steps defined in `pipeline`.
-    An interactive widget is produced when calling :meth:`plot`. After
-    interacting with the widget, the current state of all outputs which may
-    have been altered can be retrieved via one of the attributes listed below.
-
-    Parameters
-    ----------
-    pipeline : :class:`~gtda.mapper.pipeline.MapperPipeline` object
-        Mapper pipeline to act on to data.
-
-    data : array-like of shape (n_samples, n_features)
-        Data used to generate the Mapper graph. Can be a pandas dataframe.
-
-    clone_pipeline : bool, optional, default: ``True``
-        If ``True``, the input `pipeline` is cloned before computing the
-        Mapper graph to prevent unexpected side effects from in-place
-        parameter updates.
-
-    Attributes
-    ----------
-    graph_ : :class:`igraph.Graph` object
-        Current state of the graph displayed by the widget.
-
-    pipeline_ : :class:`~gtda.mapper.pipeline.MapperPipeline` object
-        Current state of the Mapper pipeline.
-
-    color_features_ : array-like of shape (n_samples, n_features)
-        Values of the features of interest for each entry in `data`, as
-        produced according to `color_data` and `color_features` when calling
-        :meth:`plot`. Not changed by interacting with the widget.
-
-    node_summaries_ : array-like of shape (n_nodes, n_features)
-        Current values of the summaries computed for each node and used as
-        node colours in the figure. Produced according to
-        `node_color_statistic`, see :meth:`plot`.
-
-    figure_ : :class:`plotly.graph_objects.FigureWidget` object
-        Current figure representing the Mapper graph with appropriate node
-        colouring and size.
-
-    Examples
-    --------
-    Instantiate the plotter object on a pipeline and data configuration, and
-    call :meth:`plot` to display the widget in a Jupyter session:
-
-    >>> import numpy as np
-    >>> np.random.seed(1)
-    >>> from gtda.mapper import make_mapper_pipeline, MapperInteractivePlotter
-    >>> pipeline = make_mapper_pipeline()
-    >>> data = np.random.random((100, 3))
-    >>> plotter = MapperInteractivePlotter(pipeline, data)
-    >>> plotter.plot()
-
-    After interacting with the widget, inspect the composition of a node with
-    "Node ID" displayed as 0 in the hovertext:
-
-    >>> plotter.graph_.vs[0]["node_elements"]
-    array([70])
-
-    Write the current figure to a file using Plotly:
-    >>> fname = "current_figure"
-    >>> plotter.fig_.write_html(fname + ".html")
-    >>> plotter.fig_.write_image(fname + ".svg")  # Requires psutil
-
-    See also
-    --------
-    plot_interactive_mapper_graph, plot_static_mapper_graph, \
-    gtda.mapper.pipeline.make_mapper_pipeline
-
-    References
-    ----------
-    .. [1] `igraph.Graph.layout
-            <https://igraph.org/python/doc/igraph.Graph-class.html#layout>`_
-            documentation.
-
-    """
-
-    def __init__(self, pipeline, data, clone_pipeline=True):
-        self.pipeline = pipeline
-        self.data = data
-        self.clone_pipeline = clone_pipeline
-
-    def plot(self, color_data=None, color_features=None,
-             node_color_statistic=None, layout="kamada_kawai", layout_dim=2,
-             n_sig_figs=3, node_scale=12, plotly_params=None):
-        """ Produce the interactive Mapper widget.
-
-        Parameters
-        ----------
-        color_data : array-like of length n_samples, or None, optional, \
-            default: ``None``
-            Data to be used to construct node colors in the Mapper graph
-            (according to `color_features` and `node_color_statistic`). Must
-            have the same length as `data`. ``None`` is the same as passing
-            ``numpy.arange(len(data))``.
-
-        color_features : object or None, optional, default: ``None``
-            Specifies one or more feature of interest from `color_data` to be
-            used, together with `node_color_statistic`, to determine node
-            colors.
-
-                1. ``None`` is equivalent to passing `color_data`.
-                2. If an object implementing :meth:`transform` or
-                   :meth:`fit_transform`, or a callable, it is applied to
-                   `color_data` to generate the features of interest.
-                3. If an index or string, or list of indices/strings, it is
-                   equivalent to selecting a column or subset of columns from
-                   `color_data`.
-
-        node_color_statistic : None or callable, optional, default: ``None``
-            If a callable, node colors will be computed as summary statistics
-            from the feature array ``y`` determined by `color_data` and
-            `color_features`. Let ``y`` have ``n`` columns (note: 1d feature
-            arrays are converted to column vectors). Then, for a node
-            representing a list ``I`` of row indices, there will be ``n``
-            colors, each computed as ``node_color_statistic(y[I, i])`` for
-            ``i`` between ``0`` and ``n``.
-
-        layout : None, str or callable, optional, default: ``"kamada-kawai"``
-            Layout algorithm for the graph. Can be any accepted value for the
-            ``layout`` parameter in the :meth:`layout` method of
-            :class:`igraph.Graph` [1]_.
-
-        layout_dim : int, default: ``2``
-            The number of dimensions for the layout. Can be 2 or 3.
-
-        n_sig_figs : int or None, optional, default: ``3``
-           If not ``None``, number of significant figures to which to round
-           node summary statistics. If ``None``, no rounding is performed.
-
-        node_scale : int or float, optional, default: ``12``
-            Sets the scale factor used to determine the rendered size of the
-            nodes. Increase for larger nodes. Implements a formula in the
-            `Plotly documentation \
-            <plotly.com/python/bubble-charts/#scaling-the-size-of-bubble-charts>`_.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"node_trace"``, ``"edge_trace"`` and ``"layout"``, and the
-            corresponding values should be dictionaries containing keyword
-            arguments as would be fed to the :meth:`update_traces` and
-            :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        box : :class:`ipywidgets.VBox` object
-            A box containing the following widgets: parameters of the
-            clustering algorithm, parameters for the covering scheme, a Mapper
-            graph arising from those parameters, a validation box, and logs.
-
-        """
-        # Clone pipeline to avoid side effects from in-place parameter changes
-        if self.clone_pipeline:
-            self._pipeline = clone(self.pipeline)
-        else:
-            self._pipeline = self.pipeline
-
-        def get_widgets_per_param(params):
-            for key, value in params.items():
-                style = {'description_width': 'initial'}
-                description = key.split("__")[1] if "__" in key else key
-                if isinstance(value, float):
-                    yield (key, widgets.FloatText(
-                        value=value,
-                        step=0.05,
-                        description=description,
-                        continuous_update=False,
-                        disabled=False,
-                        layout=Layout(width="90%"),
-                        style=style
-                    ))
-                elif isinstance(value, bool):
-                    yield (key, widgets.ToggleButton(
-                        value=value,
-                        description=description,
-                        disabled=False,
-                        layout=Layout(width="90%"),
-                        style=style
-                    ))
-                elif isinstance(value, int):
-                    yield (key, widgets.IntText(
-                        value=value,
-                        step=1,
-                        description=description,
-                        continuous_update=False,
-                        disabled=False,
-                        layout=Layout(width="90%"),
-                        style=style
-                    ))
-                elif isinstance(value, str):
-                    yield (key, widgets.Text(
-                        value=value,
-                        description=description,
-                        continuous_update=False,
-                        disabled=False,
-                        layout=Layout(width="90%"),
-                        style=style
-                    ))
-
-        def on_parameter_change(change):
-            handler.clear_logs()
-            try:
-                for param, value in cover_params.items():
-                    if isinstance(value, (int, float, str)):
-                        self._pipeline.set_params(
-                            **{param: cover_params_widgets[param].value}
-                        )
-                for param, value in cluster_params.items():
-                    if isinstance(value, (int, float, str)):
-                        self._pipeline.set_params(
-                            **{param: cluster_params_widgets[param].value}
-                        )
-                for param, value in nerve_params.items():
-                    if isinstance(value, (int, bool)):
-                        self._pipeline.set_params(
-                            **{param: nerve_params_widgets[param].value}
-                        )
-
-                logger.info("Updating figure...")
-                with self._figure.batch_update():
-                    self._graph = self._pipeline.fit_transform(self.data)
-                    (edge_trace, node_trace,
-                     self._node_colors_color_features) = \
-                        _calculate_graph_data(
-                            self._graph, self._color_data_transformed,
-                            node_color_statistic, layout, layout_dim,
-                            n_sig_figs, node_scale
-                        )
-                    if colorscale_for_hoverlabel is not None:
-                        min_col, max_col = \
-                            np.min(self._node_colors_color_features[:, 0]), \
-                            np.max(self._node_colors_color_features[:, 0])
-                        hoverlabel_bgcolor = _get_colors_for_vals(
-                            self._node_colors_color_features[:, 0],
-                            min_col, max_col, colorscale_for_hoverlabel
-                            )
-                        self._figure.update_traces(
-                            hoverlabel_bgcolor=hoverlabel_bgcolor,
-                            selector={"name": "node_trace"}
-                            )
-
-                    self._figure.update_traces(
-                        x=node_trace.x,
-                        y=node_trace.y,
-                        marker_color=node_trace.marker.color,
-                        marker_size=node_trace.marker.size,
-                        marker_sizeref=node_trace.marker.sizeref,
-                        hovertext=node_trace.hovertext,
-                        **({"z": node_trace.z} if layout_dim == 3 else dict()),
-                        selector={"name": "node_trace"}
-                    )
-                    self._figure.update_traces(
-                        x=edge_trace.x,
-                        y=edge_trace.y,
-                        **({"z": edge_trace.z} if layout_dim == 3 else dict()),
-                        selector={"name": "edge_trace"}
-                    )
-
-                    # Update color by column buttons if relevant
-                    if self._node_colors_color_features.shape[1] > 1:
-                        hovertext_color_features = node_trace.hovertext
-                        column_color_buttons = _get_column_color_buttons(
-                            self._node_colors_color_features,
-                            hovertext_color_features,
-                            colorscale_for_hoverlabel, n_sig_figs,
-                            column_names_dropdown
-                        )
-
-                        button_height = 1.1
-                        self._figure.update_layout(
-                            updatemenus=[
-                                go.layout.Updatemenu(
-                                    buttons=column_color_buttons,
-                                    direction="down",
-                                    pad={"r": 10, "t": 10},
-                                    showactive=True,
-                                    x=0.11,
-                                    xanchor="left",
-                                    y=button_height,
-                                    yanchor="top"
-                                )
-                            ])
-
-                valid.value = True
-            except Exception:
-                exception_data = traceback.format_exc().splitlines()
-                logger.exception(exception_data[-1])
-                valid.value = False
-
-        def observe_widgets(params, widgets):
-            for param, value in params.items():
-                if isinstance(value, (int, float, str)):
-                    widgets[param].observe(on_parameter_change, names="value")
-
-        # Define output widget to capture logs
-        out = widgets.Output()
-
-        @out.capture()
-        def click_box(change):
-            if logs_box.value:
-                out.clear_output()
-                handler.show_logs()
-            else:
-                out.clear_output()
-
-        # Initialise logging
-        logger = logging.getLogger(__name__)
-        handler = OutputWidgetHandler()
-        handler.setFormatter(logging.Formatter(
-            "%(asctime)s - [%(levelname)s] %(message)s"))
-        logger.addHandler(handler)
-        logger.setLevel(logging.INFO)
-
-        # Initialise cover, cluster and nerve dictionaries of parameters and
-        # widgets
-        mapper_params_items = self._pipeline.get_mapper_params().items()
-        cover_params = {key: value for key, value in mapper_params_items
-                        if key.startswith("cover__")}
-        cover_params_widgets = dict(get_widgets_per_param(cover_params))
-        cluster_params = {key: value for key, value in mapper_params_items
-                          if key.startswith("clusterer__")}
-        cluster_params_widgets = dict(get_widgets_per_param(cluster_params))
-        nerve_params = {key: value for key, value in mapper_params_items
-                        if key in ["min_intersection", "contract_nodes"]}
-        nerve_params_widgets = dict(get_widgets_per_param(nerve_params))
-
-        # Initialise widgets for validating input parameters of pipeline
-        valid = widgets.Valid(
-            value=True,
-            description="Valid parameters",
-            style={"description_width": "100px"},
-        )
-
-        # Initialise widget for showing the logs
-        logs_box = widgets.Checkbox(
-            description="Show logs: ",
-            value=False,
-            indent=False
-        )
-
-        # Initialise figure with initial pipeline and config
-        self._graph = self._pipeline.fit_transform(self.data)
-        (self._color_data_transformed, column_names_dropdown,
-         node_color_statistic) = \
-            _validate_color_kwargs(self._graph, self.data, color_data,
-                                   color_features, node_color_statistic,
-                                   interactive=True)
-        edge_trace, node_trace, self._node_colors_color_features = \
-            _calculate_graph_data(
-                self._graph, self._color_data_transformed,
-                node_color_statistic, layout, layout_dim, n_sig_figs,
-                node_scale
-            )
-
-        self._figure = _produce_static_figure(
-            edge_trace, node_trace, self._node_colors_color_features,
-            column_names_dropdown, layout_dim, n_sig_figs, plotly_params
-        )
-
-        colorscale_for_hoverlabel = None
-        if layout_dim == 3:
-            # In plot_static_mapper_graph, hoverlabel bgcolors are set to white
-            # if something goes wrong in computing them according to the
-            # colorscale.
-            is_bgcolor_not_white = \
-                self._figure.data[1].hoverlabel.bgcolor != "white"
-            user_hoverlabel_bgcolor = False
-            if plotly_params:
-                if "node_trace" in plotly_params:
-                    if "hoverlabel_bgcolor" in plotly_params["node_trace"]:
-                        user_hoverlabel_bgcolor = True
-            if is_bgcolor_not_white and not user_hoverlabel_bgcolor:
-                colorscale_for_hoverlabel = \
-                    self._figure.data[1].marker.colorscale
-
-        observe_widgets(cover_params, cover_params_widgets)
-        observe_widgets(cluster_params, cluster_params_widgets)
-        observe_widgets(nerve_params, nerve_params_widgets)
-
-        logs_box.observe(click_box, names="value")
-
-        # Define containers for input widgets
-        cover_title = HTML(value="<b>Cover parameters</b>")
-        container_cover = widgets.VBox(
-            children=[cover_title] + list(cover_params_widgets.values())
-        )
-        container_cover.layout.align_items = 'center'
-
-        cluster_title = HTML(value="<b>Clusterer parameters</b>")
-        container_cluster = widgets.VBox(
-            children=[cluster_title] + list(cluster_params_widgets.values()),
-        )
-        container_cluster.layout.align_items = 'center'
-
-        nerve_title = HTML(value="<b>Nerve parameters</b>")
-        container_nerve = widgets.VBox(
-            children=[nerve_title] + list(nerve_params_widgets.values()),
-        )
-        container_nerve.layout.align_items = 'center'
-
-        container_parameters = widgets.HBox(
-            children=[container_cover, container_cluster, container_nerve]
-        )
-
-        box = widgets.VBox([container_parameters, self._figure, valid,
-                            logs_box, out])
-
-        return box
-
-    @property
-    def graph_(self):
-        return self._graph
-
-    @property
-    def pipeline_(self):
-        return self._pipeline
-
-    @property
-    def color_features_(self):
-        return self._color_data_transformed
-
-    @property
-    def node_summaries_(self):
-        return self._node_colors_color_features
-
-    @property
-    def figure_(self):
-        return self._figure
+"""Static and interactive visualisation functions for Mapper graphs."""
+# License: GNU AGPLv3
+
+import logging
+import traceback
+
+import numpy as np
+import plotly.graph_objects as go
+from ipywidgets import widgets, Layout, HTML
+from sklearn.base import clone
+
+from .utils._logging import OutputWidgetHandler
+from .utils._visualization import (
+    _validate_color_kwargs,
+    _calculate_graph_data,
+    _produce_static_figure,
+    _get_column_color_buttons,
+    _get_colors_for_vals,
+)
+
+
+def plot_static_mapper_graph(
+        pipeline, data, color_data=None, color_features=None,
+        node_color_statistic=None, layout="kamada_kawai", layout_dim=2,
+        clone_pipeline=True, n_sig_figs=3, node_scale=12, plotly_params=None
+        ):
+    """Plot Mapper graphs without interactivity on pipeline parameters.
+
+    The output graph is a rendition of the :class:`igraph.Graph` object
+    computed by calling the :meth:`fit_transform` method of the
+    :class:`~gtda.mapper.pipeline.MapperPipeline` instance `pipeline` on the
+    input `data`. The graph's nodes correspond to subsets of elements (rows) in
+    `data`; these subsets are clusters in larger portions of `data` called
+    "pullback (cover) sets", which are computed by means of the `pipeline`'s
+    "filter function" and "cover" and correspond to the differently-colored
+    portions in `this diagram <../../../../_images/mapper_pipeline.svg>`_.
+    Two clusters from different pullback cover sets can overlap; if they do, an
+    edge between the corresponding nodes in the graph may be drawn.
+
+    Nodes are colored according to `color_features` and `node_color_statistic`
+    and are sized according to the number of elements they represent. The
+    hovertext on each node displays, in this order:
+
+        - a globally unique ID for the node, which can be used to retrieve
+          node information from the :class:`igraph.Graph` object, see
+          :class:`~gtda.mapper.nerve.Nerve`;
+        - the label of the pullback (cover) set which the node's elements
+          form a cluster in;
+        - a label identifying the node as a cluster within that pullback set;
+        - the number of elements of `data` associated with the node;
+        - the value of the summary statistic which determines the node's color.
+
+    Parameters
+    ----------
+    pipeline : :class:`~gtda.mapper.pipeline.MapperPipeline` object
+        Mapper pipeline to act onto data.
+
+    data : array-like of shape (n_samples, n_features)
+        Data used to generate the Mapper graph. Can be a pandas dataframe.
+
+    color_data : array-like of length n_samples, or None, optional, \
+        default: ``None``
+        Data to be used to construct node colors in the Mapper graph (according
+        to `color_features` and `node_color_statistic`). Must have the same
+        length as `data`. ``None`` is the same as passing
+        ``numpy.arange(len(data))``.
+
+    color_features : object or None, optional, default: ``None``
+        Specifies one or more feature of interest from `color_data` to be used,
+        together with `node_color_statistic`, to determine node colors. Ignored
+        if `node_color_statistic` is a numpy array.
+
+            1. ``None`` is equivalent to passing `color_data`.
+            2. If an object implementing :meth:`transform` or
+               :meth:`fit_transform`, or a callable, it is applied to
+               `color_data` to generate the features of interest.
+            3. If an index or string, or list of indices/strings, it is
+               equivalent to selecting a column or subset of columns from
+               `color_data`.
+
+    node_color_statistic : None, callable, or ndarray of shape (n_nodes,) or \
+        (n_nodes, 1), optional, default: ``None``
+        If a callable, node colors will be computed as summary statistics from
+        the feature array ``y`` determined by `color_data` and
+        `color_features`. Let ``y`` have ``n`` columns (note: 1d feature arrays
+        are converted to column vectors). Then, for a node representing a list
+        ``I`` of row indices, there will be ``n`` colors, each computed as
+        ``node_color_statistic(y[I, i])`` for ``i`` between ``0`` and ``n``.
+        ``None`` is equivalent to passing :func:`numpy.mean`. If a numpy array,
+        it must have the same length as the number of nodes in the Mapper graph
+        and its values are used directly as node colors (`color_features` is
+        ignored).
+
+    layout : None, str or callable, optional, default: ``"kamada-kawai"``
+        Layout algorithm for the graph. Can be any accepted value for the
+        ``layout`` parameter in the :meth:`layout` method of
+        :class:`igraph.Graph` [1]_.
+
+    layout_dim : int, default: ``2``
+        The number of dimensions for the layout. Can be 2 or 3.
+
+    clone_pipeline : bool, optional, default: ``True``
+        If ``True``, the input `pipeline` is cloned before computing the
+        Mapper graph to prevent unexpected side effects from in-place
+        parameter updates.
+
+    n_sig_figs : int or None, optional, default: ``3``
+       If not ``None``, number of significant figures to which to round node
+       summary statistics. If ``None``, no rounding is performed.
+
+    node_scale : int or float, optional, default: ``12``
+        Sets the scale factor used to determine the rendered size of the
+        nodes. Increase for larger nodes. Implements a formula in the
+        `Plotly documentation \
+        <https://plotly.com/python/bubble-charts/#scaling-the-size-of-bubble\
+        -charts>`_.
+
+    plotly_params : dict or None, optional, default: ``None``
+        Custom parameters to configure the plotly figure. Allowed keys are
+        ``"node_trace"``, ``"edge_trace"`` and ``"layout"``, and the
+        corresponding values should be dictionaries containing keyword
+        arguments as would be fed to the :meth:`update_traces` and
+        :meth:`update_layout` methods of :class:`plotly.graph_objects.Figure`.
+
+    Returns
+    -------
+    fig : :class:`plotly.graph_objects.FigureWidget` object
+        Figure representing the Mapper graph with appropriate node colouring
+        and size.
+
+    Examples
+    --------
+    Setting a colorscale different from the default one:
+
+    >>> import numpy as np
+    >>> np.random.seed(1)
+    >>> from gtda.mapper import make_mapper_pipeline, plot_static_mapper_graph
+    >>> pipeline = make_mapper_pipeline()
+    >>> data = np.random.random((100, 3))
+    >>> plotly_params = {"node_trace": {"marker_colorscale": "Blues"}}
+    >>> fig = plot_static_mapper_graph(pipeline, data,
+    ...                                plotly_params=plotly_params)
+
+    Inspect the composition of a node with "Node ID" displayed as 0 in the
+    hovertext:
+
+    >>> graph = pipeline.fit_transform(data)
+    >>> graph.vs[0]["node_elements"]
+    array([70])
+
+    Write the figure to a file using Plotly:
+    >>> fname = "current_figure"
+    >>> fig.write_html(fname + ".html")
+    >>> fig.write_image(fname + ".svg")  # Requires psutil
+
+    See also
+    --------
+    MapperInteractivePlotter, plot_interactive_mapper_graph, \
+    gtda.mapper.make_mapper_pipeline
+
+    References
+    ----------
+    .. [1] `igraph.Graph.layout
+            <https://igraph.org/python/doc/igraph.Graph-class.html#layout>`_
+            documentation.
+
+    """
+
+    # Compute the graph and fetch the indices of points in each node
+    _pipeline = clone(pipeline) if clone_pipeline else pipeline
+
+    graph = _pipeline.fit_transform(data)
+    (color_data_transformed, column_names_dropdown,
+     node_color_statistic) = \
+        _validate_color_kwargs(graph, data, color_data, color_features,
+                               node_color_statistic, interactive=False)
+    edge_trace, node_trace, node_colors_color_features = \
+        _calculate_graph_data(
+            graph, color_data_transformed, node_color_statistic, layout,
+            layout_dim, n_sig_figs, node_scale
+            )
+
+    figure = _produce_static_figure(
+        edge_trace, node_trace, node_colors_color_features,
+        column_names_dropdown, layout_dim, n_sig_figs, plotly_params
+        )
+
+    return figure
+
+
+def plot_interactive_mapper_graph(
+        pipeline, data, color_data=None, color_features=None,
+        node_color_statistic=None, layout="kamada_kawai", layout_dim=2,
+        clone_pipeline=True, n_sig_figs=3, node_scale=12, plotly_params=None
+        ):
+    """*As of version 0.5.0, we recommend using the object-oriented interface
+    provided by :class:`MapperInteractivePlotter` instead of this function.*
+
+    Plot Mapper graphs in a Jupyter session, with interactivity on pipeline
+    parameters.
+
+    Extends :func:`~gtda.mapper.visualization.plot_static_mapper_graph` by
+    providing functionality to interactively update parameters from the cover,
+    clustering and graph construction steps defined in `pipeline`.
+
+    Parameters
+    ----------
+    pipeline : :class:`~gtda.mapper.pipeline.MapperPipeline` object
+        Mapper pipeline to act on to data.
+
+    data : array-like of shape (n_samples, n_features)
+        Data used to generate the Mapper graph. Can be a pandas dataframe.
+
+    color_data : array-like of length n_samples, or None, optional, \
+        default: ``None``
+        Data to be used to construct node colors in the Mapper graph (according
+        to `color_features` and `node_color_statistic`). Must have the same
+        length as `data`. ``None`` is the same as passing
+        ``numpy.arange(len(data))``.
+
+    color_features : object or None, optional, default: ``None``
+        Specifies one or more feature of interest from `color_data` to be used,
+        together with `node_color_statistic`, to determine node colors.
+
+            1. ``None`` is equivalent to passing `color_data`.
+            2. If an object implementing :meth:`transform` or
+               :meth:`fit_transform`, or a callable, it is applied to
+               `color_data` to generate the features of interest.
+            3. If an index or string, or list of indices/strings, it is
+               equivalent to selecting a column or subset of columns from
+               `color_data`.
+
+    node_color_statistic : None or callable, optional, default: ``None``
+        If a callable, node colors will be computed as summary statistics from
+        the feature array ``y`` determined by `color_data` and
+        `color_features`. Let ``y`` have ``n`` columns (note: 1d feature arrays
+        are converted to column vectors). Then, for a node representing a list
+        ``I`` of row indices, there will be ``n`` colors, each computed as
+        ``node_color_statistic(y[I, i])`` for ``i`` between ``0`` and ``n``.
+        ``None`` is equivalent to passing :func:`numpy.mean`.
+
+    layout : None, str or callable, optional, default: ``"kamada-kawai"``
+        Layout algorithm for the graph. Can be any accepted value for the
+        ``layout`` parameter in the :meth:`layout` method of
+        :class:`igraph.Graph` [1]_.
+
+    layout_dim : int, default: ``2``
+        The number of dimensions for the layout. Can be 2 or 3.
+
+    clone_pipeline : bool, optional, default: ``True``
+        If ``True``, the input `pipeline` is cloned before computing the
+        Mapper graph to prevent unexpected side effects from in-place
+        parameter updates.
+
+    n_sig_figs : int or None, optional, default: ``3``
+       If not ``None``, number of significant figures to which to round node
+       summary statistics. If ``None``, no rounding is performed.
+
+    node_scale : int or float, optional, default: ``12``
+        Sets the scale factor used to determine the rendered size of the
+        nodes. Increase for larger nodes. Implements a formula in the
+        `Plotly documentation \
+        <plotly.com/python/bubble-charts/#scaling-the-size-of-bubble-charts>`_.
+
+    plotly_params : dict or None, optional, default: ``None``
+        Custom parameters to configure the plotly figure. Allowed keys are
+        ``"node_trace"``, ``"edge_trace"`` and ``"layout"``, and the
+        corresponding values should be dictionaries containing keyword
+        arguments as would be fed to the :meth:`update_traces` and
+        :meth:`update_layout` methods of :class:`plotly.graph_objects.Figure`.
+
+    Returns
+    -------
+    box : :class:`ipywidgets.VBox` object
+        A box containing the following widgets: parameters of the clustering
+        algorithm, parameters for the covering scheme, a Mapper graph arising
+        from those parameters, a validation box, and logs.
+
+    See also
+    --------
+    MapperInteractivePlotter, plot_static_mapper_graph, \
+    gtda.mapper.pipeline.make_mapper_pipeline
+
+    References
+    ----------
+    .. [1] `igraph.Graph.layout
+            <https://igraph.org/python/doc/igraph.Graph-class.html#layout>`_
+            documentation.
+
+    """
+
+    plotter = MapperInteractivePlotter(pipeline, data, clone_pipeline)
+
+    return plotter.plot(
+        color_data=color_data, color_features=color_features,
+        node_color_statistic=node_color_statistic, layout=layout,
+        layout_dim=layout_dim, n_sig_figs=n_sig_figs, node_scale=node_scale,
+        plotly_params=plotly_params
+        )
+
+
+class MapperInteractivePlotter:
+    """Plot Mapper graphs in a Jupyter session, with interactivity on pipeline
+    parameters.
+
+    Provides functionality to interactively update parameters from the cover,
+    clustering and graph construction steps defined in `pipeline`.
+    An interactive widget is produced when calling :meth:`plot`. After
+    interacting with the widget, the current state of all outputs which may
+    have been altered can be retrieved via one of the attributes listed below.
+
+    Parameters
+    ----------
+    pipeline : :class:`~gtda.mapper.pipeline.MapperPipeline` object
+        Mapper pipeline to act on to data.
+
+    data : array-like of shape (n_samples, n_features)
+        Data used to generate the Mapper graph. Can be a pandas dataframe.
+
+    clone_pipeline : bool, optional, default: ``True``
+        If ``True``, the input `pipeline` is cloned before computing the
+        Mapper graph to prevent unexpected side effects from in-place
+        parameter updates.
+
+    Attributes
+    ----------
+    graph_ : :class:`igraph.Graph` object
+        Current state of the graph displayed by the widget.
+
+    pipeline_ : :class:`~gtda.mapper.pipeline.MapperPipeline` object
+        Current state of the Mapper pipeline.
+
+    color_features_ : array-like of shape (n_samples, n_features)
+        Values of the features of interest for each entry in `data`, as
+        produced according to `color_data` and `color_features` when calling
+        :meth:`plot`. Not changed by interacting with the widget.
+
+    node_summaries_ : array-like of shape (n_nodes, n_features)
+        Current values of the summaries computed for each node and used as
+        node colours in the figure. Produced according to
+        `node_color_statistic`, see :meth:`plot`.
+
+    figure_ : :class:`plotly.graph_objects.FigureWidget` object
+        Current figure representing the Mapper graph with appropriate node
+        colouring and size.
+
+    Examples
+    --------
+    Instantiate the plotter object on a pipeline and data configuration, and
+    call :meth:`plot` to display the widget in a Jupyter session:
+
+    >>> import numpy as np
+    >>> np.random.seed(1)
+    >>> from gtda.mapper import make_mapper_pipeline, MapperInteractivePlotter
+    >>> pipeline = make_mapper_pipeline()
+    >>> data = np.random.random((100, 3))
+    >>> plotter = MapperInteractivePlotter(pipeline, data)
+    >>> plotter.plot()
+
+    After interacting with the widget, inspect the composition of a node with
+    "Node ID" displayed as 0 in the hovertext:
+
+    >>> plotter.graph_.vs[0]["node_elements"]
+    array([70])
+
+    Write the current figure to a file using Plotly:
+    >>> fname = "current_figure"
+    >>> plotter.fig_.write_html(fname + ".html")
+    >>> plotter.fig_.write_image(fname + ".svg")  # Requires psutil
+
+    See also
+    --------
+    plot_interactive_mapper_graph, plot_static_mapper_graph, \
+    gtda.mapper.pipeline.make_mapper_pipeline
+
+    References
+    ----------
+    .. [1] `igraph.Graph.layout
+            <https://igraph.org/python/doc/igraph.Graph-class.html#layout>`_
+            documentation.
+
+    """
+
+    def __init__(self, pipeline, data, clone_pipeline=True):
+        self.pipeline = pipeline
+        self.data = data
+        self.clone_pipeline = clone_pipeline
+
+    def plot(self, color_data=None, color_features=None,
+             node_color_statistic=None, layout="kamada_kawai", layout_dim=2,
+             n_sig_figs=3, node_scale=12, plotly_params=None):
+        """ Produce the interactive Mapper widget.
+
+        Parameters
+        ----------
+        color_data : array-like of length n_samples, or None, optional, \
+            default: ``None``
+            Data to be used to construct node colors in the Mapper graph
+            (according to `color_features` and `node_color_statistic`). Must
+            have the same length as `data`. ``None`` is the same as passing
+            ``numpy.arange(len(data))``.
+
+        color_features : object or None, optional, default: ``None``
+            Specifies one or more feature of interest from `color_data` to be
+            used, together with `node_color_statistic`, to determine node
+            colors.
+
+                1. ``None`` is equivalent to passing `color_data`.
+                2. If an object implementing :meth:`transform` or
+                   :meth:`fit_transform`, or a callable, it is applied to
+                   `color_data` to generate the features of interest.
+                3. If an index or string, or list of indices/strings, it is
+                   equivalent to selecting a column or subset of columns from
+                   `color_data`.
+
+        node_color_statistic : None or callable, optional, default: ``None``
+            If a callable, node colors will be computed as summary statistics
+            from the feature array ``y`` determined by `color_data` and
+            `color_features`. Let ``y`` have ``n`` columns (note: 1d feature
+            arrays are converted to column vectors). Then, for a node
+            representing a list ``I`` of row indices, there will be ``n``
+            colors, each computed as ``node_color_statistic(y[I, i])`` for
+            ``i`` between ``0`` and ``n``.
+
+        layout : None, str or callable, optional, default: ``"kamada-kawai"``
+            Layout algorithm for the graph. Can be any accepted value for the
+            ``layout`` parameter in the :meth:`layout` method of
+            :class:`igraph.Graph` [1]_.
+
+        layout_dim : int, default: ``2``
+            The number of dimensions for the layout. Can be 2 or 3.
+
+        n_sig_figs : int or None, optional, default: ``3``
+           If not ``None``, number of significant figures to which to round
+           node summary statistics. If ``None``, no rounding is performed.
+
+        node_scale : int or float, optional, default: ``12``
+            Sets the scale factor used to determine the rendered size of the
+            nodes. Increase for larger nodes. Implements a formula in the
+            `Plotly documentation \
+            <plotly.com/python/bubble-charts/#scaling-the-size-of-bubble-charts>`_.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"node_trace"``, ``"edge_trace"`` and ``"layout"``, and the
+            corresponding values should be dictionaries containing keyword
+            arguments as would be fed to the :meth:`update_traces` and
+            :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        box : :class:`ipywidgets.VBox` object
+            A box containing the following widgets: parameters of the
+            clustering algorithm, parameters for the covering scheme, a Mapper
+            graph arising from those parameters, a validation box, and logs.
+
+        """
+        # Clone pipeline to avoid side effects from in-place parameter changes
+        if self.clone_pipeline:
+            self._pipeline = clone(self.pipeline)
+        else:
+            self._pipeline = self.pipeline
+
+        def get_widgets_per_param(params):
+            for key, value in params.items():
+                style = {'description_width': 'initial'}
+                description = key.split("__")[1] if "__" in key else key
+                if isinstance(value, float):
+                    yield (key, widgets.FloatText(
+                        value=value,
+                        step=0.05,
+                        description=description,
+                        continuous_update=False,
+                        disabled=False,
+                        layout=Layout(width="90%"),
+                        style=style
+                    ))
+                elif isinstance(value, bool):
+                    yield (key, widgets.ToggleButton(
+                        value=value,
+                        description=description,
+                        disabled=False,
+                        layout=Layout(width="90%"),
+                        style=style
+                    ))
+                elif isinstance(value, int):
+                    yield (key, widgets.IntText(
+                        value=value,
+                        step=1,
+                        description=description,
+                        continuous_update=False,
+                        disabled=False,
+                        layout=Layout(width="90%"),
+                        style=style
+                    ))
+                elif isinstance(value, str):
+                    yield (key, widgets.Text(
+                        value=value,
+                        description=description,
+                        continuous_update=False,
+                        disabled=False,
+                        layout=Layout(width="90%"),
+                        style=style
+                    ))
+
+        def on_parameter_change(change):
+            handler.clear_logs()
+            try:
+                for param, value in cover_params.items():
+                    if isinstance(value, (int, float, str)):
+                        self._pipeline.set_params(
+                            **{param: cover_params_widgets[param].value}
+                        )
+                for param, value in cluster_params.items():
+                    if isinstance(value, (int, float, str)):
+                        self._pipeline.set_params(
+                            **{param: cluster_params_widgets[param].value}
+                        )
+                for param, value in nerve_params.items():
+                    if isinstance(value, (int, bool)):
+                        self._pipeline.set_params(
+                            **{param: nerve_params_widgets[param].value}
+                        )
+
+                logger.info("Updating figure...")
+                with self._figure.batch_update():
+                    self._graph = self._pipeline.fit_transform(self.data)
+                    (edge_trace, node_trace,
+                     self._node_colors_color_features) = \
+                        _calculate_graph_data(
+                            self._graph, self._color_data_transformed,
+                            node_color_statistic, layout, layout_dim,
+                            n_sig_figs, node_scale
+                        )
+                    if colorscale_for_hoverlabel is not None:
+                        min_col, max_col = \
+                            np.min(self._node_colors_color_features[:, 0]), \
+                            np.max(self._node_colors_color_features[:, 0])
+                        hoverlabel_bgcolor = _get_colors_for_vals(
+                            self._node_colors_color_features[:, 0],
+                            min_col, max_col, colorscale_for_hoverlabel
+                            )
+                        self._figure.update_traces(
+                            hoverlabel_bgcolor=hoverlabel_bgcolor,
+                            selector={"name": "node_trace"}
+                            )
+
+                    self._figure.update_traces(
+                        x=node_trace.x,
+                        y=node_trace.y,
+                        marker_color=node_trace.marker.color,
+                        marker_size=node_trace.marker.size,
+                        marker_sizeref=node_trace.marker.sizeref,
+                        hovertext=node_trace.hovertext,
+                        **({"z": node_trace.z} if layout_dim == 3 else dict()),
+                        selector={"name": "node_trace"}
+                    )
+                    self._figure.update_traces(
+                        x=edge_trace.x,
+                        y=edge_trace.y,
+                        **({"z": edge_trace.z} if layout_dim == 3 else dict()),
+                        selector={"name": "edge_trace"}
+                    )
+
+                    # Update color by column buttons if relevant
+                    if self._node_colors_color_features.shape[1] > 1:
+                        hovertext_color_features = node_trace.hovertext
+                        column_color_buttons = _get_column_color_buttons(
+                            self._node_colors_color_features,
+                            hovertext_color_features,
+                            colorscale_for_hoverlabel, n_sig_figs,
+                            column_names_dropdown
+                        )
+
+                        button_height = 1.1
+                        self._figure.update_layout(
+                            updatemenus=[
+                                go.layout.Updatemenu(
+                                    buttons=column_color_buttons,
+                                    direction="down",
+                                    pad={"r": 10, "t": 10},
+                                    showactive=True,
+                                    x=0.11,
+                                    xanchor="left",
+                                    y=button_height,
+                                    yanchor="top"
+                                )
+                            ])
+
+                valid.value = True
+            except Exception:
+                exception_data = traceback.format_exc().splitlines()
+                logger.exception(exception_data[-1])
+                valid.value = False
+
+        def observe_widgets(params, widgets):
+            for param, value in params.items():
+                if isinstance(value, (int, float, str)):
+                    widgets[param].observe(on_parameter_change, names="value")
+
+        # Define output widget to capture logs
+        out = widgets.Output()
+
+        @out.capture()
+        def click_box(change):
+            if logs_box.value:
+                out.clear_output()
+                handler.show_logs()
+            else:
+                out.clear_output()
+
+        # Initialise logging
+        logger = logging.getLogger(__name__)
+        handler = OutputWidgetHandler()
+        handler.setFormatter(logging.Formatter(
+            "%(asctime)s - [%(levelname)s] %(message)s"))
+        logger.addHandler(handler)
+        logger.setLevel(logging.INFO)
+
+        # Initialise cover, cluster and nerve dictionaries of parameters and
+        # widgets
+        mapper_params_items = self._pipeline.get_mapper_params().items()
+        cover_params = {key: value for key, value in mapper_params_items
+                        if key.startswith("cover__")}
+        cover_params_widgets = dict(get_widgets_per_param(cover_params))
+        cluster_params = {key: value for key, value in mapper_params_items
+                          if key.startswith("clusterer__")}
+        cluster_params_widgets = dict(get_widgets_per_param(cluster_params))
+        nerve_params = {key: value for key, value in mapper_params_items
+                        if key in ["min_intersection", "contract_nodes"]}
+        nerve_params_widgets = dict(get_widgets_per_param(nerve_params))
+
+        # Initialise widgets for validating input parameters of pipeline
+        valid = widgets.Valid(
+            value=True,
+            description="Valid parameters",
+            style={"description_width": "100px"},
+        )
+
+        # Initialise widget for showing the logs
+        logs_box = widgets.Checkbox(
+            description="Show logs: ",
+            value=False,
+            indent=False
+        )
+
+        # Initialise figure with initial pipeline and config
+        self._graph = self._pipeline.fit_transform(self.data)
+        (self._color_data_transformed, column_names_dropdown,
+         node_color_statistic) = \
+            _validate_color_kwargs(self._graph, self.data, color_data,
+                                   color_features, node_color_statistic,
+                                   interactive=True)
+        edge_trace, node_trace, self._node_colors_color_features = \
+            _calculate_graph_data(
+                self._graph, self._color_data_transformed,
+                node_color_statistic, layout, layout_dim, n_sig_figs,
+                node_scale
+            )
+
+        self._figure = _produce_static_figure(
+            edge_trace, node_trace, self._node_colors_color_features,
+            column_names_dropdown, layout_dim, n_sig_figs, plotly_params
+        )
+
+        colorscale_for_hoverlabel = None
+        if layout_dim == 3:
+            # In plot_static_mapper_graph, hoverlabel bgcolors are set to white
+            # if something goes wrong in computing them according to the
+            # colorscale.
+            is_bgcolor_not_white = \
+                self._figure.data[1].hoverlabel.bgcolor != "white"
+            user_hoverlabel_bgcolor = False
+            if plotly_params:
+                if "node_trace" in plotly_params:
+                    if "hoverlabel_bgcolor" in plotly_params["node_trace"]:
+                        user_hoverlabel_bgcolor = True
+            if is_bgcolor_not_white and not user_hoverlabel_bgcolor:
+                colorscale_for_hoverlabel = \
+                    self._figure.data[1].marker.colorscale
+
+        observe_widgets(cover_params, cover_params_widgets)
+        observe_widgets(cluster_params, cluster_params_widgets)
+        observe_widgets(nerve_params, nerve_params_widgets)
+
+        logs_box.observe(click_box, names="value")
+
+        # Define containers for input widgets
+        cover_title = HTML(value="<b>Cover parameters</b>")
+        container_cover = widgets.VBox(
+            children=[cover_title] + list(cover_params_widgets.values())
+        )
+        container_cover.layout.align_items = 'center'
+
+        cluster_title = HTML(value="<b>Clusterer parameters</b>")
+        container_cluster = widgets.VBox(
+            children=[cluster_title] + list(cluster_params_widgets.values()),
+        )
+        container_cluster.layout.align_items = 'center'
+
+        nerve_title = HTML(value="<b>Nerve parameters</b>")
+        container_nerve = widgets.VBox(
+            children=[nerve_title] + list(nerve_params_widgets.values()),
+        )
+        container_nerve.layout.align_items = 'center'
+
+        container_parameters = widgets.HBox(
+            children=[container_cover, container_cluster, container_nerve]
+        )
+
+        box = widgets.VBox([container_parameters, self._figure, valid,
+                            logs_box, out])
+
+        return box
+
+    @property
+    def graph_(self):
+        return self._graph
+
+    @property
+    def pipeline_(self):
+        return self._pipeline
+
+    @property
+    def color_features_(self):
+        return self._color_data_transformed
+
+    @property
+    def node_summaries_(self):
+        return self._node_colors_color_features
+
+    @property
+    def figure_(self):
+        return self._figure
```

## gtda/mapper/__init__.py

 * *Ordering differences only*

```diff
@@ -1,30 +1,30 @@
-"""The module :mod:`gtda.mapper` implements the Mapper algorithm for
-topological clustering and visualisation."""
-
-from .cluster import FirstHistogramGap, FirstSimpleGap, ParallelClustering
-from .cover import CubicalCover, OneDimensionalCover
-from .filter import Eccentricity, Entropy, Projection
-from .nerve import Nerve
-from .pipeline import make_mapper_pipeline
-from .utils.decorators import method_to_transform
-from .utils.pipeline import transformer_from_callable_on_rows
-from .visualization import plot_static_mapper_graph, \
-    plot_interactive_mapper_graph, MapperInteractivePlotter
-
-__all__ = [
-    'Projection',
-    'Eccentricity',
-    'Entropy',
-    'OneDimensionalCover',
-    'CubicalCover',
-    'FirstSimpleGap',
-    'FirstHistogramGap',
-    'ParallelClustering',
-    'Nerve',
-    'make_mapper_pipeline',
-    'plot_static_mapper_graph',
-    'plot_interactive_mapper_graph',
-    'MapperInteractivePlotter',
-    'method_to_transform',
-    'transformer_from_callable_on_rows'
-    ]
+"""The module :mod:`gtda.mapper` implements the Mapper algorithm for
+topological clustering and visualisation."""
+
+from .cluster import FirstHistogramGap, FirstSimpleGap, ParallelClustering
+from .cover import CubicalCover, OneDimensionalCover
+from .filter import Eccentricity, Entropy, Projection
+from .nerve import Nerve
+from .pipeline import make_mapper_pipeline
+from .utils.decorators import method_to_transform
+from .utils.pipeline import transformer_from_callable_on_rows
+from .visualization import plot_static_mapper_graph, \
+    plot_interactive_mapper_graph, MapperInteractivePlotter
+
+__all__ = [
+    'Projection',
+    'Eccentricity',
+    'Entropy',
+    'OneDimensionalCover',
+    'CubicalCover',
+    'FirstSimpleGap',
+    'FirstHistogramGap',
+    'ParallelClustering',
+    'Nerve',
+    'make_mapper_pipeline',
+    'plot_static_mapper_graph',
+    'plot_interactive_mapper_graph',
+    'MapperInteractivePlotter',
+    'method_to_transform',
+    'transformer_from_callable_on_rows'
+    ]
```

## gtda/mapper/utils/decorators.py

 * *Ordering differences only*

```diff
@@ -1,72 +1,72 @@
-"""Convenience class decorators for use in a Mapper context."""
-# License: GNU AGPLv3
-
-from sklearn.base import TransformerMixin
-
-
-def method_to_transform(cls, method_name):
-    """Wrap a class to add a :meth:`transform` method as an alias to an
-    existing method.
-
-    An example of use is for classes possessing a :meth:`score` method such as
-    kernel density estimators and anomaly/novelty detection estimators,
-    allow for these estimators are to be used as steps in a pipeline.
-
-    Note that 1D array outputs are reshaped into 2D column vectors before
-    being returned by the new :meth:`transform`.
-
-    Parameters
-    ----------
-    cls : object
-        Class to be wrapped. If `method_name` is not one of its methods,
-        :meth:`transform` always returns ``None``.
-
-    method_name : str
-        Name of the method in `cls` to which :meth:`transform` will be
-        an alias. The fist argument of this method (after ``self``) becomes
-        the ``X`` input for :meth:`transform`.
-
-    Returns
-    -------
-    wrapped_cls : object
-        New class inheriting from :class:`sklearn.base.TransformerMixin`, so
-        that both :meth:`transform` and :meth:`fit_transform` are available.
-        Its name is the name of `cls` prepended with ``'Extended'``.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from sklearn.neighbors import KernelDensity
-    >>> from gtda.mapper import method_to_transform
-    >>> X = np.random.random((100, 2))
-    >>> kde = KernelDensity()
-
-    Extend ``KernelDensity`` to give it a ``transform`` method as an alias
-    of ``score_samples`` (up to output shape). The new class is instantiated
-    with the same parameters as the original one.
-
-    >>> ExtendedKDE = method_to_transform(KernelDensity, 'score_samples')
-    >>> extended_kde = ExtendedKDE()
-    >>> Xt = kde.fit(X).score_samples(X)
-    >>> print(Xt.shape)
-    (100,)
-    >>> Xt_extended = extended_kde.fit_transform(X)
-    >>> print(Xt_extended.shape)
-    (100, 1)
-    >>> np.array_equal(Xt, Xt_extended.flatten())
-    True
-
-    """
-    class ExtendedEstimator(cls, TransformerMixin):
-        def transform(self, X, y=None):
-            has_method = hasattr(self, method_name)
-            if has_method:
-                Xt = getattr(self, method_name)(X)
-                # reshape 1D estimators to have shape (n_samples, 1)
-                if Xt.ndim == 1:
-                    Xt = Xt[:, None]
-                return Xt
-
-    ExtendedEstimator.__name__ = 'Extended' + cls.__name__
-
-    return ExtendedEstimator
+"""Convenience class decorators for use in a Mapper context."""
+# License: GNU AGPLv3
+
+from sklearn.base import TransformerMixin
+
+
+def method_to_transform(cls, method_name):
+    """Wrap a class to add a :meth:`transform` method as an alias to an
+    existing method.
+
+    An example of use is for classes possessing a :meth:`score` method such as
+    kernel density estimators and anomaly/novelty detection estimators,
+    allow for these estimators are to be used as steps in a pipeline.
+
+    Note that 1D array outputs are reshaped into 2D column vectors before
+    being returned by the new :meth:`transform`.
+
+    Parameters
+    ----------
+    cls : object
+        Class to be wrapped. If `method_name` is not one of its methods,
+        :meth:`transform` always returns ``None``.
+
+    method_name : str
+        Name of the method in `cls` to which :meth:`transform` will be
+        an alias. The fist argument of this method (after ``self``) becomes
+        the ``X`` input for :meth:`transform`.
+
+    Returns
+    -------
+    wrapped_cls : object
+        New class inheriting from :class:`sklearn.base.TransformerMixin`, so
+        that both :meth:`transform` and :meth:`fit_transform` are available.
+        Its name is the name of `cls` prepended with ``'Extended'``.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.neighbors import KernelDensity
+    >>> from gtda.mapper import method_to_transform
+    >>> X = np.random.random((100, 2))
+    >>> kde = KernelDensity()
+
+    Extend ``KernelDensity`` to give it a ``transform`` method as an alias
+    of ``score_samples`` (up to output shape). The new class is instantiated
+    with the same parameters as the original one.
+
+    >>> ExtendedKDE = method_to_transform(KernelDensity, 'score_samples')
+    >>> extended_kde = ExtendedKDE()
+    >>> Xt = kde.fit(X).score_samples(X)
+    >>> print(Xt.shape)
+    (100,)
+    >>> Xt_extended = extended_kde.fit_transform(X)
+    >>> print(Xt_extended.shape)
+    (100, 1)
+    >>> np.array_equal(Xt, Xt_extended.flatten())
+    True
+
+    """
+    class ExtendedEstimator(cls, TransformerMixin):
+        def transform(self, X, y=None):
+            has_method = hasattr(self, method_name)
+            if has_method:
+                Xt = getattr(self, method_name)(X)
+                # reshape 1D estimators to have shape (n_samples, 1)
+                if Xt.ndim == 1:
+                    Xt = Xt[:, None]
+                return Xt
+
+    ExtendedEstimator.__name__ = 'Extended' + cls.__name__
+
+    return ExtendedEstimator
```

## gtda/mapper/utils/pipeline.py

 * *Ordering differences only*

```diff
@@ -1,69 +1,69 @@
-"""Utility functions for scikit-learn pipelines."""
-# License: GNU AGPLv3
-
-from functools import partial
-from inspect import signature
-
-import numpy as np
-from sklearn.preprocessing import FunctionTransformer
-
-
-def _make_func_apply_along_axis_1(func):
-    return partial(np.apply_along_axis, func, 1)
-
-
-def _reshape_after_apply(func, arr):
-    res = func(arr)
-    if res.ndim == 1:
-        res = res[:, None]
-    return res
-
-
-def transformer_from_callable_on_rows(func, validate=True):
-    """Construct a transformer from a callable acting on 1D arrays.
-
-    Given a callable which can act on 1D arrays, this function returns a
-    fit-transformer which applies the callable to slices of 2D arrays along
-    axis 1. When possible, the array output by the transformer's
-    :meth:`fit_transform` is two-dimensional.
-
-    Parameters
-    ----------
-    func : callable or None
-        A callable object, or ``None`` which returns the identity transformer.
-
-    validate : bool, optional, default: ``True``
-        Whether the output transformer should implement input validation.
-
-    Returns
-    -------
-    function_transformer : :class:`sklearn.preprocessing.FunctionTransformer` \
-        object
-        Output fit-transformer.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from gtda.mapper import transformer_from_callable_on_rows
-    >>> function_transformer = transformer_from_callable_on_rows(np.sum)
-    >>> X = np.array([[0, 1], [2, 3]])
-    >>> print(function_transformer.fit_transform(X))
-    [[1],
-     [5]]
-
-    """
-    if func is not None:
-        func_params = signature(func).parameters
-        if 'axis' in func_params:  # Use native (faster) numpy implementation
-            func_along_axis = partial(func, axis=1, keepdims=True)
-        else:
-            func_along_axis = partial(_reshape_after_apply,
-                                      _make_func_apply_along_axis_1(func))
-    else:
-        func_along_axis = None
-
-    return FunctionTransformer(func=func_along_axis, validate=validate)
-
-
-def identity(validate=False):
-    return FunctionTransformer(validate=validate)
+"""Utility functions for scikit-learn pipelines."""
+# License: GNU AGPLv3
+
+from functools import partial
+from inspect import signature
+
+import numpy as np
+from sklearn.preprocessing import FunctionTransformer
+
+
+def _make_func_apply_along_axis_1(func):
+    return partial(np.apply_along_axis, func, 1)
+
+
+def _reshape_after_apply(func, arr):
+    res = func(arr)
+    if res.ndim == 1:
+        res = res[:, None]
+    return res
+
+
+def transformer_from_callable_on_rows(func, validate=True):
+    """Construct a transformer from a callable acting on 1D arrays.
+
+    Given a callable which can act on 1D arrays, this function returns a
+    fit-transformer which applies the callable to slices of 2D arrays along
+    axis 1. When possible, the array output by the transformer's
+    :meth:`fit_transform` is two-dimensional.
+
+    Parameters
+    ----------
+    func : callable or None
+        A callable object, or ``None`` which returns the identity transformer.
+
+    validate : bool, optional, default: ``True``
+        Whether the output transformer should implement input validation.
+
+    Returns
+    -------
+    function_transformer : :class:`sklearn.preprocessing.FunctionTransformer` \
+        object
+        Output fit-transformer.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from gtda.mapper import transformer_from_callable_on_rows
+    >>> function_transformer = transformer_from_callable_on_rows(np.sum)
+    >>> X = np.array([[0, 1], [2, 3]])
+    >>> print(function_transformer.fit_transform(X))
+    [[1],
+     [5]]
+
+    """
+    if func is not None:
+        func_params = signature(func).parameters
+        if 'axis' in func_params:  # Use native (faster) numpy implementation
+            func_along_axis = partial(func, axis=1, keepdims=True)
+        else:
+            func_along_axis = partial(_reshape_after_apply,
+                                      _make_func_apply_along_axis_1(func))
+    else:
+        func_along_axis = None
+
+    return FunctionTransformer(func=func_along_axis, validate=validate)
+
+
+def identity(validate=False):
+    return FunctionTransformer(validate=validate)
```

## gtda/mapper/utils/_cluster.py

 * *Ordering differences only*

```diff
@@ -1,71 +1,71 @@
-from functools import partial
-from math import floor
-
-import numpy as np
-
-
-def _num_clusters_histogram(distances, freq_threshold, n_bins_start, max_frac):
-    if distances.size == 1:
-        return 1
-
-    if not freq_threshold:
-        threshold_func = _zero_bins
-    else:
-        threshold_func = partial(_bins_below_threshold, freq_threshold)
-
-    zero_bins = False
-    i = 0
-    if max_frac == 1.:
-        while not zero_bins:
-            hist, edges = np.histogram(distances, bins=n_bins_start + i)
-            zero_bins_indices = threshold_func(hist)
-            zero_bins = zero_bins_indices.size
-            i += 1
-        first_gap = zero_bins_indices[0]
-        left_bin_edge_first_gap = edges[first_gap]
-        gap_idx = (distances <= left_bin_edge_first_gap).sum()
-        num_clust = distances.size + 1 - gap_idx
-    else:
-        max_num_clust = max_frac * (distances.size + 1)
-        over_max_num = True
-        while over_max_num:
-            while (not zero_bins) and over_max_num:
-                hist, edges = np.histogram(distances, bins=n_bins_start + i)
-                zero_bins_indices = threshold_func(hist)
-                zero_bins = zero_bins_indices.size
-                i += 1
-            first_gap = zero_bins_indices[0]
-            left_bin_edge_first_gap = edges[first_gap]
-            gap_idx = np.sum(distances <= left_bin_edge_first_gap)
-            num_clust = distances.size + 1 - gap_idx
-            if num_clust > max_num_clust:
-                num_clust = max_num_clust
-                break
-            else:
-                over_max_num = False
-
-    return floor(num_clust)
-
-
-def _zero_bins(hist):
-    return np.flatnonzero(~hist.astype(bool))
-
-
-def _bins_below_threshold(freq_threshold, hist):
-    return np.flatnonzero(hist < freq_threshold)
-
-
-def _num_clusters_simple(distances, min_gap_size, max_frac):
-    # Differences between subsequent elements (padding by the first distance)
-    diff = np.ediff1d(distances, to_begin=distances[0])
-    gap_indices = np.flatnonzero(diff >= min_gap_size)
-    if gap_indices.size:
-        num_clust = distances.size + 1 - gap_indices[0]
-        if max_frac is None:
-            return num_clust
-        max_num_clust = max_frac * (distances.size + 1)
-        if num_clust > max_num_clust:
-            num_clust = max_num_clust
-        return floor(num_clust)
-    # No big enough gaps -> one cluster
-    return 1
+from functools import partial
+from math import floor
+
+import numpy as np
+
+
+def _num_clusters_histogram(distances, freq_threshold, n_bins_start, max_frac):
+    if distances.size == 1:
+        return 1
+
+    if not freq_threshold:
+        threshold_func = _zero_bins
+    else:
+        threshold_func = partial(_bins_below_threshold, freq_threshold)
+
+    zero_bins = False
+    i = 0
+    if max_frac == 1.:
+        while not zero_bins:
+            hist, edges = np.histogram(distances, bins=n_bins_start + i)
+            zero_bins_indices = threshold_func(hist)
+            zero_bins = zero_bins_indices.size
+            i += 1
+        first_gap = zero_bins_indices[0]
+        left_bin_edge_first_gap = edges[first_gap]
+        gap_idx = (distances <= left_bin_edge_first_gap).sum()
+        num_clust = distances.size + 1 - gap_idx
+    else:
+        max_num_clust = max_frac * (distances.size + 1)
+        over_max_num = True
+        while over_max_num:
+            while (not zero_bins) and over_max_num:
+                hist, edges = np.histogram(distances, bins=n_bins_start + i)
+                zero_bins_indices = threshold_func(hist)
+                zero_bins = zero_bins_indices.size
+                i += 1
+            first_gap = zero_bins_indices[0]
+            left_bin_edge_first_gap = edges[first_gap]
+            gap_idx = np.sum(distances <= left_bin_edge_first_gap)
+            num_clust = distances.size + 1 - gap_idx
+            if num_clust > max_num_clust:
+                num_clust = max_num_clust
+                break
+            else:
+                over_max_num = False
+
+    return floor(num_clust)
+
+
+def _zero_bins(hist):
+    return np.flatnonzero(~hist.astype(bool))
+
+
+def _bins_below_threshold(freq_threshold, hist):
+    return np.flatnonzero(hist < freq_threshold)
+
+
+def _num_clusters_simple(distances, min_gap_size, max_frac):
+    # Differences between subsequent elements (padding by the first distance)
+    diff = np.ediff1d(distances, to_begin=distances[0])
+    gap_indices = np.flatnonzero(diff >= min_gap_size)
+    if gap_indices.size:
+        num_clust = distances.size + 1 - gap_indices[0]
+        if max_frac is None:
+            return num_clust
+        max_num_clust = max_frac * (distances.size + 1)
+        if num_clust > max_num_clust:
+            num_clust = max_num_clust
+        return floor(num_clust)
+    # No big enough gaps -> one cluster
+    return 1
```

## gtda/mapper/utils/_cover.py

 * *Ordering differences only*

```diff
@@ -1,16 +1,16 @@
-import numpy as np
-
-
-def _check_has_one_column(X):
-    if X.shape[1] > 1:
-        raise ValueError("X cannot have more than one column.")
-
-
-def _remove_empty_and_duplicate_intervals(X_masks):
-    # Remove any mask which contains only False
-    X_masks = X_masks[:, np.any(X_masks, axis=0)]
-    # Avoid repeating the same boolean masks (columns)
-    X_masks_unique, indices = np.unique(X_masks, axis=1, return_index=True)
-    # Respect the original relative column ordering
-    X_masks_unique = X_masks_unique[:, np.argsort(indices)]
-    return X_masks_unique
+import numpy as np
+
+
+def _check_has_one_column(X):
+    if X.shape[1] > 1:
+        raise ValueError("X cannot have more than one column.")
+
+
+def _remove_empty_and_duplicate_intervals(X_masks):
+    # Remove any mask which contains only False
+    X_masks = X_masks[:, np.any(X_masks, axis=0)]
+    # Avoid repeating the same boolean masks (columns)
+    X_masks_unique, indices = np.unique(X_masks, axis=1, return_index=True)
+    # Respect the original relative column ordering
+    X_masks_unique = X_masks_unique[:, np.argsort(indices)]
+    return X_masks_unique
```

## gtda/mapper/utils/_list_feature_union.py

 * *Ordering differences only*

```diff
@@ -1,50 +1,50 @@
-import numpy as np
-from joblib import Parallel, delayed
-from sklearn.pipeline import FeatureUnion, _fit_transform_one, _transform_one
-
-
-class ListFeatureUnion(FeatureUnion):
-    def fit_transform(self, X, y=None, **fit_params):
-        """Fit all transformers, transform the data and concatenate results.
-        Parameters
-        ----------
-        X : iterable or array-like, depending on transformers
-            Input data to be transformed.
-        y : array-like of shape (n_samples, ...), optional
-            Targets for supervised learning.
-        Returns
-        -------
-        Xt : list of ndarray
-            List of results of transformers.
-
-        """
-        results = self._parallel_func(X, y, fit_params, _fit_transform_one)
-        if not results:
-            # All transformers are None
-            return np.zeros((X.shape[0], 0))
-
-        Xt, transformers = zip(*results)
-        self._update_transformer_list(transformers)
-        Xt = list(Xt)
-        return Xt
-
-    def transform(self, X):
-        """Transform X separately by each transformer, concatenate results.
-        Parameters
-        ----------
-        X : iterable or array-like, depending on transformers
-            Input data to be transformed.
-        Returns
-        -------
-        Xt : list of ndarray
-            List of results of transformers.
-
-        """
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(_transform_one)(trans, X, None, weight)
-            for name, trans, weight in self._iter())
-        if not Xt:
-            # All transformers are None
-            return np.zeros((X.shape[0], 0))
-        Xt = list(Xt)
-        return Xt
+import numpy as np
+from joblib import Parallel, delayed
+from sklearn.pipeline import FeatureUnion, _fit_transform_one, _transform_one
+
+
+class ListFeatureUnion(FeatureUnion):
+    def fit_transform(self, X, y=None, **fit_params):
+        """Fit all transformers, transform the data and concatenate results.
+        Parameters
+        ----------
+        X : iterable or array-like, depending on transformers
+            Input data to be transformed.
+        y : array-like of shape (n_samples, ...), optional
+            Targets for supervised learning.
+        Returns
+        -------
+        Xt : list of ndarray
+            List of results of transformers.
+
+        """
+        results = self._parallel_func(X, y, fit_params, _fit_transform_one)
+        if not results:
+            # All transformers are None
+            return np.zeros((X.shape[0], 0))
+
+        Xt, transformers = zip(*results)
+        self._update_transformer_list(transformers)
+        Xt = list(Xt)
+        return Xt
+
+    def transform(self, X):
+        """Transform X separately by each transformer, concatenate results.
+        Parameters
+        ----------
+        X : iterable or array-like, depending on transformers
+            Input data to be transformed.
+        Returns
+        -------
+        Xt : list of ndarray
+            List of results of transformers.
+
+        """
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(_transform_one)(trans, X, None, weight)
+            for name, trans, weight in self._iter())
+        if not Xt:
+            # All transformers are None
+            return np.zeros((X.shape[0], 0))
+        Xt = list(Xt)
+        return Xt
```

## gtda/mapper/utils/_logging.py

 * *Ordering differences only*

```diff
@@ -1,36 +1,36 @@
-import logging
-
-from IPython.display import display
-from ipywidgets import widgets
-
-
-class OutputWidgetHandler(logging.Handler):
-    """Custom logging handler to send logs to an output widget"""
-
-    def __init__(self, *args, **kwargs):
-        super(OutputWidgetHandler, self).__init__(*args, **kwargs)
-        layout = {
-            'width': '100%',
-            'height': '160px',
-            'border': '1px solid black',
-            'overflow': 'auto'
-        }
-        self.out = widgets.Output(layout=layout)
-
-    def emit(self, record):
-        """Overload of logging.Handler method"""
-        formatted_record = self.format(record)
-        new_output = {
-            'name': 'stdout',
-            'output_type': 'stream',
-            'text': formatted_record+'\n'
-            }
-        self.out.outputs = (new_output,) + self.out.outputs
-
-    def show_logs(self):
-        """Show the logs"""
-        display(self.out)
-
-    def clear_logs(self):
-        """Clear the current logs"""
-        self.out.clear_output()
+import logging
+
+from IPython.display import display
+from ipywidgets import widgets
+
+
+class OutputWidgetHandler(logging.Handler):
+    """Custom logging handler to send logs to an output widget"""
+
+    def __init__(self, *args, **kwargs):
+        super(OutputWidgetHandler, self).__init__(*args, **kwargs)
+        layout = {
+            'width': '100%',
+            'height': '160px',
+            'border': '1px solid black',
+            'overflow': 'auto'
+        }
+        self.out = widgets.Output(layout=layout)
+
+    def emit(self, record):
+        """Overload of logging.Handler method"""
+        formatted_record = self.format(record)
+        new_output = {
+            'name': 'stdout',
+            'output_type': 'stream',
+            'text': formatted_record+'\n'
+            }
+        self.out.outputs = (new_output,) + self.out.outputs
+
+    def show_logs(self):
+        """Show the logs"""
+        display(self.out)
+
+    def clear_logs(self):
+        """Clear the current logs"""
+        self.out.clear_output()
```

## gtda/mapper/utils/_visualization.py

 * *Ordering differences only*

```diff
@@ -1,552 +1,552 @@
-"""Graph layout functions and plotly layout functions."""
-# License: GNU AGPLv3
-
-from copy import deepcopy
-from functools import reduce, partial
-from operator import iconcat
-from warnings import warn
-
-import numpy as np
-import plotly.graph_objs as go
-from sklearn.utils.validation import check_array
-
-PLOT_OPTIONS_NODE_TRACE_DEFAULTS = {
-    "name": "node_trace",
-    "mode": "markers",
-    "hoverinfo": "text",
-    "marker": {
-        "colorscale": "viridis",
-        "opacity": 1.,
-        "showscale": True,
-        "reversescale": False,
-        "line": {"width": 1, "color": "#888"},
-        "sizemode": "area",
-        "sizemin": 4,
-        "colorbar": {
-            "thickness": 15, "title": "", "xanchor": "left",
-            "titleside": "right"
-            }
-        }
-    }
-
-PLOT_OPTIONS_EDGE_TRACE_DEFAULTS = {
-    "name": "edge_trace",
-    "mode": "lines",
-    "line": {"width": 1, "color": "#888"},
-    "hoverinfo": "none"
-    }
-
-PLOT_OPTIONS_LAYOUT_COMMON_DEFAULTS = {
-    "showlegend": False,
-    "hovermode": "closest",
-    "title": "",
-    "margin": {"b": 20, "l": 5, "r": 5, "t": 40},
-    "autosize": False,
-    "annotations": []
-    }
-
-PLOT_OPTIONS_LAYOUT_AXES_DEFAULTS_2D = {
-    "title": "", "showgrid": False, "zeroline": False, "showticklabels": False,
-    "ticks": "", "showline": False
-    }
-
-PLOT_OPTIONS_LAYOUT_AXES_DEFAULTS_3D = {
-    "title": "", "showbackground": False, "showline": False, "zeroline": False,
-    "showgrid": False, "showticklabels": False,
-    }
-
-PLOT_OPTIONS_LAYOUT_DEFAULTS = {
-    "common": PLOT_OPTIONS_LAYOUT_COMMON_DEFAULTS,
-    2: {
-        "template": "simple_white",
-        "xaxis": PLOT_OPTIONS_LAYOUT_AXES_DEFAULTS_2D,
-        "yaxis": PLOT_OPTIONS_LAYOUT_AXES_DEFAULTS_2D
-        },
-    3: {
-        "scene": {
-            "xaxis": PLOT_OPTIONS_LAYOUT_AXES_DEFAULTS_3D,
-            "yaxis": PLOT_OPTIONS_LAYOUT_AXES_DEFAULTS_3D,
-            "zaxis": PLOT_OPTIONS_LAYOUT_AXES_DEFAULTS_3D
-            }
-        }
-    }
-
-
-def _set_node_sizeref(node_sizes, node_scale=12):
-    # Formula from Plotly https://plot.ly/python/bubble-charts/
-    return 2. * max(node_sizes) / (node_scale ** 2)
-
-
-def _round_to_n_sig_figs(x, n=3):
-    """Round a number x to n significant figures."""
-    if n is None:
-        return x
-    if not x:
-        return 0
-    return np.round(x, -int(np.floor(np.log10(np.abs(x)))) + (n - 1))
-
-
-def _get_node_size(node_elements):
-    # TODO: Add doc strings to all functions
-    return list(map(len, node_elements))
-
-
-def _get_node_text(
-        node_ids, pullback_set_labels, partial_cluster_labels,
-        num_node_elements, node_summary_statistics
-        ):
-    return [
-        f"Node ID: {node_id}<br>Pullback set label: {pullback_set_label}<br>"
-        f"Partial cluster label: {partial_cluster_label}<br>Node size: "
-        f"{num_elements}<br>Summary statistic: {node_summary_statistic}"
-        for (node_id, pullback_set_label, partial_cluster_label, num_elements,
-             node_summary_statistic)
-        in zip(node_ids, pullback_set_labels, partial_cluster_labels,
-               num_node_elements, node_summary_statistics)
-        ]
-
-
-def _get_node_statistics(color_data_transformed, node_elements,
-                         node_color_statistic):
-    n_columns = color_data_transformed.shape[1]
-
-    return np.array([[node_color_statistic(color_data_transformed[itr, i])
-                      for i in range(n_columns)]
-                     for itr in node_elements])
-
-
-def _get_column_color_buttons(
-        node_colors_color_features, hovertext_color_features,
-        colorscale_for_hoverlabel, n_sig_figs, column_names_dropdown
-        ):
-    # TODO: Consider opting for just-in-time computation instead of computing
-    # all node summary values ahead of time. Solution should preserve scroll
-    # zoom functionality of 2D static visualisation.
-    def replace_summary_statistic(current_hovertext, new_statistic):
-        pos = current_hovertext.rfind(" ")
-        new_hovertext = current_hovertext[:pos] + \
-            f" {_round_to_n_sig_figs(new_statistic, n=n_sig_figs)}"
-        return new_hovertext
-
-    column_color_buttons = [
-        {
-            "args": [{
-                "marker.color": [None, node_colors_color_features[:, 0]],
-                "hovertext": [None, hovertext_color_features]
-                }],
-            "label": f"{column_names_dropdown[0]}",
-            "method": "restyle"
-            }
-        ]
-
-    for column in range(1, len(column_names_dropdown)):
-        node_colors = node_colors_color_features[:, column]
-        hovertext = list(map(replace_summary_statistic,
-                             hovertext_color_features, node_colors))
-
-        new_button = {
-            "args": [{
-                "marker.color": [None, node_colors],
-                "hovertext": [None, hovertext]
-                }],
-            "label": f"{column_names_dropdown[column]}",
-            "method": "restyle"
-            }
-
-        if colorscale_for_hoverlabel is not None:
-            min_col = np.min(node_colors)
-            max_col = np.max(node_colors)
-            new_button["args"][0]["hoverlabel.bgcolor"] = [
-                None,
-                _get_colors_for_vals(node_colors, min_col, max_col,
-                                     colorscale_for_hoverlabel)
-                ]
-
-        column_color_buttons.append(new_button)
-
-    return column_color_buttons
-
-
-def _infer_color_features_kind(color_features):
-    """Determine whether color_features is array, pandas dataframe, callable,
-    or scikit-learn (fit-)transformer."""
-    if hasattr(color_features, "dtype") or hasattr(color_features, "dtypes"):
-        raise ValueError("`color_features` should not be a numpy array or "
-                         "pandas dataframe.")
-    elif hasattr(color_features, "fit_transform"):
-        color_features_kind = "fit_transformer"
-    elif hasattr(color_features, "transform"):
-        color_features_kind = "transformer"
-    elif callable(color_features):
-        color_features_kind = "callable"
-    elif color_features is None:
-        color_features_kind = "none"
-    else:  # Assume color_features is a selection of columns
-        color_features_kind = "else"
-
-    return color_features_kind
-
-
-def _validate_color_kwargs(graph, data, color_data, color_features,
-                           node_color_statistic, interactive=False):
-    if node_color_statistic is None:
-        node_color_statistic = np.mean
-
-    if color_data is None:
-        color_data = np.arange(len(data))
-
-    color_data_checked = check_array(color_data, ensure_2d=False, dtype=None)
-
-    # Simple duck typing to determine whether `color_data` is likely a pandas
-    # dataframe or pandas series
-    is_color_data_dataframe = hasattr(color_data, "columns")
-    is_color_data_series = hasattr(color_data, "name")
-
-    if len(color_data) != len(data):
-        raise ValueError("`color_data` and `data` must have the same length.")
-    if not (is_color_data_dataframe or is_color_data_series):
-        color_data = color_data_checked
-
-    # Determine whether node_color_statistic is an array of node colors
-    is_node_color_statistic_ndarray = hasattr(node_color_statistic, "dtype")
-    is_node_color_statistic_callable = callable(node_color_statistic)
-
-    column_names_dropdown = None
-    if interactive:
-        if not is_node_color_statistic_callable:
-            raise ValueError("`node_color_statistic` must be a callable for "
-                             "interactive plots.")
-    else:
-        node_elements = graph.vs["node_elements"]
-        if is_node_color_statistic_ndarray:
-            node_color_statistic = check_array(node_color_statistic,
-                                               ensure_2d=False)
-            len_colors = len(node_color_statistic)
-            n_nodes = len(node_elements)
-            if len_colors != n_nodes:
-                raise ValueError(f"`node_color_statistic` must have as many "
-                                 f"entries as there are nodes in the Mapper "
-                                 f"graph. {len_colors} != {n_nodes} detected.")
-            if len(node_color_statistic.shape) > 1:
-                column_names_dropdown = range(node_color_statistic.shape[1])
-            else:
-                node_color_statistic = \
-                    node_color_statistic.reshape((len_colors, -1))
-        elif not is_node_color_statistic_callable:
-            raise ValueError("`node_color_statistic` must be a callable or "
-                             "an ndarray for static plots.")
-
-    color_features_kind = _infer_color_features_kind(color_features)
-    if color_features_kind == "fit_transformer":
-        color_data_transformed = color_features.fit_transform(color_data)
-    elif color_features_kind == "transformer":
-        color_data_transformed = color_features.transform(color_data)
-    elif color_features_kind == "callable":
-        color_data_transformed = color_features(color_data)
-        # If outcome is a pandas dataframe, save column names before converting
-        # to numpy array. If a pandas series, just convert to numpy
-        if hasattr(color_data_transformed, "columns"):
-            column_names_dropdown = color_data_transformed.columns
-            color_data_transformed = color_data_transformed.to_numpy()
-        elif hasattr(color_data_transformed, "name"):
-            color_data_transformed = color_data_transformed.to_numpy()
-    elif color_features_kind == "none":
-        if not (is_color_data_dataframe or is_color_data_series):
-            color_data_transformed = color_data
-        else:  # Dataframe or series
-            if is_color_data_dataframe:
-                column_names_dropdown = color_data.columns
-            color_data_transformed = color_data.to_numpy()
-    else:  # Case of column or sequence of columns
-        if is_color_data_dataframe:
-            color_data_transformed = color_data[color_features]
-            column_names_dropdown = getattr(color_data_transformed, "columns",
-                                            None)
-            color_data_transformed = color_data_transformed.to_numpy()
-        elif is_color_data_series:
-            raise ValueError(
-                "If `color_data` is a pandas series, `color_features` can "
-                "only be a scikit-learn transformer, a callable, or None."
-                )
-        else:
-            color_data_transformed = color_data[:, color_features]
-            if len(color_data_transformed.shape) > 1:
-                column_names_dropdown = color_features
-
-    if len(color_data_transformed.shape) > 1:
-        if column_names_dropdown is None:
-            column_names_dropdown = range(color_data_transformed.shape[1])
-
-    color_data_transformed = \
-        color_data_transformed.reshape((len(color_data_transformed), -1))
-
-    return (color_data_transformed, column_names_dropdown,
-            node_color_statistic)
-
-
-def _calculate_graph_data(
-        graph, color_data_transformed, node_color_statistic, layout,
-        layout_dim, n_sig_figs, node_scale
-        ):
-    node_elements = graph.vs["node_elements"]
-
-    if not hasattr(node_color_statistic, "dtype"):
-        node_colors_color_features = \
-            _get_node_statistics(color_data_transformed, node_elements,
-                                 node_color_statistic)
-    else:
-        node_colors_color_features = node_color_statistic
-
-    # Load defaults for node and edge traces
-    plot_options = {
-        "node_trace": deepcopy(PLOT_OPTIONS_NODE_TRACE_DEFAULTS),
-        "edge_trace": deepcopy(PLOT_OPTIONS_EDGE_TRACE_DEFAULTS)
-        }
-
-    # Update size and color of nodes with zeroth column of
-    # `node_colors_color_features`
-    node_sizes = _get_node_size(node_elements)
-    plot_options["node_trace"]["marker"].update({
-        "size": node_sizes,
-        "sizeref": _set_node_sizeref(node_sizes, node_scale=node_scale),
-        "color": node_colors_color_features[:, 0]
-        })
-
-    # Generate hovertext
-    node_ids = graph.vs.indices
-    pullback_set_ids = graph.vs["pullback_set_label"]
-    partial_cluster_labels = graph.vs["partial_cluster_label"]
-    num_node_elements = map(len, node_elements)
-    node_colors_round = map(partial(_round_to_n_sig_figs, n=n_sig_figs),
-                            node_colors_color_features[:, 0])
-    plot_options["node_trace"]["hovertext"] = _get_node_text(
-        node_ids, pullback_set_ids, partial_cluster_labels,
-        num_node_elements, node_colors_round
-        )
-
-    # Compute graph layout
-    if layout_dim not in [2, 3]:
-        raise ValueError(
-            f"`layout_dim` must be either 2 or 3. {layout_dim} entered."
-            )
-    node_pos = np.asarray(graph.layout(layout, dim=layout_dim).coords)
-
-    # Store x and y coordinates of edge endpoints
-    edge_x = list(
-        reduce(
-            iconcat, map(
-                lambda e: [node_pos[e.source, 0], node_pos[e.target, 0],
-                           None],
-                graph.es
-                ), []
-            )
-        )
-    edge_y = list(
-        reduce(
-            iconcat, map(
-                lambda e: [node_pos[e.source, 1], node_pos[e.target, 1],
-                           None],
-                graph.es
-                ), []
-            )
-        )
-
-    if layout_dim == 2:
-        node_trace = go.Scatter(
-            x=node_pos[:, 0], y=node_pos[:, 1], **plot_options["node_trace"]
-            )
-
-        edge_trace = go.Scatter(
-            x=edge_x, y=edge_y, **plot_options["edge_trace"]
-            )
-
-    else:
-        node_trace = go.Scatter3d(
-            x=node_pos[:, 0], y=node_pos[:, 1], z=node_pos[:, 2],
-            **plot_options["node_trace"]
-            )
-
-        edge_z = list(
-            reduce(
-                iconcat, map(
-                    lambda e: [node_pos[e.source][2], node_pos[e.target][2],
-                               None],
-                    graph.es
-                    ), []
-                )
-            )
-        edge_trace = go.Scatter3d(
-            x=edge_x, y=edge_y, z=edge_z, **plot_options["edge_trace"]
-            )
-
-    return edge_trace, node_trace, node_colors_color_features
-
-
-def _produce_static_figure(edge_trace, node_trace, node_colors_color_features,
-                           column_names_dropdown, layout_dim, n_sig_figs,
-                           plotly_params):
-    # Define layout options
-    layout_options = go.Layout(
-        **PLOT_OPTIONS_LAYOUT_DEFAULTS["common"],
-        **PLOT_OPTIONS_LAYOUT_DEFAULTS[layout_dim]
-        )
-
-    fig = go.FigureWidget(data=[edge_trace, node_trace], layout=layout_options)
-
-    _plotly_params = deepcopy(plotly_params)
-
-    # When laying out the graph in 3D, plotly does not automatically give
-    # the background hoverlabel the same color as the respective marker,
-    # so we do this by hand here.
-    # TODO: Extract logic so as to avoid repetitions in interactive version
-    colorscale_for_hoverlabel = None
-    if layout_dim == 3:
-        compute_hoverlabel_bgcolor = True
-        if _plotly_params:
-            if "node_trace" in _plotly_params:
-                if "hoverlabel_bgcolor" in _plotly_params["node_trace"]:
-                    fig.update_traces(
-                        hoverlabel_bgcolor=_plotly_params["node_trace"].pop(
-                            "hoverlabel_bgcolor"
-                            ),
-                        selector={"name": "node_trace"}
-                        )
-                    compute_hoverlabel_bgcolor = False
-                if "marker_colorscale" in _plotly_params["node_trace"]:
-                    fig.update_traces(
-                        marker_colorscale=_plotly_params["node_trace"].pop(
-                            "marker_colorscale"
-                            ),
-                        selector={"name": "node_trace"}
-                        )
-
-        if compute_hoverlabel_bgcolor:
-            colorscale_for_hoverlabel = fig.data[1].marker.colorscale
-            min_col = np.min(node_colors_color_features[:, 0])
-            max_col = np.max(node_colors_color_features[:, 0])
-            try:
-                hoverlabel_bgcolor = _get_colors_for_vals(
-                    node_colors_color_features[:, 0], min_col, max_col,
-                    colorscale_for_hoverlabel
-                    )
-            except Exception as e:
-                if e.args[0] == "This colorscale is not supported.":
-                    warn("Data-dependent background hoverlabel colors cannot "
-                         "be generated with this choice of colorscale. Please "
-                         "use a standard hex- or RGB-formatted colorscale.",
-                         RuntimeWarning)
-                else:
-                    warn("Something went wrong in generating data-dependent "
-                         "background hoverlabel colors. All background "
-                         "hoverlabel colors will be set to white.",
-                         RuntimeWarning)
-                hoverlabel_bgcolor = "white"
-                colorscale_for_hoverlabel = None
-            fig.update_traces(
-                hoverlabel_bgcolor=hoverlabel_bgcolor,
-                selector={"name": "node_trace"}
-                )
-
-    # Produce dropdown menu if `node_colors_color_features` has more than
-    # one column
-    if node_colors_color_features.shape[1] > 1:
-        hovertext_color_features = node_trace.hovertext
-        column_color_buttons = _get_column_color_buttons(
-            node_colors_color_features, hovertext_color_features,
-            colorscale_for_hoverlabel, n_sig_figs, column_names_dropdown
-            )
-
-        button_height = 1.1
-        fig.update_layout(
-            updatemenus=[
-                go.layout.Updatemenu(buttons=column_color_buttons,
-                                     direction="down",
-                                     pad={"r": 10, "t": 10},
-                                     showactive=True,
-                                     x=0.11,
-                                     xanchor="left",
-                                     y=button_height,
-                                     yanchor="top")
-                ]
-            )
-
-        fig.add_annotation(
-            go.layout.Annotation(text="Color by:",
-                                 x=0,
-                                 xref="paper",
-                                 y=button_height - 0.045,
-                                 yref="paper",
-                                 align="left",
-                                 showarrow=False)
-            )
-
-    # Update traces and layout according to user input
-    if _plotly_params:
-        for key in ["node_trace", "edge_trace"]:
-            fig.update_traces(
-                _plotly_params.pop(key, None),
-                selector={"name": key}
-                )
-        fig.update_layout(_plotly_params.pop("layout", None))
-
-    return fig
-
-
-def _hex_to_rgb(value):
-    """Convert a hex-formatted color to rgb, ignoring alpha values."""
-    value = value.lstrip("#")
-    return [int(value[i:i + 2], 16) for i in range(0, 6, 2)]
-
-
-def _rbg_to_hex(c):
-    """Convert an rgb-formatted color to hex, ignoring alpha values."""
-    return f"#{c[0]:02x}{c[1]:02x}{c[2]:02x}"
-
-
-def _get_colors_for_vals(vals, vmin, vmax, colorscale, return_hex=True):
-    """Given a float array vals, interpolate based on a colorscale to obtain
-    rgb or hex colors. Inspired by
-    `user empet's answer in \
-    <community.plotly.com/t/hover-background-color-on-scatter-3d/9185/6>`_."""
-    from numbers import Number
-    from ast import literal_eval
-
-    if vmin >= vmax:
-        raise ValueError("`vmin` should be < `vmax`.")
-
-    if (len(colorscale[0]) == 2) and isinstance(colorscale[0][0], Number):
-        scale, colors = zip(*colorscale)
-    else:
-        scale = np.linspace(0, 1, num=len(colorscale))
-        colors = colorscale
-    scale = np.asarray(scale)
-
-    if colors[0][:3] == "rgb":
-        colors = np.asarray([literal_eval(color[3:]) for color in colors],
-                            dtype=float)
-    elif colors[0][0] == "#":
-        colors = np.asarray(list(map(_hex_to_rgb, colors)), dtype=float)
-    else:
-        raise ValueError("This colorscale is not supported.")
-
-    colorscale = np.hstack([scale.reshape(-1, 1), colors])
-    colorscale = np.vstack([colorscale, colorscale[0, :]])
-    colorscale_diffs = np.diff(colorscale, axis=0)
-    colorscale_diff_ratios = colorscale_diffs[:, 1:] / colorscale_diffs[:, [0]]
-    colorscale_diff_ratios[-1, :] = np.zeros(3)
-
-    vals_scaled = (vals - vmin) / (vmax - vmin)
-
-    left_bin_indices = np.digitize(vals_scaled, scale) - 1
-    left_endpts = colorscale[left_bin_indices]
-    vals_scaled -= left_endpts[:, 0]
-    diff_ratios = colorscale_diff_ratios[left_bin_indices]
-
-    vals_rgb = (
-            left_endpts[:, 1:] + diff_ratios * vals_scaled[:, np.newaxis] + 0.5
-        ).astype(np.uint8)
-
-    if return_hex:
-        return list(map(_rbg_to_hex, vals_rgb))
-
-    return [f"rgb{tuple(v)}" for v in vals_rgb]
+"""Graph layout functions and plotly layout functions."""
+# License: GNU AGPLv3
+
+from copy import deepcopy
+from functools import reduce, partial
+from operator import iconcat
+from warnings import warn
+
+import numpy as np
+import plotly.graph_objs as go
+from sklearn.utils.validation import check_array
+
+PLOT_OPTIONS_NODE_TRACE_DEFAULTS = {
+    "name": "node_trace",
+    "mode": "markers",
+    "hoverinfo": "text",
+    "marker": {
+        "colorscale": "viridis",
+        "opacity": 1.,
+        "showscale": True,
+        "reversescale": False,
+        "line": {"width": 1, "color": "#888"},
+        "sizemode": "area",
+        "sizemin": 4,
+        "colorbar": {
+            "thickness": 15, "title": "", "xanchor": "left",
+            "titleside": "right"
+            }
+        }
+    }
+
+PLOT_OPTIONS_EDGE_TRACE_DEFAULTS = {
+    "name": "edge_trace",
+    "mode": "lines",
+    "line": {"width": 1, "color": "#888"},
+    "hoverinfo": "none"
+    }
+
+PLOT_OPTIONS_LAYOUT_COMMON_DEFAULTS = {
+    "showlegend": False,
+    "hovermode": "closest",
+    "title": "",
+    "margin": {"b": 20, "l": 5, "r": 5, "t": 40},
+    "autosize": False,
+    "annotations": []
+    }
+
+PLOT_OPTIONS_LAYOUT_AXES_DEFAULTS_2D = {
+    "title": "", "showgrid": False, "zeroline": False, "showticklabels": False,
+    "ticks": "", "showline": False
+    }
+
+PLOT_OPTIONS_LAYOUT_AXES_DEFAULTS_3D = {
+    "title": "", "showbackground": False, "showline": False, "zeroline": False,
+    "showgrid": False, "showticklabels": False,
+    }
+
+PLOT_OPTIONS_LAYOUT_DEFAULTS = {
+    "common": PLOT_OPTIONS_LAYOUT_COMMON_DEFAULTS,
+    2: {
+        "template": "simple_white",
+        "xaxis": PLOT_OPTIONS_LAYOUT_AXES_DEFAULTS_2D,
+        "yaxis": PLOT_OPTIONS_LAYOUT_AXES_DEFAULTS_2D
+        },
+    3: {
+        "scene": {
+            "xaxis": PLOT_OPTIONS_LAYOUT_AXES_DEFAULTS_3D,
+            "yaxis": PLOT_OPTIONS_LAYOUT_AXES_DEFAULTS_3D,
+            "zaxis": PLOT_OPTIONS_LAYOUT_AXES_DEFAULTS_3D
+            }
+        }
+    }
+
+
+def _set_node_sizeref(node_sizes, node_scale=12):
+    # Formula from Plotly https://plot.ly/python/bubble-charts/
+    return 2. * max(node_sizes) / (node_scale ** 2)
+
+
+def _round_to_n_sig_figs(x, n=3):
+    """Round a number x to n significant figures."""
+    if n is None:
+        return x
+    if not x:
+        return 0
+    return np.round(x, -int(np.floor(np.log10(np.abs(x)))) + (n - 1))
+
+
+def _get_node_size(node_elements):
+    # TODO: Add doc strings to all functions
+    return list(map(len, node_elements))
+
+
+def _get_node_text(
+        node_ids, pullback_set_labels, partial_cluster_labels,
+        num_node_elements, node_summary_statistics
+        ):
+    return [
+        f"Node ID: {node_id}<br>Pullback set label: {pullback_set_label}<br>"
+        f"Partial cluster label: {partial_cluster_label}<br>Node size: "
+        f"{num_elements}<br>Summary statistic: {node_summary_statistic}"
+        for (node_id, pullback_set_label, partial_cluster_label, num_elements,
+             node_summary_statistic)
+        in zip(node_ids, pullback_set_labels, partial_cluster_labels,
+               num_node_elements, node_summary_statistics)
+        ]
+
+
+def _get_node_statistics(color_data_transformed, node_elements,
+                         node_color_statistic):
+    n_columns = color_data_transformed.shape[1]
+
+    return np.array([[node_color_statistic(color_data_transformed[itr, i])
+                      for i in range(n_columns)]
+                     for itr in node_elements])
+
+
+def _get_column_color_buttons(
+        node_colors_color_features, hovertext_color_features,
+        colorscale_for_hoverlabel, n_sig_figs, column_names_dropdown
+        ):
+    # TODO: Consider opting for just-in-time computation instead of computing
+    # all node summary values ahead of time. Solution should preserve scroll
+    # zoom functionality of 2D static visualisation.
+    def replace_summary_statistic(current_hovertext, new_statistic):
+        pos = current_hovertext.rfind(" ")
+        new_hovertext = current_hovertext[:pos] + \
+            f" {_round_to_n_sig_figs(new_statistic, n=n_sig_figs)}"
+        return new_hovertext
+
+    column_color_buttons = [
+        {
+            "args": [{
+                "marker.color": [None, node_colors_color_features[:, 0]],
+                "hovertext": [None, hovertext_color_features]
+                }],
+            "label": f"{column_names_dropdown[0]}",
+            "method": "restyle"
+            }
+        ]
+
+    for column in range(1, len(column_names_dropdown)):
+        node_colors = node_colors_color_features[:, column]
+        hovertext = list(map(replace_summary_statistic,
+                             hovertext_color_features, node_colors))
+
+        new_button = {
+            "args": [{
+                "marker.color": [None, node_colors],
+                "hovertext": [None, hovertext]
+                }],
+            "label": f"{column_names_dropdown[column]}",
+            "method": "restyle"
+            }
+
+        if colorscale_for_hoverlabel is not None:
+            min_col = np.min(node_colors)
+            max_col = np.max(node_colors)
+            new_button["args"][0]["hoverlabel.bgcolor"] = [
+                None,
+                _get_colors_for_vals(node_colors, min_col, max_col,
+                                     colorscale_for_hoverlabel)
+                ]
+
+        column_color_buttons.append(new_button)
+
+    return column_color_buttons
+
+
+def _infer_color_features_kind(color_features):
+    """Determine whether color_features is array, pandas dataframe, callable,
+    or scikit-learn (fit-)transformer."""
+    if hasattr(color_features, "dtype") or hasattr(color_features, "dtypes"):
+        raise ValueError("`color_features` should not be a numpy array or "
+                         "pandas dataframe.")
+    elif hasattr(color_features, "fit_transform"):
+        color_features_kind = "fit_transformer"
+    elif hasattr(color_features, "transform"):
+        color_features_kind = "transformer"
+    elif callable(color_features):
+        color_features_kind = "callable"
+    elif color_features is None:
+        color_features_kind = "none"
+    else:  # Assume color_features is a selection of columns
+        color_features_kind = "else"
+
+    return color_features_kind
+
+
+def _validate_color_kwargs(graph, data, color_data, color_features,
+                           node_color_statistic, interactive=False):
+    if node_color_statistic is None:
+        node_color_statistic = np.mean
+
+    if color_data is None:
+        color_data = np.arange(len(data))
+
+    color_data_checked = check_array(color_data, ensure_2d=False, dtype=None)
+
+    # Simple duck typing to determine whether `color_data` is likely a pandas
+    # dataframe or pandas series
+    is_color_data_dataframe = hasattr(color_data, "columns")
+    is_color_data_series = hasattr(color_data, "name")
+
+    if len(color_data) != len(data):
+        raise ValueError("`color_data` and `data` must have the same length.")
+    if not (is_color_data_dataframe or is_color_data_series):
+        color_data = color_data_checked
+
+    # Determine whether node_color_statistic is an array of node colors
+    is_node_color_statistic_ndarray = hasattr(node_color_statistic, "dtype")
+    is_node_color_statistic_callable = callable(node_color_statistic)
+
+    column_names_dropdown = None
+    if interactive:
+        if not is_node_color_statistic_callable:
+            raise ValueError("`node_color_statistic` must be a callable for "
+                             "interactive plots.")
+    else:
+        node_elements = graph.vs["node_elements"]
+        if is_node_color_statistic_ndarray:
+            node_color_statistic = check_array(node_color_statistic,
+                                               ensure_2d=False)
+            len_colors = len(node_color_statistic)
+            n_nodes = len(node_elements)
+            if len_colors != n_nodes:
+                raise ValueError(f"`node_color_statistic` must have as many "
+                                 f"entries as there are nodes in the Mapper "
+                                 f"graph. {len_colors} != {n_nodes} detected.")
+            if len(node_color_statistic.shape) > 1:
+                column_names_dropdown = range(node_color_statistic.shape[1])
+            else:
+                node_color_statistic = \
+                    node_color_statistic.reshape((len_colors, -1))
+        elif not is_node_color_statistic_callable:
+            raise ValueError("`node_color_statistic` must be a callable or "
+                             "an ndarray for static plots.")
+
+    color_features_kind = _infer_color_features_kind(color_features)
+    if color_features_kind == "fit_transformer":
+        color_data_transformed = color_features.fit_transform(color_data)
+    elif color_features_kind == "transformer":
+        color_data_transformed = color_features.transform(color_data)
+    elif color_features_kind == "callable":
+        color_data_transformed = color_features(color_data)
+        # If outcome is a pandas dataframe, save column names before converting
+        # to numpy array. If a pandas series, just convert to numpy
+        if hasattr(color_data_transformed, "columns"):
+            column_names_dropdown = color_data_transformed.columns
+            color_data_transformed = color_data_transformed.to_numpy()
+        elif hasattr(color_data_transformed, "name"):
+            color_data_transformed = color_data_transformed.to_numpy()
+    elif color_features_kind == "none":
+        if not (is_color_data_dataframe or is_color_data_series):
+            color_data_transformed = color_data
+        else:  # Dataframe or series
+            if is_color_data_dataframe:
+                column_names_dropdown = color_data.columns
+            color_data_transformed = color_data.to_numpy()
+    else:  # Case of column or sequence of columns
+        if is_color_data_dataframe:
+            color_data_transformed = color_data[color_features]
+            column_names_dropdown = getattr(color_data_transformed, "columns",
+                                            None)
+            color_data_transformed = color_data_transformed.to_numpy()
+        elif is_color_data_series:
+            raise ValueError(
+                "If `color_data` is a pandas series, `color_features` can "
+                "only be a scikit-learn transformer, a callable, or None."
+                )
+        else:
+            color_data_transformed = color_data[:, color_features]
+            if len(color_data_transformed.shape) > 1:
+                column_names_dropdown = color_features
+
+    if len(color_data_transformed.shape) > 1:
+        if column_names_dropdown is None:
+            column_names_dropdown = range(color_data_transformed.shape[1])
+
+    color_data_transformed = \
+        color_data_transformed.reshape((len(color_data_transformed), -1))
+
+    return (color_data_transformed, column_names_dropdown,
+            node_color_statistic)
+
+
+def _calculate_graph_data(
+        graph, color_data_transformed, node_color_statistic, layout,
+        layout_dim, n_sig_figs, node_scale
+        ):
+    node_elements = graph.vs["node_elements"]
+
+    if not hasattr(node_color_statistic, "dtype"):
+        node_colors_color_features = \
+            _get_node_statistics(color_data_transformed, node_elements,
+                                 node_color_statistic)
+    else:
+        node_colors_color_features = node_color_statistic
+
+    # Load defaults for node and edge traces
+    plot_options = {
+        "node_trace": deepcopy(PLOT_OPTIONS_NODE_TRACE_DEFAULTS),
+        "edge_trace": deepcopy(PLOT_OPTIONS_EDGE_TRACE_DEFAULTS)
+        }
+
+    # Update size and color of nodes with zeroth column of
+    # `node_colors_color_features`
+    node_sizes = _get_node_size(node_elements)
+    plot_options["node_trace"]["marker"].update({
+        "size": node_sizes,
+        "sizeref": _set_node_sizeref(node_sizes, node_scale=node_scale),
+        "color": node_colors_color_features[:, 0]
+        })
+
+    # Generate hovertext
+    node_ids = graph.vs.indices
+    pullback_set_ids = graph.vs["pullback_set_label"]
+    partial_cluster_labels = graph.vs["partial_cluster_label"]
+    num_node_elements = map(len, node_elements)
+    node_colors_round = map(partial(_round_to_n_sig_figs, n=n_sig_figs),
+                            node_colors_color_features[:, 0])
+    plot_options["node_trace"]["hovertext"] = _get_node_text(
+        node_ids, pullback_set_ids, partial_cluster_labels,
+        num_node_elements, node_colors_round
+        )
+
+    # Compute graph layout
+    if layout_dim not in [2, 3]:
+        raise ValueError(
+            f"`layout_dim` must be either 2 or 3. {layout_dim} entered."
+            )
+    node_pos = np.asarray(graph.layout(layout, dim=layout_dim).coords)
+
+    # Store x and y coordinates of edge endpoints
+    edge_x = list(
+        reduce(
+            iconcat, map(
+                lambda e: [node_pos[e.source, 0], node_pos[e.target, 0],
+                           None],
+                graph.es
+                ), []
+            )
+        )
+    edge_y = list(
+        reduce(
+            iconcat, map(
+                lambda e: [node_pos[e.source, 1], node_pos[e.target, 1],
+                           None],
+                graph.es
+                ), []
+            )
+        )
+
+    if layout_dim == 2:
+        node_trace = go.Scatter(
+            x=node_pos[:, 0], y=node_pos[:, 1], **plot_options["node_trace"]
+            )
+
+        edge_trace = go.Scatter(
+            x=edge_x, y=edge_y, **plot_options["edge_trace"]
+            )
+
+    else:
+        node_trace = go.Scatter3d(
+            x=node_pos[:, 0], y=node_pos[:, 1], z=node_pos[:, 2],
+            **plot_options["node_trace"]
+            )
+
+        edge_z = list(
+            reduce(
+                iconcat, map(
+                    lambda e: [node_pos[e.source][2], node_pos[e.target][2],
+                               None],
+                    graph.es
+                    ), []
+                )
+            )
+        edge_trace = go.Scatter3d(
+            x=edge_x, y=edge_y, z=edge_z, **plot_options["edge_trace"]
+            )
+
+    return edge_trace, node_trace, node_colors_color_features
+
+
+def _produce_static_figure(edge_trace, node_trace, node_colors_color_features,
+                           column_names_dropdown, layout_dim, n_sig_figs,
+                           plotly_params):
+    # Define layout options
+    layout_options = go.Layout(
+        **PLOT_OPTIONS_LAYOUT_DEFAULTS["common"],
+        **PLOT_OPTIONS_LAYOUT_DEFAULTS[layout_dim]
+        )
+
+    fig = go.FigureWidget(data=[edge_trace, node_trace], layout=layout_options)
+
+    _plotly_params = deepcopy(plotly_params)
+
+    # When laying out the graph in 3D, plotly does not automatically give
+    # the background hoverlabel the same color as the respective marker,
+    # so we do this by hand here.
+    # TODO: Extract logic so as to avoid repetitions in interactive version
+    colorscale_for_hoverlabel = None
+    if layout_dim == 3:
+        compute_hoverlabel_bgcolor = True
+        if _plotly_params:
+            if "node_trace" in _plotly_params:
+                if "hoverlabel_bgcolor" in _plotly_params["node_trace"]:
+                    fig.update_traces(
+                        hoverlabel_bgcolor=_plotly_params["node_trace"].pop(
+                            "hoverlabel_bgcolor"
+                            ),
+                        selector={"name": "node_trace"}
+                        )
+                    compute_hoverlabel_bgcolor = False
+                if "marker_colorscale" in _plotly_params["node_trace"]:
+                    fig.update_traces(
+                        marker_colorscale=_plotly_params["node_trace"].pop(
+                            "marker_colorscale"
+                            ),
+                        selector={"name": "node_trace"}
+                        )
+
+        if compute_hoverlabel_bgcolor:
+            colorscale_for_hoverlabel = fig.data[1].marker.colorscale
+            min_col = np.min(node_colors_color_features[:, 0])
+            max_col = np.max(node_colors_color_features[:, 0])
+            try:
+                hoverlabel_bgcolor = _get_colors_for_vals(
+                    node_colors_color_features[:, 0], min_col, max_col,
+                    colorscale_for_hoverlabel
+                    )
+            except Exception as e:
+                if e.args[0] == "This colorscale is not supported.":
+                    warn("Data-dependent background hoverlabel colors cannot "
+                         "be generated with this choice of colorscale. Please "
+                         "use a standard hex- or RGB-formatted colorscale.",
+                         RuntimeWarning)
+                else:
+                    warn("Something went wrong in generating data-dependent "
+                         "background hoverlabel colors. All background "
+                         "hoverlabel colors will be set to white.",
+                         RuntimeWarning)
+                hoverlabel_bgcolor = "white"
+                colorscale_for_hoverlabel = None
+            fig.update_traces(
+                hoverlabel_bgcolor=hoverlabel_bgcolor,
+                selector={"name": "node_trace"}
+                )
+
+    # Produce dropdown menu if `node_colors_color_features` has more than
+    # one column
+    if node_colors_color_features.shape[1] > 1:
+        hovertext_color_features = node_trace.hovertext
+        column_color_buttons = _get_column_color_buttons(
+            node_colors_color_features, hovertext_color_features,
+            colorscale_for_hoverlabel, n_sig_figs, column_names_dropdown
+            )
+
+        button_height = 1.1
+        fig.update_layout(
+            updatemenus=[
+                go.layout.Updatemenu(buttons=column_color_buttons,
+                                     direction="down",
+                                     pad={"r": 10, "t": 10},
+                                     showactive=True,
+                                     x=0.11,
+                                     xanchor="left",
+                                     y=button_height,
+                                     yanchor="top")
+                ]
+            )
+
+        fig.add_annotation(
+            go.layout.Annotation(text="Color by:",
+                                 x=0,
+                                 xref="paper",
+                                 y=button_height - 0.045,
+                                 yref="paper",
+                                 align="left",
+                                 showarrow=False)
+            )
+
+    # Update traces and layout according to user input
+    if _plotly_params:
+        for key in ["node_trace", "edge_trace"]:
+            fig.update_traces(
+                _plotly_params.pop(key, None),
+                selector={"name": key}
+                )
+        fig.update_layout(_plotly_params.pop("layout", None))
+
+    return fig
+
+
+def _hex_to_rgb(value):
+    """Convert a hex-formatted color to rgb, ignoring alpha values."""
+    value = value.lstrip("#")
+    return [int(value[i:i + 2], 16) for i in range(0, 6, 2)]
+
+
+def _rbg_to_hex(c):
+    """Convert an rgb-formatted color to hex, ignoring alpha values."""
+    return f"#{c[0]:02x}{c[1]:02x}{c[2]:02x}"
+
+
+def _get_colors_for_vals(vals, vmin, vmax, colorscale, return_hex=True):
+    """Given a float array vals, interpolate based on a colorscale to obtain
+    rgb or hex colors. Inspired by
+    `user empet's answer in \
+    <community.plotly.com/t/hover-background-color-on-scatter-3d/9185/6>`_."""
+    from numbers import Number
+    from ast import literal_eval
+
+    if vmin >= vmax:
+        raise ValueError("`vmin` should be < `vmax`.")
+
+    if (len(colorscale[0]) == 2) and isinstance(colorscale[0][0], Number):
+        scale, colors = zip(*colorscale)
+    else:
+        scale = np.linspace(0, 1, num=len(colorscale))
+        colors = colorscale
+    scale = np.asarray(scale)
+
+    if colors[0][:3] == "rgb":
+        colors = np.asarray([literal_eval(color[3:]) for color in colors],
+                            dtype=float)
+    elif colors[0][0] == "#":
+        colors = np.asarray(list(map(_hex_to_rgb, colors)), dtype=float)
+    else:
+        raise ValueError("This colorscale is not supported.")
+
+    colorscale = np.hstack([scale.reshape(-1, 1), colors])
+    colorscale = np.vstack([colorscale, colorscale[0, :]])
+    colorscale_diffs = np.diff(colorscale, axis=0)
+    colorscale_diff_ratios = colorscale_diffs[:, 1:] / colorscale_diffs[:, [0]]
+    colorscale_diff_ratios[-1, :] = np.zeros(3)
+
+    vals_scaled = (vals - vmin) / (vmax - vmin)
+
+    left_bin_indices = np.digitize(vals_scaled, scale) - 1
+    left_endpts = colorscale[left_bin_indices]
+    vals_scaled -= left_endpts[:, 0]
+    diff_ratios = colorscale_diff_ratios[left_bin_indices]
+
+    vals_rgb = (
+            left_endpts[:, 1:] + diff_ratios * vals_scaled[:, np.newaxis] + 0.5
+        ).astype(np.uint8)
+
+    if return_hex:
+        return list(map(_rbg_to_hex, vals_rgb))
+
+    return [f"rgb{tuple(v)}" for v in vals_rgb]
```

## gtda/metaestimators/collection_transformer.py

```diff
@@ -1,190 +1,196 @@
-"""CollectionTransformer meta-estimator."""
-# License: GNU AGPLv3
-
-from functools import reduce
-from operator import and_
-from warnings import warn
-
-import numpy as np
-from joblib import Parallel, delayed
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.base import clone
-from sklearn.utils.metaestimators import if_delegate_has_method
-
-from gtda.utils import check_collection
-
-
-class CollectionTransformer(BaseEstimator, TransformerMixin):
-    """Meta-transformer for applying a fit-transformer to each input in a
-    collection.
-
-    If `transformer` possesses a ``fit_transform`` method,
-    ``CollectionTransformer(transformer)`` also possesses a
-    :meth:`fit_transform` method which, on each entry in its input ``X``,
-    fit-transforms a clone of `transformer`. A collection (list or ndarray) of
-    outputs is returned.
-
-    Note: to have compatibility with scikit-learn and giotto-tda pipelines, a
-    :meth:`transform` method is also present but it is simply an alias for
-    :meth:`fit_transform`.
-
-    Parameters
-    ----------
-    transformer : object
-        The fit-transformer instance from which the transformer acting on
-        collections is built. Should implement ``fit_transform``.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use in a joblib-parallel application of
-        `transformer`'s ``fit_transform`` to each input. ``None`` means 1
-        unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using
-        all processors.
-
-    parallel_backend_prefer :  ``"processes"`` | ``"threads"`` | ``None``, \
-        optional, default: ``None``
-        Soft hint for the default joblib backend to use in a joblib-parallel
-        application  of `transformer`'s ``fit_transform`` to each input. See
-        [1]_.
-
-    parallel_backend_require : ``"sharedmem"`` or None, optional, default: \
-        ``None``
-        Hard constraint to select the backend. If set to ``'sharedmem'``, the
-        selected backend will be single-host and thread-based even if the user
-        asked for a non-thread based backend with parallel_backend.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from sklearn.decomposition import PCA
-    >>> from gtda.metaestimators import CollectionTransformer
-    >>> rng = np.random.default_rng()
-
-    Create a collection of 1000 2D inputs for PCA, as a single 3D ndarray (we
-    could also create a list of 2D inputs instead).
-
-    >>> X = rng.random((1000, 100, 50))
-
-    In the case of PCA, joblib parallelism can be very beneficial!
-
-    >>> multi_pca = CollectionTransformer(PCA(n_components=3), n_jobs=-1)
-    >>> Xt = multi_pca.fit_transform(X)
-
-    Since all PCA outputs have the same shape, ``Xt`` is an  ndarray.
-    >>> print(Xt.shape)
-    (1000, 100, 3)
-
-    See also
-    --------
-    gtda.mapper.utils.pipeline.transformer_from_callable_on_rows, \
-    gtda.mapper.utils.decorators.method_to_transform
-
-    References
-    ----------
-    .. [1] "Thread-based parallelism vs process-based parallelism", in
-           `joblib documentation
-           <https://joblib.readthedocs.io/en/latest/parallel.html>`_.
-
-    """
-
-    def __init__(self, transformer, n_jobs=None, parallel_backend_prefer=None,
-                 parallel_backend_require=None):
-        self.transformer = transformer
-        self.n_jobs = n_jobs
-        self.parallel_backend_prefer = parallel_backend_prefer
-        self.parallel_backend_require = parallel_backend_require
-
-    def _validate_transformer(self):
-        if not hasattr(self.transformer, "fit_transform"):
-            raise TypeError("`transformer` must possess a fit_transform "
-                            "method.")
-        if not isinstance(self.transformer, BaseEstimator):
-            warn("`transformer` is not an instance of "
-                 "sklearn.base.BaseEstimator. This will lead to limited "
-                 "functionality in a scikit-learn context.", UserWarning)
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : list of length n_samples, or ndarray of shape (n_samples, ...)
-            Collection of inputs to be fit-transformed by `transformer`.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_collection(X, accept_sparse=True, accept_large_sparse=True,
-                         force_all_finite=False)
-        self._validate_transformer()
-
-        self._is_fitted = True
-        return self
-
-    @if_delegate_has_method(delegate="transformer")
-    def fit_transform(self, X, y=None):
-        """Fit-transform a clone of `transformer` to each element in the
-        collection `X`.
-
-        Parameters
-        ----------
-        X : list of length n_samples, or ndarray of shape (n_samples, ...)
-            Collection of inputs to be fit-transformed by `transformer`.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : list of length n_samples, or ndarray of shape (n_samples, ...)
-            Collection of outputs. It is a list unless all outputs have the
-            same shape, in which case it is converted to an ndarray.
-
-        """
-        Xt = check_collection(X, accept_sparse=True, accept_large_sparse=True,
-                              force_all_finite=False)
-        self._validate_transformer()
-
-        Xt = Parallel(n_jobs=self.n_jobs, prefer=self.parallel_backend_prefer,
-                      require=self.parallel_backend_require)(
-            delayed(clone(self.transformer).fit_transform)(x) for x in Xt
-            )
-
-        x0_shape = Xt[0].shape
-        if reduce(and_, (x.shape == x0_shape for x in Xt), True):
-            Xt = np.asarray(Xt)
-
-        return Xt
-
-    def transform(self, X, y=None):
-        """Alias for :meth:`fit_transform`.
-
-        Allows for this class to be used as an intermediate step in a
-        scikit-learn pipeline.
-
-        Parameters
-        ----------
-        X : list of length n_samples, or ndarray of shape (n_samples, ...)
-            Collection of inputs to be fit-transformed by `transformer`.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : list of length n_samples, or ndarray of shape (n_samples, ...)
-            Collection of outputs. It is a list unless all outputs have the
-            same shape, in which case it is converted to an ndarray.
-
-        """
-        return self.fit_transform(X, y)
+"""CollectionTransformer meta-estimator."""
+# License: GNU AGPLv3
+
+from functools import reduce
+from operator import and_
+from warnings import warn
+
+import numpy as np
+from joblib import Parallel, delayed
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.base import clone
+from sklearn.utils.metaestimators import available_if
+
+from gtda.utils import check_collection
+
+
+class CollectionTransformer(BaseEstimator, TransformerMixin):
+    """Meta-transformer for applying a fit-transformer to each input in a
+    collection.
+
+    If `transformer` possesses a ``fit_transform`` method,
+    ``CollectionTransformer(transformer)`` also possesses a
+    :meth:`fit_transform` method which, on each entry in its input ``X``,
+    fit-transforms a clone of `transformer`. A collection (list or ndarray) of
+    outputs is returned.
+
+    Note: to have compatibility with scikit-learn and giotto-tda pipelines, a
+    :meth:`transform` method is also present but it is simply an alias for
+    :meth:`fit_transform`.
+
+    Parameters
+    ----------
+    transformer : object
+        The fit-transformer instance from which the transformer acting on
+        collections is built. Should implement ``fit_transform``.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use in a joblib-parallel application of
+        `transformer`'s ``fit_transform`` to each input. ``None`` means 1
+        unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using
+        all processors.
+
+    parallel_backend_prefer :  ``"processes"`` | ``"threads"`` | ``None``, \
+        optional, default: ``None``
+        Soft hint for the default joblib backend to use in a joblib-parallel
+        application  of `transformer`'s ``fit_transform`` to each input. See
+        [1]_.
+
+    parallel_backend_require : ``"sharedmem"`` or None, optional, default: \
+        ``None``
+        Hard constraint to select the backend. If set to ``'sharedmem'``, the
+        selected backend will be single-host and thread-based even if the user
+        asked for a non-thread based backend with parallel_backend.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.decomposition import PCA
+    >>> from gtda.metaestimators import CollectionTransformer
+    >>> rng = np.random.default_rng()
+
+    Create a collection of 1000 2D inputs for PCA, as a single 3D ndarray (we
+    could also create a list of 2D inputs instead).
+
+    >>> X = rng.random((1000, 100, 50))
+
+    In the case of PCA, joblib parallelism can be very beneficial!
+
+    >>> multi_pca = CollectionTransformer(PCA(n_components=3), n_jobs=-1)
+    >>> Xt = multi_pca.fit_transform(X)
+
+    Since all PCA outputs have the same shape, ``Xt`` is an  ndarray.
+    >>> print(Xt.shape)
+    (1000, 100, 3)
+
+    See also
+    --------
+    gtda.mapper.utils.pipeline.transformer_from_callable_on_rows, \
+    gtda.mapper.utils.decorators.method_to_transform
+
+    References
+    ----------
+    .. [1] "Thread-based parallelism vs process-based parallelism", in
+           `joblib documentation
+           <https://joblib.readthedocs.io/en/latest/parallel.html>`_.
+
+    """
+
+    def __init__(self, transformer, n_jobs=None, parallel_backend_prefer=None,
+                 parallel_backend_require=None):
+        self.transformer = transformer
+        self.n_jobs = n_jobs
+        self.parallel_backend_prefer = parallel_backend_prefer
+        self.parallel_backend_require = parallel_backend_require
+
+    def _validate_transformer(self):
+        if not hasattr(self.transformer, "fit_transform"):
+            raise TypeError("`transformer` must possess a fit_transform "
+                            "method.")
+        if not isinstance(self.transformer, BaseEstimator):
+            warn("`transformer` is not an instance of "
+                 "sklearn.base.BaseEstimator. This will lead to limited "
+                 "functionality in a scikit-learn context.", UserWarning)
+
+    def _transformer_has(attr):
+        def check(self):
+            return hasattr(self.transformer, attr)
+
+        return check
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : list of length n_samples, or ndarray of shape (n_samples, ...)
+            Collection of inputs to be fit-transformed by `transformer`.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_collection(X, accept_sparse=True, accept_large_sparse=True,
+                         force_all_finite=False)
+        self._validate_transformer()
+
+        self._is_fitted = True
+        return self
+
+    @available_if(_transformer_has("fit_transform"))
+    def fit_transform(self, X, y=None):
+        """Fit-transform a clone of `transformer` to each element in the
+        collection `X`.
+
+        Parameters
+        ----------
+        X : list of length n_samples, or ndarray of shape (n_samples, ...)
+            Collection of inputs to be fit-transformed by `transformer`.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : list of length n_samples, or ndarray of shape (n_samples, ...)
+            Collection of outputs. It is a list unless all outputs have the
+            same shape, in which case it is converted to an ndarray.
+
+        """
+        Xt = check_collection(X, accept_sparse=True, accept_large_sparse=True,
+                              force_all_finite=False)
+        self._validate_transformer()
+
+        Xt = Parallel(n_jobs=self.n_jobs, prefer=self.parallel_backend_prefer,
+                      require=self.parallel_backend_require)(
+            delayed(clone(self.transformer).fit_transform)(x) for x in Xt
+            )
+
+        x0_shape = Xt[0].shape
+        if reduce(and_, (x.shape == x0_shape for x in Xt), True):
+            Xt = np.asarray(Xt)
+
+        return Xt
+
+    def transform(self, X, y=None):
+        """Alias for :meth:`fit_transform`.
+
+        Allows for this class to be used as an intermediate step in a
+        scikit-learn pipeline.
+
+        Parameters
+        ----------
+        X : list of length n_samples, or ndarray of shape (n_samples, ...)
+            Collection of inputs to be fit-transformed by `transformer`.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : list of length n_samples, or ndarray of shape (n_samples, ...)
+            Collection of outputs. It is a list unless all outputs have the
+            same shape, in which case it is converted to an ndarray.
+
+        """
+        return self.fit_transform(X, y)
```

## gtda/metaestimators/__init__.py

 * *Ordering differences only*

```diff
@@ -1,8 +1,8 @@
-"""The module :mod:`gtda.metaestimators` implements meta-estimators, i.e.
-estimators which take other estimators as parameters."""
-
-from .collection_transformer import CollectionTransformer
-
-__all__ = [
-    'CollectionTransformer'
-    ]
+"""The module :mod:`gtda.metaestimators` implements meta-estimators, i.e.
+estimators which take other estimators as parameters."""
+
+from .collection_transformer import CollectionTransformer
+
+__all__ = [
+    'CollectionTransformer'
+    ]
```

## gtda/plotting/diagram_representations.py

 * *Ordering differences only*

```diff
@@ -1,187 +1,187 @@
-"""Plotting functions for (vector) representations of persistence diagrams."""
-# License: GNU AGPLv3
-
-import numpy as np
-import plotly.graph_objs as gobj
-
-
-def plot_betti_curves(betti_numbers, samplings, homology_dimensions=None,
-                      plotly_params=None):
-    """Plot Betti curves by homology dimension.
-
-    Parameters
-    ----------
-    betti_numbers : ndarray of shape (n_homology_dimensions, n_bins)
-        Betti numbers, i.e. the y-coordinates of Betti curves. Entry i along
-        axis 0 is assumed to contain the Betti numbers for a discretised Betti
-        curve in homology dimension i.
-
-    samplings : ndarray of shape (n_homology_dimensions, n_bins)
-        Filtration parameter values to be used as the x-coordinates of the
-        Betti curves.
-
-    homology_dimensions : list, tuple or None, optional, default: ``None``
-        Which homology dimensions to include in the plot. If ``None``,
-        all available homology dimensions will be used.
-
-    plotly_params : dict or None, optional, default: ``None``
-        Custom parameters to configure the plotly figure. Allowed keys are
-        ``"traces"`` and ``"layout"``, and the corresponding values should be
-        dictionaries containing keyword arguments as would be fed to the
-        :meth:`update_traces` and :meth:`update_layout` methods of
-        :class:`plotly.graph_objects.Figure`.
-
-    Returns
-    -------
-    fig : :class:`plotly.graph_objects.Figure` object
-        Figure representing the Betti curves.
-
-    """
-    if homology_dimensions is None:
-        _homology_dimensions = list(range(betti_numbers.shape[0]))
-    else:
-        _homology_dimensions = homology_dimensions
-
-    layout = {
-        "xaxis1": {
-            "title": "Filtration parameter",
-            "side": "bottom",
-            "type": "linear",
-            "ticks": "outside",
-            "anchor": "x1",
-            "showline": True,
-            "zeroline": True,
-            "showexponent": "all",
-            "exponentformat": "e"
-            },
-        "yaxis1": {
-            "title": "Betti number",
-            "side": "left",
-            "type": "linear",
-            "ticks": "outside",
-            "anchor": "y1",
-            "showline": True,
-            "zeroline": True,
-            "showexponent": "all",
-            "exponentformat": "e"
-            },
-        "plot_bgcolor": "white"
-        }
-
-    fig = gobj.Figure(layout=layout)
-    fig.update_xaxes(zeroline=True, linewidth=1, linecolor="black",
-                     mirror=False)
-    fig.update_yaxes(zeroline=True, linewidth=1, linecolor="black",
-                     mirror=False)
-
-    for dim in _homology_dimensions:
-        fig.add_trace(gobj.Scatter(x=samplings[dim],
-                                   y=betti_numbers[dim],
-                                   mode="lines", showlegend=True,
-                                   hoverinfo="none",
-                                   name=f"H{int(dim)}"))
-
-    # Update traces and layout according to user input
-    if plotly_params:
-        fig.update_traces(plotly_params.get("traces", None))
-        fig.update_layout(plotly_params.get("layout", None))
-
-    return fig
-
-
-def plot_betti_surfaces(betti_curves, samplings=None,
-                        homology_dimensions=None, plotly_params=None):
-    """Plot Betti surfaces (Betti numbers against "time" and filtration
-    parameter) by homology dimension.
-
-    Parameters
-    ----------
-    betti_curves : ndarray of shape (n_samples, n_homology_dimensions, \
-        n_bins)
-        Collection whose each entry contains the Betti numbers for
-        ``n_homology_dimensions`` discretised Betti curves. Index i along axis
-        1 is assumed to correspond to homology dimension i.
-
-    samplings : ndarray of shape (n_homology_dimensions, n_bins)
-        Filtration parameter values to be used as one of the independent
-        variables when plotting the Betti surfaces. The other independent
-        variable is "time", i.e. the sample index.
-
-    homology_dimensions : list, tuple or None, optional, default: ``None``
-        Homology dimensions for which the Betti surfaces should be plotted.
-        If ``None``, all available dimensions will be used.
-
-    samplings : ndarray of shape (n_homology_dimensions, n_bins)
-        For each homology dimension, (filtration parameter) values to be used
-        on the x-axis against the corresponding values in `betti_curves` on the
-        y-axis.
-
-    plotly_params : dict or None, optional, default: ``None``
-        Custom parameters to configure the plotly figure. Allowed keys are
-        ``"traces"`` and ``"layout"``, and the corresponding values should be
-        dictionaries containing keyword arguments as would be fed to the
-        :meth:`update_traces` and :meth:`update_layout` methods of
-        :class:`plotly.graph_objects.Figure`.
-
-    Returns
-    -------
-    figs/fig : tuple of :class:`plotly.graph_objects.Figure`/\
-        :class:`plotly.graph_objects.Figure` object
-        If ``n_samples > 1``, a tuple of figures representing the Betti
-        surfaces, with one figure per dimension in `homology_dimensions`.
-        Otherwise, a single figure representing the Betti curve of the
-        single sample present.
-
-    """
-    if homology_dimensions is None:
-        _homology_dimensions = list(range(betti_curves.shape[1]))
-    else:
-        _homology_dimensions = homology_dimensions
-
-    scene = {
-        "xaxis": {
-            "title": "Filtration parameter",
-            "type": "linear",
-            "showexponent": "all",
-            "exponentformat": "e"
-            },
-        "yaxis": {
-            "title": "Time",
-            "type": "linear",
-            "showexponent": "all",
-            "exponentformat": "e"
-            },
-        "zaxis": {
-            "title": "Betti number",
-            "type": "linear",
-            "showexponent": "all",
-            "exponentformat": "e"
-            }
-        }
-
-    if betti_curves.shape[0] == 1:
-        return plot_betti_curves(
-            betti_curves[0], samplings,
-            homology_dimensions=homology_dimensions,
-            plotly_params=plotly_params
-            )
-    else:
-        figs = []
-        for dim in _homology_dimensions:
-            fig = gobj.Figure()
-            fig.update_layout(scene=scene,
-                              title=f"Betti surface for homology "
-                                    f"dimension {int(dim)}")
-            fig.add_trace(gobj.Surface(x=samplings[dim],
-                                       y=np.arange(betti_curves.shape[0]),
-                                       z=betti_curves[:, dim],
-                                       connectgaps=True, hoverinfo="none"))
-
-            # Update traces and layout according to user input
-            if plotly_params:
-                fig.update_traces(plotly_params.get("traces", None))
-                fig.update_layout(plotly_params.get("layout", None))
-
-            figs.append(fig)
-
-        return tuple(figs)
+"""Plotting functions for (vector) representations of persistence diagrams."""
+# License: GNU AGPLv3
+
+import numpy as np
+import plotly.graph_objs as gobj
+
+
+def plot_betti_curves(betti_numbers, samplings, homology_dimensions=None,
+                      plotly_params=None):
+    """Plot Betti curves by homology dimension.
+
+    Parameters
+    ----------
+    betti_numbers : ndarray of shape (n_homology_dimensions, n_bins)
+        Betti numbers, i.e. the y-coordinates of Betti curves. Entry i along
+        axis 0 is assumed to contain the Betti numbers for a discretised Betti
+        curve in homology dimension i.
+
+    samplings : ndarray of shape (n_homology_dimensions, n_bins)
+        Filtration parameter values to be used as the x-coordinates of the
+        Betti curves.
+
+    homology_dimensions : list, tuple or None, optional, default: ``None``
+        Which homology dimensions to include in the plot. If ``None``,
+        all available homology dimensions will be used.
+
+    plotly_params : dict or None, optional, default: ``None``
+        Custom parameters to configure the plotly figure. Allowed keys are
+        ``"traces"`` and ``"layout"``, and the corresponding values should be
+        dictionaries containing keyword arguments as would be fed to the
+        :meth:`update_traces` and :meth:`update_layout` methods of
+        :class:`plotly.graph_objects.Figure`.
+
+    Returns
+    -------
+    fig : :class:`plotly.graph_objects.Figure` object
+        Figure representing the Betti curves.
+
+    """
+    if homology_dimensions is None:
+        _homology_dimensions = list(range(betti_numbers.shape[0]))
+    else:
+        _homology_dimensions = homology_dimensions
+
+    layout = {
+        "xaxis1": {
+            "title": "Filtration parameter",
+            "side": "bottom",
+            "type": "linear",
+            "ticks": "outside",
+            "anchor": "x1",
+            "showline": True,
+            "zeroline": True,
+            "showexponent": "all",
+            "exponentformat": "e"
+            },
+        "yaxis1": {
+            "title": "Betti number",
+            "side": "left",
+            "type": "linear",
+            "ticks": "outside",
+            "anchor": "y1",
+            "showline": True,
+            "zeroline": True,
+            "showexponent": "all",
+            "exponentformat": "e"
+            },
+        "plot_bgcolor": "white"
+        }
+
+    fig = gobj.Figure(layout=layout)
+    fig.update_xaxes(zeroline=True, linewidth=1, linecolor="black",
+                     mirror=False)
+    fig.update_yaxes(zeroline=True, linewidth=1, linecolor="black",
+                     mirror=False)
+
+    for dim in _homology_dimensions:
+        fig.add_trace(gobj.Scatter(x=samplings[dim],
+                                   y=betti_numbers[dim],
+                                   mode="lines", showlegend=True,
+                                   hoverinfo="none",
+                                   name=f"H{int(dim)}"))
+
+    # Update traces and layout according to user input
+    if plotly_params:
+        fig.update_traces(plotly_params.get("traces", None))
+        fig.update_layout(plotly_params.get("layout", None))
+
+    return fig
+
+
+def plot_betti_surfaces(betti_curves, samplings=None,
+                        homology_dimensions=None, plotly_params=None):
+    """Plot Betti surfaces (Betti numbers against "time" and filtration
+    parameter) by homology dimension.
+
+    Parameters
+    ----------
+    betti_curves : ndarray of shape (n_samples, n_homology_dimensions, \
+        n_bins)
+        Collection whose each entry contains the Betti numbers for
+        ``n_homology_dimensions`` discretised Betti curves. Index i along axis
+        1 is assumed to correspond to homology dimension i.
+
+    samplings : ndarray of shape (n_homology_dimensions, n_bins)
+        Filtration parameter values to be used as one of the independent
+        variables when plotting the Betti surfaces. The other independent
+        variable is "time", i.e. the sample index.
+
+    homology_dimensions : list, tuple or None, optional, default: ``None``
+        Homology dimensions for which the Betti surfaces should be plotted.
+        If ``None``, all available dimensions will be used.
+
+    samplings : ndarray of shape (n_homology_dimensions, n_bins)
+        For each homology dimension, (filtration parameter) values to be used
+        on the x-axis against the corresponding values in `betti_curves` on the
+        y-axis.
+
+    plotly_params : dict or None, optional, default: ``None``
+        Custom parameters to configure the plotly figure. Allowed keys are
+        ``"traces"`` and ``"layout"``, and the corresponding values should be
+        dictionaries containing keyword arguments as would be fed to the
+        :meth:`update_traces` and :meth:`update_layout` methods of
+        :class:`plotly.graph_objects.Figure`.
+
+    Returns
+    -------
+    figs/fig : tuple of :class:`plotly.graph_objects.Figure`/\
+        :class:`plotly.graph_objects.Figure` object
+        If ``n_samples > 1``, a tuple of figures representing the Betti
+        surfaces, with one figure per dimension in `homology_dimensions`.
+        Otherwise, a single figure representing the Betti curve of the
+        single sample present.
+
+    """
+    if homology_dimensions is None:
+        _homology_dimensions = list(range(betti_curves.shape[1]))
+    else:
+        _homology_dimensions = homology_dimensions
+
+    scene = {
+        "xaxis": {
+            "title": "Filtration parameter",
+            "type": "linear",
+            "showexponent": "all",
+            "exponentformat": "e"
+            },
+        "yaxis": {
+            "title": "Time",
+            "type": "linear",
+            "showexponent": "all",
+            "exponentformat": "e"
+            },
+        "zaxis": {
+            "title": "Betti number",
+            "type": "linear",
+            "showexponent": "all",
+            "exponentformat": "e"
+            }
+        }
+
+    if betti_curves.shape[0] == 1:
+        return plot_betti_curves(
+            betti_curves[0], samplings,
+            homology_dimensions=homology_dimensions,
+            plotly_params=plotly_params
+            )
+    else:
+        figs = []
+        for dim in _homology_dimensions:
+            fig = gobj.Figure()
+            fig.update_layout(scene=scene,
+                              title=f"Betti surface for homology "
+                                    f"dimension {int(dim)}")
+            fig.add_trace(gobj.Surface(x=samplings[dim],
+                                       y=np.arange(betti_curves.shape[0]),
+                                       z=betti_curves[:, dim],
+                                       connectgaps=True, hoverinfo="none"))
+
+            # Update traces and layout according to user input
+            if plotly_params:
+                fig.update_traces(plotly_params.get("traces", None))
+                fig.update_layout(plotly_params.get("layout", None))
+
+            figs.append(fig)
+
+        return tuple(figs)
```

## gtda/plotting/images.py

 * *Ordering differences only*

```diff
@@ -1,62 +1,62 @@
-"""Image-related plotting functions and classes."""
-# License: GNU AGPLv3
-
-import plotly.graph_objects as gobj
-
-
-def plot_heatmap(data, x=None, y=None, colorscale="greys", origin="upper",
-                 title=None, plotly_params=None):
-    """Plot a 2D single-channel image, as a heat map from 2D array data.
-
-    Parameters
-    ----------
-    data : ndarray of shape (n_pixels_x, n_pixels_y)
-        Data describing the heat map value-to-color mapping.
-
-    x : ndarray of shape (n_pixels_x,) or None, optional, default: ``None``
-        Horizontal coordinates of the pixels in `data`.
-
-    y : ndarray of shape (n_pixels_y,) or None, optional, default: ``None``
-        Vertical coordinates of the pixels in `data`.
-
-    colorscale : str, optional, default: ``"greys"``
-        Color scale to be used in the heat map. Can be anything allowed by
-        :class:`plotly.graph_objects.Heatmap`.
-
-    origin : ``"upper"`` | ``"lower"``, optional, default: ``"upper"``
-        Position of the [0, 0] pixel of `data`, in the upper left or lower
-        left corner. The convention ``"upper"`` is typically used for
-        matrices and images.
-
-    title : str or None, optional, default: ``None``
-        Title of the resulting figure.
-
-    plotly_params : dict or None, optional, default: ``None``
-        Custom parameters to configure the plotly figure. Allowed keys are
-        ``"trace"`` and ``"layout"``, and the corresponding values should be
-        dictionaries containing keyword arguments as would be fed to the
-        :meth:`update_traces` and :meth:`update_layout` methods of
-        :class:`plotly.graph_objects.Figure`.
-
-    Returns
-    -------
-    fig : :class:`plotly.graph_objects.Figure` object
-        Figure representing the 2D single-channel image.
-
-    """
-    autorange = True if origin == "lower" else "reversed"
-    layout = {
-        "xaxis": {"scaleanchor": "y", "constrain": "domain"},
-        "yaxis": {"autorange": autorange, "constrain": "domain"},
-        "plot_bgcolor": "white",
-        "title": title
-        }
-    fig = gobj.Figure(layout=layout)
-    fig.add_trace(gobj.Heatmap(z=data * 1, x=x, y=y, colorscale=colorscale))
-
-    # Update trace and layout according to user input
-    if plotly_params:
-        fig.update_traces(plotly_params.get("trace", None))
-        fig.update_layout(plotly_params.get("layout", None))
-
-    return fig
+"""Image-related plotting functions and classes."""
+# License: GNU AGPLv3
+
+import plotly.graph_objects as gobj
+
+
+def plot_heatmap(data, x=None, y=None, colorscale="greys", origin="upper",
+                 title=None, plotly_params=None):
+    """Plot a 2D single-channel image, as a heat map from 2D array data.
+
+    Parameters
+    ----------
+    data : ndarray of shape (n_pixels_x, n_pixels_y)
+        Data describing the heat map value-to-color mapping.
+
+    x : ndarray of shape (n_pixels_x,) or None, optional, default: ``None``
+        Horizontal coordinates of the pixels in `data`.
+
+    y : ndarray of shape (n_pixels_y,) or None, optional, default: ``None``
+        Vertical coordinates of the pixels in `data`.
+
+    colorscale : str, optional, default: ``"greys"``
+        Color scale to be used in the heat map. Can be anything allowed by
+        :class:`plotly.graph_objects.Heatmap`.
+
+    origin : ``"upper"`` | ``"lower"``, optional, default: ``"upper"``
+        Position of the [0, 0] pixel of `data`, in the upper left or lower
+        left corner. The convention ``"upper"`` is typically used for
+        matrices and images.
+
+    title : str or None, optional, default: ``None``
+        Title of the resulting figure.
+
+    plotly_params : dict or None, optional, default: ``None``
+        Custom parameters to configure the plotly figure. Allowed keys are
+        ``"trace"`` and ``"layout"``, and the corresponding values should be
+        dictionaries containing keyword arguments as would be fed to the
+        :meth:`update_traces` and :meth:`update_layout` methods of
+        :class:`plotly.graph_objects.Figure`.
+
+    Returns
+    -------
+    fig : :class:`plotly.graph_objects.Figure` object
+        Figure representing the 2D single-channel image.
+
+    """
+    autorange = True if origin == "lower" else "reversed"
+    layout = {
+        "xaxis": {"scaleanchor": "y", "constrain": "domain"},
+        "yaxis": {"autorange": autorange, "constrain": "domain"},
+        "plot_bgcolor": "white",
+        "title": title
+        }
+    fig = gobj.Figure(layout=layout)
+    fig.add_trace(gobj.Heatmap(z=data * 1, x=x, y=y, colorscale=colorscale))
+
+    # Update trace and layout according to user input
+    if plotly_params:
+        fig.update_traces(plotly_params.get("trace", None))
+        fig.update_layout(plotly_params.get("layout", None))
+
+    return fig
```

## gtda/plotting/persistence_diagrams.py

 * *Ordering differences only*

```diff
@@ -1,145 +1,145 @@
-"""Persistent-homologyrelated plotting functions and classes."""
-# License: GNU AGPLv3
-
-import numpy as np
-import plotly.graph_objs as gobj
-
-
-def plot_diagram(diagram, homology_dimensions=None, plotly_params=None):
-    """Plot a single persistence diagram.
-
-    Parameters
-    ----------
-    diagram : ndarray of shape (n_points, 3)
-        The persistence diagram to plot, where the third dimension along axis 1
-        contains homology dimensions, and the first two contain (birth, death)
-        pairs to be used as coordinates in the two-dimensional plot.
-
-    homology_dimensions : list of int or None, optional, default: ``None``
-        Homology dimensions which will appear on the plot. If ``None``, all
-        homology dimensions which appear in `diagram` will be plotted.
-
-    plotly_params : dict or None, optional, default: ``None``
-        Custom parameters to configure the plotly figure. Allowed keys are
-        ``"traces"`` and ``"layout"``, and the corresponding values should be
-        dictionaries containing keyword arguments as would be fed to the
-        :meth:`update_traces` and :meth:`update_layout` methods of
-        :class:`plotly.graph_objects.Figure`.
-
-    Returns
-    -------
-    fig : :class:`plotly.graph_objects.Figure` object
-        Figure representing the persistence diagram.
-
-    """
-    # TODO: increase the marker size
-    if homology_dimensions is None:
-        homology_dimensions = np.unique(diagram[:, 2])
-
-    diagram = diagram[diagram[:, 0] != diagram[:, 1]]
-    diagram_no_dims = diagram[:, :2]
-    posinfinite_mask = np.isposinf(diagram_no_dims)
-    neginfinite_mask = np.isneginf(diagram_no_dims)
-    if diagram_no_dims.size:
-        max_val = np.max(np.where(posinfinite_mask, -np.inf, diagram_no_dims))
-        min_val = np.min(np.where(neginfinite_mask, np.inf, diagram_no_dims))
-    else:
-        # Dummy values if diagram is empty
-        max_val = 1
-        min_val = 0
-    parameter_range = max_val - min_val
-    extra_space_factor = 0.02
-    has_posinfinite_death = np.any(posinfinite_mask[:, 1])
-    if has_posinfinite_death:
-        posinfinity_val = max_val + 0.1 * parameter_range
-        extra_space_factor += 0.1
-    extra_space = extra_space_factor * parameter_range
-    min_val_display = min_val - extra_space
-    max_val_display = max_val + extra_space
-
-    fig = gobj.Figure()
-    fig.add_trace(gobj.Scatter(
-        x=[min_val_display, max_val_display],
-        y=[min_val_display, max_val_display],
-        mode="lines",
-        line={"dash": "dash", "width": 1, "color": "black"},
-        showlegend=False,
-        hoverinfo="none"
-        ))
-
-    for dim in homology_dimensions:
-        name = f"H{int(dim)}" if dim != np.inf else "Any homology dimension"
-        subdiagram = diagram[diagram[:, 2] == dim]
-        unique, inverse, counts = np.unique(
-            subdiagram, axis=0, return_inverse=True, return_counts=True
-            )
-        hovertext = [
-            f"{tuple(unique[unique_row_index][:2])}" +
-            (
-                f", multiplicity: {counts[unique_row_index]}"
-                if counts[unique_row_index] > 1 else ""
-            )
-            for unique_row_index in inverse
-            ]
-        y = subdiagram[:, 1]
-        if has_posinfinite_death:
-            y[np.isposinf(y)] = posinfinity_val
-        fig.add_trace(gobj.Scatter(
-            x=subdiagram[:, 0], y=y, mode="markers",
-            hoverinfo="text", hovertext=hovertext, name=name
-        ))
-
-    fig.update_layout(
-        width=500,
-        height=500,
-        xaxis1={
-            "title": "Birth",
-            "side": "bottom",
-            "type": "linear",
-            "range": [min_val_display, max_val_display],
-            "autorange": False,
-            "ticks": "outside",
-            "showline": True,
-            "zeroline": True,
-            "linewidth": 1,
-            "linecolor": "black",
-            "mirror": False,
-            "showexponent": "all",
-            "exponentformat": "e"
-            },
-        yaxis1={
-            "title": "Death",
-            "side": "left",
-            "type": "linear",
-            "range": [min_val_display, max_val_display],
-            "autorange": False, "scaleanchor": "x", "scaleratio": 1,
-            "ticks": "outside",
-            "showline": True,
-            "zeroline": True,
-            "linewidth": 1,
-            "linecolor": "black",
-            "mirror": False,
-            "showexponent": "all",
-            "exponentformat": "e"
-            },
-        plot_bgcolor="white"
-        )
-
-    # Add a horizontal dashed line for points with infinite death
-    if has_posinfinite_death:
-        fig.add_trace(gobj.Scatter(
-            x=[min_val_display, max_val_display],
-            y=[posinfinity_val, posinfinity_val],
-            mode="lines",
-            line={"dash": "dash", "width": 0.5, "color": "black"},
-            showlegend=True,
-            name=u"\u221E",
-            hoverinfo="none"
-        ))
-
-    # Update traces and layout according to user input
-    if plotly_params:
-        fig.update_traces(plotly_params.get("traces", None))
-        fig.update_layout(plotly_params.get("layout", None))
-
-    return fig
+"""Persistent-homologyrelated plotting functions and classes."""
+# License: GNU AGPLv3
+
+import numpy as np
+import plotly.graph_objs as gobj
+
+
+def plot_diagram(diagram, homology_dimensions=None, plotly_params=None):
+    """Plot a single persistence diagram.
+
+    Parameters
+    ----------
+    diagram : ndarray of shape (n_points, 3)
+        The persistence diagram to plot, where the third dimension along axis 1
+        contains homology dimensions, and the first two contain (birth, death)
+        pairs to be used as coordinates in the two-dimensional plot.
+
+    homology_dimensions : list of int or None, optional, default: ``None``
+        Homology dimensions which will appear on the plot. If ``None``, all
+        homology dimensions which appear in `diagram` will be plotted.
+
+    plotly_params : dict or None, optional, default: ``None``
+        Custom parameters to configure the plotly figure. Allowed keys are
+        ``"traces"`` and ``"layout"``, and the corresponding values should be
+        dictionaries containing keyword arguments as would be fed to the
+        :meth:`update_traces` and :meth:`update_layout` methods of
+        :class:`plotly.graph_objects.Figure`.
+
+    Returns
+    -------
+    fig : :class:`plotly.graph_objects.Figure` object
+        Figure representing the persistence diagram.
+
+    """
+    # TODO: increase the marker size
+    if homology_dimensions is None:
+        homology_dimensions = np.unique(diagram[:, 2])
+
+    diagram = diagram[diagram[:, 0] != diagram[:, 1]]
+    diagram_no_dims = diagram[:, :2]
+    posinfinite_mask = np.isposinf(diagram_no_dims)
+    neginfinite_mask = np.isneginf(diagram_no_dims)
+    if diagram_no_dims.size:
+        max_val = np.max(np.where(posinfinite_mask, -np.inf, diagram_no_dims))
+        min_val = np.min(np.where(neginfinite_mask, np.inf, diagram_no_dims))
+    else:
+        # Dummy values if diagram is empty
+        max_val = 1
+        min_val = 0
+    parameter_range = max_val - min_val
+    extra_space_factor = 0.02
+    has_posinfinite_death = np.any(posinfinite_mask[:, 1])
+    if has_posinfinite_death:
+        posinfinity_val = max_val + 0.1 * parameter_range
+        extra_space_factor += 0.1
+    extra_space = extra_space_factor * parameter_range
+    min_val_display = min_val - extra_space
+    max_val_display = max_val + extra_space
+
+    fig = gobj.Figure()
+    fig.add_trace(gobj.Scatter(
+        x=[min_val_display, max_val_display],
+        y=[min_val_display, max_val_display],
+        mode="lines",
+        line={"dash": "dash", "width": 1, "color": "black"},
+        showlegend=False,
+        hoverinfo="none"
+        ))
+
+    for dim in homology_dimensions:
+        name = f"H{int(dim)}" if dim != np.inf else "Any homology dimension"
+        subdiagram = diagram[diagram[:, 2] == dim]
+        unique, inverse, counts = np.unique(
+            subdiagram, axis=0, return_inverse=True, return_counts=True
+            )
+        hovertext = [
+            f"{tuple(unique[unique_row_index][:2])}" +
+            (
+                f", multiplicity: {counts[unique_row_index]}"
+                if counts[unique_row_index] > 1 else ""
+            )
+            for unique_row_index in inverse
+            ]
+        y = subdiagram[:, 1]
+        if has_posinfinite_death:
+            y[np.isposinf(y)] = posinfinity_val
+        fig.add_trace(gobj.Scatter(
+            x=subdiagram[:, 0], y=y, mode="markers",
+            hoverinfo="text", hovertext=hovertext, name=name
+        ))
+
+    fig.update_layout(
+        width=500,
+        height=500,
+        xaxis1={
+            "title": "Birth",
+            "side": "bottom",
+            "type": "linear",
+            "range": [min_val_display, max_val_display],
+            "autorange": False,
+            "ticks": "outside",
+            "showline": True,
+            "zeroline": True,
+            "linewidth": 1,
+            "linecolor": "black",
+            "mirror": False,
+            "showexponent": "all",
+            "exponentformat": "e"
+            },
+        yaxis1={
+            "title": "Death",
+            "side": "left",
+            "type": "linear",
+            "range": [min_val_display, max_val_display],
+            "autorange": False, "scaleanchor": "x", "scaleratio": 1,
+            "ticks": "outside",
+            "showline": True,
+            "zeroline": True,
+            "linewidth": 1,
+            "linecolor": "black",
+            "mirror": False,
+            "showexponent": "all",
+            "exponentformat": "e"
+            },
+        plot_bgcolor="white"
+        )
+
+    # Add a horizontal dashed line for points with infinite death
+    if has_posinfinite_death:
+        fig.add_trace(gobj.Scatter(
+            x=[min_val_display, max_val_display],
+            y=[posinfinity_val, posinfinity_val],
+            mode="lines",
+            line={"dash": "dash", "width": 0.5, "color": "black"},
+            showlegend=True,
+            name=u"\u221E",
+            hoverinfo="none"
+        ))
+
+    # Update traces and layout according to user input
+    if plotly_params:
+        fig.update_traces(plotly_params.get("traces", None))
+        fig.update_layout(plotly_params.get("layout", None))
+
+    return fig
```

## gtda/plotting/point_clouds.py

 * *Ordering differences only*

```diff
@@ -1,135 +1,135 @@
-"""Point-cloudrelated plotting functions and classes."""
-# License: GNU AGPLv3
-
-import numpy as np
-import plotly.graph_objs as gobj
-
-from ..utils.validation import validate_params
-
-
-def plot_point_cloud(point_cloud, dimension=None, plotly_params=None):
-    """Plot the first 2 or 3 coordinates of a point cloud.
-
-    Note: this function does not work on 1D arrays.
-
-    Parameters
-    ----------
-    point_cloud : ndarray of shape (n_samples, n_dimensions)
-        Data points to be represented in a 2D or 3D scatter plot. Only the
-        first 2 or 3 dimensions will be considered for plotting.
-
-    dimension : int or None, default: ``None``
-        Sets the dimension of the resulting plot. If ``None``, the dimension
-        will be chosen between 2 and 3 depending on the shape of `point_cloud`.
-
-    plotly_params : dict or None, optional, default: ``None``
-        Custom parameters to configure the plotly figure. Allowed keys are
-        ``"trace"`` and ``"layout"``, and the corresponding values should be
-        dictionaries containing keyword arguments as would be fed to the
-        :meth:`update_traces` and :meth:`update_layout` methods of
-        :class:`plotly.graph_objects.Figure`.
-
-    Returns
-    -------
-    fig : :class:`plotly.graph_objects.Figure` object
-        Figure representing a point cloud in 2D or 3D.
-
-    """
-    # TODO: increase the marker size
-    validate_params({"dimension": dimension},
-                    {"dimension": {"type": (int, type(None)), "in": [2, 3]}})
-    if dimension is None:
-        dimension = np.min((3, point_cloud.shape[1]))
-
-    # Check consistency between point_cloud and dimension
-    if point_cloud.shape[1] < dimension:
-        raise ValueError("Not enough dimensions available in the input point "
-                         "cloud.")
-
-    elif dimension == 2:
-        layout = {
-            "width": 600,
-            "height": 600,
-            "xaxis1": {
-                "title": "0th",
-                "side": "bottom",
-                "type": "linear",
-                "ticks": "outside",
-                "anchor": "x1",
-                "showline": True,
-                "zeroline": True,
-                "showexponent": "all",
-                "exponentformat": "e"
-                },
-            "yaxis1": {
-                "title": "1st",
-                "side": "left",
-                "type": "linear",
-                "ticks": "outside",
-                "anchor": "y1",
-                "showline": True,
-                "zeroline": True,
-                "showexponent": "all",
-                "exponentformat": "e"
-                },
-            "plot_bgcolor": "white"
-            }
-
-        fig = gobj.Figure(layout=layout)
-        fig.update_xaxes(zeroline=True, linewidth=1, linecolor="black",
-                         mirror=False)
-        fig.update_yaxes(zeroline=True, linewidth=1, linecolor="black",
-                         mirror=False)
-
-        fig.add_trace(gobj.Scatter(
-            x=point_cloud[:, 0],
-            y=point_cloud[:, 1],
-            mode="markers",
-            marker={"size": 4,
-                    "color": list(range(point_cloud.shape[0])),
-                    "colorscale": "Viridis",
-                    "opacity": 0.8}
-            ))
-
-    elif dimension == 3:
-        scene = {
-            "xaxis": {
-                "title": "0th",
-                "type": "linear",
-                "showexponent": "all",
-                "exponentformat": "e"
-                },
-            "yaxis": {
-                "title": "1st",
-                "type": "linear",
-                "showexponent": "all",
-                "exponentformat": "e"
-                },
-            "zaxis": {
-                "title": "2nd",
-                "type": "linear",
-                "showexponent": "all",
-                "exponentformat": "e"
-                }
-            }
-
-        fig = gobj.Figure()
-        fig.update_layout(scene=scene)
-
-        fig.add_trace(gobj.Scatter3d(
-            x=point_cloud[:, 0],
-            y=point_cloud[:, 1],
-            z=point_cloud[:, 2],
-            mode="markers",
-            marker={"size": 4,
-                    "color": list(range(point_cloud.shape[0])),
-                    "colorscale": "Viridis",
-                    "opacity": 0.8}
-            ))
-
-    # Update trace and layout according to user input
-    if plotly_params:
-        fig.update_traces(plotly_params.get("trace", None))
-        fig.update_layout(plotly_params.get("layout", None))
-
-    return fig
+"""Point-cloudrelated plotting functions and classes."""
+# License: GNU AGPLv3
+
+import numpy as np
+import plotly.graph_objs as gobj
+
+from ..utils.validation import validate_params
+
+
+def plot_point_cloud(point_cloud, dimension=None, plotly_params=None):
+    """Plot the first 2 or 3 coordinates of a point cloud.
+
+    Note: this function does not work on 1D arrays.
+
+    Parameters
+    ----------
+    point_cloud : ndarray of shape (n_samples, n_dimensions)
+        Data points to be represented in a 2D or 3D scatter plot. Only the
+        first 2 or 3 dimensions will be considered for plotting.
+
+    dimension : int or None, default: ``None``
+        Sets the dimension of the resulting plot. If ``None``, the dimension
+        will be chosen between 2 and 3 depending on the shape of `point_cloud`.
+
+    plotly_params : dict or None, optional, default: ``None``
+        Custom parameters to configure the plotly figure. Allowed keys are
+        ``"trace"`` and ``"layout"``, and the corresponding values should be
+        dictionaries containing keyword arguments as would be fed to the
+        :meth:`update_traces` and :meth:`update_layout` methods of
+        :class:`plotly.graph_objects.Figure`.
+
+    Returns
+    -------
+    fig : :class:`plotly.graph_objects.Figure` object
+        Figure representing a point cloud in 2D or 3D.
+
+    """
+    # TODO: increase the marker size
+    validate_params({"dimension": dimension},
+                    {"dimension": {"type": (int, type(None)), "in": [2, 3]}})
+    if dimension is None:
+        dimension = np.min((3, point_cloud.shape[1]))
+
+    # Check consistency between point_cloud and dimension
+    if point_cloud.shape[1] < dimension:
+        raise ValueError("Not enough dimensions available in the input point "
+                         "cloud.")
+
+    elif dimension == 2:
+        layout = {
+            "width": 600,
+            "height": 600,
+            "xaxis1": {
+                "title": "0th",
+                "side": "bottom",
+                "type": "linear",
+                "ticks": "outside",
+                "anchor": "x1",
+                "showline": True,
+                "zeroline": True,
+                "showexponent": "all",
+                "exponentformat": "e"
+                },
+            "yaxis1": {
+                "title": "1st",
+                "side": "left",
+                "type": "linear",
+                "ticks": "outside",
+                "anchor": "y1",
+                "showline": True,
+                "zeroline": True,
+                "showexponent": "all",
+                "exponentformat": "e"
+                },
+            "plot_bgcolor": "white"
+            }
+
+        fig = gobj.Figure(layout=layout)
+        fig.update_xaxes(zeroline=True, linewidth=1, linecolor="black",
+                         mirror=False)
+        fig.update_yaxes(zeroline=True, linewidth=1, linecolor="black",
+                         mirror=False)
+
+        fig.add_trace(gobj.Scatter(
+            x=point_cloud[:, 0],
+            y=point_cloud[:, 1],
+            mode="markers",
+            marker={"size": 4,
+                    "color": list(range(point_cloud.shape[0])),
+                    "colorscale": "Viridis",
+                    "opacity": 0.8}
+            ))
+
+    elif dimension == 3:
+        scene = {
+            "xaxis": {
+                "title": "0th",
+                "type": "linear",
+                "showexponent": "all",
+                "exponentformat": "e"
+                },
+            "yaxis": {
+                "title": "1st",
+                "type": "linear",
+                "showexponent": "all",
+                "exponentformat": "e"
+                },
+            "zaxis": {
+                "title": "2nd",
+                "type": "linear",
+                "showexponent": "all",
+                "exponentformat": "e"
+                }
+            }
+
+        fig = gobj.Figure()
+        fig.update_layout(scene=scene)
+
+        fig.add_trace(gobj.Scatter3d(
+            x=point_cloud[:, 0],
+            y=point_cloud[:, 1],
+            z=point_cloud[:, 2],
+            mode="markers",
+            marker={"size": 4,
+                    "color": list(range(point_cloud.shape[0])),
+                    "colorscale": "Viridis",
+                    "opacity": 0.8}
+            ))
+
+    # Update trace and layout according to user input
+    if plotly_params:
+        fig.update_traces(plotly_params.get("trace", None))
+        fig.update_layout(plotly_params.get("layout", None))
+
+    return fig
```

## gtda/plotting/__init__.py

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-"""The module :mod:`gtda.plotting` implements function to plot the outputs of
-giotto-tda transformers."""
-
-from .point_clouds import plot_point_cloud
-from .persistence_diagrams import plot_diagram
-from .diagram_representations import plot_betti_curves, plot_betti_surfaces
-from .images import plot_heatmap
-
-__all__ = [
-    'plot_point_cloud',
-    'plot_diagram',
-    'plot_heatmap',
-    'plot_betti_curves',
-    'plot_betti_surfaces'
-    ]
+"""The module :mod:`gtda.plotting` implements function to plot the outputs of
+giotto-tda transformers."""
+
+from .point_clouds import plot_point_cloud
+from .persistence_diagrams import plot_diagram
+from .diagram_representations import plot_betti_curves, plot_betti_surfaces
+from .images import plot_heatmap
+
+__all__ = [
+    'plot_point_cloud',
+    'plot_diagram',
+    'plot_heatmap',
+    'plot_betti_curves',
+    'plot_betti_surfaces'
+    ]
```

## gtda/point_clouds/rescaling.py

```diff
@@ -1,422 +1,422 @@
-"""Rescaling methods for persistent homology."""
-# License: GNU AGPLv3
-
-import itertools
-from numbers import Real
-from types import FunctionType
-
-import numpy as np
-from joblib import Parallel, delayed
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.metrics import pairwise_distances
-from sklearn.utils.validation import check_array, check_is_fitted
-
-from ..base import PlotterMixin
-from ..plotting import plot_heatmap
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.intervals import Interval
-from ..utils.validation import validate_params
-
-
-@adapt_fit_transform_docs
-class ConsistentRescaling(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Rescaling of distances between pairs of points by the geometric mean
-    of the distances to the respective :math:`k`-th nearest neighbours.
-
-    Based on ideas in [1]_. The computation during :meth:`transform` depends on
-    the nature of the array `X`. If each entry in `X` along axis 0 represents a
-    distance matrix :math:`D`, then the corresponding entry in the transformed
-    array is the distance matrix
-    :math:`D'_{i,j} = D_{i,j}/\\sqrt{D_{i,k_i}D_{j,k_j}}`, where :math:`k_i` is
-    the index of the :math:`k`-th largest value in row :math:`i` (and similarly
-    for :math:`j`). If the entries in `X` represent point clouds, their
-    distance matrices are first computed, and then rescaled according to the
-    same formula.
-
-    Parameters
-    ----------
-    metric : string or callable, optional, default: ``'euclidean'``
-        If set to ``'precomputed'``, each entry in `X` along axis 0 is
-        interpreted to be a distance matrix. Otherwise, entries are
-        interpreted as feature arrays, and `metric` determines a rule with
-        which to calculate distances between pairs of instances (i.e. rows)
-        in these arrays.
-        If `metric` is a string, it must be one of the options allowed by
-        :func:`scipy.spatial.distance.pdist` for its metric parameter, or a
-        metric listed in :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`,
-        including "euclidean", "manhattan" or "cosine".
-        If `metric` is a callable function, it is called on each pair of
-        instances and the resulting value recorded. The callable should take
-        two arrays from the entry in `X` as input, and return a value
-        indicating the distance between them.
-
-    metric_params : dict or None, optional, default: ``None``
-        Additional keyword arguments for the metric function.
-
-    neighbor_rank : int, optional, default: ``1``
-        Rank of the neighbors used to modify the metric structure according
-        to the "consistent rescaling" procedure.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    effective_metric_params_ : dict
-        Dictionary containing all information present in `metric_params`.
-        If `metric_params` is ``None``, it is set to the empty dictionary.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from gtda.point_clouds import ConsistentRescaling
-    >>> X = np.array([[[0, 0], [1, 2], [5, 6]]])
-    >>> cr = ConsistentRescaling()
-    >>> X_rescaled = cr.fit_transform(X)
-    >>> print(X_rescaled.shape)
-    (1, 3, 3)
-
-    See also
-    --------
-    ConsecutiveRescaling
-
-    References
-    ----------
-    .. [1] T. Berry and T. Sauer, "Consistent manifold representation for
-           topological data analysis"; *Foundations of data analysis* **1**,
-           pp. 1--38, 2019; `DOI: 10.3934/fods.2019001
-           <http://dx.doi.org/10.3934/fods.2019001>`_.
-
-    """
-
-    _hyperparameters = {
-        'metric': {'type': (str, FunctionType)},
-        'metric_params': {'type': (dict, type(None))},
-        'neighbor_rank': {'type': int,
-                          'in': Interval(1, np.inf, closed='left')}
-        }
-
-    def __init__(self, metric='euclidean', metric_params=None, neighbor_rank=1,
-                 n_jobs=None):
-        self.metric = metric
-        self.metric_params = metric_params
-        self.neighbor_rank = neighbor_rank
-        self.n_jobs = n_jobs
-
-    def _consistent_rescaling(self, X):
-        Xm = pairwise_distances(X, metric=self.metric, n_jobs=1,
-                                **self.effective_metric_params_)
-
-        indices_k_neighbor = np.argsort(Xm)[:, self.neighbor_rank]
-        distance_k_neighbor = Xm[np.arange(X.shape[0]),
-                                 indices_k_neighbor]
-
-        # Only calculate metric for upper triangle
-        Xc = np.zeros(Xm.shape)
-        iterator = itertools.combinations(range(Xm.shape[0]), 2)
-        for i, j in iterator:
-            Xc[i, j] = Xm[i, j] / (np.sqrt(distance_k_neighbor[i] *
-                                           distance_k_neighbor[j]))
-        return Xc + Xc.T
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`effective_metric_params_`. Then, return the
-        estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_points, n_points) or (n_samples, \
-            n_points, n_dimensions)
-            Input data. If ``metric == 'precomputed'``, the input should be an
-            ndarray whose each entry along axis 0 is a distance matrix of shape
-            ``(n_points, n_points)``. Otherwise, each such entry will be
-            interpreted as an array of ``n_points`` row vectors in
-            ``n_dimensions``-dimensional space.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_array(X, allow_nd=True)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        if self.metric_params is None:
-            self.effective_metric_params_ = {}
-        else:
-            self.effective_metric_params_ = self.metric_params.copy()
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each entry in the input data array X, find the metric structure
-        after consistent rescaling and encode it as a distance matrix.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_points, n_points) or (n_samples, \
-            n_points, n_dimensions)
-            Input data. If ``metric == 'precomputed'``, the input should be an
-            ndarray whose each entry along axis 0 is a distance matrix of shape
-            ``(n_points, n_points)``. Otherwise, each such entry will be
-            interpreted as an array of ``n_points`` row vectors in
-            ``n_dimensions``-dimensional space.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_points, n_points)
-            Array containing (as entries along axis 0) the distance matrices
-            after consistent rescaling.
-
-        """
-        check_is_fitted(self)
-        Xt = check_array(X, allow_nd=True)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(self._consistent_rescaling)(x) for x in Xt)
-        Xt = np.array(Xt)
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, colorscale='blues', plotly_params=None):
-        """Plot a sample from a collection of distance matrices.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_points, n_points)
-            Collection of distance matrices, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample to be plotted.
-
-        colorscale : str, optional, default: ``'blues'``
-            Color scale to be used in the heat map. Can be anything allowed by
-            :class:`plotly.graph_objects.Heatmap`.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_heatmap(
-            Xt[sample], colorscale=colorscale,
-            title=f"{sample}-th distance matrix after consistent rescaling",
-            plotly_params=plotly_params
-            )
-
-
-@adapt_fit_transform_docs
-class ConsecutiveRescaling(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Rescaling of distances between consecutive pairs of points by a fixed
-    factor.
-
-    The computation during :meth:`transform` depends on the nature of the array
-    `X`. If each entry in `X` along axis 0 represents a distance matrix
-    :math:`D`, then the corresponding entry in the transformed array is the
-    distance matrix :math:`D'_{i,i+1} = \\alpha D_{i,i+1}` where
-    :math:`\\alpha` is a positive factor. If the entries in `X` represent point
-    clouds, their distance matrices are first computed, and then rescaled
-    according to the same formula.
-
-    Parameters
-    ----------
-    metric : string or callable, optional, default: ``'euclidean'``
-        If set to ``'precomputed'``, each entry in `X` along axis 0 is
-        interpreted to be a distance matrix. Otherwise, entries are
-        interpreted as feature arrays, and `metric` determines a rule with
-        which to calculate distances between pairs of instances (i.e. rows)
-        in these arrays.
-        If `metric` is a string, it must be one of the options allowed by
-        :func:`scipy.spatial.distance.pdist` for its metric parameter, or a
-        metric listed in :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`,
-        including "euclidean", "manhattan" or "cosine".
-        If `metric` is a callable function, it is called on each pair of
-        instances and the resulting value recorded. The callable should take
-        two arrays from the entry in `X` as input, and return a value
-        indicating the distance between them.
-
-    metric_params : dict or None, optional, default: ``None``
-        Additional keyword arguments for the metric function.
-
-    factor : float, optional, default: ``0.``
-        Factor by which to multiply the distance between consecutive
-        points.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    effective_metric_params_ : dict
-        Dictionary containing all information present in `metric_params`.
-        If `metric_params` is ``None``, it is set to the empty dictionary.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from gtda.point_clouds import ConsecutiveRescaling
-    >>> X = np.array([[[0, 0], [1, 2], [5, 6]]])
-    >>> cr = ConsecutiveRescaling()
-    >>> X_rescaled = cr.fit_transform(X)
-    >>> print(X_rescaled.shape)
-    (1, 3, 3)
-
-    See also
-    --------
-    ConsistentRescaling
-
-    """
-
-    _hyperparameters = {
-        'metric': {'type': (str, FunctionType)},
-        'metric_params': {'type': (dict, type(None))},
-        'factor': {'type': Real, 'in': Interval(0, np.inf, closed='both')}
-        }
-
-    def __init__(self, metric='euclidean', metric_params=None, factor=0.,
-                 n_jobs=None):
-        self.metric = metric
-        self.metric_params = metric_params
-        self.factor = factor
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Calculate :attr:`effective_metric_params_`. Then, return the
-        estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_points, n_points) or (n_samples, \
-            n_points, n_dimensions)
-            Input data. If ``metric == 'precomputed'``, the input should be an
-            ndarray whose each entry along axis 0 is a distance matrix of shape
-            ``(n_points, n_points)``. Otherwise, each such entry will be
-            interpreted as an array of ``n_points`` row vectors in
-            ``n_dimensions``-dimensional space.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_array(X, allow_nd=True)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        if self.metric_params is None:
-            self.effective_metric_params_ = {}
-        else:
-            self.effective_metric_params_ = self.metric_params.copy()
-
-        return self
-
-    def transform(self, X, y=None):
-        """For each entry in the input data array X, find the metric structure
-        after consecutive rescaling and encode it as a distance matrix.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_points, n_points) or (n_samples, \
-            n_points, n_dimensions)
-            Input data. If ``metric == 'precomputed'``, the input should be an
-            ndarray whose each entry along axis 0 is a distance matrix of shape
-            ``(n_points, n_points)``. Otherwise, each such entry will be
-            interpreted as an array of ``n_points`` row vectors in
-            ``n_dimensions``-dimensional space.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_points, n_points)
-            Array containing (as entries along axis 0) the distance matrices
-            after consecutive rescaling.
-
-        """
-        check_is_fitted(self)
-        is_precomputed = self.metric == 'precomputed'
-        X = check_array(X, allow_nd=True, copy=is_precomputed)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(
-            delayed(pairwise_distances)(
-                x, metric=self.metric, n_jobs=1,
-                **self.effective_metric_params_)
-            for x in X)
-
-        if is_precomputed:
-            # Parallel loop above serves only as additional input validation
-            Xt = X
-        else:
-            Xt = np.array(Xt)
-        Xt[:, range(Xt.shape[1] - 1), range(1, Xt.shape[1])] *= self.factor
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, colorscale='blues', plotly_params=None):
-        """Plot a sample from a collection of distance matrices.
-
-        Parameters
-        ----------
-        Xt : ndarray of shape (n_samples, n_points, n_points)
-            Collection of distance matrices, such as returned by
-            :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample to be plotted.
-
-        colorscale : str, optional, default: ``'blues'``
-            Color scale to be used in the heat map. Can be anything allowed by
-            :class:`plotly.graph_objects.Heatmap`.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_heatmap(
-            Xt[sample], colorscale=colorscale,
-            title=f"{sample}-th distance matrix after consecutive rescaling",
-            plotly_params=plotly_params
-            )
+"""Rescaling methods for persistent homology."""
+# License: GNU AGPLv3
+
+import itertools
+from numbers import Real
+from typing import Callable
+
+import numpy as np
+from joblib import Parallel, delayed
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.metrics import pairwise_distances
+from sklearn.utils.validation import check_array, check_is_fitted
+
+from ..base import PlotterMixin
+from ..plotting import plot_heatmap
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.intervals import Interval
+from ..utils.validation import validate_params
+
+
+@adapt_fit_transform_docs
+class ConsistentRescaling(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Rescaling of distances between pairs of points by the geometric mean
+    of the distances to the respective :math:`k`-th nearest neighbours.
+
+    Based on ideas in [1]_. The computation during :meth:`transform` depends on
+    the nature of the array `X`. If each entry in `X` along axis 0 represents a
+    distance matrix :math:`D`, then the corresponding entry in the transformed
+    array is the distance matrix
+    :math:`D'_{i,j} = D_{i,j}/\\sqrt{D_{i,k_i}D_{j,k_j}}`, where :math:`k_i` is
+    the index of the :math:`k`-th largest value in row :math:`i` (and similarly
+    for :math:`j`). If the entries in `X` represent point clouds, their
+    distance matrices are first computed, and then rescaled according to the
+    same formula.
+
+    Parameters
+    ----------
+    metric : string or callable, optional, default: ``'euclidean'``
+        If set to ``'precomputed'``, each entry in `X` along axis 0 is
+        interpreted to be a distance matrix. Otherwise, entries are
+        interpreted as feature arrays, and `metric` determines a rule with
+        which to calculate distances between pairs of instances (i.e. rows)
+        in these arrays.
+        If `metric` is a string, it must be one of the options allowed by
+        :func:`scipy.spatial.distance.pdist` for its metric parameter, or a
+        metric listed in :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`,
+        including "euclidean", "manhattan" or "cosine".
+        If `metric` is a callable function, it is called on each pair of
+        instances and the resulting value recorded. The callable should take
+        two arrays from the entry in `X` as input, and return a value
+        indicating the distance between them.
+
+    metric_params : dict or None, optional, default: ``None``
+        Additional keyword arguments for the metric function.
+
+    neighbor_rank : int, optional, default: ``1``
+        Rank of the neighbors used to modify the metric structure according
+        to the "consistent rescaling" procedure.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    effective_metric_params_ : dict
+        Dictionary containing all information present in `metric_params`.
+        If `metric_params` is ``None``, it is set to the empty dictionary.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from gtda.point_clouds import ConsistentRescaling
+    >>> X = np.array([[[0, 0], [1, 2], [5, 6]]])
+    >>> cr = ConsistentRescaling()
+    >>> X_rescaled = cr.fit_transform(X)
+    >>> print(X_rescaled.shape)
+    (1, 3, 3)
+
+    See also
+    --------
+    ConsecutiveRescaling
+
+    References
+    ----------
+    .. [1] T. Berry and T. Sauer, "Consistent manifold representation for
+           topological data analysis"; *Foundations of data analysis* **1**,
+           pp. 1--38, 2019; `DOI: 10.3934/fods.2019001
+           <http://dx.doi.org/10.3934/fods.2019001>`_.
+
+    """
+
+    _hyperparameters = {
+        'metric': {'type': (str, Callable)},
+        'metric_params': {'type': (dict, type(None))},
+        'neighbor_rank': {'type': int,
+                          'in': Interval(1, np.inf, closed='left')}
+        }
+
+    def __init__(self, metric='euclidean', metric_params=None, neighbor_rank=1,
+                 n_jobs=None):
+        self.metric = metric
+        self.metric_params = metric_params
+        self.neighbor_rank = neighbor_rank
+        self.n_jobs = n_jobs
+
+    def _consistent_rescaling(self, X):
+        Xm = pairwise_distances(X, metric=self.metric, n_jobs=1,
+                                **self.effective_metric_params_)
+
+        indices_k_neighbor = np.argsort(Xm)[:, self.neighbor_rank]
+        distance_k_neighbor = Xm[np.arange(X.shape[0]),
+                                 indices_k_neighbor]
+
+        # Only calculate metric for upper triangle
+        Xc = np.zeros(Xm.shape)
+        iterator = itertools.combinations(range(Xm.shape[0]), 2)
+        for i, j in iterator:
+            Xc[i, j] = Xm[i, j] / (np.sqrt(distance_k_neighbor[i] *
+                                           distance_k_neighbor[j]))
+        return Xc + Xc.T
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`effective_metric_params_`. Then, return the
+        estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_points, n_points) or (n_samples, \
+            n_points, n_dimensions)
+            Input data. If ``metric == 'precomputed'``, the input should be an
+            ndarray whose each entry along axis 0 is a distance matrix of shape
+            ``(n_points, n_points)``. Otherwise, each such entry will be
+            interpreted as an array of ``n_points`` row vectors in
+            ``n_dimensions``-dimensional space.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_array(X, allow_nd=True)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        if self.metric_params is None:
+            self.effective_metric_params_ = {}
+        else:
+            self.effective_metric_params_ = self.metric_params.copy()
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each entry in the input data array X, find the metric structure
+        after consistent rescaling and encode it as a distance matrix.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_points, n_points) or (n_samples, \
+            n_points, n_dimensions)
+            Input data. If ``metric == 'precomputed'``, the input should be an
+            ndarray whose each entry along axis 0 is a distance matrix of shape
+            ``(n_points, n_points)``. Otherwise, each such entry will be
+            interpreted as an array of ``n_points`` row vectors in
+            ``n_dimensions``-dimensional space.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_points, n_points)
+            Array containing (as entries along axis 0) the distance matrices
+            after consistent rescaling.
+
+        """
+        check_is_fitted(self)
+        Xt = check_array(X, allow_nd=True)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(self._consistent_rescaling)(x) for x in Xt)
+        Xt = np.array(Xt)
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, colorscale='blues', plotly_params=None):
+        """Plot a sample from a collection of distance matrices.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_points, n_points)
+            Collection of distance matrices, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample to be plotted.
+
+        colorscale : str, optional, default: ``'blues'``
+            Color scale to be used in the heat map. Can be anything allowed by
+            :class:`plotly.graph_objects.Heatmap`.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_heatmap(
+            Xt[sample], colorscale=colorscale,
+            title=f"{sample}-th distance matrix after consistent rescaling",
+            plotly_params=plotly_params
+            )
+
+
+@adapt_fit_transform_docs
+class ConsecutiveRescaling(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Rescaling of distances between consecutive pairs of points by a fixed
+    factor.
+
+    The computation during :meth:`transform` depends on the nature of the array
+    `X`. If each entry in `X` along axis 0 represents a distance matrix
+    :math:`D`, then the corresponding entry in the transformed array is the
+    distance matrix :math:`D'_{i,i+1} = \\alpha D_{i,i+1}` where
+    :math:`\\alpha` is a positive factor. If the entries in `X` represent point
+    clouds, their distance matrices are first computed, and then rescaled
+    according to the same formula.
+
+    Parameters
+    ----------
+    metric : string or callable, optional, default: ``'euclidean'``
+        If set to ``'precomputed'``, each entry in `X` along axis 0 is
+        interpreted to be a distance matrix. Otherwise, entries are
+        interpreted as feature arrays, and `metric` determines a rule with
+        which to calculate distances between pairs of instances (i.e. rows)
+        in these arrays.
+        If `metric` is a string, it must be one of the options allowed by
+        :func:`scipy.spatial.distance.pdist` for its metric parameter, or a
+        metric listed in :obj:`sklearn.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`,
+        including "euclidean", "manhattan" or "cosine".
+        If `metric` is a callable function, it is called on each pair of
+        instances and the resulting value recorded. The callable should take
+        two arrays from the entry in `X` as input, and return a value
+        indicating the distance between them.
+
+    metric_params : dict or None, optional, default: ``None``
+        Additional keyword arguments for the metric function.
+
+    factor : float, optional, default: ``0.``
+        Factor by which to multiply the distance between consecutive
+        points.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    effective_metric_params_ : dict
+        Dictionary containing all information present in `metric_params`.
+        If `metric_params` is ``None``, it is set to the empty dictionary.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from gtda.point_clouds import ConsecutiveRescaling
+    >>> X = np.array([[[0, 0], [1, 2], [5, 6]]])
+    >>> cr = ConsecutiveRescaling()
+    >>> X_rescaled = cr.fit_transform(X)
+    >>> print(X_rescaled.shape)
+    (1, 3, 3)
+
+    See also
+    --------
+    ConsistentRescaling
+
+    """
+
+    _hyperparameters = {
+        'metric': {'type': (str, Callable)},
+        'metric_params': {'type': (dict, type(None))},
+        'factor': {'type': Real, 'in': Interval(0, np.inf, closed='both')}
+        }
+
+    def __init__(self, metric='euclidean', metric_params=None, factor=0.,
+                 n_jobs=None):
+        self.metric = metric
+        self.metric_params = metric_params
+        self.factor = factor
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Calculate :attr:`effective_metric_params_`. Then, return the
+        estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_points, n_points) or (n_samples, \
+            n_points, n_dimensions)
+            Input data. If ``metric == 'precomputed'``, the input should be an
+            ndarray whose each entry along axis 0 is a distance matrix of shape
+            ``(n_points, n_points)``. Otherwise, each such entry will be
+            interpreted as an array of ``n_points`` row vectors in
+            ``n_dimensions``-dimensional space.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_array(X, allow_nd=True)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        if self.metric_params is None:
+            self.effective_metric_params_ = {}
+        else:
+            self.effective_metric_params_ = self.metric_params.copy()
+
+        return self
+
+    def transform(self, X, y=None):
+        """For each entry in the input data array X, find the metric structure
+        after consecutive rescaling and encode it as a distance matrix.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_points, n_points) or (n_samples, \
+            n_points, n_dimensions)
+            Input data. If ``metric == 'precomputed'``, the input should be an
+            ndarray whose each entry along axis 0 is a distance matrix of shape
+            ``(n_points, n_points)``. Otherwise, each such entry will be
+            interpreted as an array of ``n_points`` row vectors in
+            ``n_dimensions``-dimensional space.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_points, n_points)
+            Array containing (as entries along axis 0) the distance matrices
+            after consecutive rescaling.
+
+        """
+        check_is_fitted(self)
+        is_precomputed = self.metric == 'precomputed'
+        X = check_array(X, allow_nd=True, copy=is_precomputed)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(
+            delayed(pairwise_distances)(
+                x, metric=self.metric, n_jobs=1,
+                **self.effective_metric_params_)
+            for x in X)
+
+        if is_precomputed:
+            # Parallel loop above serves only as additional input validation
+            Xt = X
+        else:
+            Xt = np.array(Xt)
+        Xt[:, range(Xt.shape[1] - 1), range(1, Xt.shape[1])] *= self.factor
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, colorscale='blues', plotly_params=None):
+        """Plot a sample from a collection of distance matrices.
+
+        Parameters
+        ----------
+        Xt : ndarray of shape (n_samples, n_points, n_points)
+            Collection of distance matrices, such as returned by
+            :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample to be plotted.
+
+        colorscale : str, optional, default: ``'blues'``
+            Color scale to be used in the heat map. Can be anything allowed by
+            :class:`plotly.graph_objects.Heatmap`.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_heatmap(
+            Xt[sample], colorscale=colorscale,
+            title=f"{sample}-th distance matrix after consecutive rescaling",
+            plotly_params=plotly_params
+            )
```

## gtda/point_clouds/__init__.py

 * *Ordering differences only*

```diff
@@ -1,10 +1,10 @@
-"""The module :mod:`gtda.homology` implements transformers to process point
-clouds and modify metric spaces."""
-# License: GNU AGPLv3
-
-from .rescaling import ConsistentRescaling, ConsecutiveRescaling
-
-__all__ = [
-    'ConsistentRescaling',
-    'ConsecutiveRescaling',
-    ]
+"""The module :mod:`gtda.homology` implements transformers to process point
+clouds and modify metric spaces."""
+# License: GNU AGPLv3
+
+from .rescaling import ConsistentRescaling, ConsecutiveRescaling
+
+__all__ = [
+    'ConsistentRescaling',
+    'ConsecutiveRescaling',
+    ]
```

## gtda/time_series/embedding.py

 * *Ordering differences only*

```diff
@@ -1,754 +1,754 @@
-"""Time series embedding."""
-# License: GNU AGPLv3
-
-import numpy as np
-from joblib import Parallel, delayed
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils.validation import check_is_fitted, check_array, column_or_1d
-
-from ._utils import _time_delay_embedding, _mutual_information, \
-    _false_nearest_neighbors
-from ..base import TransformerResamplerMixin, PlotterMixin
-from ..plotting import plot_point_cloud
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.intervals import Interval
-from ..utils.validation import validate_params, check_collection
-
-_TAKENS_EMBEDDING_HYPERPARAMETERS = {
-        'time_delay': {'type': int, 'in': Interval(1, np.inf, closed='left')},
-        'dimension': {'type': int, 'in': Interval(1, np.inf, closed='left')},
-        'stride': {'type': int, 'in': Interval(1, np.inf, closed='left')}
-        }
-
-
-def takens_embedding_optimal_parameters(X, max_time_delay, max_dimension,
-                                        stride=1, n_jobs=None, validate=True):
-    """Compute the "optimal" parameters for a Takens (time-delay) embedding
-    [1]_ of a univariate time series.
-
-    First, an optimal time delay is found by minimising the time-delayed mutual
-    information among values no greater than `max_time_delay`. Then, a
-    heuristic based on an algorithm in [2]_ is used to select an embedding
-    dimension which, when increased, does not reveal a large proportion of
-    "false nearest neighbors".
-
-    Parameters
-    ----------
-    X : ndarray of shape (n_samples,) or (n_samples, 1)
-        Input data representing a single univariate time series.
-
-    max_time_delay : int, required
-        Maximum time delay between two consecutive values for constructing one
-        embedded point.
-
-    max_dimension : int, required
-        Maximum embedding dimension that will be considered in the
-        optimization.
-
-    stride : int, optional, default: ``1``
-        Stride duration between two consecutive embedded points. It defaults to
-        1 as this is the usual value in the statement of Takens's embedding
-        theorem.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    validate : bool, optional, default: ``True``
-        Whether the input and hyperparameters should be validated.
-
-    Returns
-    -------
-    time_delay : int
-        The "optimal" time delay less than or equal to `max_dimension`, as
-        determined by minimizing the time-delayed mutual information.
-
-    dimension : int
-        The "optimal" embedding dimension less than or equal to
-        `max_dimension`, as determined by a false nearest neighbors heuristic
-        once `time_delay` is computed.
-
-    See also
-    --------
-    SingleTakensEmbedding, TakensEmbedding, SlidingWindow
-
-    References
-    ----------
-    .. [1] F. Takens, "Detecting strange attractors in turbulence". In: Rand
-           D., Young LS. (eds) *Dynamical Systems and Turbulence, Warwick
-           1980*. Lecture Notes in Mathematics, vol. 898. Springer, 1981;
-           `DOI: 10.1007/BFb0091924 <https://doi.org/10.1007/BFb0091924>`_.
-
-    .. [2] M. B. Kennel, R. Brown, and H. D. I. Abarbanel, "Determining
-           embedding dimension for phase-space reconstruction using a
-           geometrical construction"; *Phys. Rev. A* **45**, pp. 3403--3411,
-           1992; `DOI: 10.1103/PhysRevA.45.3403
-           <https://doi.org/10.1103/PhysRevA.45.3403>`_.
-
-    """
-    if validate:
-        _hyperparameters = _TAKENS_EMBEDDING_HYPERPARAMETERS.copy()
-        validate_params({'validate': validate}, {'validate': {'type': bool}})
-        validate_params({'time_delay': max_time_delay,
-                         'dimension': max_dimension, 'stride': stride},
-                        _hyperparameters)
-        X = column_or_1d(X)
-
-    mutual_information_list = Parallel(n_jobs=n_jobs)(
-        delayed(_mutual_information)(X, time_delay, n_bins=100)
-        for time_delay in range(1, max_time_delay + 1))
-    time_delay = \
-        mutual_information_list.index(min(mutual_information_list)) + 1
-
-    n_false_nbhrs_list = Parallel(n_jobs=n_jobs)(
-        delayed(_false_nearest_neighbors)(X, time_delay, dim, stride=stride)
-        for dim in range(1, max_dimension + 3))
-    variation_list = [np.abs(n_false_nbhrs_list[dim - 1]
-                             - 2 * n_false_nbhrs_list[dim] +
-                             n_false_nbhrs_list[dim + 1])
-                      / (n_false_nbhrs_list[dim] + 1) / dim
-                      for dim in range(2, max_dimension + 1)]
-    dimension = variation_list.index(min(variation_list)) + 2
-
-    return time_delay, dimension
-
-
-@adapt_fit_transform_docs
-class SlidingWindow(BaseEstimator, TransformerResamplerMixin):
-    """Sliding windows onto the data.
-
-    Useful in time series analysis to convert a sequence of objects (scalar or
-    array-like) into a sequence of windows on the original sequence. Each
-    window stacks together consecutive objects, and consecutive windows are
-    separated by a constant stride.
-
-    Parameters
-    ----------
-    size : int, optional, default: ``10``
-        Size of each sliding window.
-
-    stride : int, optional, default: ``1``
-        Stride between consecutive windows.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from gtda.time_series import SlidingWindow
-    >>> # Create a time series of two-dimensional vectors, and a corresponding
-    >>> # time series of scalars
-    >>> X = np.arange(20).reshape(-1, 2)
-    >>> y = np.arange(10)
-    >>> windows = SlidingWindow(size=3, stride=3)
-    >>> # Fit and transform X
-    >>> X_windows = windows.fit_transform(X)
-    >>> print(X_windows)
-    [[[ 2  3]
-      [ 4  5]
-      [ 6  7]]
-     [[ 8  9]
-      [10 11]
-      [12 13]]
-     [[14 15]
-      [16 17]
-      [18 19]]]
-    >>> # Resample y
-    >>> yr = windows.resample(y)
-    >>> print(yr)
-    [3 6 9]
-
-    See also
-    --------
-    SingleTakensEmbedding, TakensEmbedding
-
-    Notes
-    -----
-    The current implementation favours the last entry over the first one, in
-    the sense that the last entry of the last window always equals the last
-    entry in the original time series. Hence, a number of initial entries
-    (depending on the remainder of the division between ``n_samples - size``
-    and ``stride``) may be lost.
-
-    """
-
-    _hyperparameters = {
-        'size': {'type': int, 'in': Interval(1, np.inf, closed='left')},
-        'stride': {'type': int, 'in': Interval(1, np.inf, closed='left')}
-        }
-
-    def __init__(self, size=10, stride=1):
-        self.size = size
-        self.stride = stride
-
-    def _window_indices(self, X):
-        n_samples = X.shape[0]
-        n_windows, offset = divmod(n_samples - self.size, self.stride)
-        n_windows += 1
-        if n_windows <= 0:
-            raise ValueError(
-                f"Number of samples ({n_samples}) cannot be less than window "
-                f"size ({self.size})."
-                )
-        indices = np.tile(np.arange(self.size), (n_windows, 1))
-        indices += np.arange(n_windows)[:, None] * self.stride + offset
-        return indices
-
-    def slice_windows(self, X):
-        indices = self._window_indices(X)
-        return indices[:, [0, -1]] + np.array([0, 1])
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, ...)
-            Input data.
-
-        y : None
-            Ignored.
-
-        Returns
-        -------
-        self
-
-        """
-        check_array(X, ensure_2d=False, allow_nd=True)
-        validate_params(self.get_params(), self._hyperparameters)
-
-        self._is_fitted = True
-        return self
-
-    def transform(self, X, y=None):
-        """Slide windows over X.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, ...)
-            Input data.
-
-        y : None
-            Ignored.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_windows, size, ...)
-            Windows of consecutive entries of the original time series.
-            ``n_windows = (n_samples - size) // stride  + 1``.
-
-        """
-        check_is_fitted(self, '_is_fitted')
-        Xt = check_array(X, ensure_2d=False, allow_nd=True)
-
-        window_indices = self._window_indices(Xt)
-
-        Xt = Xt[window_indices]
-        return Xt
-
-    def resample(self, y, X=None):
-        """Resample `y` so that, for any i > 0, the minus i-th entry of the
-        resampled vector corresponds in time to the last entry of the minus
-        i-th window produced by :meth:`transform`.
-
-        Parameters
-        ----------
-        y : ndarray of shape (n_samples,)
-            Target.
-
-        X : None
-            There is no need for input data, yet the pipeline API requires this
-            parameter.
-
-        Returns
-        -------
-        yr : ndarray of shape (n_samples_new,)
-            The resampled target. ``n_samples_new = (n_samples - size)
-            // stride + 1``.
-
-        """
-        check_is_fitted(self, '_is_fitted')
-        yr = column_or_1d(y)
-
-        yr = yr[:self.size - 2:-self.stride][::-1]
-        return yr
-
-
-@adapt_fit_transform_docs
-class SingleTakensEmbedding(BaseEstimator, TransformerResamplerMixin):
-    """Representation of a single univariate time series as a point cloud.
-
-    Based on a time-delay embedding technique named after F. Takens [1]_ [2]_.
-    Given a discrete time series :math:`(X_0, X_1, \\ldots)` and a sequence of
-    evenly sampled times :math:`t_0, t_1, \\ldots`, one extracts a set of
-    :math:`d`-dimensional vectors of the form :math:`(X_{t_i}, X_{t_i + \\tau},
-    \\ldots , X_{t_i + (d-1)\\tau})` for :math:`i = 0, 1, \\ldots`. This set is
-    called the :ref:`Takens embedding <takens_embedding>` of the time series
-    and can be interpreted as a point cloud.
-
-    The difference between :math:`t_{i+1}` and :math:`t_i` is called the
-    stride, :math:`\\tau` is called the time delay, and :math:`d` is called the
-    (embedding) dimension.
-
-    If :math:`d` and :math:`\\tau` are not explicitly set, suitable values are
-    searched for during :meth:`fit` [3]_ [4]_.
-
-    To compute time-delay embeddings of several time series simultaneously, use
-    :class:`TakensEmbedding` instead.
-
-    Parameters
-    ----------
-    parameters_type : ``'search'`` | ``'fixed'``, optional, default: \
-        ``'search'``
-        If set to ``'fixed'``, the values of `time_delay` and `dimension` are
-        used directly in :meth:`transform`. If set to ``'search'``,
-        :func:`takens_embedding_optimal_parameter` is run in :meth:`fit` to
-        estimate optimal values for these quantities and store them as
-        :attr:`time_delay_` and :attr:`dimension_`.
-
-    time_delay : int, optional, default: ``1``
-        Time delay between two consecutive values for constructing one embedded
-        point. If `parameters_type` is ``'search'``, it corresponds to the
-        maximum time delay that will be considered.
-
-    dimension : int, optional, default: ``5``
-        Dimension of the embedding space. If `parameters_type` is ``'search'``,
-        it corresponds to the maximum embedding dimension that will be
-        considered.
-
-    stride : int, optional, default: ``1``
-        Stride duration between two consecutive embedded points. It defaults to
-        1 as this is the usual value in the statement of Takens's embedding
-        theorem.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    Attributes
-    ----------
-    time_delay_ : int
-        Actual time delay used to embed. If
-        `parameters_type` is ``'search'``, it is the calculated optimal time
-        delay and is less than or equal to `time_delay`. Otherwise it is equal
-        to `time_delay`.
-
-    dimension_ : int
-        Actual embedding dimension used to embed. If `parameters_type` is
-        ``'search'``, it is the calculated optimal embedding dimension and is
-        less than or equal to `dimension`. Otherwise it is equal to
-        `dimension`.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from gtda.time_series import SingleTakensEmbedding
-    >>> # Create a noisy signal
-    >>> rng = np.random.default_rng()
-    >>> n_samples = 10000
-    >>> signal = np.asarray([np.sin(x / 50) + 0.5 * rng.random()
-    ...                      for x in range(n_samples)])
-    >>> # Set up the transformer
-    >>> STE = SingleTakensEmbedding(parameters_type='search', dimension=5,
-    ...                             time_delay=5, n_jobs=-1)
-    >>> # Fit and transform
-    >>> signal_embedded = STE.fit_transform(signal)
-    >>> print('Optimal time delay based on mutual information:',
-    ...       STE.time_delay_)
-    Optimal time delay based on mutual information: 5
-    >>> print('Optimal embedding dimension based on false nearest neighbors:',
-    ...       STE.dimension_)
-    Optimal embedding dimension based on false nearest neighbors: 2
-    >>> print(signal_embedded.shape)
-    (9995, 2)
-
-    See also
-    --------
-    TakensEmbedding, SlidingWindow, takens_embedding_optimal_parameters
-
-    Notes
-    -----
-    The current implementation favours the last value over the first one, in
-    the sense that the last coordinate of the last vector in a Takens embedded
-    time series always equals the last value in the original time series.
-    Hence, a number of initial values (depending on the remainder of the
-    division between ``n_samples - dimension * (time_delay - 1) - 1`` and the
-    stride) may be lost.
-
-    References
-    ----------
-    .. [1] F. Takens, "Detecting strange attractors in turbulence". In: Rand
-           D., Young LS. (eds) *Dynamical Systems and Turbulence, Warwick
-           1980*. Lecture Notes in Mathematics, vol. 898. Springer, 1981;
-           `DOI: 10.1007/BFb0091924 <https://doi.org/10.1007/BFb0091924>`_.
-
-    .. [2] J. A. Perea and J. Harer, "Sliding Windows and Persistence: An \
-           Application of Topological Methods to Signal Analysis"; \
-           *Foundations of Computational Mathematics*, **15**, \
-            pp. 799--838; `DOI: 10.1007/s10208-014-9206-z
-           <https://doi.org/10.1007/s10208-014-9206-z>`_.
-
-    .. [3] M. B. Kennel, R. Brown, and H. D. I. Abarbanel, "Determining
-           embedding dimension for phase-space reconstruction using a
-           geometrical construction"; *Phys. Rev. A* **45**, pp. 3403--3411,
-           1992; `DOI: 10.1103/PhysRevA.45.3403
-           <https://doi.org/10.1103/PhysRevA.45.3403>`_.
-
-    .. [4] N. Sanderson, "Topological Data Analysis of Time Series using
-           Witness Complexes"; PhD thesis, University of Colorado at
-           Boulder, 2018; `https://scholar.colorado.edu/math_gradetds/67
-           <https://scholar.colorado.edu/math_gradetds/67>`_.
-
-    """
-
-    _hyperparameters = _TAKENS_EMBEDDING_HYPERPARAMETERS.copy()
-    _hyperparameters['parameters_type'] = \
-        {'type': str, 'in': ['fixed', 'search']}
-
-    def __init__(self, parameters_type='search', time_delay=1, dimension=5,
-                 stride=1, n_jobs=None):
-        self.parameters_type = parameters_type
-        self.time_delay = time_delay
-        self.dimension = dimension
-        self.stride = stride
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """If necessary, compute the optimal time delay and embedding
-        dimension. Then, return the estimator.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples,) or (n_samples, 1)
-            Input data.
-
-        y : None
-            There is no need for a target, yet the pipeline API requires this
-            parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = column_or_1d(X)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        if self.parameters_type == 'search':
-            self.time_delay_, self.dimension_ = \
-                takens_embedding_optimal_parameters(
-                    X, self.time_delay, self.dimension, stride=self.stride,
-                    n_jobs=self.n_jobs, validate=False
-                    )
-        else:
-            self.time_delay_ = self.time_delay
-            self.dimension_ = self.dimension
-
-        return self
-
-    def transform(self, X, y=None):
-        """Compute the Takens embedding of `X`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples,) or (n_samples, 1)
-            Input data.
-
-        y : None
-            Ignored.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_points, n_dimensions)
-            Output point cloud in Euclidean space of dimension given by
-            :attr:`dimension_`. ``n_points = (n_samples - time_delay *
-            (dimension - 1) - 1) // stride + 1``.
-
-        """
-        check_is_fitted(self)
-        Xt = column_or_1d(X).copy()
-
-        Xt = _time_delay_embedding(
-            Xt, time_delay=self.time_delay_, dimension=self.dimension_,
-            stride=self.stride
-            )
-
-        return Xt
-
-    def resample(self, y, X=None):
-        """Resample `y` so that, for any i > 0, the minus i-th entry of the
-        resampled vector corresponds in time to the last coordinate of the
-        minus i-th embedding vector produced by :meth:`transform`.
-
-        Parameters
-        ----------
-        y : ndarray of shape (n_samples,)
-            Target.
-
-        X : None
-            There is no need for input data, yet the pipeline API requires this
-            parameter.
-
-        Returns
-        -------
-        yr : ndarray of shape (n_samples_new,)
-            The resampled target. ``n_samples_new = (n_samples - time_delay *
-            (dimension - 1) - 1) // stride + 1``.
-
-        """
-        check_is_fitted(self)
-        yr = column_or_1d(y)
-
-        final_index = self.time_delay_ * (self.dimension_ - 1)
-        yr = yr[:final_index - 1:-self.stride][::-1]
-        return yr
-
-
-@adapt_fit_transform_docs
-class TakensEmbedding(BaseEstimator, TransformerMixin, PlotterMixin):
-    """Point clouds from collections of time series via independent Takens
-    embeddings.
-
-    This transformer takes collections of (possibly multivariate) time series
-    as input, applies the Takens embedding algorithm described in
-    :class:`SingleTakensEmbedding` to each independently, and returns a
-    corresponding collection of point clouds in Euclidean space (or possibly
-    higher-dimensional structures, see `flatten`).
-
-    Parameters
-    ----------
-    time_delay : int, optional, default: ``1``
-        Time delay between two consecutive values for constructing one embedded
-        point.
-
-    dimension : int, optional, default: ``2``
-        Dimension of the embedding space (per variable, in the multivariate
-        case).
-
-    stride : int, optional, default: ``1``
-        Stride duration between two consecutive embedded points.
-
-    flatten : bool, optional, default: ``True``
-        Only relevant when the input of :meth:`transform` represents a
-        collection of multivariate or tensor-valued time series. If ``True``,
-        ensures that the output is a 3D ndarray or list of 2D arrays. If
-        ``False``, each entry of the input collection leads to an array of
-        dimension one higher than the entry's dimension. See Examples.
-
-    ensure_last_value : bool, optional, default: ``True``
-        Whether the value(s) representing the last measurement(s) must be
-        be present in the output as the last coordinate(s) of the last
-        embedding vector(s). If ``False``, the first measurement(s) is (are)
-        present as the 0-th coordinate(s) of the 0-th vector(s) instead.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from gtda.time_series import TakensEmbedding
-
-    Two univariate time series of duration 4:
-
-    >>> X = np.arange(8).reshape(2, 4)
-    >>> print(X)
-    [[0 1 2 3]
-     [4 5 6 7]]
-    >>> TE = TakensEmbedding(time_delay=1, dimension=2)
-    >>> print(TE.fit_transform(X))
-    [[[0 1]
-      [1 2]
-      [2 3]]
-     [[5 6]
-      [6 7]
-      [7 8]]]
-
-    Two multivariate time series of duration 4, with 2 variables:
-
-    >>> x = np.arange(8).reshape(2, 1, 4)
-    >>> X = np.concatenate([x, -x], axis=1)
-    >>> print(X)
-    [[[ 0  1  2  3]
-      [ 0 -1 -2 -3]]
-     [[ 4  5  6  7]
-      [-4 -5 -6 -7]]]
-
-    Pass `flatten` as ``True`` (default):
-
-    >>> TE = TakensEmbedding(time_delay=1, dimension=2, flatten=True)
-    >>> print(TE.fit_transform(X))
-    [[[ 0  1  0 -1]
-      [ 1  2 -1 -2]
-      [ 2  3 -2 -3]]
-     [[ 4  5 -4 -5]
-      [ 5  6 -5 -6]
-      [ 6  7 -6 -7]]]
-
-    Pass `flatten` as ``False``:
-
-    >>> TE = TakensEmbedding(time_delay=1, dimension=2, flatten=False)
-    >>> print(TE.fit_transform(X))
-    [[[[ 0  1]
-       [ 1  2]
-       [ 2  3]]
-      [[ 0 -1]
-       [-1 -2]
-       [-2 -3]]]
-     [[[ 4  5]
-       [ 5  6]
-       [ 6  7]]
-      [[-4 -5]
-       [-5 -6]
-       [-6 -7]]]]
-
-    See also
-    --------
-    SingleTakensEmbedding, SlidingWindow, takens_embedding_optimal_parameters
-
-    Notes
-    -----
-    To compute the Takens embedding of a single univariate time series in the
-    form of a 1D array or column vector, use :class:`SingleTakensEmbedding`
-    instead.
-
-    Unlike :class:`SingleTakensEmbedding`, this transformer does not include
-    heuristics to optimize the choice of time delay and embedding dimension.
-    The function :func:`takens_embedding_optimal_parameters` is specifically
-    dedicated to this task, but only on a single univariate time series.
-
-    If dealing with a forecasting problem on a single time series, this
-    transformer can be used after an instance of :class:`SlidingWindow` and
-    before an instance of a homology transformer, to produce topological
-    features from sliding windows over the time series.
-
-    """
-
-    _hyperparameters = _TAKENS_EMBEDDING_HYPERPARAMETERS.copy()
-    _hyperparameters.update({'flatten': {'type': bool},
-                             'ensure_last_value': {'type': bool}})
-
-    def __init__(self, time_delay=1, dimension=2, stride=1, flatten=True,
-                 ensure_last_value=True):
-        self.time_delay = time_delay
-        self.dimension = dimension
-        self.stride = stride
-        self.flatten = flatten
-        self.ensure_last_value = ensure_last_value
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray or list of length n_samples
-            Input collection of time series. A 2D array or list of 1D arrays is
-            interpreted as a collection of univariate time series. A 3D array
-            or list of 2D arrays is interpreted as a collection of multivariate
-            time series, each with shape ``(n_variables, n_timestamps)``. More
-            generally, :math`N`-dimensional arrays or lists of
-            (:math`N-1`)-dimensional arrays (:math:`N \\geq 3`) are interpreted
-            as collections of tensor-valued time series, each with time indexed
-            by the last axis.
-
-        y : None
-            There is no need for a target, yet the pipeline API requires this
-            parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_collection(X, copy=False)
-        validate_params(self.get_params(), self._hyperparameters)
-        self._is_fitted = True
-
-        return self
-
-    def transform(self, X, y=None):
-        """Compute the Takens embedding of each entry in `X`.
-
-        Parameters
-        ----------
-        X : ndarray or list of length n_samples
-            Input collection of time series. A 2D array or list of 1D arrays is
-            interpreted as a collection of univariate time series. A 3D array
-            or list of 2D arrays is interpreted as a collection of multivariate
-            time series, each with shape ``(n_variables, n_timestamps)``. More
-            generally, :math`N`-dimensional arrays or lists of
-            (:math`N-1`)-dimensional arrays (:math:`N \\geq 3`) are interpreted
-            as collections of tensor-valued time series, each with time indexed
-            by the last axis.
-
-        y : None
-            Ignored.
-
-        Returns
-        -------
-        Xt : ndarray or list of length n_samples
-            The result of performing a Takens embedding of each entry in `X`
-            with the given parameters. If `X` is a 2D array or a list of 1D
-            arrays, `Xt` is a 3D array or a list of 2D arrays (respectively),
-            each entry of which has shape ``(n_points, dimension)`` where
-            ``n_points = (n_timestamps - time_delay * (dimension - 1) - 1) // \
-            stride + 1``. If `X` is an :math`N`-dimensional array or a list of
-            (:math`N-1`)-dimensional arrays (:math:`N \\geq 3`), the output
-            shapes depend on the `flatten` parameter:
-
-                - if `flatten` is ``True``, `Xt` is still a 3D array or a
-                  list of 2D arrays (respectively), each entry of which has
-                  shape ``(n_points, dimension * n_variables)`` where
-                  ``n_points`` is as above and ``n_variables`` is the product
-                  of the sizes of all axes in said entry except the last.
-                - if `flatten` is ``False``, `Xt` is an
-                  (:math`N+1`)-dimensional array or list of
-                  :math`N`-dimensional arrays.
-
-        """
-        check_is_fitted(self, '_is_fitted')
-        Xt = check_collection(X, copy=True)
-
-        Xt = _time_delay_embedding(
-            Xt, time_delay=self.time_delay, dimension=self.dimension,
-            stride=self.stride, flatten=self.flatten,
-            ensure_last_value=self.ensure_last_value
-            )
-
-        return Xt
-
-    @staticmethod
-    def plot(Xt, sample=0, plotly_params=None):
-        """Plot a sample from a collection of Takens embeddings of time series,
-        as a point cloud in 2D or 3D. If points in the window have more than
-        three dimensions, only the first three are plotted.
-
-        Parameters
-        ----------
-        Xt : ndarray or list of length n_samples
-            Collection of point clouds, such as returned by :meth:`transform`.
-
-        sample : int, optional, default: ``0``
-            Index of the sample in `Xt` to be plotted.
-
-        plotly_params : dict or None, optional, default: ``None``
-            Custom parameters to configure the plotly figure. Allowed keys are
-            ``"trace"`` and ``"layout"``, and the corresponding values should
-            be dictionaries containing keyword arguments as would be fed to the
-            :meth:`update_traces` and :meth:`update_layout` methods of
-            :class:`plotly.graph_objects.Figure`.
-
-        Returns
-        -------
-        fig : :class:`plotly.graph_objects.Figure` object
-            Plotly figure.
-
-        """
-        return plot_point_cloud(Xt[sample], plotly_params=plotly_params)
+"""Time series embedding."""
+# License: GNU AGPLv3
+
+import numpy as np
+from joblib import Parallel, delayed
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils.validation import check_is_fitted, check_array, column_or_1d
+
+from ._utils import _time_delay_embedding, _mutual_information, \
+    _false_nearest_neighbors
+from ..base import TransformerResamplerMixin, PlotterMixin
+from ..plotting import plot_point_cloud
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.intervals import Interval
+from ..utils.validation import validate_params, check_collection
+
+_TAKENS_EMBEDDING_HYPERPARAMETERS = {
+        'time_delay': {'type': int, 'in': Interval(1, np.inf, closed='left')},
+        'dimension': {'type': int, 'in': Interval(1, np.inf, closed='left')},
+        'stride': {'type': int, 'in': Interval(1, np.inf, closed='left')}
+        }
+
+
+def takens_embedding_optimal_parameters(X, max_time_delay, max_dimension,
+                                        stride=1, n_jobs=None, validate=True):
+    """Compute the "optimal" parameters for a Takens (time-delay) embedding
+    [1]_ of a univariate time series.
+
+    First, an optimal time delay is found by minimising the time-delayed mutual
+    information among values no greater than `max_time_delay`. Then, a
+    heuristic based on an algorithm in [2]_ is used to select an embedding
+    dimension which, when increased, does not reveal a large proportion of
+    "false nearest neighbors".
+
+    Parameters
+    ----------
+    X : ndarray of shape (n_samples,) or (n_samples, 1)
+        Input data representing a single univariate time series.
+
+    max_time_delay : int, required
+        Maximum time delay between two consecutive values for constructing one
+        embedded point.
+
+    max_dimension : int, required
+        Maximum embedding dimension that will be considered in the
+        optimization.
+
+    stride : int, optional, default: ``1``
+        Stride duration between two consecutive embedded points. It defaults to
+        1 as this is the usual value in the statement of Takens's embedding
+        theorem.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    validate : bool, optional, default: ``True``
+        Whether the input and hyperparameters should be validated.
+
+    Returns
+    -------
+    time_delay : int
+        The "optimal" time delay less than or equal to `max_dimension`, as
+        determined by minimizing the time-delayed mutual information.
+
+    dimension : int
+        The "optimal" embedding dimension less than or equal to
+        `max_dimension`, as determined by a false nearest neighbors heuristic
+        once `time_delay` is computed.
+
+    See also
+    --------
+    SingleTakensEmbedding, TakensEmbedding, SlidingWindow
+
+    References
+    ----------
+    .. [1] F. Takens, "Detecting strange attractors in turbulence". In: Rand
+           D., Young LS. (eds) *Dynamical Systems and Turbulence, Warwick
+           1980*. Lecture Notes in Mathematics, vol. 898. Springer, 1981;
+           `DOI: 10.1007/BFb0091924 <https://doi.org/10.1007/BFb0091924>`_.
+
+    .. [2] M. B. Kennel, R. Brown, and H. D. I. Abarbanel, "Determining
+           embedding dimension for phase-space reconstruction using a
+           geometrical construction"; *Phys. Rev. A* **45**, pp. 3403--3411,
+           1992; `DOI: 10.1103/PhysRevA.45.3403
+           <https://doi.org/10.1103/PhysRevA.45.3403>`_.
+
+    """
+    if validate:
+        _hyperparameters = _TAKENS_EMBEDDING_HYPERPARAMETERS.copy()
+        validate_params({'validate': validate}, {'validate': {'type': bool}})
+        validate_params({'time_delay': max_time_delay,
+                         'dimension': max_dimension, 'stride': stride},
+                        _hyperparameters)
+        X = column_or_1d(X)
+
+    mutual_information_list = Parallel(n_jobs=n_jobs)(
+        delayed(_mutual_information)(X, time_delay, n_bins=100)
+        for time_delay in range(1, max_time_delay + 1))
+    time_delay = \
+        mutual_information_list.index(min(mutual_information_list)) + 1
+
+    n_false_nbhrs_list = Parallel(n_jobs=n_jobs)(
+        delayed(_false_nearest_neighbors)(X, time_delay, dim, stride=stride)
+        for dim in range(1, max_dimension + 3))
+    variation_list = [np.abs(n_false_nbhrs_list[dim - 1]
+                             - 2 * n_false_nbhrs_list[dim] +
+                             n_false_nbhrs_list[dim + 1])
+                      / (n_false_nbhrs_list[dim] + 1) / dim
+                      for dim in range(2, max_dimension + 1)]
+    dimension = variation_list.index(min(variation_list)) + 2
+
+    return time_delay, dimension
+
+
+@adapt_fit_transform_docs
+class SlidingWindow(BaseEstimator, TransformerResamplerMixin):
+    """Sliding windows onto the data.
+
+    Useful in time series analysis to convert a sequence of objects (scalar or
+    array-like) into a sequence of windows on the original sequence. Each
+    window stacks together consecutive objects, and consecutive windows are
+    separated by a constant stride.
+
+    Parameters
+    ----------
+    size : int, optional, default: ``10``
+        Size of each sliding window.
+
+    stride : int, optional, default: ``1``
+        Stride between consecutive windows.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from gtda.time_series import SlidingWindow
+    >>> # Create a time series of two-dimensional vectors, and a corresponding
+    >>> # time series of scalars
+    >>> X = np.arange(20).reshape(-1, 2)
+    >>> y = np.arange(10)
+    >>> windows = SlidingWindow(size=3, stride=3)
+    >>> # Fit and transform X
+    >>> X_windows = windows.fit_transform(X)
+    >>> print(X_windows)
+    [[[ 2  3]
+      [ 4  5]
+      [ 6  7]]
+     [[ 8  9]
+      [10 11]
+      [12 13]]
+     [[14 15]
+      [16 17]
+      [18 19]]]
+    >>> # Resample y
+    >>> yr = windows.resample(y)
+    >>> print(yr)
+    [3 6 9]
+
+    See also
+    --------
+    SingleTakensEmbedding, TakensEmbedding
+
+    Notes
+    -----
+    The current implementation favours the last entry over the first one, in
+    the sense that the last entry of the last window always equals the last
+    entry in the original time series. Hence, a number of initial entries
+    (depending on the remainder of the division between ``n_samples - size``
+    and ``stride``) may be lost.
+
+    """
+
+    _hyperparameters = {
+        'size': {'type': int, 'in': Interval(1, np.inf, closed='left')},
+        'stride': {'type': int, 'in': Interval(1, np.inf, closed='left')}
+        }
+
+    def __init__(self, size=10, stride=1):
+        self.size = size
+        self.stride = stride
+
+    def _window_indices(self, X):
+        n_samples = X.shape[0]
+        n_windows, offset = divmod(n_samples - self.size, self.stride)
+        n_windows += 1
+        if n_windows <= 0:
+            raise ValueError(
+                f"Number of samples ({n_samples}) cannot be less than window "
+                f"size ({self.size})."
+                )
+        indices = np.tile(np.arange(self.size), (n_windows, 1))
+        indices += np.arange(n_windows)[:, None] * self.stride + offset
+        return indices
+
+    def slice_windows(self, X):
+        indices = self._window_indices(X)
+        return indices[:, [0, -1]] + np.array([0, 1])
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, ...)
+            Input data.
+
+        y : None
+            Ignored.
+
+        Returns
+        -------
+        self
+
+        """
+        check_array(X, ensure_2d=False, allow_nd=True)
+        validate_params(self.get_params(), self._hyperparameters)
+
+        self._is_fitted = True
+        return self
+
+    def transform(self, X, y=None):
+        """Slide windows over X.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, ...)
+            Input data.
+
+        y : None
+            Ignored.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_windows, size, ...)
+            Windows of consecutive entries of the original time series.
+            ``n_windows = (n_samples - size) // stride  + 1``.
+
+        """
+        check_is_fitted(self, '_is_fitted')
+        Xt = check_array(X, ensure_2d=False, allow_nd=True)
+
+        window_indices = self._window_indices(Xt)
+
+        Xt = Xt[window_indices]
+        return Xt
+
+    def resample(self, y, X=None):
+        """Resample `y` so that, for any i > 0, the minus i-th entry of the
+        resampled vector corresponds in time to the last entry of the minus
+        i-th window produced by :meth:`transform`.
+
+        Parameters
+        ----------
+        y : ndarray of shape (n_samples,)
+            Target.
+
+        X : None
+            There is no need for input data, yet the pipeline API requires this
+            parameter.
+
+        Returns
+        -------
+        yr : ndarray of shape (n_samples_new,)
+            The resampled target. ``n_samples_new = (n_samples - size)
+            // stride + 1``.
+
+        """
+        check_is_fitted(self, '_is_fitted')
+        yr = column_or_1d(y)
+
+        yr = yr[:self.size - 2:-self.stride][::-1]
+        return yr
+
+
+@adapt_fit_transform_docs
+class SingleTakensEmbedding(BaseEstimator, TransformerResamplerMixin):
+    """Representation of a single univariate time series as a point cloud.
+
+    Based on a time-delay embedding technique named after F. Takens [1]_ [2]_.
+    Given a discrete time series :math:`(X_0, X_1, \\ldots)` and a sequence of
+    evenly sampled times :math:`t_0, t_1, \\ldots`, one extracts a set of
+    :math:`d`-dimensional vectors of the form :math:`(X_{t_i}, X_{t_i + \\tau},
+    \\ldots , X_{t_i + (d-1)\\tau})` for :math:`i = 0, 1, \\ldots`. This set is
+    called the :ref:`Takens embedding <takens_embedding>` of the time series
+    and can be interpreted as a point cloud.
+
+    The difference between :math:`t_{i+1}` and :math:`t_i` is called the
+    stride, :math:`\\tau` is called the time delay, and :math:`d` is called the
+    (embedding) dimension.
+
+    If :math:`d` and :math:`\\tau` are not explicitly set, suitable values are
+    searched for during :meth:`fit` [3]_ [4]_.
+
+    To compute time-delay embeddings of several time series simultaneously, use
+    :class:`TakensEmbedding` instead.
+
+    Parameters
+    ----------
+    parameters_type : ``'search'`` | ``'fixed'``, optional, default: \
+        ``'search'``
+        If set to ``'fixed'``, the values of `time_delay` and `dimension` are
+        used directly in :meth:`transform`. If set to ``'search'``,
+        :func:`takens_embedding_optimal_parameter` is run in :meth:`fit` to
+        estimate optimal values for these quantities and store them as
+        :attr:`time_delay_` and :attr:`dimension_`.
+
+    time_delay : int, optional, default: ``1``
+        Time delay between two consecutive values for constructing one embedded
+        point. If `parameters_type` is ``'search'``, it corresponds to the
+        maximum time delay that will be considered.
+
+    dimension : int, optional, default: ``5``
+        Dimension of the embedding space. If `parameters_type` is ``'search'``,
+        it corresponds to the maximum embedding dimension that will be
+        considered.
+
+    stride : int, optional, default: ``1``
+        Stride duration between two consecutive embedded points. It defaults to
+        1 as this is the usual value in the statement of Takens's embedding
+        theorem.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    Attributes
+    ----------
+    time_delay_ : int
+        Actual time delay used to embed. If
+        `parameters_type` is ``'search'``, it is the calculated optimal time
+        delay and is less than or equal to `time_delay`. Otherwise it is equal
+        to `time_delay`.
+
+    dimension_ : int
+        Actual embedding dimension used to embed. If `parameters_type` is
+        ``'search'``, it is the calculated optimal embedding dimension and is
+        less than or equal to `dimension`. Otherwise it is equal to
+        `dimension`.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from gtda.time_series import SingleTakensEmbedding
+    >>> # Create a noisy signal
+    >>> rng = np.random.default_rng()
+    >>> n_samples = 10000
+    >>> signal = np.asarray([np.sin(x / 50) + 0.5 * rng.random()
+    ...                      for x in range(n_samples)])
+    >>> # Set up the transformer
+    >>> STE = SingleTakensEmbedding(parameters_type='search', dimension=5,
+    ...                             time_delay=5, n_jobs=-1)
+    >>> # Fit and transform
+    >>> signal_embedded = STE.fit_transform(signal)
+    >>> print('Optimal time delay based on mutual information:',
+    ...       STE.time_delay_)
+    Optimal time delay based on mutual information: 5
+    >>> print('Optimal embedding dimension based on false nearest neighbors:',
+    ...       STE.dimension_)
+    Optimal embedding dimension based on false nearest neighbors: 2
+    >>> print(signal_embedded.shape)
+    (9995, 2)
+
+    See also
+    --------
+    TakensEmbedding, SlidingWindow, takens_embedding_optimal_parameters
+
+    Notes
+    -----
+    The current implementation favours the last value over the first one, in
+    the sense that the last coordinate of the last vector in a Takens embedded
+    time series always equals the last value in the original time series.
+    Hence, a number of initial values (depending on the remainder of the
+    division between ``n_samples - dimension * (time_delay - 1) - 1`` and the
+    stride) may be lost.
+
+    References
+    ----------
+    .. [1] F. Takens, "Detecting strange attractors in turbulence". In: Rand
+           D., Young LS. (eds) *Dynamical Systems and Turbulence, Warwick
+           1980*. Lecture Notes in Mathematics, vol. 898. Springer, 1981;
+           `DOI: 10.1007/BFb0091924 <https://doi.org/10.1007/BFb0091924>`_.
+
+    .. [2] J. A. Perea and J. Harer, "Sliding Windows and Persistence: An \
+           Application of Topological Methods to Signal Analysis"; \
+           *Foundations of Computational Mathematics*, **15**, \
+            pp. 799--838; `DOI: 10.1007/s10208-014-9206-z
+           <https://doi.org/10.1007/s10208-014-9206-z>`_.
+
+    .. [3] M. B. Kennel, R. Brown, and H. D. I. Abarbanel, "Determining
+           embedding dimension for phase-space reconstruction using a
+           geometrical construction"; *Phys. Rev. A* **45**, pp. 3403--3411,
+           1992; `DOI: 10.1103/PhysRevA.45.3403
+           <https://doi.org/10.1103/PhysRevA.45.3403>`_.
+
+    .. [4] N. Sanderson, "Topological Data Analysis of Time Series using
+           Witness Complexes"; PhD thesis, University of Colorado at
+           Boulder, 2018; `https://scholar.colorado.edu/math_gradetds/67
+           <https://scholar.colorado.edu/math_gradetds/67>`_.
+
+    """
+
+    _hyperparameters = _TAKENS_EMBEDDING_HYPERPARAMETERS.copy()
+    _hyperparameters['parameters_type'] = \
+        {'type': str, 'in': ['fixed', 'search']}
+
+    def __init__(self, parameters_type='search', time_delay=1, dimension=5,
+                 stride=1, n_jobs=None):
+        self.parameters_type = parameters_type
+        self.time_delay = time_delay
+        self.dimension = dimension
+        self.stride = stride
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """If necessary, compute the optimal time delay and embedding
+        dimension. Then, return the estimator.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples,) or (n_samples, 1)
+            Input data.
+
+        y : None
+            There is no need for a target, yet the pipeline API requires this
+            parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = column_or_1d(X)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        if self.parameters_type == 'search':
+            self.time_delay_, self.dimension_ = \
+                takens_embedding_optimal_parameters(
+                    X, self.time_delay, self.dimension, stride=self.stride,
+                    n_jobs=self.n_jobs, validate=False
+                    )
+        else:
+            self.time_delay_ = self.time_delay
+            self.dimension_ = self.dimension
+
+        return self
+
+    def transform(self, X, y=None):
+        """Compute the Takens embedding of `X`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples,) or (n_samples, 1)
+            Input data.
+
+        y : None
+            Ignored.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_points, n_dimensions)
+            Output point cloud in Euclidean space of dimension given by
+            :attr:`dimension_`. ``n_points = (n_samples - time_delay *
+            (dimension - 1) - 1) // stride + 1``.
+
+        """
+        check_is_fitted(self)
+        Xt = column_or_1d(X).copy()
+
+        Xt = _time_delay_embedding(
+            Xt, time_delay=self.time_delay_, dimension=self.dimension_,
+            stride=self.stride
+            )
+
+        return Xt
+
+    def resample(self, y, X=None):
+        """Resample `y` so that, for any i > 0, the minus i-th entry of the
+        resampled vector corresponds in time to the last coordinate of the
+        minus i-th embedding vector produced by :meth:`transform`.
+
+        Parameters
+        ----------
+        y : ndarray of shape (n_samples,)
+            Target.
+
+        X : None
+            There is no need for input data, yet the pipeline API requires this
+            parameter.
+
+        Returns
+        -------
+        yr : ndarray of shape (n_samples_new,)
+            The resampled target. ``n_samples_new = (n_samples - time_delay *
+            (dimension - 1) - 1) // stride + 1``.
+
+        """
+        check_is_fitted(self)
+        yr = column_or_1d(y)
+
+        final_index = self.time_delay_ * (self.dimension_ - 1)
+        yr = yr[:final_index - 1:-self.stride][::-1]
+        return yr
+
+
+@adapt_fit_transform_docs
+class TakensEmbedding(BaseEstimator, TransformerMixin, PlotterMixin):
+    """Point clouds from collections of time series via independent Takens
+    embeddings.
+
+    This transformer takes collections of (possibly multivariate) time series
+    as input, applies the Takens embedding algorithm described in
+    :class:`SingleTakensEmbedding` to each independently, and returns a
+    corresponding collection of point clouds in Euclidean space (or possibly
+    higher-dimensional structures, see `flatten`).
+
+    Parameters
+    ----------
+    time_delay : int, optional, default: ``1``
+        Time delay between two consecutive values for constructing one embedded
+        point.
+
+    dimension : int, optional, default: ``2``
+        Dimension of the embedding space (per variable, in the multivariate
+        case).
+
+    stride : int, optional, default: ``1``
+        Stride duration between two consecutive embedded points.
+
+    flatten : bool, optional, default: ``True``
+        Only relevant when the input of :meth:`transform` represents a
+        collection of multivariate or tensor-valued time series. If ``True``,
+        ensures that the output is a 3D ndarray or list of 2D arrays. If
+        ``False``, each entry of the input collection leads to an array of
+        dimension one higher than the entry's dimension. See Examples.
+
+    ensure_last_value : bool, optional, default: ``True``
+        Whether the value(s) representing the last measurement(s) must be
+        be present in the output as the last coordinate(s) of the last
+        embedding vector(s). If ``False``, the first measurement(s) is (are)
+        present as the 0-th coordinate(s) of the 0-th vector(s) instead.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from gtda.time_series import TakensEmbedding
+
+    Two univariate time series of duration 4:
+
+    >>> X = np.arange(8).reshape(2, 4)
+    >>> print(X)
+    [[0 1 2 3]
+     [4 5 6 7]]
+    >>> TE = TakensEmbedding(time_delay=1, dimension=2)
+    >>> print(TE.fit_transform(X))
+    [[[0 1]
+      [1 2]
+      [2 3]]
+     [[5 6]
+      [6 7]
+      [7 8]]]
+
+    Two multivariate time series of duration 4, with 2 variables:
+
+    >>> x = np.arange(8).reshape(2, 1, 4)
+    >>> X = np.concatenate([x, -x], axis=1)
+    >>> print(X)
+    [[[ 0  1  2  3]
+      [ 0 -1 -2 -3]]
+     [[ 4  5  6  7]
+      [-4 -5 -6 -7]]]
+
+    Pass `flatten` as ``True`` (default):
+
+    >>> TE = TakensEmbedding(time_delay=1, dimension=2, flatten=True)
+    >>> print(TE.fit_transform(X))
+    [[[ 0  1  0 -1]
+      [ 1  2 -1 -2]
+      [ 2  3 -2 -3]]
+     [[ 4  5 -4 -5]
+      [ 5  6 -5 -6]
+      [ 6  7 -6 -7]]]
+
+    Pass `flatten` as ``False``:
+
+    >>> TE = TakensEmbedding(time_delay=1, dimension=2, flatten=False)
+    >>> print(TE.fit_transform(X))
+    [[[[ 0  1]
+       [ 1  2]
+       [ 2  3]]
+      [[ 0 -1]
+       [-1 -2]
+       [-2 -3]]]
+     [[[ 4  5]
+       [ 5  6]
+       [ 6  7]]
+      [[-4 -5]
+       [-5 -6]
+       [-6 -7]]]]
+
+    See also
+    --------
+    SingleTakensEmbedding, SlidingWindow, takens_embedding_optimal_parameters
+
+    Notes
+    -----
+    To compute the Takens embedding of a single univariate time series in the
+    form of a 1D array or column vector, use :class:`SingleTakensEmbedding`
+    instead.
+
+    Unlike :class:`SingleTakensEmbedding`, this transformer does not include
+    heuristics to optimize the choice of time delay and embedding dimension.
+    The function :func:`takens_embedding_optimal_parameters` is specifically
+    dedicated to this task, but only on a single univariate time series.
+
+    If dealing with a forecasting problem on a single time series, this
+    transformer can be used after an instance of :class:`SlidingWindow` and
+    before an instance of a homology transformer, to produce topological
+    features from sliding windows over the time series.
+
+    """
+
+    _hyperparameters = _TAKENS_EMBEDDING_HYPERPARAMETERS.copy()
+    _hyperparameters.update({'flatten': {'type': bool},
+                             'ensure_last_value': {'type': bool}})
+
+    def __init__(self, time_delay=1, dimension=2, stride=1, flatten=True,
+                 ensure_last_value=True):
+        self.time_delay = time_delay
+        self.dimension = dimension
+        self.stride = stride
+        self.flatten = flatten
+        self.ensure_last_value = ensure_last_value
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray or list of length n_samples
+            Input collection of time series. A 2D array or list of 1D arrays is
+            interpreted as a collection of univariate time series. A 3D array
+            or list of 2D arrays is interpreted as a collection of multivariate
+            time series, each with shape ``(n_variables, n_timestamps)``. More
+            generally, :math`N`-dimensional arrays or lists of
+            (:math`N-1`)-dimensional arrays (:math:`N \\geq 3`) are interpreted
+            as collections of tensor-valued time series, each with time indexed
+            by the last axis.
+
+        y : None
+            There is no need for a target, yet the pipeline API requires this
+            parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_collection(X, copy=False)
+        validate_params(self.get_params(), self._hyperparameters)
+        self._is_fitted = True
+
+        return self
+
+    def transform(self, X, y=None):
+        """Compute the Takens embedding of each entry in `X`.
+
+        Parameters
+        ----------
+        X : ndarray or list of length n_samples
+            Input collection of time series. A 2D array or list of 1D arrays is
+            interpreted as a collection of univariate time series. A 3D array
+            or list of 2D arrays is interpreted as a collection of multivariate
+            time series, each with shape ``(n_variables, n_timestamps)``. More
+            generally, :math`N`-dimensional arrays or lists of
+            (:math`N-1`)-dimensional arrays (:math:`N \\geq 3`) are interpreted
+            as collections of tensor-valued time series, each with time indexed
+            by the last axis.
+
+        y : None
+            Ignored.
+
+        Returns
+        -------
+        Xt : ndarray or list of length n_samples
+            The result of performing a Takens embedding of each entry in `X`
+            with the given parameters. If `X` is a 2D array or a list of 1D
+            arrays, `Xt` is a 3D array or a list of 2D arrays (respectively),
+            each entry of which has shape ``(n_points, dimension)`` where
+            ``n_points = (n_timestamps - time_delay * (dimension - 1) - 1) // \
+            stride + 1``. If `X` is an :math`N`-dimensional array or a list of
+            (:math`N-1`)-dimensional arrays (:math:`N \\geq 3`), the output
+            shapes depend on the `flatten` parameter:
+
+                - if `flatten` is ``True``, `Xt` is still a 3D array or a
+                  list of 2D arrays (respectively), each entry of which has
+                  shape ``(n_points, dimension * n_variables)`` where
+                  ``n_points`` is as above and ``n_variables`` is the product
+                  of the sizes of all axes in said entry except the last.
+                - if `flatten` is ``False``, `Xt` is an
+                  (:math`N+1`)-dimensional array or list of
+                  :math`N`-dimensional arrays.
+
+        """
+        check_is_fitted(self, '_is_fitted')
+        Xt = check_collection(X, copy=True)
+
+        Xt = _time_delay_embedding(
+            Xt, time_delay=self.time_delay, dimension=self.dimension,
+            stride=self.stride, flatten=self.flatten,
+            ensure_last_value=self.ensure_last_value
+            )
+
+        return Xt
+
+    @staticmethod
+    def plot(Xt, sample=0, plotly_params=None):
+        """Plot a sample from a collection of Takens embeddings of time series,
+        as a point cloud in 2D or 3D. If points in the window have more than
+        three dimensions, only the first three are plotted.
+
+        Parameters
+        ----------
+        Xt : ndarray or list of length n_samples
+            Collection of point clouds, such as returned by :meth:`transform`.
+
+        sample : int, optional, default: ``0``
+            Index of the sample in `Xt` to be plotted.
+
+        plotly_params : dict or None, optional, default: ``None``
+            Custom parameters to configure the plotly figure. Allowed keys are
+            ``"trace"`` and ``"layout"``, and the corresponding values should
+            be dictionaries containing keyword arguments as would be fed to the
+            :meth:`update_traces` and :meth:`update_layout` methods of
+            :class:`plotly.graph_objects.Figure`.
+
+        Returns
+        -------
+        fig : :class:`plotly.graph_objects.Figure` object
+            Plotly figure.
+
+        """
+        return plot_point_cloud(Xt[sample], plotly_params=plotly_params)
```

## gtda/time_series/features.py

 * *Ordering differences only*

```diff
@@ -1,110 +1,110 @@
-"""Features from time series."""
-# License: GNU AGPLv3
-
-import numpy as np
-from joblib import Parallel, delayed, effective_n_jobs
-from scipy.stats import entropy
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils import gen_even_slices
-from sklearn.utils.validation import check_is_fitted, check_array
-
-from ..utils._docs import adapt_fit_transform_docs
-
-
-@adapt_fit_transform_docs
-class PermutationEntropy(BaseEstimator, TransformerMixin):
-    """Entropies from sets of permutations arg-sorting rows in arrays.
-
-    Given a two-dimensional array `A`, another array `A'` of the same size is
-    computed by arg-sorting each row in `A`. The permutation entropy [1]_ of
-    `A` is the (base 2) Shannon entropy of the probability distribution given
-    by the relative frequencies of each arg-sorting permutation in `A'`.
-
-    Parameters
-    ----------
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    See also
-    --------
-    SlidingWindow, TakensEmbedding, \
-    SingleTakensEmbedding, gtda.diagrams.PersistenceEntropy
-
-    References
-    ----------
-    .. [1] C. Bandt and B. Pompe, "Permutation Entropy: A Natural Complexity
-           Measure for Time Series"; *Phys. Rev. Lett.*, **88**.17, 2002;
-           `DOI: 10.1103/physrevlett.88.174102
-           <https://doi.org/10.1103/physrevlett.88.174102>`_.
-
-    """
-
-    def __init__(self, n_jobs=None):
-        self.n_jobs = n_jobs
-
-    @staticmethod
-    def _entropy_2d(x):
-        unique_row_counts = np.unique(x, axis=0, return_counts=True)[1]
-        return entropy(unique_row_counts, base=2)
-
-    def _permutation_entropy(self, X):
-        X_permutations = np.argsort(X, axis=2)
-        X_permutation_entropy = np.asarray(
-            [self._entropy_2d(x) for x in X_permutations]
-            )[:, None]
-        return X_permutation_entropy
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_points, n_dimensions)
-            Input data.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_array(X, allow_nd=True)
-
-        self._is_fitted = True
-        return self
-
-    def transform(self, X, y=None):
-        """Calculate the permutation entropy of each two-dimensional array in
-        `X`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_points, n_dimensions)
-            Input data.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of int, shape (n_samples, 1)
-            One permutation entropy per entry in `X` along axis 0.
-
-        """
-        check_is_fitted(self, '_is_fitted')
-        Xt = check_array(X, allow_nd=True)
-
-        Xt = Parallel(n_jobs=self.n_jobs)(delayed(
-            self._permutation_entropy)(Xt[s])
-            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
-        Xt = np.concatenate(Xt)
-        return Xt
+"""Features from time series."""
+# License: GNU AGPLv3
+
+import numpy as np
+from joblib import Parallel, delayed, effective_n_jobs
+from scipy.stats import entropy
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils import gen_even_slices
+from sklearn.utils.validation import check_is_fitted, check_array
+
+from ..utils._docs import adapt_fit_transform_docs
+
+
+@adapt_fit_transform_docs
+class PermutationEntropy(BaseEstimator, TransformerMixin):
+    """Entropies from sets of permutations arg-sorting rows in arrays.
+
+    Given a two-dimensional array `A`, another array `A'` of the same size is
+    computed by arg-sorting each row in `A`. The permutation entropy [1]_ of
+    `A` is the (base 2) Shannon entropy of the probability distribution given
+    by the relative frequencies of each arg-sorting permutation in `A'`.
+
+    Parameters
+    ----------
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    See also
+    --------
+    SlidingWindow, TakensEmbedding, \
+    SingleTakensEmbedding, gtda.diagrams.PersistenceEntropy
+
+    References
+    ----------
+    .. [1] C. Bandt and B. Pompe, "Permutation Entropy: A Natural Complexity
+           Measure for Time Series"; *Phys. Rev. Lett.*, **88**.17, 2002;
+           `DOI: 10.1103/physrevlett.88.174102
+           <https://doi.org/10.1103/physrevlett.88.174102>`_.
+
+    """
+
+    def __init__(self, n_jobs=None):
+        self.n_jobs = n_jobs
+
+    @staticmethod
+    def _entropy_2d(x):
+        unique_row_counts = np.unique(x, axis=0, return_counts=True)[1]
+        return entropy(unique_row_counts, base=2)
+
+    def _permutation_entropy(self, X):
+        X_permutations = np.argsort(X, axis=2)
+        X_permutation_entropy = np.asarray(
+            [self._entropy_2d(x) for x in X_permutations]
+            )[:, None]
+        return X_permutation_entropy
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_points, n_dimensions)
+            Input data.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_array(X, allow_nd=True)
+
+        self._is_fitted = True
+        return self
+
+    def transform(self, X, y=None):
+        """Calculate the permutation entropy of each two-dimensional array in
+        `X`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_points, n_dimensions)
+            Input data.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of int, shape (n_samples, 1)
+            One permutation entropy per entry in `X` along axis 0.
+
+        """
+        check_is_fitted(self, '_is_fitted')
+        Xt = check_array(X, allow_nd=True)
+
+        Xt = Parallel(n_jobs=self.n_jobs)(delayed(
+            self._permutation_entropy)(Xt[s])
+            for s in gen_even_slices(len(Xt), effective_n_jobs(self.n_jobs)))
+        Xt = np.concatenate(Xt)
+        return Xt
```

## gtda/time_series/multivariate.py

 * *Ordering differences only*

```diff
@@ -1,109 +1,109 @@
-"""Processing of multivariate time series."""
-# License: GNU AGPLv3
-
-import numpy as np
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.utils.validation import check_is_fitted, check_array
-
-from ..utils import validate_params
-from ..utils._docs import adapt_fit_transform_docs
-
-
-@adapt_fit_transform_docs
-class PearsonDissimilarity(BaseEstimator, TransformerMixin):
-    """Pearson dissimilarities from collections of multivariate time series.
-
-    The sample Pearson correlation coefficients between pairs of components of
-    an :math:`N`-variate time series form an :math:`N \\times N` matrix
-    :math:`R` with entries
-
-    .. math:: R_{ij} = \\frac{ C_{ij} }{ \\sqrt{ C_{ii} C_{jj} } },
-
-    where :math:`C` is the covariance matrix. Setting :math:`D_{ij} =
-    (1 - R_{ij})/2` or :math:`D_{ij} = 1 - |R_{ij}|` we obtain a dissimilarity
-    matrix with entries between 0 and 1.
-
-    This transformer computes one dissimilarity matrix per multivariate time
-    series in a collection. Examples of such collections are the outputs of
-    :class:`SlidingWindow`.
-
-    Parameters
-    ----------
-    absolute_value : bool, default: ``False``
-        Whether absolute values of the Pearson correlation coefficients should
-        be taken. Doing so makes pairs of strongly anti-correlated variables as
-        similar as pairs of strongly correlated ones.
-
-    n_jobs : int or None, optional, default: ``None``
-        The number of jobs to use for the computation. ``None`` means 1 unless
-        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
-        processors.
-
-    See also
-    --------
-    SlidingWindow, gtda.homology.VietorisRipsPersistence
-
-    """
-
-    _hyperparameters = {'absolute_value': {'type': bool}}
-
-    def __init__(self, absolute_value=False, n_jobs=None):
-        self.absolute_value = absolute_value
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_observations, n_features)
-            Input data. Each entry along axis 0 is a sample of ``n_features``
-            different variables, of size ``n_observations``.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_array(X, allow_nd=True)
-        validate_params(
-            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
-
-        self._is_fitted = True
-        return self
-
-    def transform(self, X, y=None):
-        """Compute Pearson dissimilarities.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples, n_observations, n_features)
-            Input data. Each entry along axis 0 is a sample of ``n_features``
-            different variables, of size ``n_observations``.
-
-        y : None
-            There is no need for a target in a transformer, yet the pipeline
-            API requires this parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples, n_features, n_features)
-            Array of Pearson dissimilarities.
-
-        """
-        check_is_fitted(self, '_is_fitted')
-        X = check_array(X, allow_nd=True)
-
-        Xt = np.empty((X.shape[0], X.shape[2], X.shape[2]))
-        for i, sample in enumerate(X):
-            Xt[i, :, :] = np.corrcoef(sample, rowvar=False)
-        Xt = 0.5 - Xt/2 if not self.absolute_value else 1 - np.abs(Xt)
-
-        return Xt
+"""Processing of multivariate time series."""
+# License: GNU AGPLv3
+
+import numpy as np
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.utils.validation import check_is_fitted, check_array
+
+from ..utils import validate_params
+from ..utils._docs import adapt_fit_transform_docs
+
+
+@adapt_fit_transform_docs
+class PearsonDissimilarity(BaseEstimator, TransformerMixin):
+    """Pearson dissimilarities from collections of multivariate time series.
+
+    The sample Pearson correlation coefficients between pairs of components of
+    an :math:`N`-variate time series form an :math:`N \\times N` matrix
+    :math:`R` with entries
+
+    .. math:: R_{ij} = \\frac{ C_{ij} }{ \\sqrt{ C_{ii} C_{jj} } },
+
+    where :math:`C` is the covariance matrix. Setting :math:`D_{ij} =
+    (1 - R_{ij})/2` or :math:`D_{ij} = 1 - |R_{ij}|` we obtain a dissimilarity
+    matrix with entries between 0 and 1.
+
+    This transformer computes one dissimilarity matrix per multivariate time
+    series in a collection. Examples of such collections are the outputs of
+    :class:`SlidingWindow`.
+
+    Parameters
+    ----------
+    absolute_value : bool, default: ``False``
+        Whether absolute values of the Pearson correlation coefficients should
+        be taken. Doing so makes pairs of strongly anti-correlated variables as
+        similar as pairs of strongly correlated ones.
+
+    n_jobs : int or None, optional, default: ``None``
+        The number of jobs to use for the computation. ``None`` means 1 unless
+        in a :obj:`joblib.parallel_backend` context. ``-1`` means using all
+        processors.
+
+    See also
+    --------
+    SlidingWindow, gtda.homology.VietorisRipsPersistence
+
+    """
+
+    _hyperparameters = {'absolute_value': {'type': bool}}
+
+    def __init__(self, absolute_value=False, n_jobs=None):
+        self.absolute_value = absolute_value
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_observations, n_features)
+            Input data. Each entry along axis 0 is a sample of ``n_features``
+            different variables, of size ``n_observations``.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_array(X, allow_nd=True)
+        validate_params(
+            self.get_params(), self._hyperparameters, exclude=['n_jobs'])
+
+        self._is_fitted = True
+        return self
+
+    def transform(self, X, y=None):
+        """Compute Pearson dissimilarities.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples, n_observations, n_features)
+            Input data. Each entry along axis 0 is a sample of ``n_features``
+            different variables, of size ``n_observations``.
+
+        y : None
+            There is no need for a target in a transformer, yet the pipeline
+            API requires this parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples, n_features, n_features)
+            Array of Pearson dissimilarities.
+
+        """
+        check_is_fitted(self, '_is_fitted')
+        X = check_array(X, allow_nd=True)
+
+        Xt = np.empty((X.shape[0], X.shape[2], X.shape[2]))
+        for i, sample in enumerate(X):
+            Xt[i, :, :] = np.corrcoef(sample, rowvar=False)
+        Xt = 0.5 - Xt/2 if not self.absolute_value else 1 - np.abs(Xt)
+
+        return Xt
```

## gtda/time_series/preprocessing.py

 * *Ordering differences only*

```diff
@@ -1,245 +1,245 @@
-"""Resampling and stationarization of time series data."""
-# License: GNU AGPLv3
-
-import numpy as np
-from sklearn.base import BaseEstimator
-from sklearn.utils.validation import check_array, column_or_1d
-from sklearn.utils.validation import check_is_fitted
-
-from ..base import TransformerResamplerMixin
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.intervals import Interval
-from ..utils.validation import validate_params
-
-
-@adapt_fit_transform_docs
-class Resampler(BaseEstimator, TransformerResamplerMixin):
-    """Time series resampling at regular intervals.
-
-    Parameters
-    ----------
-    period : int, default: ``2``
-        The sampling period, i.e. one point every period will be kept.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from gtda.time_series import Resampler
-    >>> # Create a noisy signal
-    >>> signal = np.asarray([np.sin(x /40) + np.random.random()
-    ...                      for x in range(0, 300)])
-    >>> # Set up the Resampler
-    >>> period = 10
-    >>> periodic_sampler = Resampler(period=period)
-    >>> # Fit and transform the signal
-    >>> signal_resampled = periodic_sampler.fit_transform(signal)
-    >>> print(signal_resampled.shape)
-    (30,)
-
-    """
-
-    _hyperparameters = {
-        'period': {'type': int, 'in': Interval(1, np.inf, closed='left')}
-        }
-
-    def __init__(self, period=2):
-        self.period = period
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples,) or (n_samples, ...)
-            Input data.
-
-        y : None
-            Ignored.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_array(X, ensure_2d=False, allow_nd=True)
-        validate_params(self.get_params(), self._hyperparameters)
-
-        self._is_fitted = True
-        return self
-
-    def transform(self, X, y=None):
-        """Resample `X`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples,) or (n_samples, ...)
-            Input data.
-
-        y : None
-            There is no need for a target, yet the pipeline API requires this
-            parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples_new, ...)
-            Resampled array. ``n_samples_new = n_samples // period``.
-
-        """
-        check_is_fitted(self, '_is_fitted')
-        Xt = check_array(X, ensure_2d=False, allow_nd=True, copy=True)
-
-        if Xt.ndim == 1:
-            Xt = Xt[: None]
-        Xt = Xt[::self.period]
-
-        return Xt
-
-    def resample(self, y, X=None):
-        """Resample `y`.
-
-        Parameters
-        ----------
-        y : ndarray of shape (n_samples,)
-            Target.
-
-        X : None
-            There is no need for input data, yet the pipeline API requires this
-            parameter.
-
-        Returns
-        -------
-        yr : ndarray of shape (n_samples_new,)
-            Resampled target. ``n_samples_new = n_samples // period``.
-
-        """
-        check_is_fitted(self, '_is_fitted')
-        yr = column_or_1d(y)
-        yr = yr[::self.period]
-
-        return yr
-
-
-class Stationarizer(BaseEstimator, TransformerResamplerMixin):
-    """Methods for stationarizing time series data.
-
-    Time series may be stationarized to remove or reduce linear or exponential
-    trends.
-
-    Parameters
-    ----------
-    operation : ``'return'`` | ``'log-return'``, default: ``'return'``
-        The type of stationarization operation to perform. It can have two
-        values:
-
-        - ``'return'``:
-          This option transforms the time series :math:`{X_t}_t` into the
-          time series of relative returns, i.e. the ratio :math:`(X_t-X_{
-          t-1})/X_t`.
-
-        - ``'log-return'``:
-          This option transforms the time series :math:`{X_t}_t` into the
-          time series of relative log-returns, i.e. :math:`\\log(X_t/X_{
-          t-1})`.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from gtda.time_series import Stationarizer
-    >>> # Create a noisy signal
-    >>> signal = np.asarray([np.sin(x /40) + 5 + np.random.random()
-    >>>                      for x in range(0, 300)]).reshape(-1, 1)
-    >>> # Initialize the stationarizer
-    >>> stationarizer = Stationarizer(operation='return')
-    >>> # Fit and transform the signal
-    >>> signal_stationarized = stationarizer.fit_transform(signal)
-    >>> print(signal_stationarized.shape)
-    (299,)
-
-    """
-
-    _hyperparameters = {
-        'operation': {'type': str, 'in': ['return', 'log-return']}
-        }
-
-    def __init__(self, operation='return'):
-        self.operation = operation
-
-    def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged.
-
-        This method is here to implement the usual scikit-learn API and hence
-        work in pipelines.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples,) or (n_samples, ...)
-            Input data.
-
-        y : None
-            Ignored.
-
-        Returns
-        -------
-        self : object
-
-        """
-        check_array(X, ensure_2d=False, allow_nd=True)
-        validate_params(self.get_params(), self._hyperparameters)
-
-        self._is_fitted = True
-        return self
-
-    def transform(self, X, y=None):
-        """Stationarize `X` by applying the procedure given by `operation`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples,) or (n_samples, ...)
-            Input data.
-
-        y : None
-            There is no need for a target, yet the pipeline API requires this
-            parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples_new, ...)
-            Stationarized array. ``n_samples_new = n_samples - 1``.
-
-        """
-        check_is_fitted(self, '_is_fitted')
-        Xt = check_array(X, ensure_2d=False, allow_nd=True)
-
-        if Xt.ndim == 1:
-            Xt = Xt[:, None]
-
-        if self.operation == 'return':
-            return np.diff(Xt, n=1, axis=0) / Xt[1:]
-        else:  # Assumes 'log-return' operation
-            return np.diff(np.log(Xt), n=1, axis=0)
-
-    def resample(self, y, X=None):
-        """Resample `y`.
-
-        Parameters
-        ----------
-        y : ndarray of shape (n_samples,)
-            Target.
-
-        X : None
-            There is no need for input data, yet the pipeline API requires this
-            parameter.
-
-        Returns
-        -------
-        yr : ndarray of shape (n_samples_new,)
-            Resampled target. ``n_samples_new = n_samples - 1``.
-
-        """
-        check_is_fitted(self, '_is_fitted')
-        y = column_or_1d(y)
-
-        return y[1:]
+"""Resampling and stationarization of time series data."""
+# License: GNU AGPLv3
+
+import numpy as np
+from sklearn.base import BaseEstimator
+from sklearn.utils.validation import check_array, column_or_1d
+from sklearn.utils.validation import check_is_fitted
+
+from ..base import TransformerResamplerMixin
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.intervals import Interval
+from ..utils.validation import validate_params
+
+
+@adapt_fit_transform_docs
+class Resampler(BaseEstimator, TransformerResamplerMixin):
+    """Time series resampling at regular intervals.
+
+    Parameters
+    ----------
+    period : int, default: ``2``
+        The sampling period, i.e. one point every period will be kept.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from gtda.time_series import Resampler
+    >>> # Create a noisy signal
+    >>> signal = np.asarray([np.sin(x /40) + np.random.random()
+    ...                      for x in range(0, 300)])
+    >>> # Set up the Resampler
+    >>> period = 10
+    >>> periodic_sampler = Resampler(period=period)
+    >>> # Fit and transform the signal
+    >>> signal_resampled = periodic_sampler.fit_transform(signal)
+    >>> print(signal_resampled.shape)
+    (30,)
+
+    """
+
+    _hyperparameters = {
+        'period': {'type': int, 'in': Interval(1, np.inf, closed='left')}
+        }
+
+    def __init__(self, period=2):
+        self.period = period
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples,) or (n_samples, ...)
+            Input data.
+
+        y : None
+            Ignored.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_array(X, ensure_2d=False, allow_nd=True)
+        validate_params(self.get_params(), self._hyperparameters)
+
+        self._is_fitted = True
+        return self
+
+    def transform(self, X, y=None):
+        """Resample `X`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples,) or (n_samples, ...)
+            Input data.
+
+        y : None
+            There is no need for a target, yet the pipeline API requires this
+            parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples_new, ...)
+            Resampled array. ``n_samples_new = n_samples // period``.
+
+        """
+        check_is_fitted(self, '_is_fitted')
+        Xt = check_array(X, ensure_2d=False, allow_nd=True, copy=True)
+
+        if Xt.ndim == 1:
+            Xt = Xt[: None]
+        Xt = Xt[::self.period]
+
+        return Xt
+
+    def resample(self, y, X=None):
+        """Resample `y`.
+
+        Parameters
+        ----------
+        y : ndarray of shape (n_samples,)
+            Target.
+
+        X : None
+            There is no need for input data, yet the pipeline API requires this
+            parameter.
+
+        Returns
+        -------
+        yr : ndarray of shape (n_samples_new,)
+            Resampled target. ``n_samples_new = n_samples // period``.
+
+        """
+        check_is_fitted(self, '_is_fitted')
+        yr = column_or_1d(y)
+        yr = yr[::self.period]
+
+        return yr
+
+
+class Stationarizer(BaseEstimator, TransformerResamplerMixin):
+    """Methods for stationarizing time series data.
+
+    Time series may be stationarized to remove or reduce linear or exponential
+    trends.
+
+    Parameters
+    ----------
+    operation : ``'return'`` | ``'log-return'``, default: ``'return'``
+        The type of stationarization operation to perform. It can have two
+        values:
+
+        - ``'return'``:
+          This option transforms the time series :math:`{X_t}_t` into the
+          time series of relative returns, i.e. the ratio :math:`(X_t-X_{
+          t-1})/X_t`.
+
+        - ``'log-return'``:
+          This option transforms the time series :math:`{X_t}_t` into the
+          time series of relative log-returns, i.e. :math:`\\log(X_t/X_{
+          t-1})`.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from gtda.time_series import Stationarizer
+    >>> # Create a noisy signal
+    >>> signal = np.asarray([np.sin(x /40) + 5 + np.random.random()
+    >>>                      for x in range(0, 300)]).reshape(-1, 1)
+    >>> # Initialize the stationarizer
+    >>> stationarizer = Stationarizer(operation='return')
+    >>> # Fit and transform the signal
+    >>> signal_stationarized = stationarizer.fit_transform(signal)
+    >>> print(signal_stationarized.shape)
+    (299,)
+
+    """
+
+    _hyperparameters = {
+        'operation': {'type': str, 'in': ['return', 'log-return']}
+        }
+
+    def __init__(self, operation='return'):
+        self.operation = operation
+
+    def fit(self, X, y=None):
+        """Do nothing and return the estimator unchanged.
+
+        This method is here to implement the usual scikit-learn API and hence
+        work in pipelines.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples,) or (n_samples, ...)
+            Input data.
+
+        y : None
+            Ignored.
+
+        Returns
+        -------
+        self : object
+
+        """
+        check_array(X, ensure_2d=False, allow_nd=True)
+        validate_params(self.get_params(), self._hyperparameters)
+
+        self._is_fitted = True
+        return self
+
+    def transform(self, X, y=None):
+        """Stationarize `X` by applying the procedure given by `operation`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples,) or (n_samples, ...)
+            Input data.
+
+        y : None
+            There is no need for a target, yet the pipeline API requires this
+            parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples_new, ...)
+            Stationarized array. ``n_samples_new = n_samples - 1``.
+
+        """
+        check_is_fitted(self, '_is_fitted')
+        Xt = check_array(X, ensure_2d=False, allow_nd=True)
+
+        if Xt.ndim == 1:
+            Xt = Xt[:, None]
+
+        if self.operation == 'return':
+            return np.diff(Xt, n=1, axis=0) / Xt[1:]
+        else:  # Assumes 'log-return' operation
+            return np.diff(np.log(Xt), n=1, axis=0)
+
+    def resample(self, y, X=None):
+        """Resample `y`.
+
+        Parameters
+        ----------
+        y : ndarray of shape (n_samples,)
+            Target.
+
+        X : None
+            There is no need for input data, yet the pipeline API requires this
+            parameter.
+
+        Returns
+        -------
+        yr : ndarray of shape (n_samples_new,)
+            Resampled target. ``n_samples_new = n_samples - 1``.
+
+        """
+        check_is_fitted(self, '_is_fitted')
+        y = column_or_1d(y)
+
+        return y[1:]
```

## gtda/time_series/target.py

```diff
@@ -1,188 +1,188 @@
-"""Time series labelling."""
-# License: GNU AGPLv3
-
-from numbers import Real
-from types import FunctionType
-
-import numpy as np
-from sklearn.base import BaseEstimator
-from sklearn.utils.validation import check_is_fitted, column_or_1d
-
-from .embedding import SlidingWindow
-from ..base import TransformerResamplerMixin
-from ..utils._docs import adapt_fit_transform_docs
-from ..utils.intervals import Interval
-from ..utils.validation import validate_params
-
-
-@adapt_fit_transform_docs
-class Labeller(BaseEstimator, TransformerResamplerMixin):
-    """Target creation from sliding windows over a univariate time series.
-
-    Useful to define a time series forecasting task in which labels are
-    obtained from future values of the input time series, via the application
-    of a function to time windows.
-
-    Parameters
-    ----------
-    size : int, optional, default: ``10``
-        Size of each sliding window.
-
-    func : callable, optional, default: ``numpy.std``
-        Function to be applied to each window.
-
-    func_params : dict or None, optional, default: ``None``
-        Additional keyword arguments for `func`.
-
-    percentiles : list of real numbers between 0 and 100 inclusive, or \
-        None, optional, default: ``None``
-        If ``None``, creates a target for a regression task. Otherwise, creates
-        a target for an n-class classification task where
-        ``n = len(percentiles) + 1``.
-
-    n_steps_future : int, optional, default: ``1``
-        Number of steps in the future for the predictive task.
-
-    Attributes
-    ----------
-    thresholds_ : list of floats or ``None`` if percentiles is ``None``
-        Values corresponding to each percentile, based on data seen in
-        :meth:`fit`.
-
-    Examples
-    --------
-    >>> import numpy as np
-    >>> from gtda.time_series import Labeller
-    >>> # Create a time series
-    >>> X = np.arange(10)
-    >>> labeller = Labeller(size=3, func=np.min)
-    >>> # Fit and transform X
-    >>> X, y = labeller.fit_transform_resample(X, X)
-    >>> print(X)
-    [1 2 3 4 5 6 7 8]
-    >>> print(y)
-    [0 1 2 3 4 5 6 7]
-
-    """
-
-    _hyperparameters = {
-        'size': {'type': int, 'in': Interval(1, np.inf, closed='left')},
-        'func': {'type': FunctionType},
-        'func_params': {'type': (dict, type(None))},
-        'percentiles': {
-            'type': (list, type(None)),
-            'of': {'type': Real, 'in': Interval(0, 100, closed='both')}
-            },
-        'n_steps_future': {'type': int,
-                           'in': Interval(1, np.inf, closed='left')}
-        }
-
-    def __init__(self, size=10, func=np.std,
-                 func_params=None, percentiles=None, n_steps_future=1):
-        self.size = size
-        self.func = func
-        self.func_params = func_params
-        self.percentiles = percentiles
-        self.n_steps_future = n_steps_future
-
-    def fit(self, X, y=None):
-        """Compute :attr:`thresholds_` and return the estimator.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples,) or (n_samples, 1)
-            Univariate time series to build a target for.
-
-        y : None
-            There is no need for a target, yet the pipeline API requires this
-            parameter.
-
-        Returns
-        -------
-        self : object
-
-        """
-        X = column_or_1d(X)
-        validate_params(self.get_params(), self._hyperparameters)
-
-        self._sliding_window = SlidingWindow(size=self.size, stride=1).fit(X)
-        _X = self._sliding_window.transform(X)
-        if self.func_params is None:
-            self._effective_func_params = {}
-        else:
-            self._effective_func_params = self.func_params
-        _X = self.func(_X, axis=1, **self._effective_func_params)[:, None]
-
-        if self.percentiles is None:
-            self.thresholds_ = None
-        else:
-            self.thresholds_ = [np.percentile(np.abs(_X.flatten()), percentile)
-                                for percentile in self.percentiles]
-
-        return self
-
-    def transform(self, X, y=None):
-        """Cuts `X` so it is aligned with `y`.
-
-        Parameters
-        ----------
-        X : ndarray of shape (n_samples,) or (n_samples, 1)
-            Univariate time series to build a target for.
-
-        y : None
-            There is no need for a target, yet the pipeline API requires this
-            parameter.
-
-        Returns
-        -------
-        Xt : ndarray of shape (n_samples_new,)
-            The cut input time series.
-
-        """
-        check_is_fitted(self)
-        Xt = column_or_1d(X)
-
-        Xt = Xt[:-self.n_steps_future]
-
-        if self.n_steps_future < self.size - 1:
-            Xt = Xt[self.size - 1 - self.n_steps_future:]
-        return Xt
-
-    def resample(self, y, X=None):
-        """Resample `y`.
-
-        Parameters
-        ----------
-        y : ndarray of shape (n_samples,)
-            Time series to build a target for.
-
-        X : None
-            There is no need for `X`, yet the pipeline API requires this
-            parameter.
-
-        Returns
-        -------
-        yr : ndarray of shape (n_samples_new,)
-            Target for the prediction task.
-
-        """
-        check_is_fitted(self)
-        y = column_or_1d(y)
-
-        yr = self._sliding_window.transform(y)
-        yr = self.func(yr, axis=1, **self._effective_func_params)[:, None]
-
-        if self.thresholds_ is not None:
-            yr = np.abs(yr)
-            yr = np.concatenate(
-                [1 * (yr >= 0) * (yr < self.thresholds_[0])] +
-                [1 * (yr >= self.thresholds_[i]) *
-                 (yr < self.thresholds_[i + 1]) for i in range(
-                    len(self.thresholds_) - 1)] +
-                [1 * (yr >= self.thresholds_[-1])], axis=1)
-            yr = np.nonzero(yr)[1].reshape(yr.shape[0], 1)
-
-        if self.n_steps_future > self.size - 1:
-            yr = yr[self.n_steps_future - self.size + 1:]
-
-        return yr.reshape(-1)
+"""Time series labelling."""
+# License: GNU AGPLv3
+
+from numbers import Real
+from typing import Callable
+
+import numpy as np
+from sklearn.base import BaseEstimator
+from sklearn.utils.validation import check_is_fitted, column_or_1d
+
+from .embedding import SlidingWindow
+from ..base import TransformerResamplerMixin
+from ..utils._docs import adapt_fit_transform_docs
+from ..utils.intervals import Interval
+from ..utils.validation import validate_params
+
+
+@adapt_fit_transform_docs
+class Labeller(BaseEstimator, TransformerResamplerMixin):
+    """Target creation from sliding windows over a univariate time series.
+
+    Useful to define a time series forecasting task in which labels are
+    obtained from future values of the input time series, via the application
+    of a function to time windows.
+
+    Parameters
+    ----------
+    size : int, optional, default: ``10``
+        Size of each sliding window.
+
+    func : callable, optional, default: ``numpy.std``
+        Function to be applied to each window.
+
+    func_params : dict or None, optional, default: ``None``
+        Additional keyword arguments for `func`.
+
+    percentiles : list of real numbers between 0 and 100 inclusive, or \
+        None, optional, default: ``None``
+        If ``None``, creates a target for a regression task. Otherwise, creates
+        a target for an n-class classification task where
+        ``n = len(percentiles) + 1``.
+
+    n_steps_future : int, optional, default: ``1``
+        Number of steps in the future for the predictive task.
+
+    Attributes
+    ----------
+    thresholds_ : list of floats or ``None`` if percentiles is ``None``
+        Values corresponding to each percentile, based on data seen in
+        :meth:`fit`.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from gtda.time_series import Labeller
+    >>> # Create a time series
+    >>> X = np.arange(10)
+    >>> labeller = Labeller(size=3, func=np.min)
+    >>> # Fit and transform X
+    >>> X, y = labeller.fit_transform_resample(X, X)
+    >>> print(X)
+    [1 2 3 4 5 6 7 8]
+    >>> print(y)
+    [0 1 2 3 4 5 6 7]
+
+    """
+
+    _hyperparameters = {
+        'size': {'type': int, 'in': Interval(1, np.inf, closed='left')},
+        'func': {'type': Callable},
+        'func_params': {'type': (dict, type(None))},
+        'percentiles': {
+            'type': (list, type(None)),
+            'of': {'type': Real, 'in': Interval(0, 100, closed='both')}
+            },
+        'n_steps_future': {'type': int,
+                           'in': Interval(1, np.inf, closed='left')}
+        }
+
+    def __init__(self, size=10, func=np.std,
+                 func_params=None, percentiles=None, n_steps_future=1):
+        self.size = size
+        self.func = func
+        self.func_params = func_params
+        self.percentiles = percentiles
+        self.n_steps_future = n_steps_future
+
+    def fit(self, X, y=None):
+        """Compute :attr:`thresholds_` and return the estimator.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples,) or (n_samples, 1)
+            Univariate time series to build a target for.
+
+        y : None
+            There is no need for a target, yet the pipeline API requires this
+            parameter.
+
+        Returns
+        -------
+        self : object
+
+        """
+        X = column_or_1d(X)
+        validate_params(self.get_params(), self._hyperparameters)
+
+        self._sliding_window = SlidingWindow(size=self.size, stride=1).fit(X)
+        _X = self._sliding_window.transform(X)
+        if self.func_params is None:
+            self._effective_func_params = {}
+        else:
+            self._effective_func_params = self.func_params
+        _X = self.func(_X, axis=1, **self._effective_func_params)[:, None]
+
+        if self.percentiles is None:
+            self.thresholds_ = None
+        else:
+            self.thresholds_ = [np.percentile(np.abs(_X.flatten()), percentile)
+                                for percentile in self.percentiles]
+
+        return self
+
+    def transform(self, X, y=None):
+        """Cuts `X` so it is aligned with `y`.
+
+        Parameters
+        ----------
+        X : ndarray of shape (n_samples,) or (n_samples, 1)
+            Univariate time series to build a target for.
+
+        y : None
+            There is no need for a target, yet the pipeline API requires this
+            parameter.
+
+        Returns
+        -------
+        Xt : ndarray of shape (n_samples_new,)
+            The cut input time series.
+
+        """
+        check_is_fitted(self)
+        Xt = column_or_1d(X)
+
+        Xt = Xt[:-self.n_steps_future]
+
+        if self.n_steps_future < self.size - 1:
+            Xt = Xt[self.size - 1 - self.n_steps_future:]
+        return Xt
+
+    def resample(self, y, X=None):
+        """Resample `y`.
+
+        Parameters
+        ----------
+        y : ndarray of shape (n_samples,)
+            Time series to build a target for.
+
+        X : None
+            There is no need for `X`, yet the pipeline API requires this
+            parameter.
+
+        Returns
+        -------
+        yr : ndarray of shape (n_samples_new,)
+            Target for the prediction task.
+
+        """
+        check_is_fitted(self)
+        y = column_or_1d(y)
+
+        yr = self._sliding_window.transform(y)
+        yr = self.func(yr, axis=1, **self._effective_func_params)[:, None]
+
+        if self.thresholds_ is not None:
+            yr = np.abs(yr)
+            yr = np.concatenate(
+                [1 * (yr >= 0) * (yr < self.thresholds_[0])] +
+                [1 * (yr >= self.thresholds_[i]) *
+                 (yr < self.thresholds_[i + 1]) for i in range(
+                    len(self.thresholds_) - 1)] +
+                [1 * (yr >= self.thresholds_[-1])], axis=1)
+            yr = np.nonzero(yr)[1].reshape(yr.shape[0], 1)
+
+        if self.n_steps_future > self.size - 1:
+            yr = yr[self.n_steps_future - self.size + 1:]
+
+        return yr.reshape(-1)
```

## gtda/time_series/_utils.py

 * *Ordering differences only*

```diff
@@ -1,89 +1,89 @@
-"""Utility functions for time series processing."""
-# License: GNU AGPLv3
-
-from functools import partial
-
-import numpy as np
-from sklearn.metrics import mutual_info_score
-from sklearn.neighbors import NearestNeighbors
-
-
-def _time_delay_embedding(X, time_delay=1, dimension=2, stride=1,
-                          flatten=False, ensure_last_value=True):
-    if hasattr(X, 'shape') and hasattr(X, 'ndim'):  # ndarray input
-        n_timestamps = X.shape[-1]
-        n_points, offset = \
-            divmod(n_timestamps - time_delay * (dimension - 1) - 1, stride)
-        n_points += 1
-        if n_points <= 0:
-            raise ValueError(
-                f"Not enough time stamps ({n_timestamps}) to produce at least "
-                f"one {dimension}-dimensional vector under the current choice "
-                f"of time delay ({time_delay})."
-                )
-        indices = np.tile(np.arange(0, time_delay * dimension, time_delay),
-                          (n_points, 1))
-        indices += np.arange(n_points)[:, None] * stride
-        if ensure_last_value:
-            indices += offset
-
-        X_embedded = X[..., indices]
-        if flatten and (X.ndim > 2):
-            transpose_axes = (0, *range(1, X.ndim)[::-1], X.ndim)
-            X_embedded = np.transpose(X_embedded, axes=transpose_axes).\
-                reshape(len(X), -1, dimension * np.prod(X.shape[1:-1]))
-    else:  # list of ndarray input
-        func = partial(_time_delay_embedding, time_delay=time_delay,
-                       dimension=dimension, stride=stride, flatten=flatten,
-                       ensure_last_value=ensure_last_value)
-        X_embedded = [func(x[None, ...])[0] for x in X]
-
-    return X_embedded
-
-
-def _mutual_information(X, time_delay, n_bins):
-    """Calculate the mutual information given the time delay."""
-    contingency = np.histogram2d(X[:-time_delay], X[time_delay:],
-                                 bins=n_bins)[0]
-    mutual_information = mutual_info_score(None, None,
-                                           contingency=contingency)
-    return mutual_information
-
-
-def _false_nearest_neighbors(X, time_delay, dimension, stride=1):
-    """Calculate the number of false nearest neighbours in a certain
-    embedding dimension, based on heuristics."""
-    X_embedded = _time_delay_embedding(X, time_delay=time_delay,
-                                       dimension=dimension, stride=stride)
-
-    neighbor = \
-        NearestNeighbors(n_neighbors=2, algorithm='auto').fit(X_embedded)
-    distances, indices = neighbor.kneighbors(X_embedded)
-    distance = distances[:, 1]
-    X_first_nbhrs = X[indices[:, 1]]
-
-    epsilon = 2. * np.std(X)
-    tolerance = 10
-
-    neg_dim_delay = - dimension * time_delay
-    distance_slice = distance[:neg_dim_delay]
-    X_rolled = np.roll(X, neg_dim_delay)
-    X_rolled_slice = slice(len(X) - len(X_embedded), neg_dim_delay)
-    X_first_nbhrs_rolled = np.roll(X_first_nbhrs, neg_dim_delay)
-
-    neighbor_abs_diff = np.abs(
-        X_rolled[X_rolled_slice] - X_first_nbhrs_rolled[:neg_dim_delay]
-        )
-
-    false_neighbor_ratio = np.divide(
-        neighbor_abs_diff, distance_slice,
-        out=np.zeros_like(neighbor_abs_diff, dtype=float),
-        where=(distance_slice != 0)
-        )
-    false_neighbor_criteria = false_neighbor_ratio > tolerance
-
-    limited_dataset_criteria = distance_slice < epsilon
-
-    n_false_neighbors = \
-        np.sum(false_neighbor_criteria * limited_dataset_criteria)
-    return n_false_neighbors
+"""Utility functions for time series processing."""
+# License: GNU AGPLv3
+
+from functools import partial
+
+import numpy as np
+from sklearn.metrics import mutual_info_score
+from sklearn.neighbors import NearestNeighbors
+
+
+def _time_delay_embedding(X, time_delay=1, dimension=2, stride=1,
+                          flatten=False, ensure_last_value=True):
+    if hasattr(X, 'shape') and hasattr(X, 'ndim'):  # ndarray input
+        n_timestamps = X.shape[-1]
+        n_points, offset = \
+            divmod(n_timestamps - time_delay * (dimension - 1) - 1, stride)
+        n_points += 1
+        if n_points <= 0:
+            raise ValueError(
+                f"Not enough time stamps ({n_timestamps}) to produce at least "
+                f"one {dimension}-dimensional vector under the current choice "
+                f"of time delay ({time_delay})."
+                )
+        indices = np.tile(np.arange(0, time_delay * dimension, time_delay),
+                          (n_points, 1))
+        indices += np.arange(n_points)[:, None] * stride
+        if ensure_last_value:
+            indices += offset
+
+        X_embedded = X[..., indices]
+        if flatten and (X.ndim > 2):
+            transpose_axes = (0, *range(1, X.ndim)[::-1], X.ndim)
+            X_embedded = np.transpose(X_embedded, axes=transpose_axes).\
+                reshape(len(X), -1, dimension * np.prod(X.shape[1:-1]))
+    else:  # list of ndarray input
+        func = partial(_time_delay_embedding, time_delay=time_delay,
+                       dimension=dimension, stride=stride, flatten=flatten,
+                       ensure_last_value=ensure_last_value)
+        X_embedded = [func(x[None, ...])[0] for x in X]
+
+    return X_embedded
+
+
+def _mutual_information(X, time_delay, n_bins):
+    """Calculate the mutual information given the time delay."""
+    contingency = np.histogram2d(X[:-time_delay], X[time_delay:],
+                                 bins=n_bins)[0]
+    mutual_information = mutual_info_score(None, None,
+                                           contingency=contingency)
+    return mutual_information
+
+
+def _false_nearest_neighbors(X, time_delay, dimension, stride=1):
+    """Calculate the number of false nearest neighbours in a certain
+    embedding dimension, based on heuristics."""
+    X_embedded = _time_delay_embedding(X, time_delay=time_delay,
+                                       dimension=dimension, stride=stride)
+
+    neighbor = \
+        NearestNeighbors(n_neighbors=2, algorithm='auto').fit(X_embedded)
+    distances, indices = neighbor.kneighbors(X_embedded)
+    distance = distances[:, 1]
+    X_first_nbhrs = X[indices[:, 1]]
+
+    epsilon = 2. * np.std(X)
+    tolerance = 10
+
+    neg_dim_delay = - dimension * time_delay
+    distance_slice = distance[:neg_dim_delay]
+    X_rolled = np.roll(X, neg_dim_delay)
+    X_rolled_slice = slice(len(X) - len(X_embedded), neg_dim_delay)
+    X_first_nbhrs_rolled = np.roll(X_first_nbhrs, neg_dim_delay)
+
+    neighbor_abs_diff = np.abs(
+        X_rolled[X_rolled_slice] - X_first_nbhrs_rolled[:neg_dim_delay]
+        )
+
+    false_neighbor_ratio = np.divide(
+        neighbor_abs_diff, distance_slice,
+        out=np.zeros_like(neighbor_abs_diff, dtype=float),
+        where=(distance_slice != 0)
+        )
+    false_neighbor_criteria = false_neighbor_ratio > tolerance
+
+    limited_dataset_criteria = distance_slice < epsilon
+
+    n_false_neighbors = \
+        np.sum(false_neighbor_criteria * limited_dataset_criteria)
+    return n_false_neighbors
```

## gtda/time_series/__init__.py

 * *Ordering differences only*

```diff
@@ -1,22 +1,22 @@
-"""The module :mod:`gtda.time_series` implements transformers to preprocess
-time series or embed them in a higher dimensional space for persistent
-homology."""
-
-from .embedding import SlidingWindow, takens_embedding_optimal_parameters, \
-    SingleTakensEmbedding, TakensEmbedding
-from .features import PermutationEntropy
-from .preprocessing import Resampler, Stationarizer
-from .multivariate import PearsonDissimilarity
-from .target import Labeller
-
-__all__ = [
-    'Resampler',
-    'Stationarizer',
-    'PermutationEntropy',
-    'takens_embedding_optimal_parameters',
-    'SingleTakensEmbedding',
-    'TakensEmbedding',
-    'SlidingWindow',
-    'Labeller',
-    'PearsonDissimilarity'
-    ]
+"""The module :mod:`gtda.time_series` implements transformers to preprocess
+time series or embed them in a higher dimensional space for persistent
+homology."""
+
+from .embedding import SlidingWindow, takens_embedding_optimal_parameters, \
+    SingleTakensEmbedding, TakensEmbedding
+from .features import PermutationEntropy
+from .preprocessing import Resampler, Stationarizer
+from .multivariate import PearsonDissimilarity
+from .target import Labeller
+
+__all__ = [
+    'Resampler',
+    'Stationarizer',
+    'PermutationEntropy',
+    'takens_embedding_optimal_parameters',
+    'SingleTakensEmbedding',
+    'TakensEmbedding',
+    'SlidingWindow',
+    'Labeller',
+    'PearsonDissimilarity'
+    ]
```

## gtda/utils/intervals.py

 * *Ordering differences only*

```diff
@@ -1,193 +1,193 @@
-"""Base class for real intervals."""
-# License: GNU AGPLv3
-
-from numbers import Real
-from operator import le, lt
-
-
-def _interval_like(other):
-    return (hasattr(other, 'left')
-            and hasattr(other, 'right')
-            and hasattr(other, 'closed'))
-
-
-class Interval:
-    """Immutable object implementing an interval.
-
-    Parameters
-    ----------
-    left : real scalar, required
-        Left bound for the interval.
-
-    right : real scalar, required
-        Right bound for the interval.
-
-    closed : ``'right'`` | ``'left'`` | ``'both'`` | ``'neither'``, required
-        Whether the interval is closed on the left-side, right-side, both or
-        neither.
-
-    """
-    _VALID_CLOSED = frozenset(['left', 'right', 'both', 'neither'])
-
-    def __init__(self, left, right, *, closed):
-        self._validate_endpoint(left)
-        self._validate_endpoint(right)
-        if closed not in self._VALID_CLOSED:
-            raise ValueError(
-                f"Invalid option for `closed`: {closed}. Argument must be "
-                f"one of {list(self._VALID_CLOSED)}.")
-        if not left <= right:
-            raise ValueError("Left side of interval must be <= right side")
-
-        self.left = left
-        self.right = right
-        self.closed = closed
-
-    @staticmethod
-    def _validate_endpoint(endpoint):
-        if not isinstance(endpoint, Real):
-            raise ValueError(
-                "Only real (finite or infinite) endpoints are allowed when "
-                "constructing an Interval.")
-
-    @property
-    def closed_left(self):
-        """Check if the interval is closed on the left side.
-
-        """
-        return self.closed in ('left', 'both')
-
-    @property
-    def closed_right(self):
-        """Check if the interval is closed on the right side.
-
-        """
-        return self.closed in ('right', 'both')
-
-    @property
-    def open_left(self):
-        """Check if the interval is open on the left side.
-
-        """
-        return not self.closed_left
-
-    @property
-    def open_right(self):
-        """Check if the interval is closed on the left side.
-
-        """
-        return not self.closed_right
-
-    @property
-    def mid(self):
-        """Return the midpoint of the interval. Take care when the left or
-        right sides are infinite.
-
-        """
-        return 0.5 * (self.left + self.right)
-
-    @property
-    def length(self):
-        """Return the length of the interval. Take care when the left or
-        right sides are infinite.
-
-        """
-        return self.right - self.left
-
-    @property
-    def is_empty(self):
-        """Indicates if an interval is empty, meaning it contains no points.
-
-        """
-        return (self.right == self.left) & (self.closed != 'both')
-
-    def __hash__(self):
-        return hash((self.left, self.right, self.closed))
-
-    def __contains__(self, key):
-        if _interval_like(key):
-            raise TypeError("__contains__ not defined for two intervals")
-        return ((self.left < key if self.open_left else self.left <= key) &
-                (key < self.right if self.open_right else key <= self.right))
-
-    def __reduce__(self):
-        args = (self.left, self.right, self.closed)
-        return type(self), args
-
-    def __repr__(self):
-        left, right = self.left, self.right
-        name = type(self).__name__
-        repr_str = f"{name}({repr(left)}, {repr(right)}, " \
-                   f"closed={repr(self.closed)})"
-        return repr_str
-
-    def __str__(self):
-        left, right = self.left, self.right
-        start_symbol = '[' if self.closed_left else '('
-        end_symbol = ']' if self.closed_right else ')'
-        return f'{start_symbol}{left}, {right}{end_symbol}'
-
-    def __add__(self, y):
-        if isinstance(y, Real):
-            return Interval(self.left + y, self.right + y, closed=self.closed)
-        elif isinstance(y, Interval) and isinstance(self, Real):
-            return Interval(y.left + self, y.right + self, closed=y.closed)
-        return NotImplemented
-
-    def __sub__(self, y):
-        if isinstance(y, Real):
-            return Interval(self.left - y, self.right - y, closed=self.closed)
-        return NotImplemented
-
-    def __mul__(self, y):
-        if isinstance(y, Real):
-            return Interval(self.left * y, self.right * y, closed=self.closed)
-        elif isinstance(y, Interval) and isinstance(self, Real):
-            return Interval(y.left * self, y.right * self, closed=y.closed)
-        return NotImplemented
-
-    def __div__(self, y):
-        if isinstance(y, Real):
-            return Interval(self.left / y, self.right / y, closed=self.closed)
-        return NotImplemented
-
-    def __truediv__(self, y):
-        if isinstance(y, Real):
-            return Interval(self.left / y, self.right / y, closed=self.closed)
-        return NotImplemented
-
-    def __floordiv__(self, y):
-        if isinstance(y, Real):
-            return Interval(
-                self.left // y, self.right // y, closed=self.closed)
-        return NotImplemented
-
-    def intersects(self, other):
-        """Check whether two :cls:`Interval` objects intersect. Two
-        intervals intersect if they share a common point, including closed
-        endpoints. Intervals that only have an open endpoint in common do not
-        intersect.
-
-        Parameters
-        ----------
-        other : Interval object
-            Interval to check against for an overlap.
-
-        Returns
-        -------
-        bool
-            ``True`` if the two intervals overlap.
-
-        """
-        if not isinstance(other, Interval):
-            raise TypeError("`other` must be an Interval, "
-                            f"got {type(other).__name__}")
-
-        # equality is okay if both endpoints are closed (overlap at a point)
-        op1 = le if (self.closed_left and other.closed_right) else lt
-        op2 = le if (other.closed_left and self.closed_right) else lt
-
-        # overlaps is equivalent negation of two interval being disjoint:
-        # disjoint = (A.left > B.right) or (B.left > A.right)
-        # (simplifying the negation allows this to be done in fewer operations)
-        return op1(self.left, other.right) and op2(other.left, self.right)
+"""Base class for real intervals."""
+# License: GNU AGPLv3
+
+from numbers import Real
+from operator import le, lt
+
+
+def _interval_like(other):
+    return (hasattr(other, 'left')
+            and hasattr(other, 'right')
+            and hasattr(other, 'closed'))
+
+
+class Interval:
+    """Immutable object implementing an interval.
+
+    Parameters
+    ----------
+    left : real scalar, required
+        Left bound for the interval.
+
+    right : real scalar, required
+        Right bound for the interval.
+
+    closed : ``'right'`` | ``'left'`` | ``'both'`` | ``'neither'``, required
+        Whether the interval is closed on the left-side, right-side, both or
+        neither.
+
+    """
+    _VALID_CLOSED = frozenset(['left', 'right', 'both', 'neither'])
+
+    def __init__(self, left, right, *, closed):
+        self._validate_endpoint(left)
+        self._validate_endpoint(right)
+        if closed not in self._VALID_CLOSED:
+            raise ValueError(
+                f"Invalid option for `closed`: {closed}. Argument must be "
+                f"one of {list(self._VALID_CLOSED)}.")
+        if not left <= right:
+            raise ValueError("Left side of interval must be <= right side")
+
+        self.left = left
+        self.right = right
+        self.closed = closed
+
+    @staticmethod
+    def _validate_endpoint(endpoint):
+        if not isinstance(endpoint, Real):
+            raise ValueError(
+                "Only real (finite or infinite) endpoints are allowed when "
+                "constructing an Interval.")
+
+    @property
+    def closed_left(self):
+        """Check if the interval is closed on the left side.
+
+        """
+        return self.closed in ('left', 'both')
+
+    @property
+    def closed_right(self):
+        """Check if the interval is closed on the right side.
+
+        """
+        return self.closed in ('right', 'both')
+
+    @property
+    def open_left(self):
+        """Check if the interval is open on the left side.
+
+        """
+        return not self.closed_left
+
+    @property
+    def open_right(self):
+        """Check if the interval is closed on the left side.
+
+        """
+        return not self.closed_right
+
+    @property
+    def mid(self):
+        """Return the midpoint of the interval. Take care when the left or
+        right sides are infinite.
+
+        """
+        return 0.5 * (self.left + self.right)
+
+    @property
+    def length(self):
+        """Return the length of the interval. Take care when the left or
+        right sides are infinite.
+
+        """
+        return self.right - self.left
+
+    @property
+    def is_empty(self):
+        """Indicates if an interval is empty, meaning it contains no points.
+
+        """
+        return (self.right == self.left) & (self.closed != 'both')
+
+    def __hash__(self):
+        return hash((self.left, self.right, self.closed))
+
+    def __contains__(self, key):
+        if _interval_like(key):
+            raise TypeError("__contains__ not defined for two intervals")
+        return ((self.left < key if self.open_left else self.left <= key) &
+                (key < self.right if self.open_right else key <= self.right))
+
+    def __reduce__(self):
+        args = (self.left, self.right, self.closed)
+        return type(self), args
+
+    def __repr__(self):
+        left, right = self.left, self.right
+        name = type(self).__name__
+        repr_str = f"{name}({repr(left)}, {repr(right)}, " \
+                   f"closed={repr(self.closed)})"
+        return repr_str
+
+    def __str__(self):
+        left, right = self.left, self.right
+        start_symbol = '[' if self.closed_left else '('
+        end_symbol = ']' if self.closed_right else ')'
+        return f'{start_symbol}{left}, {right}{end_symbol}'
+
+    def __add__(self, y):
+        if isinstance(y, Real):
+            return Interval(self.left + y, self.right + y, closed=self.closed)
+        elif isinstance(y, Interval) and isinstance(self, Real):
+            return Interval(y.left + self, y.right + self, closed=y.closed)
+        return NotImplemented
+
+    def __sub__(self, y):
+        if isinstance(y, Real):
+            return Interval(self.left - y, self.right - y, closed=self.closed)
+        return NotImplemented
+
+    def __mul__(self, y):
+        if isinstance(y, Real):
+            return Interval(self.left * y, self.right * y, closed=self.closed)
+        elif isinstance(y, Interval) and isinstance(self, Real):
+            return Interval(y.left * self, y.right * self, closed=y.closed)
+        return NotImplemented
+
+    def __div__(self, y):
+        if isinstance(y, Real):
+            return Interval(self.left / y, self.right / y, closed=self.closed)
+        return NotImplemented
+
+    def __truediv__(self, y):
+        if isinstance(y, Real):
+            return Interval(self.left / y, self.right / y, closed=self.closed)
+        return NotImplemented
+
+    def __floordiv__(self, y):
+        if isinstance(y, Real):
+            return Interval(
+                self.left // y, self.right // y, closed=self.closed)
+        return NotImplemented
+
+    def intersects(self, other):
+        """Check whether two :cls:`Interval` objects intersect. Two
+        intervals intersect if they share a common point, including closed
+        endpoints. Intervals that only have an open endpoint in common do not
+        intersect.
+
+        Parameters
+        ----------
+        other : Interval object
+            Interval to check against for an overlap.
+
+        Returns
+        -------
+        bool
+            ``True`` if the two intervals overlap.
+
+        """
+        if not isinstance(other, Interval):
+            raise TypeError("`other` must be an Interval, "
+                            f"got {type(other).__name__}")
+
+        # equality is okay if both endpoints are closed (overlap at a point)
+        op1 = le if (self.closed_left and other.closed_right) else lt
+        op2 = le if (other.closed_left and self.closed_right) else lt
+
+        # overlaps is equivalent negation of two interval being disjoint:
+        # disjoint = (A.left > B.right) or (B.left > A.right)
+        # (simplifying the negation allows this to be done in fewer operations)
+        return op1(self.left, other.right) and op2(other.left, self.right)
```

## gtda/utils/validation.py

 * *Ordering differences only*

```diff
@@ -1,355 +1,355 @@
-"""Utilities for input validation."""
-# License: GNU AGPLv3
-
-from functools import reduce
-from operator import and_
-from warnings import warn
-
-import numpy as np
-from scipy.sparse import issparse
-from sklearn.exceptions import DataDimensionalityWarning
-from sklearn.utils.validation import check_array
-
-
-def _check_array_mod(X, **kwargs):
-    """Modified version of :func:`sklearn.utils.validation.check_array. When
-    keyword parameter `force_all_finite` is set to False, NaNs are not
-    accepted but infinity is."""
-    if not kwargs.get('force_all_finite', True):
-        Xnew = check_array(X, **kwargs)
-        if np.isnan(Xnew if not issparse(Xnew) else Xnew.data).any():
-            raise ValueError("Input contains NaNs. Only finite values and "
-                             "infinity are allowed when parameter "
-                             "`force_all_finite` is False.")
-        return Xnew
-    return check_array(X, **kwargs)
-
-
-def check_diagrams(X, copy=False):
-    """Input validation for collections of persistence diagrams.
-
-    Basic type and sanity checks are run on the input collection and the
-    array is converted to float type before returning. In particular,
-    the input is checked to be an ndarray of shape ``(n_samples, n_points,
-    3)``.
-
-    Parameters
-    ----------
-    X : object
-        Input object to check/convert.
-
-    copy : bool, optional, default: ``False``
-        Whether a forced copy should be triggered.
-
-    Returns
-    -------
-    X_validated : ndarray of shape (n_samples, n_points, 3)
-        The converted and validated array of persistence diagrams.
-
-    """
-    X_array = _check_array_mod(X, ensure_2d=False, allow_nd=True,
-                               force_all_finite=False, copy=copy)
-    if X_array.ndim != 3:
-        raise ValueError(
-            f"Input should be a 3D ndarray, the shape is {X_array.shape}."
-            )
-    if X_array.shape[2] != 3:
-        raise ValueError(
-            f"Input should be a 3D ndarray with a 3rd dimension of 3 "
-            f"components, but there are {X_array.shape[2]} components."
-            )
-
-    homology_dimensions = sorted(np.unique(X_array[0, :, 2]))
-    for dim in homology_dimensions:
-        if dim == np.inf:
-            if len(homology_dimensions) != 1:
-                raise ValueError(
-                    f"numpy.inf is a valid homology dimension for a stacked "
-                    f"diagram but it should be the only one: "
-                    f"homology_dimensions = {homology_dimensions}."
-                    )
-        else:
-            if (dim != int(dim)) or (dim < 0):
-                raise ValueError(
-                    f"Homology dimensions should be positive integers or "
-                    f"numpy.inf: {dim} can't be cast to an int of the same "
-                    f"value."
-                    )
-
-    n_points_below_diag = np.sum(X_array[:, :, 1] < X_array[:, :, 0])
-    if n_points_below_diag:
-        raise ValueError(
-            f"All points of all persistence diagrams should be above the "
-            f"diagonal, i.e. X[:, :, 1] >= X[:, :, 0]. {n_points_below_diag} "
-            f"points are below the diagonal."
-            )
-
-    return X_array
-
-
-def check_graph(X):
-    # TODO
-    return X
-
-
-def _validate_params(parameters, references, rec_name=None):
-    types_tuple = (list, tuple, np.ndarray, dict)
-
-    def _validate_params_single(_parameter, _reference, _name):
-        if _reference is None:
-            return
-
-        _ref_type = _reference.get('type', None)
-
-        # Check that _parameter has the correct type
-        if not ((_ref_type is None) or isinstance(_parameter, _ref_type)):
-            raise TypeError(f"Parameter `{_name}` is of type "
-                            f"{type(_parameter)} while it should be of type "
-                            f"{_ref_type}.")
-
-        # If neither the reference type is list, tuple, np.ndarray or dict,
-        # nor _parameter is an instance of one of these types, the checks are
-        # performed on _parameter directly.
-        elif not ((_ref_type in types_tuple)
-                  or isinstance(_parameter, types_tuple)):
-            ref_in = _reference.get('in', None)
-            ref_other = _reference.get('other', None)
-            if _parameter is not None:
-                if not ((ref_in is None) or _parameter in ref_in):
-                    raise ValueError(f"Parameter `{_name}` is {_parameter}, "
-                                     f"which is not in {ref_in}.")
-            # Perform any other checks via the callable ref_others
-            if ref_other is not None:
-                return ref_other(_parameter)
-
-        # Explicitly return the type of _reference if one of list, tuple,
-        # np.ndarray or dict.
-        else:
-            return _ref_type
-
-    for name, parameter in parameters.items():
-        if name not in references.keys():
-            name_extras = "" if rec_name is None else f" in `{rec_name}`"
-            raise KeyError(f"`{name}`{name_extras} is not an available "
-                           f"parameter. Available parameters are in "
-                           f"{tuple(references.keys())}.")
-
-        reference = references[name]
-        ref_type = _validate_params_single(parameter, reference, name)
-        if ref_type:
-            ref_of = reference.get('of', None)
-            if ref_of is None:
-                # if ref_of is None, the elements are not to be validated
-                continue
-            elif ref_type == dict:
-                _validate_params(parameter, ref_of, rec_name=name)
-            else:  # List, tuple or ndarray type
-                for i, parameter_elem in enumerate(parameter):
-                    _validate_params_single(parameter_elem, ref_of,
-                                            f"{name}[{i}]")
-
-
-def validate_params(parameters, references, exclude=None):
-    """Function to automate the validation of (hyper)parameters.
-
-    Parameters
-    ----------
-    parameters : dict, required
-        Dictionary in which the keys parameter names (as strings) and the
-        corresponding values are parameter values. Unless `exclude` (see
-        below) contains some of the keys in this dictionary, all parameters
-        are checked against `references`.
-
-    references : dict, required
-        Dictionary in which the keys are parameter names (as strings). Let
-        ``name`` and ``parameter`` denote a key-value pair in `parameters`.
-        Since ``name`` should also be a key in `references`, let ``reference``
-        be the corresponding value there. Then, ``reference`` must be a
-        dictionary containing any of the following keys:
-
-        - ``'type'``, mapping to a class or tuple of classes. ``parameter``
-          is checked to be an instance of this class or tuple of classes.
-
-        - ``'in'``, mapping to an object, when the value of ``'type'`` is
-          not one of ``list``, ``tuple``, ``numpy.ndarray`` or ``dict``.
-          Letting ``ref_in`` denote that object, the following check is
-          performed: ``parameter in ref_in``.
-
-        - ``'of'``, mapping to a dictionary, when the value of ``'type'``
-          is one of ``list``, ``tuple``, ``numpy.ndarray`` or ``dict``.
-          Let ``ref_of`` denote that dictionary. Then:
-
-          a) If ``reference['type'] == dict``  meaning that ``parameter``
-             should be a dictionary  ``ref_of`` should have a similar
-             structure as `references`, and :func:`validate_params` is called
-             recursively on ``(parameter, ref_of)``.
-          b) Otherwise, ``ref_of`` should have a similar structure as
-             ``reference`` and each entry in ``parameter`` is checked to
-             satisfy the constraints in ``ref_of``.
-
-        - ``'other'``, which should map to a callable defining custom checks on
-          ``parameter``.
-
-    exclude : list or None, optional, default: ``None``
-        List of parameter names which are among the keys in `parameters` but
-        should be excluded from validation. ``None`` is equivalent to
-        passing the empty list.
-
-    """
-    exclude_ = [] if exclude is None else exclude
-    parameters_ = {key: value for key, value in parameters.items()
-                   if key not in exclude_}
-    return _validate_params(parameters_, references)
-
-
-def check_point_clouds(X, distance_matrices=False, **kwargs):
-    """Input validation on arrays or lists representing collections of point
-    clouds or of distance/adjacency matrices.
-
-    The input is checked to be either a single 3D array using a single call
-    to :func:`sklearn.utils.validation.check_array`, or a list of 2D arrays by
-    calling :func:`sklearn.utils.validation.check_array` on each entry.
-
-    Parameters
-    ----------
-    X : object
-        Input object to check / convert.
-
-    distance_matrices : bool, optional, default: ``False``
-        Whether the input represents a collection of distance matrices or of
-        concrete point clouds in Euclidean space. In the first case, entries
-        are allowed to be infinite unless otherwise specified in `kwargs`.
-
-    **kwargs
-        Keyword arguments accepted by
-        :func:`sklearn.utils.validation.check_array`, with the following
-        caveats: 1) `ensure_2d` and `allow_nd` are ignored; 2) if not passed
-        explicitly, `force_all_finite` is set to be the boolean negation of
-        `distance_matrices`; 3) when `force_all_finite` is set to ``False``,
-        NaN inputs are not allowed; 4) `accept_sparse` and
-        `accept_large_sparse` are only meaningful in the case of lists of 2D
-        arrays, in which case they are passed to individual instances of
-        :func:`sklearn.utils.validation.check_array` validating each entry
-        in the list.
-
-    Returns
-    -------
-    Xnew : ndarray or list
-        The converted and validated object.
-
-    """
-    kwargs_ = {'force_all_finite': not distance_matrices}
-    kwargs_.update(kwargs)
-    kwargs_.pop('allow_nd', None)
-    kwargs_.pop('ensure_2d', None)
-    if hasattr(X, 'shape') and hasattr(X, 'ndim'):
-        if X.ndim != 3:
-            if X.ndim == 2:
-                extra_2D = \
-                    "\nReshape your input X using X.reshape(1, *X.shape) or " \
-                    "X[None, :, :] if X is a single point cloud/distance " \
-                    "matrix/adjacency matrix of a weighted graph."
-            else:
-                extra_2D = ""
-            raise ValueError(
-                f"Input must be a single 3D array or a list of 2D arrays or "
-                f"sparse matrices. Structure of dimension {X.ndim} passed."
-                + extra_2D
-                )
-        if (X.shape[1] != X.shape[2]) and distance_matrices:
-            raise ValueError(
-                f"Input array X must have X.shape[1] == X.shape[2]: "
-                f"{X.shape[1]} != {X.shape[2]} passed.")
-        elif (X.shape[1] == X.shape[2]) and not distance_matrices:
-            warn(
-                "Input array X has X.shape[1] == X.shape[2]. This is "
-                "consistent with a collection of distance/adjacency "
-                "matrices, but the input is being treated as a collection "
-                "of vectors in Euclidean space.",
-                DataDimensionalityWarning, stacklevel=2
-                )
-        Xnew = _check_array_mod(X, allow_nd=True, **kwargs_)
-    else:
-        has_check_failed = False
-        messages = []
-        Xnew = []
-        for i, x in enumerate(X):
-            try:
-                xnew = _check_array_mod(x, ensure_2d=True, **kwargs_)
-                if distance_matrices and not issparse(xnew):
-                    if not x.shape[0] == x.shape[1]:
-                        raise ValueError(
-                            f"All arrays must be square: {x.shape[0]} rows "
-                            f"and {x.shape[1]} columns found in this array."
-                            )
-                Xnew.append(xnew)
-            except ValueError as e:
-                has_check_failed = True
-                messages.append(f"Entry {i}:\n{e}")
-        if has_check_failed:
-            raise ValueError(
-                "The following errors were raised by the inputs:\n\n" +
-                "\n\n".join(messages)
-                )
-
-        if not distance_matrices:
-            if reduce(and_, (x.shape[0] == x.shape[1] for x in X), True):
-                warn(
-                    "All arrays/matrices are square. This is consistent with "
-                    "a collection of distance/adjacency matrices, but the "
-                    "entries will be treated as collections of vectors in "
-                    "Euclidean space.", DataDimensionalityWarning,
-                    stacklevel=2
-                    )
-
-        ref_dim = X[0].shape  # Shape of first sample
-        if reduce(and_, (x.shape == ref_dim for x in X[1:]), True):
-            Xnew = np.asarray(Xnew)
-
-    return Xnew
-
-
-def check_collection(X, **kwargs):
-    """Generic input validation on arrays or lists of arrays.
-
-    Parameters
-    ----------
-    X : object
-        Input object to check / convert.
-
-    **kwargs
-        Keyword arguments accepted by
-        :func:`sklearn.utils.validation.check_array`, with the following
-        caveats: 1) `ensure_2d` and `allow_nd` are ignored; 2) when
-        `force_all_finite` is set to ``False``, NaN inputs are not allowed.
-
-    Returns
-    -------
-    Xnew : ndarray or list
-        The converted and validated object.
-
-    """
-    kwargs_ = kwargs.copy()
-    kwargs_.pop('allow_nd', None)
-    kwargs_.pop('ensure_2d', None)
-    if hasattr(X, 'shape') and hasattr(X, 'ndim'):
-        Xnew = _check_array_mod(X, ensure_2d=True, allow_nd=True, **kwargs_)
-    else:
-        has_check_failed = False
-        messages = []
-        Xnew = []
-        for i, x in enumerate(X):
-            try:
-                xnew = _check_array_mod(x, ensure_2d=False, allow_nd=True,
-                                        **kwargs_)
-                Xnew.append(xnew)
-            except ValueError as e:
-                has_check_failed = True
-                messages.append(f"Entry {i}:\n{e}")
-        if has_check_failed:
-            raise ValueError(
-                "The following errors were raised by the inputs:\n\n" +
-                "\n\n".join(messages)
-                )
-
-    return Xnew
+"""Utilities for input validation."""
+# License: GNU AGPLv3
+
+from functools import reduce
+from operator import and_
+from warnings import warn
+
+import numpy as np
+from scipy.sparse import issparse
+from sklearn.exceptions import DataDimensionalityWarning
+from sklearn.utils.validation import check_array
+
+
+def _check_array_mod(X, **kwargs):
+    """Modified version of :func:`sklearn.utils.validation.check_array. When
+    keyword parameter `force_all_finite` is set to False, NaNs are not
+    accepted but infinity is."""
+    if not kwargs.get('force_all_finite', True):
+        Xnew = check_array(X, **kwargs)
+        if np.isnan(Xnew if not issparse(Xnew) else Xnew.data).any():
+            raise ValueError("Input contains NaNs. Only finite values and "
+                             "infinity are allowed when parameter "
+                             "`force_all_finite` is False.")
+        return Xnew
+    return check_array(X, **kwargs)
+
+
+def check_diagrams(X, copy=False):
+    """Input validation for collections of persistence diagrams.
+
+    Basic type and sanity checks are run on the input collection and the
+    array is converted to float type before returning. In particular,
+    the input is checked to be an ndarray of shape ``(n_samples, n_points,
+    3)``.
+
+    Parameters
+    ----------
+    X : object
+        Input object to check/convert.
+
+    copy : bool, optional, default: ``False``
+        Whether a forced copy should be triggered.
+
+    Returns
+    -------
+    X_validated : ndarray of shape (n_samples, n_points, 3)
+        The converted and validated array of persistence diagrams.
+
+    """
+    X_array = _check_array_mod(X, ensure_2d=False, allow_nd=True,
+                               force_all_finite=False, copy=copy)
+    if X_array.ndim != 3:
+        raise ValueError(
+            f"Input should be a 3D ndarray, the shape is {X_array.shape}."
+            )
+    if X_array.shape[2] != 3:
+        raise ValueError(
+            f"Input should be a 3D ndarray with a 3rd dimension of 3 "
+            f"components, but there are {X_array.shape[2]} components."
+            )
+
+    homology_dimensions = sorted(np.unique(X_array[0, :, 2]))
+    for dim in homology_dimensions:
+        if dim == np.inf:
+            if len(homology_dimensions) != 1:
+                raise ValueError(
+                    f"numpy.inf is a valid homology dimension for a stacked "
+                    f"diagram but it should be the only one: "
+                    f"homology_dimensions = {homology_dimensions}."
+                    )
+        else:
+            if (dim != int(dim)) or (dim < 0):
+                raise ValueError(
+                    f"Homology dimensions should be positive integers or "
+                    f"numpy.inf: {dim} can't be cast to an int of the same "
+                    f"value."
+                    )
+
+    n_points_below_diag = np.sum(X_array[:, :, 1] < X_array[:, :, 0])
+    if n_points_below_diag:
+        raise ValueError(
+            f"All points of all persistence diagrams should be above the "
+            f"diagonal, i.e. X[:, :, 1] >= X[:, :, 0]. {n_points_below_diag} "
+            f"points are below the diagonal."
+            )
+
+    return X_array
+
+
+def check_graph(X):
+    # TODO
+    return X
+
+
+def _validate_params(parameters, references, rec_name=None):
+    types_tuple = (list, tuple, np.ndarray, dict)
+
+    def _validate_params_single(_parameter, _reference, _name):
+        if _reference is None:
+            return
+
+        _ref_type = _reference.get('type', None)
+
+        # Check that _parameter has the correct type
+        if not ((_ref_type is None) or isinstance(_parameter, _ref_type)):
+            raise TypeError(f"Parameter `{_name}` is of type "
+                            f"{type(_parameter)} while it should be of type "
+                            f"{_ref_type}.")
+
+        # If neither the reference type is list, tuple, np.ndarray or dict,
+        # nor _parameter is an instance of one of these types, the checks are
+        # performed on _parameter directly.
+        elif not ((_ref_type in types_tuple)
+                  or isinstance(_parameter, types_tuple)):
+            ref_in = _reference.get('in', None)
+            ref_other = _reference.get('other', None)
+            if _parameter is not None:
+                if not ((ref_in is None) or _parameter in ref_in):
+                    raise ValueError(f"Parameter `{_name}` is {_parameter}, "
+                                     f"which is not in {ref_in}.")
+            # Perform any other checks via the callable ref_others
+            if ref_other is not None:
+                return ref_other(_parameter)
+
+        # Explicitly return the type of _reference if one of list, tuple,
+        # np.ndarray or dict.
+        else:
+            return _ref_type
+
+    for name, parameter in parameters.items():
+        if name not in references.keys():
+            name_extras = "" if rec_name is None else f" in `{rec_name}`"
+            raise KeyError(f"`{name}`{name_extras} is not an available "
+                           f"parameter. Available parameters are in "
+                           f"{tuple(references.keys())}.")
+
+        reference = references[name]
+        ref_type = _validate_params_single(parameter, reference, name)
+        if ref_type:
+            ref_of = reference.get('of', None)
+            if ref_of is None:
+                # if ref_of is None, the elements are not to be validated
+                continue
+            elif ref_type == dict:
+                _validate_params(parameter, ref_of, rec_name=name)
+            else:  # List, tuple or ndarray type
+                for i, parameter_elem in enumerate(parameter):
+                    _validate_params_single(parameter_elem, ref_of,
+                                            f"{name}[{i}]")
+
+
+def validate_params(parameters, references, exclude=None):
+    """Function to automate the validation of (hyper)parameters.
+
+    Parameters
+    ----------
+    parameters : dict, required
+        Dictionary in which the keys parameter names (as strings) and the
+        corresponding values are parameter values. Unless `exclude` (see
+        below) contains some of the keys in this dictionary, all parameters
+        are checked against `references`.
+
+    references : dict, required
+        Dictionary in which the keys are parameter names (as strings). Let
+        ``name`` and ``parameter`` denote a key-value pair in `parameters`.
+        Since ``name`` should also be a key in `references`, let ``reference``
+        be the corresponding value there. Then, ``reference`` must be a
+        dictionary containing any of the following keys:
+
+        - ``'type'``, mapping to a class or tuple of classes. ``parameter``
+          is checked to be an instance of this class or tuple of classes.
+
+        - ``'in'``, mapping to an object, when the value of ``'type'`` is
+          not one of ``list``, ``tuple``, ``numpy.ndarray`` or ``dict``.
+          Letting ``ref_in`` denote that object, the following check is
+          performed: ``parameter in ref_in``.
+
+        - ``'of'``, mapping to a dictionary, when the value of ``'type'``
+          is one of ``list``, ``tuple``, ``numpy.ndarray`` or ``dict``.
+          Let ``ref_of`` denote that dictionary. Then:
+
+          a) If ``reference['type'] == dict``  meaning that ``parameter``
+             should be a dictionary  ``ref_of`` should have a similar
+             structure as `references`, and :func:`validate_params` is called
+             recursively on ``(parameter, ref_of)``.
+          b) Otherwise, ``ref_of`` should have a similar structure as
+             ``reference`` and each entry in ``parameter`` is checked to
+             satisfy the constraints in ``ref_of``.
+
+        - ``'other'``, which should map to a callable defining custom checks on
+          ``parameter``.
+
+    exclude : list or None, optional, default: ``None``
+        List of parameter names which are among the keys in `parameters` but
+        should be excluded from validation. ``None`` is equivalent to
+        passing the empty list.
+
+    """
+    exclude_ = [] if exclude is None else exclude
+    parameters_ = {key: value for key, value in parameters.items()
+                   if key not in exclude_}
+    return _validate_params(parameters_, references)
+
+
+def check_point_clouds(X, distance_matrices=False, **kwargs):
+    """Input validation on arrays or lists representing collections of point
+    clouds or of distance/adjacency matrices.
+
+    The input is checked to be either a single 3D array using a single call
+    to :func:`sklearn.utils.validation.check_array`, or a list of 2D arrays by
+    calling :func:`sklearn.utils.validation.check_array` on each entry.
+
+    Parameters
+    ----------
+    X : object
+        Input object to check / convert.
+
+    distance_matrices : bool, optional, default: ``False``
+        Whether the input represents a collection of distance matrices or of
+        concrete point clouds in Euclidean space. In the first case, entries
+        are allowed to be infinite unless otherwise specified in `kwargs`.
+
+    **kwargs
+        Keyword arguments accepted by
+        :func:`sklearn.utils.validation.check_array`, with the following
+        caveats: 1) `ensure_2d` and `allow_nd` are ignored; 2) if not passed
+        explicitly, `force_all_finite` is set to be the boolean negation of
+        `distance_matrices`; 3) when `force_all_finite` is set to ``False``,
+        NaN inputs are not allowed; 4) `accept_sparse` and
+        `accept_large_sparse` are only meaningful in the case of lists of 2D
+        arrays, in which case they are passed to individual instances of
+        :func:`sklearn.utils.validation.check_array` validating each entry
+        in the list.
+
+    Returns
+    -------
+    Xnew : ndarray or list
+        The converted and validated object.
+
+    """
+    kwargs_ = {'force_all_finite': not distance_matrices}
+    kwargs_.update(kwargs)
+    kwargs_.pop('allow_nd', None)
+    kwargs_.pop('ensure_2d', None)
+    if hasattr(X, 'shape') and hasattr(X, 'ndim'):
+        if X.ndim != 3:
+            if X.ndim == 2:
+                extra_2D = \
+                    "\nReshape your input X using X.reshape(1, *X.shape) or " \
+                    "X[None, :, :] if X is a single point cloud/distance " \
+                    "matrix/adjacency matrix of a weighted graph."
+            else:
+                extra_2D = ""
+            raise ValueError(
+                f"Input must be a single 3D array or a list of 2D arrays or "
+                f"sparse matrices. Structure of dimension {X.ndim} passed."
+                + extra_2D
+                )
+        if (X.shape[1] != X.shape[2]) and distance_matrices:
+            raise ValueError(
+                f"Input array X must have X.shape[1] == X.shape[2]: "
+                f"{X.shape[1]} != {X.shape[2]} passed.")
+        elif (X.shape[1] == X.shape[2]) and not distance_matrices:
+            warn(
+                "Input array X has X.shape[1] == X.shape[2]. This is "
+                "consistent with a collection of distance/adjacency "
+                "matrices, but the input is being treated as a collection "
+                "of vectors in Euclidean space.",
+                DataDimensionalityWarning, stacklevel=2
+                )
+        Xnew = _check_array_mod(X, allow_nd=True, **kwargs_)
+    else:
+        has_check_failed = False
+        messages = []
+        Xnew = []
+        for i, x in enumerate(X):
+            try:
+                xnew = _check_array_mod(x, ensure_2d=True, **kwargs_)
+                if distance_matrices and not issparse(xnew):
+                    if not x.shape[0] == x.shape[1]:
+                        raise ValueError(
+                            f"All arrays must be square: {x.shape[0]} rows "
+                            f"and {x.shape[1]} columns found in this array."
+                            )
+                Xnew.append(xnew)
+            except ValueError as e:
+                has_check_failed = True
+                messages.append(f"Entry {i}:\n{e}")
+        if has_check_failed:
+            raise ValueError(
+                "The following errors were raised by the inputs:\n\n" +
+                "\n\n".join(messages)
+                )
+
+        if not distance_matrices:
+            if reduce(and_, (x.shape[0] == x.shape[1] for x in X), True):
+                warn(
+                    "All arrays/matrices are square. This is consistent with "
+                    "a collection of distance/adjacency matrices, but the "
+                    "entries will be treated as collections of vectors in "
+                    "Euclidean space.", DataDimensionalityWarning,
+                    stacklevel=2
+                    )
+
+        ref_dim = X[0].shape  # Shape of first sample
+        if reduce(and_, (x.shape == ref_dim for x in X[1:]), True):
+            Xnew = np.asarray(Xnew)
+
+    return Xnew
+
+
+def check_collection(X, **kwargs):
+    """Generic input validation on arrays or lists of arrays.
+
+    Parameters
+    ----------
+    X : object
+        Input object to check / convert.
+
+    **kwargs
+        Keyword arguments accepted by
+        :func:`sklearn.utils.validation.check_array`, with the following
+        caveats: 1) `ensure_2d` and `allow_nd` are ignored; 2) when
+        `force_all_finite` is set to ``False``, NaN inputs are not allowed.
+
+    Returns
+    -------
+    Xnew : ndarray or list
+        The converted and validated object.
+
+    """
+    kwargs_ = kwargs.copy()
+    kwargs_.pop('allow_nd', None)
+    kwargs_.pop('ensure_2d', None)
+    if hasattr(X, 'shape') and hasattr(X, 'ndim'):
+        Xnew = _check_array_mod(X, ensure_2d=True, allow_nd=True, **kwargs_)
+    else:
+        has_check_failed = False
+        messages = []
+        Xnew = []
+        for i, x in enumerate(X):
+            try:
+                xnew = _check_array_mod(x, ensure_2d=False, allow_nd=True,
+                                        **kwargs_)
+                Xnew.append(xnew)
+            except ValueError as e:
+                has_check_failed = True
+                messages.append(f"Entry {i}:\n{e}")
+        if has_check_failed:
+            raise ValueError(
+                "The following errors were raised by the inputs:\n\n" +
+                "\n\n".join(messages)
+                )
+
+    return Xnew
```

## gtda/utils/_docs.py

 * *Ordering differences only*

```diff
@@ -1,114 +1,114 @@
-"""Utilities for docstring building."""
-# License: GNU AGPLv3
-
-import re
-from functools import wraps
-from inspect import getdoc
-
-from sklearn.base import TransformerMixin
-
-inputs_start = 'Parameters\n----------\n'
-inputs_end = 'Returns\n-------\n'
-outputs_end = '\n\n'
-
-
-def get_preamble_docs(docs):
-    re_search = re.search(
-        f'^(.*){inputs_start}', docs, flags=re.DOTALL)
-    return re_search.group(1)
-
-
-def get_inputs_docs(docs):
-    re_search = re.search(
-        f'{inputs_start}(.*){inputs_end}', docs, flags=re.DOTALL)
-    return re_search.group(1)
-
-
-def get_outputs_docs(docs):
-    re_search = re.search(
-        f'{inputs_end}(.*){outputs_end}', docs, flags=re.DOTALL)
-    if re_search is None:
-        re_search = re.search(
-            f'{inputs_end}(.*)$', docs, flags=re.DOTALL)
-    return re_search.group(1)
-
-
-standard_fit_transform_docs = getdoc(TransformerMixin.fit_transform)
-standard_intro_docs = get_preamble_docs(standard_fit_transform_docs)
-intro_docs = re.sub(r'\bX\b', '`X`', standard_intro_docs)
-intro_docs = re.sub(r'\by\b', '`y`', intro_docs)
-intro_docs = re.sub(r'\bfit_params\b', '`fit_params`', intro_docs)
-standard_inputs_docs = get_inputs_docs(standard_fit_transform_docs)
-standard_outputs_docs = get_outputs_docs(standard_fit_transform_docs)
-
-
-def make_fit_transform_docs(fit_docs, transform_docs):
-    """Create docstring for a :meth:`fit_transform` method.
-
-    Uses the standard documentation for
-    :class:`sklearn.base.TransformerMixin` as a template, but replaces the
-    "Parameters" section with the corresponding section from `fit_docs`,
-    and the "Returns" section with the corresponding section from
-    `transform_docs`. Also performs other cosmetic changes.
-
-    Parameters
-    ----------
-    fit_docs : str
-        Docstring for :meth:`fit`. Should contain a "Parameters" section.
-
-    transform_docs : str
-        Docstring for :meth:`transform`. Should contain a "Returns" section.
-
-    """
-
-    inputs_docs = get_inputs_docs(fit_docs)
-    outputs_docs = get_outputs_docs(transform_docs)
-    new_docstring = standard_fit_transform_docs.\
-        replace(standard_intro_docs, intro_docs).\
-        replace(standard_inputs_docs, inputs_docs).\
-        replace(standard_outputs_docs, outputs_docs)
-    return new_docstring
-
-
-def adapt_fit_transform_docs(transformermixin_cls):
-    """Class decorator changing the docstring for :meth:`fit_transform`.
-
-    Fetches the :meth:`fit` and :meth:`transform` docstrings of a class
-    implementing :meth:`fit_transform`, creates adapted docstring using
-    :func:`gtda.utils._docs.make_fit_transform_docs`, and
-    wraps the original :meth:`fit_transform` implementation with one with
-    the new docstring.
-
-    This is particularly useful for classes inheriting from
-    :class:`sklearn.base.TransformerMixin`, when the standard docstring is
-    inadequate because of exotic input shapes or types.
-
-    Parameters
-    ----------
-    transformermixin_cls : type
-        A class containing a method :meth:`fit_transform`.
-
-    Returns
-    -------
-    transformermixin_cls : type
-        Input class in which the :meth:`fit_transform` method has been
-        replaced by a version with a modified docstring but otherwise identical
-        behaviour.
-
-    """
-
-    fit_docs = getdoc(getattr(transformermixin_cls, 'fit'))
-    transform_docs = getdoc(getattr(transformermixin_cls, 'transform'))
-
-    def make_new_fit_transform(original_fit_transform):
-        @wraps(original_fit_transform)
-        def fit_transform_wrapper(*args, **kwargs):
-            return original_fit_transform(*args, **kwargs)
-        fit_transform_wrapper.__doc__ = \
-            make_fit_transform_docs(fit_docs, transform_docs)
-        return fit_transform_wrapper
-
-    new_fit_transform = make_new_fit_transform(
-        getattr(transformermixin_cls, 'fit_transform'))
-    setattr(transformermixin_cls, 'fit_transform', new_fit_transform)
-    return transformermixin_cls
+"""Utilities for docstring building."""
+# License: GNU AGPLv3
+
+import re
+from functools import wraps
+from inspect import getdoc
+
+from sklearn.base import TransformerMixin
+
+inputs_start = 'Parameters\n----------\n'
+inputs_end = 'Returns\n-------\n'
+outputs_end = '\n\n'
+
+
+def get_preamble_docs(docs):
+    re_search = re.search(
+        f'^(.*){inputs_start}', docs, flags=re.DOTALL)
+    return re_search.group(1)
+
+
+def get_inputs_docs(docs):
+    re_search = re.search(
+        f'{inputs_start}(.*){inputs_end}', docs, flags=re.DOTALL)
+    return re_search.group(1)
+
+
+def get_outputs_docs(docs):
+    re_search = re.search(
+        f'{inputs_end}(.*){outputs_end}', docs, flags=re.DOTALL)
+    if re_search is None:
+        re_search = re.search(
+            f'{inputs_end}(.*)$', docs, flags=re.DOTALL)
+    return re_search.group(1)
+
+
+standard_fit_transform_docs = getdoc(TransformerMixin.fit_transform)
+standard_intro_docs = get_preamble_docs(standard_fit_transform_docs)
+intro_docs = re.sub(r'\bX\b', '`X`', standard_intro_docs)
+intro_docs = re.sub(r'\by\b', '`y`', intro_docs)
+intro_docs = re.sub(r'\bfit_params\b', '`fit_params`', intro_docs)
+standard_inputs_docs = get_inputs_docs(standard_fit_transform_docs)
+standard_outputs_docs = get_outputs_docs(standard_fit_transform_docs)
+
+
+def make_fit_transform_docs(fit_docs, transform_docs):
+    """Create docstring for a :meth:`fit_transform` method.
+
+    Uses the standard documentation for
+    :class:`sklearn.base.TransformerMixin` as a template, but replaces the
+    "Parameters" section with the corresponding section from `fit_docs`,
+    and the "Returns" section with the corresponding section from
+    `transform_docs`. Also performs other cosmetic changes.
+
+    Parameters
+    ----------
+    fit_docs : str
+        Docstring for :meth:`fit`. Should contain a "Parameters" section.
+
+    transform_docs : str
+        Docstring for :meth:`transform`. Should contain a "Returns" section.
+
+    """
+
+    inputs_docs = get_inputs_docs(fit_docs)
+    outputs_docs = get_outputs_docs(transform_docs)
+    new_docstring = standard_fit_transform_docs.\
+        replace(standard_intro_docs, intro_docs).\
+        replace(standard_inputs_docs, inputs_docs).\
+        replace(standard_outputs_docs, outputs_docs)
+    return new_docstring
+
+
+def adapt_fit_transform_docs(transformermixin_cls):
+    """Class decorator changing the docstring for :meth:`fit_transform`.
+
+    Fetches the :meth:`fit` and :meth:`transform` docstrings of a class
+    implementing :meth:`fit_transform`, creates adapted docstring using
+    :func:`gtda.utils._docs.make_fit_transform_docs`, and
+    wraps the original :meth:`fit_transform` implementation with one with
+    the new docstring.
+
+    This is particularly useful for classes inheriting from
+    :class:`sklearn.base.TransformerMixin`, when the standard docstring is
+    inadequate because of exotic input shapes or types.
+
+    Parameters
+    ----------
+    transformermixin_cls : type
+        A class containing a method :meth:`fit_transform`.
+
+    Returns
+    -------
+    transformermixin_cls : type
+        Input class in which the :meth:`fit_transform` method has been
+        replaced by a version with a modified docstring but otherwise identical
+        behaviour.
+
+    """
+
+    fit_docs = getdoc(getattr(transformermixin_cls, 'fit'))
+    transform_docs = getdoc(getattr(transformermixin_cls, 'transform'))
+
+    def make_new_fit_transform(original_fit_transform):
+        @wraps(original_fit_transform)
+        def fit_transform_wrapper(*args, **kwargs):
+            return original_fit_transform(*args, **kwargs)
+        fit_transform_wrapper.__doc__ = \
+            make_fit_transform_docs(fit_docs, transform_docs)
+        return fit_transform_wrapper
+
+    new_fit_transform = make_new_fit_transform(
+        getattr(transformermixin_cls, 'fit_transform'))
+    setattr(transformermixin_cls, 'fit_transform', new_fit_transform)
+    return transformermixin_cls
```

## gtda/utils/__init__.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-"""The module :mod:`gtda.utils` includes various utilities."""
-
-from .validation import check_collection, check_point_clouds, check_diagrams, \
-    validate_params
-
-__all__ = [
-    "check_collection",
-    "check_point_clouds",
-    "check_diagrams",
-    "validate_params"
-    ]
+"""The module :mod:`gtda.utils` includes various utilities."""
+
+from .validation import check_collection, check_point_clouds, check_diagrams, \
+    validate_params
+
+__all__ = [
+    "check_collection",
+    "check_point_clouds",
+    "check_diagrams",
+    "validate_params"
+    ]
```

## Comparing `giotto_tda-0.6.0.dist-info/LICENSE` & `giotto_tda-0.6.1.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,676 +1,676 @@
-Copyright 2021 L2F SA.
-
-If you need a different distribution license, please contact the L2F team
-at business@l2f.ch.
-
-Licensed under the GNU Affero General Public License (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License below or at https://www.gnu.org/licenses/agpl-3.0.html
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-
-                    GNU AFFERO GENERAL PUBLIC LICENSE
-                       Version 3, 19 November 2007
-
- Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
- Everyone is permitted to copy and distribute verbatim copies
- of this license document, but changing it is not allowed.
-
-                            Preamble
-
-  The GNU Affero General Public License is a free, copyleft license for
-software and other kinds of works, specifically designed to ensure
-cooperation with the community in the case of network server software.
-
-  The licenses for most software and other practical works are designed
-to take away your freedom to share and change the works.  By contrast,
-our General Public Licenses are intended to guarantee your freedom to
-share and change all versions of a program--to make sure it remains free
-software for all its users.
-
-  When we speak of free software, we are referring to freedom, not
-price.  Our General Public Licenses are designed to make sure that you
-have the freedom to distribute copies of free software (and charge for
-them if you wish), that you receive source code or can get it if you
-want it, that you can change the software or use pieces of it in new
-free programs, and that you know you can do these things.
-
-  Developers that use our General Public Licenses protect your rights
-with two steps: (1) assert copyright on the software, and (2) offer
-you this License which gives you legal permission to copy, distribute
-and/or modify the software.
-
-  A secondary benefit of defending all users' freedom is that
-improvements made in alternate versions of the program, if they
-receive widespread use, become available for other developers to
-incorporate.  Many developers of free software are heartened and
-encouraged by the resulting cooperation.  However, in the case of
-software used on network servers, this result may fail to come about.
-The GNU General Public License permits making a modified version and
-letting the public access it on a server without ever releasing its
-source code to the public.
-
-  The GNU Affero General Public License is designed specifically to
-ensure that, in such cases, the modified source code becomes available
-to the community.  It requires the operator of a network server to
-provide the source code of the modified version running there to the
-users of that server.  Therefore, public use of a modified version, on
-a publicly accessible server, gives the public access to the source
-code of the modified version.
-
-  An older license, called the Affero General Public License and
-published by Affero, was designed to accomplish similar goals.  This is
-a different license, not a version of the Affero GPL, but Affero has
-released a new version of the Affero GPL which permits relicensing under
-this license.
-
-  The precise terms and conditions for copying, distribution and
-modification follow.
-
-                       TERMS AND CONDITIONS
-
-  0. Definitions.
-
-  "This License" refers to version 3 of the GNU Affero General Public License.
-
-  "Copyright" also means copyright-like laws that apply to other kinds of
-works, such as semiconductor masks.
-
-  "The Program" refers to any copyrightable work licensed under this
-License.  Each licensee is addressed as "you".  "Licensees" and
-"recipients" may be individuals or organizations.
-
-  To "modify" a work means to copy from or adapt all or part of the work
-in a fashion requiring copyright permission, other than the making of an
-exact copy.  The resulting work is called a "modified version" of the
-earlier work or a work "based on" the earlier work.
-
-  A "covered work" means either the unmodified Program or a work based
-on the Program.
-
-  To "propagate" a work means to do anything with it that, without
-permission, would make you directly or secondarily liable for
-infringement under applicable copyright law, except executing it on a
-computer or modifying a private copy.  Propagation includes copying,
-distribution (with or without modification), making available to the
-public, and in some countries other activities as well.
-
-  To "convey" a work means any kind of propagation that enables other
-parties to make or receive copies.  Mere interaction with a user through
-a computer network, with no transfer of a copy, is not conveying.
-
-  An interactive user interface displays "Appropriate Legal Notices"
-to the extent that it includes a convenient and prominently visible
-feature that (1) displays an appropriate copyright notice, and (2)
-tells the user that there is no warranty for the work (except to the
-extent that warranties are provided), that licensees may convey the
-work under this License, and how to view a copy of this License.  If
-the interface presents a list of user commands or options, such as a
-menu, a prominent item in the list meets this criterion.
-
-  1. Source Code.
-
-  The "source code" for a work means the preferred form of the work
-for making modifications to it.  "Object code" means any non-source
-form of a work.
-
-  A "Standard Interface" means an interface that either is an official
-standard defined by a recognized standards body, or, in the case of
-interfaces specified for a particular programming language, one that
-is widely used among developers working in that language.
-
-  The "System Libraries" of an executable work include anything, other
-than the work as a whole, that (a) is included in the normal form of
-packaging a Major Component, but which is not part of that Major
-Component, and (b) serves only to enable use of the work with that
-Major Component, or to implement a Standard Interface for which an
-implementation is available to the public in source code form.  A
-"Major Component", in this context, means a major essential component
-(kernel, window system, and so on) of the specific operating system
-(if any) on which the executable work runs, or a compiler used to
-produce the work, or an object code interpreter used to run it.
-
-  The "Corresponding Source" for a work in object code form means all
-the source code needed to generate, install, and (for an executable
-work) run the object code and to modify the work, including scripts to
-control those activities.  However, it does not include the work's
-System Libraries, or general-purpose tools or generally available free
-programs which are used unmodified in performing those activities but
-which are not part of the work.  For example, Corresponding Source
-includes interface definition files associated with source files for
-the work, and the source code for shared libraries and dynamically
-linked subprograms that the work is specifically designed to require,
-such as by intimate data communication or control flow between those
-subprograms and other parts of the work.
-
-  The Corresponding Source need not include anything that users
-can regenerate automatically from other parts of the Corresponding
-Source.
-
-  The Corresponding Source for a work in source code form is that
-same work.
-
-  2. Basic Permissions.
-
-  All rights granted under this License are granted for the term of
-copyright on the Program, and are irrevocable provided the stated
-conditions are met.  This License explicitly affirms your unlimited
-permission to run the unmodified Program.  The output from running a
-covered work is covered by this License only if the output, given its
-content, constitutes a covered work.  This License acknowledges your
-rights of fair use or other equivalent, as provided by copyright law.
-
-  You may make, run and propagate covered works that you do not
-convey, without conditions so long as your license otherwise remains
-in force.  You may convey covered works to others for the sole purpose
-of having them make modifications exclusively for you, or provide you
-with facilities for running those works, provided that you comply with
-the terms of this License in conveying all material for which you do
-not control copyright.  Those thus making or running the covered works
-for you must do so exclusively on your behalf, under your direction
-and control, on terms that prohibit them from making any copies of
-your copyrighted material outside their relationship with you.
-
-  Conveying under any other circumstances is permitted solely under
-the conditions stated below.  Sublicensing is not allowed; section 10
-makes it unnecessary.
-
-  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
-
-  No covered work shall be deemed part of an effective technological
-measure under any applicable law fulfilling obligations under article
-11 of the WIPO copyright treaty adopted on 20 December 1996, or
-similar laws prohibiting or restricting circumvention of such
-measures.
-
-  When you convey a covered work, you waive any legal power to forbid
-circumvention of technological measures to the extent such circumvention
-is effected by exercising rights under this License with respect to
-the covered work, and you disclaim any intention to limit operation or
-modification of the work as a means of enforcing, against the work's
-users, your or third parties' legal rights to forbid circumvention of
-technological measures.
-
-  4. Conveying Verbatim Copies.
-
-  You may convey verbatim copies of the Program's source code as you
-receive it, in any medium, provided that you conspicuously and
-appropriately publish on each copy an appropriate copyright notice;
-keep intact all notices stating that this License and any
-non-permissive terms added in accord with section 7 apply to the code;
-keep intact all notices of the absence of any warranty; and give all
-recipients a copy of this License along with the Program.
-
-  You may charge any price or no price for each copy that you convey,
-and you may offer support or warranty protection for a fee.
-
-  5. Conveying Modified Source Versions.
-
-  You may convey a work based on the Program, or the modifications to
-produce it from the Program, in the form of source code under the
-terms of section 4, provided that you also meet all of these conditions:
-
-    a) The work must carry prominent notices stating that you modified
-    it, and giving a relevant date.
-
-    b) The work must carry prominent notices stating that it is
-    released under this License and any conditions added under section
-    7.  This requirement modifies the requirement in section 4 to
-    "keep intact all notices".
-
-    c) You must license the entire work, as a whole, under this
-    License to anyone who comes into possession of a copy.  This
-    License will therefore apply, along with any applicable section 7
-    additional terms, to the whole of the work, and all its parts,
-    regardless of how they are packaged.  This License gives no
-    permission to license the work in any other way, but it does not
-    invalidate such permission if you have separately received it.
-
-    d) If the work has interactive user interfaces, each must display
-    Appropriate Legal Notices; however, if the Program has interactive
-    interfaces that do not display Appropriate Legal Notices, your
-    work need not make them do so.
-
-  A compilation of a covered work with other separate and independent
-works, which are not by their nature extensions of the covered work,
-and which are not combined with it such as to form a larger program,
-in or on a volume of a storage or distribution medium, is called an
-"aggregate" if the compilation and its resulting copyright are not
-used to limit the access or legal rights of the compilation's users
-beyond what the individual works permit.  Inclusion of a covered work
-in an aggregate does not cause this License to apply to the other
-parts of the aggregate.
-
-  6. Conveying Non-Source Forms.
-
-  You may convey a covered work in object code form under the terms
-of sections 4 and 5, provided that you also convey the
-machine-readable Corresponding Source under the terms of this License,
-in one of these ways:
-
-    a) Convey the object code in, or embodied in, a physical product
-    (including a physical distribution medium), accompanied by the
-    Corresponding Source fixed on a durable physical medium
-    customarily used for software interchange.
-
-    b) Convey the object code in, or embodied in, a physical product
-    (including a physical distribution medium), accompanied by a
-    written offer, valid for at least three years and valid for as
-    long as you offer spare parts or customer support for that product
-    model, to give anyone who possesses the object code either (1) a
-    copy of the Corresponding Source for all the software in the
-    product that is covered by this License, on a durable physical
-    medium customarily used for software interchange, for a price no
-    more than your reasonable cost of physically performing this
-    conveying of source, or (2) access to copy the
-    Corresponding Source from a network server at no charge.
-
-    c) Convey individual copies of the object code with a copy of the
-    written offer to provide the Corresponding Source.  This
-    alternative is allowed only occasionally and noncommercially, and
-    only if you received the object code with such an offer, in accord
-    with subsection 6b.
-
-    d) Convey the object code by offering access from a designated
-    place (gratis or for a charge), and offer equivalent access to the
-    Corresponding Source in the same way through the same place at no
-    further charge.  You need not require recipients to copy the
-    Corresponding Source along with the object code.  If the place to
-    copy the object code is a network server, the Corresponding Source
-    may be on a different server (operated by you or a third party)
-    that supports equivalent copying facilities, provided you maintain
-    clear directions next to the object code saying where to find the
-    Corresponding Source.  Regardless of what server hosts the
-    Corresponding Source, you remain obligated to ensure that it is
-    available for as long as needed to satisfy these requirements.
-
-    e) Convey the object code using peer-to-peer transmission, provided
-    you inform other peers where the object code and Corresponding
-    Source of the work are being offered to the general public at no
-    charge under subsection 6d.
-
-  A separable portion of the object code, whose source code is excluded
-from the Corresponding Source as a System Library, need not be
-included in conveying the object code work.
-
-  A "User Product" is either (1) a "consumer product", which means any
-tangible personal property which is normally used for personal, family,
-or household purposes, or (2) anything designed or sold for incorporation
-into a dwelling.  In determining whether a product is a consumer product,
-doubtful cases shall be resolved in favor of coverage.  For a particular
-product received by a particular user, "normally used" refers to a
-typical or common use of that class of product, regardless of the status
-of the particular user or of the way in which the particular user
-actually uses, or expects or is expected to use, the product.  A product
-is a consumer product regardless of whether the product has substantial
-commercial, industrial or non-consumer uses, unless such uses represent
-the only significant mode of use of the product.
-
-  "Installation Information" for a User Product means any methods,
-procedures, authorization keys, or other information required to install
-and execute modified versions of a covered work in that User Product from
-a modified version of its Corresponding Source.  The information must
-suffice to ensure that the continued functioning of the modified object
-code is in no case prevented or interfered with solely because
-modification has been made.
-
-  If you convey an object code work under this section in, or with, or
-specifically for use in, a User Product, and the conveying occurs as
-part of a transaction in which the right of possession and use of the
-User Product is transferred to the recipient in perpetuity or for a
-fixed term (regardless of how the transaction is characterized), the
-Corresponding Source conveyed under this section must be accompanied
-by the Installation Information.  But this requirement does not apply
-if neither you nor any third party retains the ability to install
-modified object code on the User Product (for example, the work has
-been installed in ROM).
-
-  The requirement to provide Installation Information does not include a
-requirement to continue to provide support service, warranty, or updates
-for a work that has been modified or installed by the recipient, or for
-the User Product in which it has been modified or installed.  Access to a
-network may be denied when the modification itself materially and
-adversely affects the operation of the network or violates the rules and
-protocols for communication across the network.
-
-  Corresponding Source conveyed, and Installation Information provided,
-in accord with this section must be in a format that is publicly
-documented (and with an implementation available to the public in
-source code form), and must require no special password or key for
-unpacking, reading or copying.
-
-  7. Additional Terms.
-
-  "Additional permissions" are terms that supplement the terms of this
-License by making exceptions from one or more of its conditions.
-Additional permissions that are applicable to the entire Program shall
-be treated as though they were included in this License, to the extent
-that they are valid under applicable law.  If additional permissions
-apply only to part of the Program, that part may be used separately
-under those permissions, but the entire Program remains governed by
-this License without regard to the additional permissions.
-
-  When you convey a copy of a covered work, you may at your option
-remove any additional permissions from that copy, or from any part of
-it.  (Additional permissions may be written to require their own
-removal in certain cases when you modify the work.)  You may place
-additional permissions on material, added by you to a covered work,
-for which you have or can give appropriate copyright permission.
-
-  Notwithstanding any other provision of this License, for material you
-add to a covered work, you may (if authorized by the copyright holders of
-that material) supplement the terms of this License with terms:
-
-    a) Disclaiming warranty or limiting liability differently from the
-    terms of sections 15 and 16 of this License; or
-
-    b) Requiring preservation of specified reasonable legal notices or
-    author attributions in that material or in the Appropriate Legal
-    Notices displayed by works containing it; or
-
-    c) Prohibiting misrepresentation of the origin of that material, or
-    requiring that modified versions of such material be marked in
-    reasonable ways as different from the original version; or
-
-    d) Limiting the use for publicity purposes of names of licensors or
-    authors of the material; or
-
-    e) Declining to grant rights under trademark law for use of some
-    trade names, trademarks, or service marks; or
-
-    f) Requiring indemnification of licensors and authors of that
-    material by anyone who conveys the material (or modified versions of
-    it) with contractual assumptions of liability to the recipient, for
-    any liability that these contractual assumptions directly impose on
-    those licensors and authors.
-
-  All other non-permissive additional terms are considered "further
-restrictions" within the meaning of section 10.  If the Program as you
-received it, or any part of it, contains a notice stating that it is
-governed by this License along with a term that is a further
-restriction, you may remove that term.  If a license document contains
-a further restriction but permits relicensing or conveying under this
-License, you may add to a covered work material governed by the terms
-of that license document, provided that the further restriction does
-not survive such relicensing or conveying.
-
-  If you add terms to a covered work in accord with this section, you
-must place, in the relevant source files, a statement of the
-additional terms that apply to those files, or a notice indicating
-where to find the applicable terms.
-
-  Additional terms, permissive or non-permissive, may be stated in the
-form of a separately written license, or stated as exceptions;
-the above requirements apply either way.
-
-  8. Termination.
-
-  You may not propagate or modify a covered work except as expressly
-provided under this License.  Any attempt otherwise to propagate or
-modify it is void, and will automatically terminate your rights under
-this License (including any patent licenses granted under the third
-paragraph of section 11).
-
-  However, if you cease all violation of this License, then your
-license from a particular copyright holder is reinstated (a)
-provisionally, unless and until the copyright holder explicitly and
-finally terminates your license, and (b) permanently, if the copyright
-holder fails to notify you of the violation by some reasonable means
-prior to 60 days after the cessation.
-
-  Moreover, your license from a particular copyright holder is
-reinstated permanently if the copyright holder notifies you of the
-violation by some reasonable means, this is the first time you have
-received notice of violation of this License (for any work) from that
-copyright holder, and you cure the violation prior to 30 days after
-your receipt of the notice.
-
-  Termination of your rights under this section does not terminate the
-licenses of parties who have received copies or rights from you under
-this License.  If your rights have been terminated and not permanently
-reinstated, you do not qualify to receive new licenses for the same
-material under section 10.
-
-  9. Acceptance Not Required for Having Copies.
-
-  You are not required to accept this License in order to receive or
-run a copy of the Program.  Ancillary propagation of a covered work
-occurring solely as a consequence of using peer-to-peer transmission
-to receive a copy likewise does not require acceptance.  However,
-nothing other than this License grants you permission to propagate or
-modify any covered work.  These actions infringe copyright if you do
-not accept this License.  Therefore, by modifying or propagating a
-covered work, you indicate your acceptance of this License to do so.
-
-  10. Automatic Licensing of Downstream Recipients.
-
-  Each time you convey a covered work, the recipient automatically
-receives a license from the original licensors, to run, modify and
-propagate that work, subject to this License.  You are not responsible
-for enforcing compliance by third parties with this License.
-
-  An "entity transaction" is a transaction transferring control of an
-organization, or substantially all assets of one, or subdividing an
-organization, or merging organizations.  If propagation of a covered
-work results from an entity transaction, each party to that
-transaction who receives a copy of the work also receives whatever
-licenses to the work the party's predecessor in interest had or could
-give under the previous paragraph, plus a right to possession of the
-Corresponding Source of the work from the predecessor in interest, if
-the predecessor has it or can get it with reasonable efforts.
-
-  You may not impose any further restrictions on the exercise of the
-rights granted or affirmed under this License.  For example, you may
-not impose a license fee, royalty, or other charge for exercise of
-rights granted under this License, and you may not initiate litigation
-(including a cross-claim or counterclaim in a lawsuit) alleging that
-any patent claim is infringed by making, using, selling, offering for
-sale, or importing the Program or any portion of it.
-
-  11. Patents.
-
-  A "contributor" is a copyright holder who authorizes use under this
-License of the Program or a work on which the Program is based.  The
-work thus licensed is called the contributor's "contributor version".
-
-  A contributor's "essential patent claims" are all patent claims
-owned or controlled by the contributor, whether already acquired or
-hereafter acquired, that would be infringed by some manner, permitted
-by this License, of making, using, or selling its contributor version,
-but do not include claims that would be infringed only as a
-consequence of further modification of the contributor version.  For
-purposes of this definition, "control" includes the right to grant
-patent sublicenses in a manner consistent with the requirements of
-this License.
-
-  Each contributor grants you a non-exclusive, worldwide, royalty-free
-patent license under the contributor's essential patent claims, to
-make, use, sell, offer for sale, import and otherwise run, modify and
-propagate the contents of its contributor version.
-
-  In the following three paragraphs, a "patent license" is any express
-agreement or commitment, however denominated, not to enforce a patent
-(such as an express permission to practice a patent or covenant not to
-sue for patent infringement).  To "grant" such a patent license to a
-party means to make such an agreement or commitment not to enforce a
-patent against the party.
-
-  If you convey a covered work, knowingly relying on a patent license,
-and the Corresponding Source of the work is not available for anyone
-to copy, free of charge and under the terms of this License, through a
-publicly available network server or other readily accessible means,
-then you must either (1) cause the Corresponding Source to be so
-available, or (2) arrange to deprive yourself of the benefit of the
-patent license for this particular work, or (3) arrange, in a manner
-consistent with the requirements of this License, to extend the patent
-license to downstream recipients.  "Knowingly relying" means you have
-actual knowledge that, but for the patent license, your conveying the
-covered work in a country, or your recipient's use of the covered work
-in a country, would infringe one or more identifiable patents in that
-country that you have reason to believe are valid.
-
-  If, pursuant to or in connection with a single transaction or
-arrangement, you convey, or propagate by procuring conveyance of, a
-covered work, and grant a patent license to some of the parties
-receiving the covered work authorizing them to use, propagate, modify
-or convey a specific copy of the covered work, then the patent license
-you grant is automatically extended to all recipients of the covered
-work and works based on it.
-
-  A patent license is "discriminatory" if it does not include within
-the scope of its coverage, prohibits the exercise of, or is
-conditioned on the non-exercise of one or more of the rights that are
-specifically granted under this License.  You may not convey a covered
-work if you are a party to an arrangement with a third party that is
-in the business of distributing software, under which you make payment
-to the third party based on the extent of your activity of conveying
-the work, and under which the third party grants, to any of the
-parties who would receive the covered work from you, a discriminatory
-patent license (a) in connection with copies of the covered work
-conveyed by you (or copies made from those copies), or (b) primarily
-for and in connection with specific products or compilations that
-contain the covered work, unless you entered into that arrangement,
-or that patent license was granted, prior to 28 March 2007.
-
-  Nothing in this License shall be construed as excluding or limiting
-any implied license or other defenses to infringement that may
-otherwise be available to you under applicable patent law.
-
-  12. No Surrender of Others' Freedom.
-
-  If conditions are imposed on you (whether by court order, agreement or
-otherwise) that contradict the conditions of this License, they do not
-excuse you from the conditions of this License.  If you cannot convey a
-covered work so as to satisfy simultaneously your obligations under this
-License and any other pertinent obligations, then as a consequence you may
-not convey it at all.  For example, if you agree to terms that obligate you
-to collect a royalty for further conveying from those to whom you convey
-the Program, the only way you could satisfy both those terms and this
-License would be to refrain entirely from conveying the Program.
-
-  13. Remote Network Interaction; Use with the GNU General Public License.
-
-  Notwithstanding any other provision of this License, if you modify the
-Program, your modified version must prominently offer all users
-interacting with it remotely through a computer network (if your version
-supports such interaction) an opportunity to receive the Corresponding
-Source of your version by providing access to the Corresponding Source
-from a network server at no charge, through some standard or customary
-means of facilitating copying of software.  This Corresponding Source
-shall include the Corresponding Source for any work covered by version 3
-of the GNU General Public License that is incorporated pursuant to the
-following paragraph.
-
-  Notwithstanding any other provision of this License, you have
-permission to link or combine any covered work with a work licensed
-under version 3 of the GNU General Public License into a single
-combined work, and to convey the resulting work.  The terms of this
-License will continue to apply to the part which is the covered work,
-but the work with which it is combined will remain governed by version
-3 of the GNU General Public License.
-
-  14. Revised Versions of this License.
-
-  The Free Software Foundation may publish revised and/or new versions of
-the GNU Affero General Public License from time to time.  Such new versions
-will be similar in spirit to the present version, but may differ in detail to
-address new problems or concerns.
-
-  Each version is given a distinguishing version number.  If the
-Program specifies that a certain numbered version of the GNU Affero General
-Public License "or any later version" applies to it, you have the
-option of following the terms and conditions either of that numbered
-version or of any later version published by the Free Software
-Foundation.  If the Program does not specify a version number of the
-GNU Affero General Public License, you may choose any version ever published
-by the Free Software Foundation.
-
-  If the Program specifies that a proxy can decide which future
-versions of the GNU Affero General Public License can be used, that proxy's
-public statement of acceptance of a version permanently authorizes you
-to choose that version for the Program.
-
-  Later license versions may give you additional or different
-permissions.  However, no additional obligations are imposed on any
-author or copyright holder as a result of your choosing to follow a
-later version.
-
-  15. Disclaimer of Warranty.
-
-  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
-APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
-HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
-OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
-THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
-PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
-IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
-ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
-
-  16. Limitation of Liability.
-
-  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
-WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
-THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
-GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
-USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
-DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
-PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
-EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
-SUCH DAMAGES.
-
-  17. Interpretation of Sections 15 and 16.
-
-  If the disclaimer of warranty and limitation of liability provided
-above cannot be given local legal effect according to their terms,
-reviewing courts shall apply local law that most closely approximates
-an absolute waiver of all civil liability in connection with the
-Program, unless a warranty or assumption of liability accompanies a
-copy of the Program in return for a fee.
-
-                     END OF TERMS AND CONDITIONS
-
-            How to Apply These Terms to Your New Programs
-
-  If you develop a new program, and you want it to be of the greatest
-possible use to the public, the best way to achieve this is to make it
-free software which everyone can redistribute and change under these terms.
-
-  To do so, attach the following notices to the program.  It is safest
-to attach them to the start of each source file to most effectively
-state the exclusion of warranty; and each file should have at least
-the "copyright" line and a pointer to where the full notice is found.
-
-    <one line to give the program's name and a brief idea of what it does.>
-    Copyright (C) <year>  <name of author>
-
-    This program is free software: you can redistribute it and/or modify
-    it under the terms of the GNU Affero General Public License as published
-    by the Free Software Foundation, either version 3 of the License, or
-    (at your option) any later version.
-
-    This program is distributed in the hope that it will be useful,
-    but WITHOUT ANY WARRANTY; without even the implied warranty of
-    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-    GNU Affero General Public License for more details.
-
-    You should have received a copy of the GNU Affero General Public License
-    along with this program.  If not, see <https://www.gnu.org/licenses/>.
-
-Also add information on how to contact you by electronic and paper mail.
-
-  If your software can interact with users remotely through a computer
-network, you should also make sure that it provides a way for users to
-get its source.  For example, if your program is a web application, its
-interface could display a "Source" link that leads users to an archive
-of the code.  There are many ways you could offer source, and different
-solutions will be better for different programs; see section 13 for the
-specific requirements.
-
-  You should also get your employer (if you work as a programmer) or school,
-if any, to sign a "copyright disclaimer" for the program, if necessary.
-For more information on this, and how to apply and follow the GNU AGPL, see
-<https://www.gnu.org/licenses/>.
+Copyright 2021 L2F SA.
+
+If you need a different distribution license, please contact the L2F team
+at business@l2f.ch.
+
+Licensed under the GNU Affero General Public License (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License below or at https://www.gnu.org/licenses/agpl-3.0.html
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+                    GNU AFFERO GENERAL PUBLIC LICENSE
+                       Version 3, 19 November 2007
+
+ Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+                            Preamble
+
+  The GNU Affero General Public License is a free, copyleft license for
+software and other kinds of works, specifically designed to ensure
+cooperation with the community in the case of network server software.
+
+  The licenses for most software and other practical works are designed
+to take away your freedom to share and change the works.  By contrast,
+our General Public Licenses are intended to guarantee your freedom to
+share and change all versions of a program--to make sure it remains free
+software for all its users.
+
+  When we speak of free software, we are referring to freedom, not
+price.  Our General Public Licenses are designed to make sure that you
+have the freedom to distribute copies of free software (and charge for
+them if you wish), that you receive source code or can get it if you
+want it, that you can change the software or use pieces of it in new
+free programs, and that you know you can do these things.
+
+  Developers that use our General Public Licenses protect your rights
+with two steps: (1) assert copyright on the software, and (2) offer
+you this License which gives you legal permission to copy, distribute
+and/or modify the software.
+
+  A secondary benefit of defending all users' freedom is that
+improvements made in alternate versions of the program, if they
+receive widespread use, become available for other developers to
+incorporate.  Many developers of free software are heartened and
+encouraged by the resulting cooperation.  However, in the case of
+software used on network servers, this result may fail to come about.
+The GNU General Public License permits making a modified version and
+letting the public access it on a server without ever releasing its
+source code to the public.
+
+  The GNU Affero General Public License is designed specifically to
+ensure that, in such cases, the modified source code becomes available
+to the community.  It requires the operator of a network server to
+provide the source code of the modified version running there to the
+users of that server.  Therefore, public use of a modified version, on
+a publicly accessible server, gives the public access to the source
+code of the modified version.
+
+  An older license, called the Affero General Public License and
+published by Affero, was designed to accomplish similar goals.  This is
+a different license, not a version of the Affero GPL, but Affero has
+released a new version of the Affero GPL which permits relicensing under
+this license.
+
+  The precise terms and conditions for copying, distribution and
+modification follow.
+
+                       TERMS AND CONDITIONS
+
+  0. Definitions.
+
+  "This License" refers to version 3 of the GNU Affero General Public License.
+
+  "Copyright" also means copyright-like laws that apply to other kinds of
+works, such as semiconductor masks.
+
+  "The Program" refers to any copyrightable work licensed under this
+License.  Each licensee is addressed as "you".  "Licensees" and
+"recipients" may be individuals or organizations.
+
+  To "modify" a work means to copy from or adapt all or part of the work
+in a fashion requiring copyright permission, other than the making of an
+exact copy.  The resulting work is called a "modified version" of the
+earlier work or a work "based on" the earlier work.
+
+  A "covered work" means either the unmodified Program or a work based
+on the Program.
+
+  To "propagate" a work means to do anything with it that, without
+permission, would make you directly or secondarily liable for
+infringement under applicable copyright law, except executing it on a
+computer or modifying a private copy.  Propagation includes copying,
+distribution (with or without modification), making available to the
+public, and in some countries other activities as well.
+
+  To "convey" a work means any kind of propagation that enables other
+parties to make or receive copies.  Mere interaction with a user through
+a computer network, with no transfer of a copy, is not conveying.
+
+  An interactive user interface displays "Appropriate Legal Notices"
+to the extent that it includes a convenient and prominently visible
+feature that (1) displays an appropriate copyright notice, and (2)
+tells the user that there is no warranty for the work (except to the
+extent that warranties are provided), that licensees may convey the
+work under this License, and how to view a copy of this License.  If
+the interface presents a list of user commands or options, such as a
+menu, a prominent item in the list meets this criterion.
+
+  1. Source Code.
+
+  The "source code" for a work means the preferred form of the work
+for making modifications to it.  "Object code" means any non-source
+form of a work.
+
+  A "Standard Interface" means an interface that either is an official
+standard defined by a recognized standards body, or, in the case of
+interfaces specified for a particular programming language, one that
+is widely used among developers working in that language.
+
+  The "System Libraries" of an executable work include anything, other
+than the work as a whole, that (a) is included in the normal form of
+packaging a Major Component, but which is not part of that Major
+Component, and (b) serves only to enable use of the work with that
+Major Component, or to implement a Standard Interface for which an
+implementation is available to the public in source code form.  A
+"Major Component", in this context, means a major essential component
+(kernel, window system, and so on) of the specific operating system
+(if any) on which the executable work runs, or a compiler used to
+produce the work, or an object code interpreter used to run it.
+
+  The "Corresponding Source" for a work in object code form means all
+the source code needed to generate, install, and (for an executable
+work) run the object code and to modify the work, including scripts to
+control those activities.  However, it does not include the work's
+System Libraries, or general-purpose tools or generally available free
+programs which are used unmodified in performing those activities but
+which are not part of the work.  For example, Corresponding Source
+includes interface definition files associated with source files for
+the work, and the source code for shared libraries and dynamically
+linked subprograms that the work is specifically designed to require,
+such as by intimate data communication or control flow between those
+subprograms and other parts of the work.
+
+  The Corresponding Source need not include anything that users
+can regenerate automatically from other parts of the Corresponding
+Source.
+
+  The Corresponding Source for a work in source code form is that
+same work.
+
+  2. Basic Permissions.
+
+  All rights granted under this License are granted for the term of
+copyright on the Program, and are irrevocable provided the stated
+conditions are met.  This License explicitly affirms your unlimited
+permission to run the unmodified Program.  The output from running a
+covered work is covered by this License only if the output, given its
+content, constitutes a covered work.  This License acknowledges your
+rights of fair use or other equivalent, as provided by copyright law.
+
+  You may make, run and propagate covered works that you do not
+convey, without conditions so long as your license otherwise remains
+in force.  You may convey covered works to others for the sole purpose
+of having them make modifications exclusively for you, or provide you
+with facilities for running those works, provided that you comply with
+the terms of this License in conveying all material for which you do
+not control copyright.  Those thus making or running the covered works
+for you must do so exclusively on your behalf, under your direction
+and control, on terms that prohibit them from making any copies of
+your copyrighted material outside their relationship with you.
+
+  Conveying under any other circumstances is permitted solely under
+the conditions stated below.  Sublicensing is not allowed; section 10
+makes it unnecessary.
+
+  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
+
+  No covered work shall be deemed part of an effective technological
+measure under any applicable law fulfilling obligations under article
+11 of the WIPO copyright treaty adopted on 20 December 1996, or
+similar laws prohibiting or restricting circumvention of such
+measures.
+
+  When you convey a covered work, you waive any legal power to forbid
+circumvention of technological measures to the extent such circumvention
+is effected by exercising rights under this License with respect to
+the covered work, and you disclaim any intention to limit operation or
+modification of the work as a means of enforcing, against the work's
+users, your or third parties' legal rights to forbid circumvention of
+technological measures.
+
+  4. Conveying Verbatim Copies.
+
+  You may convey verbatim copies of the Program's source code as you
+receive it, in any medium, provided that you conspicuously and
+appropriately publish on each copy an appropriate copyright notice;
+keep intact all notices stating that this License and any
+non-permissive terms added in accord with section 7 apply to the code;
+keep intact all notices of the absence of any warranty; and give all
+recipients a copy of this License along with the Program.
+
+  You may charge any price or no price for each copy that you convey,
+and you may offer support or warranty protection for a fee.
+
+  5. Conveying Modified Source Versions.
+
+  You may convey a work based on the Program, or the modifications to
+produce it from the Program, in the form of source code under the
+terms of section 4, provided that you also meet all of these conditions:
+
+    a) The work must carry prominent notices stating that you modified
+    it, and giving a relevant date.
+
+    b) The work must carry prominent notices stating that it is
+    released under this License and any conditions added under section
+    7.  This requirement modifies the requirement in section 4 to
+    "keep intact all notices".
+
+    c) You must license the entire work, as a whole, under this
+    License to anyone who comes into possession of a copy.  This
+    License will therefore apply, along with any applicable section 7
+    additional terms, to the whole of the work, and all its parts,
+    regardless of how they are packaged.  This License gives no
+    permission to license the work in any other way, but it does not
+    invalidate such permission if you have separately received it.
+
+    d) If the work has interactive user interfaces, each must display
+    Appropriate Legal Notices; however, if the Program has interactive
+    interfaces that do not display Appropriate Legal Notices, your
+    work need not make them do so.
+
+  A compilation of a covered work with other separate and independent
+works, which are not by their nature extensions of the covered work,
+and which are not combined with it such as to form a larger program,
+in or on a volume of a storage or distribution medium, is called an
+"aggregate" if the compilation and its resulting copyright are not
+used to limit the access or legal rights of the compilation's users
+beyond what the individual works permit.  Inclusion of a covered work
+in an aggregate does not cause this License to apply to the other
+parts of the aggregate.
+
+  6. Conveying Non-Source Forms.
+
+  You may convey a covered work in object code form under the terms
+of sections 4 and 5, provided that you also convey the
+machine-readable Corresponding Source under the terms of this License,
+in one of these ways:
+
+    a) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by the
+    Corresponding Source fixed on a durable physical medium
+    customarily used for software interchange.
+
+    b) Convey the object code in, or embodied in, a physical product
+    (including a physical distribution medium), accompanied by a
+    written offer, valid for at least three years and valid for as
+    long as you offer spare parts or customer support for that product
+    model, to give anyone who possesses the object code either (1) a
+    copy of the Corresponding Source for all the software in the
+    product that is covered by this License, on a durable physical
+    medium customarily used for software interchange, for a price no
+    more than your reasonable cost of physically performing this
+    conveying of source, or (2) access to copy the
+    Corresponding Source from a network server at no charge.
+
+    c) Convey individual copies of the object code with a copy of the
+    written offer to provide the Corresponding Source.  This
+    alternative is allowed only occasionally and noncommercially, and
+    only if you received the object code with such an offer, in accord
+    with subsection 6b.
+
+    d) Convey the object code by offering access from a designated
+    place (gratis or for a charge), and offer equivalent access to the
+    Corresponding Source in the same way through the same place at no
+    further charge.  You need not require recipients to copy the
+    Corresponding Source along with the object code.  If the place to
+    copy the object code is a network server, the Corresponding Source
+    may be on a different server (operated by you or a third party)
+    that supports equivalent copying facilities, provided you maintain
+    clear directions next to the object code saying where to find the
+    Corresponding Source.  Regardless of what server hosts the
+    Corresponding Source, you remain obligated to ensure that it is
+    available for as long as needed to satisfy these requirements.
+
+    e) Convey the object code using peer-to-peer transmission, provided
+    you inform other peers where the object code and Corresponding
+    Source of the work are being offered to the general public at no
+    charge under subsection 6d.
+
+  A separable portion of the object code, whose source code is excluded
+from the Corresponding Source as a System Library, need not be
+included in conveying the object code work.
+
+  A "User Product" is either (1) a "consumer product", which means any
+tangible personal property which is normally used for personal, family,
+or household purposes, or (2) anything designed or sold for incorporation
+into a dwelling.  In determining whether a product is a consumer product,
+doubtful cases shall be resolved in favor of coverage.  For a particular
+product received by a particular user, "normally used" refers to a
+typical or common use of that class of product, regardless of the status
+of the particular user or of the way in which the particular user
+actually uses, or expects or is expected to use, the product.  A product
+is a consumer product regardless of whether the product has substantial
+commercial, industrial or non-consumer uses, unless such uses represent
+the only significant mode of use of the product.
+
+  "Installation Information" for a User Product means any methods,
+procedures, authorization keys, or other information required to install
+and execute modified versions of a covered work in that User Product from
+a modified version of its Corresponding Source.  The information must
+suffice to ensure that the continued functioning of the modified object
+code is in no case prevented or interfered with solely because
+modification has been made.
+
+  If you convey an object code work under this section in, or with, or
+specifically for use in, a User Product, and the conveying occurs as
+part of a transaction in which the right of possession and use of the
+User Product is transferred to the recipient in perpetuity or for a
+fixed term (regardless of how the transaction is characterized), the
+Corresponding Source conveyed under this section must be accompanied
+by the Installation Information.  But this requirement does not apply
+if neither you nor any third party retains the ability to install
+modified object code on the User Product (for example, the work has
+been installed in ROM).
+
+  The requirement to provide Installation Information does not include a
+requirement to continue to provide support service, warranty, or updates
+for a work that has been modified or installed by the recipient, or for
+the User Product in which it has been modified or installed.  Access to a
+network may be denied when the modification itself materially and
+adversely affects the operation of the network or violates the rules and
+protocols for communication across the network.
+
+  Corresponding Source conveyed, and Installation Information provided,
+in accord with this section must be in a format that is publicly
+documented (and with an implementation available to the public in
+source code form), and must require no special password or key for
+unpacking, reading or copying.
+
+  7. Additional Terms.
+
+  "Additional permissions" are terms that supplement the terms of this
+License by making exceptions from one or more of its conditions.
+Additional permissions that are applicable to the entire Program shall
+be treated as though they were included in this License, to the extent
+that they are valid under applicable law.  If additional permissions
+apply only to part of the Program, that part may be used separately
+under those permissions, but the entire Program remains governed by
+this License without regard to the additional permissions.
+
+  When you convey a copy of a covered work, you may at your option
+remove any additional permissions from that copy, or from any part of
+it.  (Additional permissions may be written to require their own
+removal in certain cases when you modify the work.)  You may place
+additional permissions on material, added by you to a covered work,
+for which you have or can give appropriate copyright permission.
+
+  Notwithstanding any other provision of this License, for material you
+add to a covered work, you may (if authorized by the copyright holders of
+that material) supplement the terms of this License with terms:
+
+    a) Disclaiming warranty or limiting liability differently from the
+    terms of sections 15 and 16 of this License; or
+
+    b) Requiring preservation of specified reasonable legal notices or
+    author attributions in that material or in the Appropriate Legal
+    Notices displayed by works containing it; or
+
+    c) Prohibiting misrepresentation of the origin of that material, or
+    requiring that modified versions of such material be marked in
+    reasonable ways as different from the original version; or
+
+    d) Limiting the use for publicity purposes of names of licensors or
+    authors of the material; or
+
+    e) Declining to grant rights under trademark law for use of some
+    trade names, trademarks, or service marks; or
+
+    f) Requiring indemnification of licensors and authors of that
+    material by anyone who conveys the material (or modified versions of
+    it) with contractual assumptions of liability to the recipient, for
+    any liability that these contractual assumptions directly impose on
+    those licensors and authors.
+
+  All other non-permissive additional terms are considered "further
+restrictions" within the meaning of section 10.  If the Program as you
+received it, or any part of it, contains a notice stating that it is
+governed by this License along with a term that is a further
+restriction, you may remove that term.  If a license document contains
+a further restriction but permits relicensing or conveying under this
+License, you may add to a covered work material governed by the terms
+of that license document, provided that the further restriction does
+not survive such relicensing or conveying.
+
+  If you add terms to a covered work in accord with this section, you
+must place, in the relevant source files, a statement of the
+additional terms that apply to those files, or a notice indicating
+where to find the applicable terms.
+
+  Additional terms, permissive or non-permissive, may be stated in the
+form of a separately written license, or stated as exceptions;
+the above requirements apply either way.
+
+  8. Termination.
+
+  You may not propagate or modify a covered work except as expressly
+provided under this License.  Any attempt otherwise to propagate or
+modify it is void, and will automatically terminate your rights under
+this License (including any patent licenses granted under the third
+paragraph of section 11).
+
+  However, if you cease all violation of this License, then your
+license from a particular copyright holder is reinstated (a)
+provisionally, unless and until the copyright holder explicitly and
+finally terminates your license, and (b) permanently, if the copyright
+holder fails to notify you of the violation by some reasonable means
+prior to 60 days after the cessation.
+
+  Moreover, your license from a particular copyright holder is
+reinstated permanently if the copyright holder notifies you of the
+violation by some reasonable means, this is the first time you have
+received notice of violation of this License (for any work) from that
+copyright holder, and you cure the violation prior to 30 days after
+your receipt of the notice.
+
+  Termination of your rights under this section does not terminate the
+licenses of parties who have received copies or rights from you under
+this License.  If your rights have been terminated and not permanently
+reinstated, you do not qualify to receive new licenses for the same
+material under section 10.
+
+  9. Acceptance Not Required for Having Copies.
+
+  You are not required to accept this License in order to receive or
+run a copy of the Program.  Ancillary propagation of a covered work
+occurring solely as a consequence of using peer-to-peer transmission
+to receive a copy likewise does not require acceptance.  However,
+nothing other than this License grants you permission to propagate or
+modify any covered work.  These actions infringe copyright if you do
+not accept this License.  Therefore, by modifying or propagating a
+covered work, you indicate your acceptance of this License to do so.
+
+  10. Automatic Licensing of Downstream Recipients.
+
+  Each time you convey a covered work, the recipient automatically
+receives a license from the original licensors, to run, modify and
+propagate that work, subject to this License.  You are not responsible
+for enforcing compliance by third parties with this License.
+
+  An "entity transaction" is a transaction transferring control of an
+organization, or substantially all assets of one, or subdividing an
+organization, or merging organizations.  If propagation of a covered
+work results from an entity transaction, each party to that
+transaction who receives a copy of the work also receives whatever
+licenses to the work the party's predecessor in interest had or could
+give under the previous paragraph, plus a right to possession of the
+Corresponding Source of the work from the predecessor in interest, if
+the predecessor has it or can get it with reasonable efforts.
+
+  You may not impose any further restrictions on the exercise of the
+rights granted or affirmed under this License.  For example, you may
+not impose a license fee, royalty, or other charge for exercise of
+rights granted under this License, and you may not initiate litigation
+(including a cross-claim or counterclaim in a lawsuit) alleging that
+any patent claim is infringed by making, using, selling, offering for
+sale, or importing the Program or any portion of it.
+
+  11. Patents.
+
+  A "contributor" is a copyright holder who authorizes use under this
+License of the Program or a work on which the Program is based.  The
+work thus licensed is called the contributor's "contributor version".
+
+  A contributor's "essential patent claims" are all patent claims
+owned or controlled by the contributor, whether already acquired or
+hereafter acquired, that would be infringed by some manner, permitted
+by this License, of making, using, or selling its contributor version,
+but do not include claims that would be infringed only as a
+consequence of further modification of the contributor version.  For
+purposes of this definition, "control" includes the right to grant
+patent sublicenses in a manner consistent with the requirements of
+this License.
+
+  Each contributor grants you a non-exclusive, worldwide, royalty-free
+patent license under the contributor's essential patent claims, to
+make, use, sell, offer for sale, import and otherwise run, modify and
+propagate the contents of its contributor version.
+
+  In the following three paragraphs, a "patent license" is any express
+agreement or commitment, however denominated, not to enforce a patent
+(such as an express permission to practice a patent or covenant not to
+sue for patent infringement).  To "grant" such a patent license to a
+party means to make such an agreement or commitment not to enforce a
+patent against the party.
+
+  If you convey a covered work, knowingly relying on a patent license,
+and the Corresponding Source of the work is not available for anyone
+to copy, free of charge and under the terms of this License, through a
+publicly available network server or other readily accessible means,
+then you must either (1) cause the Corresponding Source to be so
+available, or (2) arrange to deprive yourself of the benefit of the
+patent license for this particular work, or (3) arrange, in a manner
+consistent with the requirements of this License, to extend the patent
+license to downstream recipients.  "Knowingly relying" means you have
+actual knowledge that, but for the patent license, your conveying the
+covered work in a country, or your recipient's use of the covered work
+in a country, would infringe one or more identifiable patents in that
+country that you have reason to believe are valid.
+
+  If, pursuant to or in connection with a single transaction or
+arrangement, you convey, or propagate by procuring conveyance of, a
+covered work, and grant a patent license to some of the parties
+receiving the covered work authorizing them to use, propagate, modify
+or convey a specific copy of the covered work, then the patent license
+you grant is automatically extended to all recipients of the covered
+work and works based on it.
+
+  A patent license is "discriminatory" if it does not include within
+the scope of its coverage, prohibits the exercise of, or is
+conditioned on the non-exercise of one or more of the rights that are
+specifically granted under this License.  You may not convey a covered
+work if you are a party to an arrangement with a third party that is
+in the business of distributing software, under which you make payment
+to the third party based on the extent of your activity of conveying
+the work, and under which the third party grants, to any of the
+parties who would receive the covered work from you, a discriminatory
+patent license (a) in connection with copies of the covered work
+conveyed by you (or copies made from those copies), or (b) primarily
+for and in connection with specific products or compilations that
+contain the covered work, unless you entered into that arrangement,
+or that patent license was granted, prior to 28 March 2007.
+
+  Nothing in this License shall be construed as excluding or limiting
+any implied license or other defenses to infringement that may
+otherwise be available to you under applicable patent law.
+
+  12. No Surrender of Others' Freedom.
+
+  If conditions are imposed on you (whether by court order, agreement or
+otherwise) that contradict the conditions of this License, they do not
+excuse you from the conditions of this License.  If you cannot convey a
+covered work so as to satisfy simultaneously your obligations under this
+License and any other pertinent obligations, then as a consequence you may
+not convey it at all.  For example, if you agree to terms that obligate you
+to collect a royalty for further conveying from those to whom you convey
+the Program, the only way you could satisfy both those terms and this
+License would be to refrain entirely from conveying the Program.
+
+  13. Remote Network Interaction; Use with the GNU General Public License.
+
+  Notwithstanding any other provision of this License, if you modify the
+Program, your modified version must prominently offer all users
+interacting with it remotely through a computer network (if your version
+supports such interaction) an opportunity to receive the Corresponding
+Source of your version by providing access to the Corresponding Source
+from a network server at no charge, through some standard or customary
+means of facilitating copying of software.  This Corresponding Source
+shall include the Corresponding Source for any work covered by version 3
+of the GNU General Public License that is incorporated pursuant to the
+following paragraph.
+
+  Notwithstanding any other provision of this License, you have
+permission to link or combine any covered work with a work licensed
+under version 3 of the GNU General Public License into a single
+combined work, and to convey the resulting work.  The terms of this
+License will continue to apply to the part which is the covered work,
+but the work with which it is combined will remain governed by version
+3 of the GNU General Public License.
+
+  14. Revised Versions of this License.
+
+  The Free Software Foundation may publish revised and/or new versions of
+the GNU Affero General Public License from time to time.  Such new versions
+will be similar in spirit to the present version, but may differ in detail to
+address new problems or concerns.
+
+  Each version is given a distinguishing version number.  If the
+Program specifies that a certain numbered version of the GNU Affero General
+Public License "or any later version" applies to it, you have the
+option of following the terms and conditions either of that numbered
+version or of any later version published by the Free Software
+Foundation.  If the Program does not specify a version number of the
+GNU Affero General Public License, you may choose any version ever published
+by the Free Software Foundation.
+
+  If the Program specifies that a proxy can decide which future
+versions of the GNU Affero General Public License can be used, that proxy's
+public statement of acceptance of a version permanently authorizes you
+to choose that version for the Program.
+
+  Later license versions may give you additional or different
+permissions.  However, no additional obligations are imposed on any
+author or copyright holder as a result of your choosing to follow a
+later version.
+
+  15. Disclaimer of Warranty.
+
+  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
+APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
+HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
+OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
+THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
+IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
+ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
+
+  16. Limitation of Liability.
+
+  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
+WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
+THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
+GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
+USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
+DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
+PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
+EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
+SUCH DAMAGES.
+
+  17. Interpretation of Sections 15 and 16.
+
+  If the disclaimer of warranty and limitation of liability provided
+above cannot be given local legal effect according to their terms,
+reviewing courts shall apply local law that most closely approximates
+an absolute waiver of all civil liability in connection with the
+Program, unless a warranty or assumption of liability accompanies a
+copy of the Program in return for a fee.
+
+                     END OF TERMS AND CONDITIONS
+
+            How to Apply These Terms to Your New Programs
+
+  If you develop a new program, and you want it to be of the greatest
+possible use to the public, the best way to achieve this is to make it
+free software which everyone can redistribute and change under these terms.
+
+  To do so, attach the following notices to the program.  It is safest
+to attach them to the start of each source file to most effectively
+state the exclusion of warranty; and each file should have at least
+the "copyright" line and a pointer to where the full notice is found.
+
+    <one line to give the program's name and a brief idea of what it does.>
+    Copyright (C) <year>  <name of author>
+
+    This program is free software: you can redistribute it and/or modify
+    it under the terms of the GNU Affero General Public License as published
+    by the Free Software Foundation, either version 3 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU Affero General Public License for more details.
+
+    You should have received a copy of the GNU Affero General Public License
+    along with this program.  If not, see <https://www.gnu.org/licenses/>.
+
+Also add information on how to contact you by electronic and paper mail.
+
+  If your software can interact with users remotely through a computer
+network, you should also make sure that it provides a way for users to
+get its source.  For example, if your program is a web application, its
+interface could display a "Source" link that leads users to an archive
+of the code.  There are many ways you could offer source, and different
+solutions will be better for different programs; see section 13 for the
+specific requirements.
+
+  You should also get your employer (if you work as a programmer) or school,
+if any, to sign a "copyright disclaimer" for the program, if necessary.
+For more information on this, and how to apply and follow the GNU AGPL, see
+<https://www.gnu.org/licenses/>.
```

## Comparing `giotto_tda-0.6.0.dist-info/METADATA` & `giotto_tda-0.6.1.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 Metadata-Version: 2.1
 Name: giotto-tda
-Version: 0.6.0
+Version: 0.6.1
 Summary: Toolbox for Machine Learning using Topological Data Analysis.
 Home-page: https://github.com/giotto-ai/giotto-tda
-Download-URL: https://github.com/giotto-ai/giotto-tda/tarball/v0.6.0
+Download-URL: https://github.com/giotto-ai/giotto-tda/tarball/v0.6.1
 Maintainer: Umberto Lupo, Wojciech Reise
 Maintainer-email: maintainers@giotto.ai
 License: GNU AGPLv3
 Keywords: machine learning,topological data analysis,persistent homology,persistence diagrams,Mapper
 Classifier: Intended Audience :: Science/Research
 Classifier: Intended Audience :: Developers
 Classifier: License :: OSI Approved
@@ -19,40 +19,42 @@
 Classifier: Operating System :: POSIX
 Classifier: Operating System :: Unix
 Classifier: Operating System :: MacOS
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Classifier: Programming Language :: Python :: 3.12
 Description-Content-Type: text/x-rst
 License-File: LICENSE
-Requires-Dist: numpy (>=1.19.1)
-Requires-Dist: scipy (>=1.5.0)
-Requires-Dist: joblib (>=0.16.0)
-Requires-Dist: scikit-learn (>=0.23.1)
-Requires-Dist: giotto-ph (>=0.2.1)
-Requires-Dist: pyflagser (>=0.4.3)
-Requires-Dist: igraph (>=0.9.8)
-Requires-Dist: plotly (>=4.8.2)
-Requires-Dist: ipywidgets (>=7.5.1)
+Requires-Dist: numpy >=1.19.1
+Requires-Dist: scipy >=1.5.0
+Requires-Dist: joblib >=0.16.0
+Requires-Dist: scikit-learn ==1.3.2
+Requires-Dist: giotto-ph >=0.2.1
+Requires-Dist: pyflagser >=0.4.3
+Requires-Dist: igraph >=0.9.8
+Requires-Dist: plotly >=4.8.2
+Requires-Dist: ipywidgets >=7.5.1
 Provides-Extra: dev
-Requires-Dist: nbconvert ; extra == 'dev'
-Requires-Dist: pytest-benchmark ; extra == 'dev'
+Requires-Dist: pandas ; extra == 'dev'
+Requires-Dist: hypothesis ; extra == 'dev'
 Requires-Dist: pytest-cov ; extra == 'dev'
-Requires-Dist: sphinx-rtd-theme ; extra == 'dev'
-Requires-Dist: sphinx ; extra == 'dev'
+Requires-Dist: jupyter-contrib-nbextensions ; extra == 'dev'
+Requires-Dist: flake8 ; extra == 'dev'
 Requires-Dist: sphinx-issues ; extra == 'dev'
-Requires-Dist: pytest ; extra == 'dev'
-Requires-Dist: hypothesis ; extra == 'dev'
+Requires-Dist: numpydoc ; extra == 'dev'
 Requires-Dist: openml ; extra == 'dev'
-Requires-Dist: flake8 ; extra == 'dev'
+Requires-Dist: pytest-benchmark ; extra == 'dev'
 Requires-Dist: pytest-azurepipelines ; extra == 'dev'
-Requires-Dist: jupyter-contrib-nbextensions ; extra == 'dev'
-Requires-Dist: pandas ; extra == 'dev'
-Requires-Dist: numpydoc ; extra == 'dev'
+Requires-Dist: pytest ; extra == 'dev'
+Requires-Dist: nbconvert ; extra == 'dev'
+Requires-Dist: sphinx-rtd-theme ; extra == 'dev'
+Requires-Dist: sphinx ; extra == 'dev'
 Provides-Extra: doc
 Requires-Dist: openml ; extra == 'doc'
 Requires-Dist: sphinx ; extra == 'doc'
 Requires-Dist: nbconvert ; extra == 'doc'
 Requires-Dist: sphinx-issues ; extra == 'doc'
 Requires-Dist: sphinx-rtd-theme ; extra == 'doc'
 Requires-Dist: numpydoc ; extra == 'doc'
```

