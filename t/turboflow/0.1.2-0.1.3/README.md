# Comparing `tmp/turboflow-0.1.2.tar.gz` & `tmp/turboflow-0.1.3.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "turboflow-0.1.2.tar", max compression
+gzip compressed data, was "turboflow-0.1.3.tar", max compression
```

## Comparing `turboflow-0.1.2.tar` & `turboflow-0.1.3.tar`

### file list

```diff
@@ -1,43 +1,43 @@
--rw-r--r--   0        0        0     1088 2024-05-29 10:11:47.879151 turboflow-0.1.2/LICENSE.md
--rw-r--r--   0        0        0      898 2024-05-30 10:20:03.550800 turboflow-0.1.2/pyproject.toml
--rw-r--r--   0        0        0    10917 2024-05-30 07:59:04.140453 turboflow-0.1.2/README.md
--rw-r--r--   0        0        0      793 2024-05-30 09:59:28.006383 turboflow-0.1.2/turboflow/__init__.py
--rw-r--r--   0        0        0     1556 2024-05-29 18:10:11.113738 turboflow-0.1.2/turboflow/add_to_pythonpath.py
--rw-r--r--   0        0        0      213 2024-05-29 18:10:11.060013 turboflow-0.1.2/turboflow/axial_turbine/__init__.py
--rw-r--r--   0        0        0    26417 2024-05-29 18:10:11.888334 turboflow-0.1.2/turboflow/axial_turbine/choking_model.py
--rw-r--r--   0        0        0    30820 2024-05-29 18:10:12.113230 turboflow-0.1.2/turboflow/axial_turbine/design_optimization.py
--rw-r--r--   0        0        0     8169 2024-05-30 08:01:40.239922 turboflow-0.1.2/turboflow/axial_turbine/deviation_model.py
--rw-r--r--   0        0        0    39279 2024-05-29 18:10:12.179897 turboflow-0.1.2/turboflow/axial_turbine/flow_model.py
--rw-r--r--   0        0        0    26232 2024-05-29 18:10:11.959885 turboflow-0.1.2/turboflow/axial_turbine/geometry_model.py
--rw-r--r--   0        0        0     4506 2024-05-29 18:10:11.272522 turboflow-0.1.2/turboflow/axial_turbine/loss_model.py
--rw-r--r--   0        0        0    36512 2024-05-29 18:10:12.037475 turboflow-0.1.2/turboflow/axial_turbine/loss_model_benner.py
--rw-r--r--   0        0        0    21342 2024-05-29 18:10:11.745554 turboflow-0.1.2/turboflow/axial_turbine/loss_model_kacker_okapuu.py
--rw-r--r--   0        0        0    29756 2024-05-29 18:10:12.096159 turboflow-0.1.2/turboflow/axial_turbine/loss_model_moustapha.py
--rw-r--r--   0        0        0    56467 2024-05-30 08:01:40.241927 turboflow-0.1.2/turboflow/axial_turbine/performance_analysis.py
--rw-r--r--   0        0        0    11150 2024-05-29 18:10:11.566240 turboflow-0.1.2/turboflow/config_validation.py
--rw-r--r--   0        0        0    13970 2024-05-30 08:01:40.255152 turboflow-0.1.2/turboflow/math.py
--rw-r--r--   0        0        0    24083 2024-05-29 18:10:12.173368 turboflow-0.1.2/turboflow/plot_functions.py
--rw-r--r--   0        0        0       33 2024-05-29 15:54:27.494792 turboflow-0.1.2/turboflow/properties/__init__.py
--rw-r--r--   0        0        0    56211 2024-05-29 15:54:27.495791 turboflow-0.1.2/turboflow/properties/fluid_properties.py
--rw-r--r--   0        0        0      624 2024-05-29 18:10:11.322234 turboflow-0.1.2/turboflow/pysolver_view/__init__.py
--rw-r--r--   0        0        0    22285 2024-05-29 18:10:12.037475 turboflow-0.1.2/turboflow/pysolver_view/nonlinear_system.py
--rw-r--r--   0        0        0     1589 2024-05-29 18:10:11.398534 turboflow-0.1.2/turboflow/pysolver_view/nonlinear_system_problems.py
--rw-r--r--   0        0        0     7886 2024-05-29 18:10:11.698379 turboflow-0.1.2/turboflow/pysolver_view/numerical_differentiation.py
--rw-r--r--   0        0        0    38015 2024-05-29 18:10:12.215035 turboflow-0.1.2/turboflow/pysolver_view/optimization.py
--rw-r--r--   0        0        0     8340 2024-05-29 18:10:11.764637 turboflow-0.1.2/turboflow/pysolver_view/optimization_problems.py
--rw-r--r--   0        0        0    16992 2024-05-29 18:10:12.022417 turboflow-0.1.2/turboflow/pysolver_view/optimization_wrappers.py
--rw-r--r--   0        0        0     9509 2024-05-29 18:10:11.837627 turboflow-0.1.2/turboflow/pysolver_view/pysolver_utilities.py
--rw-r--r--   0        0        0      146 2024-05-29 18:10:11.498980 turboflow-0.1.2/turboflow/thermodynamic_cycles/__init__.py
--rw-r--r--   0        0        0    12417 2024-05-29 18:10:12.113230 turboflow-0.1.2/turboflow/thermodynamic_cycles/brayton_recuperated.py
--rw-r--r--   0        0        0    15506 2024-05-29 18:10:12.238907 turboflow-0.1.2/turboflow/thermodynamic_cycles/brayton_split_compression.py
--rw-r--r--   0        0        0    10147 2024-05-29 18:10:12.027929 turboflow-0.1.2/turboflow/thermodynamic_cycles/components.py
--rw-r--r--   0        0        0    40848 2024-05-29 18:10:12.482257 turboflow-0.1.2/turboflow/thermodynamic_cycles/cycle_optimization.py
--rw-r--r--   0        0        0    37542 2024-05-29 18:10:11.881812 turboflow-0.1.2/turboflow/thermodynamic_cycles/cycle_optimization_old.py
--rw-r--r--   0        0        0      115 2024-05-29 18:10:11.714934 turboflow-0.1.2/turboflow/turbo_configurations/__init__.py
--rw-r--r--   0        0        0    26252 2024-05-29 18:10:12.329252 turboflow-0.1.2/turboflow/turbo_configurations/config_axial_turbine.py
--rw-r--r--   0        0        0      110 2024-05-29 15:57:26.631202 turboflow-0.1.2/turboflow/utilities/__init__.py
--rw-r--r--   0        0        0    13796 2024-05-29 18:10:12.087627 turboflow-0.1.2/turboflow/utilities/file_utils.py
--rw-r--r--   0        0        0     8862 2024-05-29 18:10:12.026414 turboflow-0.1.2/turboflow/utilities/graphics.py
--rw-r--r--   0        0        0     6273 2024-05-29 18:10:11.899884 turboflow-0.1.2/turboflow/utilities/numerics.py
--rw-r--r--   0        0        0    10779 2024-05-29 18:10:12.035960 turboflow-0.1.2/turboflow/utilities/optimization_utils.py
--rw-r--r--   0        0        0    11462 1970-01-01 00:00:00.000000 turboflow-0.1.2/PKG-INFO
+-rw-r--r--   0        0        0     1067 2024-05-30 10:49:28.380781 turboflow-0.1.3/LICENSE.md
+-rw-r--r--   0        0        0    10926 2024-05-30 10:49:28.380781 turboflow-0.1.3/README.md
+-rw-r--r--   0        0        0      860 2024-05-30 10:49:29.068785 turboflow-0.1.3/pyproject.toml
+-rw-r--r--   0        0        0      763 2024-05-30 10:49:29.068785 turboflow-0.1.3/turboflow/__init__.py
+-rw-r--r--   0        0        0     1515 2024-05-30 10:49:29.068785 turboflow-0.1.3/turboflow/add_to_pythonpath.py
+-rw-r--r--   0        0        0      213 2024-05-30 10:49:29.068785 turboflow-0.1.3/turboflow/axial_turbine/__init__.py
+-rw-r--r--   0        0        0    25660 2024-05-30 10:49:29.068785 turboflow-0.1.3/turboflow/axial_turbine/choking_model.py
+-rw-r--r--   0        0        0    30032 2024-05-30 10:49:29.068785 turboflow-0.1.3/turboflow/axial_turbine/design_optimization.py
+-rw-r--r--   0        0        0     7936 2024-05-30 10:49:29.068785 turboflow-0.1.3/turboflow/axial_turbine/deviation_model.py
+-rw-r--r--   0        0        0    38154 2024-05-30 10:49:29.068785 turboflow-0.1.3/turboflow/axial_turbine/flow_model.py
+-rw-r--r--   0        0        0    25638 2024-05-30 10:49:29.068785 turboflow-0.1.3/turboflow/axial_turbine/geometry_model.py
+-rw-r--r--   0        0        0     4365 2024-05-30 10:49:29.068785 turboflow-0.1.3/turboflow/axial_turbine/loss_model.py
+-rw-r--r--   0        0        0    35498 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/axial_turbine/loss_model_benner.py
+-rw-r--r--   0        0        0    20734 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/axial_turbine/loss_model_kacker_okapuu.py
+-rw-r--r--   0        0        0    28906 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/axial_turbine/loss_model_moustapha.py
+-rw-r--r--   0        0        0    55104 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/axial_turbine/performance_analysis.py
+-rw-r--r--   0        0        0    10850 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/config_validation.py
+-rw-r--r--   0        0        0    13530 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/math.py
+-rw-r--r--   0        0        0    23376 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/plot_functions.py
+-rw-r--r--   0        0        0       32 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/properties/__init__.py
+-rw-r--r--   0        0        0    54806 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/properties/fluid_properties.py
+-rw-r--r--   0        0        0      604 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/pysolver_view/__init__.py
+-rw-r--r--   0        0        0    21671 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/pysolver_view/nonlinear_system.py
+-rw-r--r--   0        0        0     1530 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/pysolver_view/nonlinear_system_problems.py
+-rw-r--r--   0        0        0     7650 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/pysolver_view/numerical_differentiation.py
+-rw-r--r--   0        0        0    37039 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/pysolver_view/optimization.py
+-rw-r--r--   0        0        0     8067 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/pysolver_view/optimization_problems.py
+-rw-r--r--   0        0        0    16483 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/pysolver_view/optimization_wrappers.py
+-rw-r--r--   0        0        0     9222 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/pysolver_view/pysolver_utilities.py
+-rw-r--r--   0        0        0      141 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/thermodynamic_cycles/__init__.py
+-rw-r--r--   0        0        0    12089 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/thermodynamic_cycles/brayton_recuperated.py
+-rw-r--r--   0        0        0    15116 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/thermodynamic_cycles/brayton_split_compression.py
+-rw-r--r--   0        0        0     9834 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/thermodynamic_cycles/components.py
+-rw-r--r--   0        0        0    39883 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/thermodynamic_cycles/cycle_optimization.py
+-rw-r--r--   0        0        0    36705 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/thermodynamic_cycles/cycle_optimization_old.py
+-rw-r--r--   0        0        0      113 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/turbo_configurations/__init__.py
+-rw-r--r--   0        0        0    25494 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/turbo_configurations/config_axial_turbine.py
+-rw-r--r--   0        0        0      110 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/utilities/__init__.py
+-rw-r--r--   0        0        0    13374 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/utilities/file_utils.py
+-rw-r--r--   0        0        0     8589 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/utilities/graphics.py
+-rw-r--r--   0        0        0     6040 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/utilities/numerics.py
+-rw-r--r--   0        0        0    10504 2024-05-30 10:49:29.072785 turboflow-0.1.3/turboflow/utilities/optimization_utils.py
+-rw-r--r--   0        0        0    11738 1970-01-01 00:00:00.000000 turboflow-0.1.3/PKG-INFO
```

### Comparing `turboflow-0.1.2/LICENSE.md` & `turboflow-0.1.3/LICENSE.md`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-MIT License
-
-Copyright (c) 2023 Lasse Borg
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
+MIT License
+
+Copyright (c) 2023 Lasse Borg
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
```

### Comparing `turboflow-0.1.2/README.md` & `turboflow-0.1.3/README.md`

 * *Files 23% similar despite different names*

```diff
@@ -1,268 +1,278 @@
-
-# TurboFlow: Axial Turbine Mean-line Modelling
-
-**Meanline Axial** is a tool for mean-line modelling of axial turbines. It provides a systematic approach to analyze and optimize axial turbines based on specified requirements.
-
-## Core Features
-- **Performance Analysis Mode**: 
-  - Evaluate single operating points
-  - Produce performance maps
-- **Design Optimization Mode**: 
-  - Single point optimization
-  - Multi-point optimization
-- **Problem formulation and solution**
-  - Equation-oriented problem formulation for performance analysis and design optimization
-  - Consistency between both calculation modes is guaranteed by design
-  - Efficient solution with gradient-based root-finding and optimization solvers
-  - Multi-start strategies or derivative-free optimizers for global optimization
-- **Fluid Property Analysis**:
-  - Use CoolProp to determine real gas fluid properties.
-- **Design Flexibility**: 
-  - Supports modelling with any number of turbine stages.
-  - Specify turbine geometry using main geometric parameters.
-- **Choking Calculations**:
-  - General computational strategy to evaluate cascade choking.
-  - Formulation autonomously identifies choked cascades for a set of operating conditions
-- **Loss Models**: 
-  - Kacker Okapuu model.
-  - Benner model.
-
-
-## Installation Instructions
-
-1. **Install Conda**:
-
-   Before proceeding, ensure you have [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/windows.html) installed on your system.
-
-2. **Create a virtual environment with dependencies**:
-
-   Run the following command in [Bash](https://gitforwindows.org/) to set up a new virtual environment with all the required dependencies:
-
-   ```bash
-   conda env create --file environment.yaml
-   ```
-
-   This command will create a virtual environment named `meanline_env` and will install all the packages specified in the `environment.yaml` file.
-
-3. **Activate the virtual environment**:
-
-   ```bash
-   conda activate meanline_env
-   ```
-
-4. **Installing additional packages (optional)**:
-
-   If you need any additional packages, they can be installed using:
-
-   ```bash
-   conda install <name of the package>
-   ```
-
-   Alternatively, you can add package names directly to the `environment.yaml` file and then update the environment:
-
-   ```bash
-   conda env update --file environment.yaml --prune
-   ```
-
-5. **Setting up for local development**
-
-   To ensure that you can import the package for local development, you need to add the package directory to the `PYTHONPATH` variable. We have provided a convenient script named `install_local.py` to do this for you.
-
-   Just run:
-
-   ```bash
-   python install_local.py
-   ```
-
-   This will append the current working directory (which should be the root of this repository) to your `PYTHONPATH` by adding a line to your `~/.bashrc` file.
-
-   **Note:** This is a temporary development solution. In the future the package will be installed via pip/conda.
-
-
-## To-do list
-- [x] Verify torque and efficiency deviation match
-- [x] Check if the correlation for incidence losses from Moustapha 1990 is better
-- [x] Check if baseline kacker-okapuu + moustapha 1990 is better
-- [x] Create clean dataset including torque/efficiency/flow angle for each pressure ration/speed
-- [x] Extend validation Kofskey1972 to exit flow angle
-- [x] Add x=y comparison plots for the validation
-- [ ] Verify the displacement thickness value for kofskey1974
-- [ ] Try to extract the shock loss at the inlet fromm the profile loss to have a more clear split of the losses
-- [x] Add better smoother for Mach number constraint
-- [x] Add generic smoothing functions for min/max/abs/piecewise
-- [ ] Replace all non-differentiable functions of the code (specially in the loss models)
-- [x] Improve initial guess computation for supersonic case (call the function `initialize()`)
-- [ ] Validate model with experimental data
-- [x] Add automatic testing (pytest) to ensure that new code pushes do not introduce breaking changes
-- [x] Make function to plot performance maps more general
-- [x] Add functionality to export/import all the parameters corresponding to a calculated operating point
-- [x] Make the plotting of performance maps more general and add functionality to save figures
-- [x] Add environment.yaml file to install all dependencies at once
-- [x] Add Sphinx documentation structure
-
-- [ ] Implement design optimization
-  - [ ] Single-point
-  - [ ] Multi-point
-- [ ] Add CI/CD pipeline to run tests and update documentation on new pushes to main branch
-- [ ] Think of a nice catchy name for the project
-  - MeanFlow?
-  - MeanStream?
-  - MeanTurbo
-  - MeanTurboPy
-  - TurboCompuCore
-  - TurboCore
-  - Meanpy
-  - Others?
-  - TurboFlow?
-
-
-## To-do list 10.11.2023
-- [x] Residuals and independent variables should be dictionaries
-- [x] Improve initial guess functionality (Lasse)
-  - [x] Move initial guess generation into the CascadesNonlinearSystemProblem() class
-  - [x] Move the solution scaling into the CascadesNonlinearSystemProblem() class [how to re-use scaling for optimization?]
-  - [x] Generation within CascadesNonlinearSystemProblem() class
-  - [x] Initial guess should be a dictionary with keys
-  - [x] Initial guess specification in YAML file (this will be used for the first operation point)
-  - [x] Improve initial guess calculation and extend to multistage
-  - [ ] 1D correlation for Ma_crit as a function of the loss coefficient of the cascade
-- [x] Geometry processing (Roberto)
-  - [x] Clean up code
-  - [x] Improve docstrings
-  - [x] Discuss and improve the throat area calculation (Roberto/Lasse)
-- [ ] Update PySolverView to solve root finding problems with the optimization solver.
-- [x] Add computation time in solver report
-- [ ] Improve robustness with random generation of initial guesses
-  - [ ] Specify the number of initial guesses that will be tried, e.g., 50
-  - [ ] Specify the ranges of variables to be sampled from (enthalpy frac distribution, efficiency ts and tt)
-  - [ ] Use a sampling techniques like latin hypercube, montecarlo, or orthogonal sampling'
-  - [ ] https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.LatinHypercube.html
-- [x] Add regression tests
-- [ ] 
-
-## TODO 17.11.2023
-- [x] Add validation of configuration file
-- [x] Add configuration file options to documentation (automated process)
-- [ ] Add logger object to the performance analysis and optimization functions
-- [ ] Log errors and warnings during model evaluation (print after convergence history?)
-- [ ] Improve geometry generation functionality to create full geometry from design optimization variables
-- [ ] Improve printing/logging with imporve control over verbosity
-- [ ] Solver improvements
-  - [ ] Unify optimization problem so that it can be solver with root finding and optimization methods
-  - [ ] Add pygmo solvers to the pysolverview interface (IPOPT/SNOPT)
-- [x] Improve deviation modeling to cover cases when:
-  - [x] Area changes from throat to exit
-  - [x] Radius changes from throat to exit
-  - [x] There is an additional blockage factor at the throat
-- [ ] Add plotting functionality
-  - [ ] Plot velocity triangles
-  - [ ] Plot blade to plade plane
-  - [ ] Plot meridional view
-  - [ ] Plot h-s or T-s diagrams of the expansion
-  - [ ] Nice validation plotting against experimental data
-  - [ ] 
-
-
-
-## TODO 06.12.2023
-- [x] Use gauge and and not opening-to-pitch in deviation model functions
-- [x] Make sure opening is only used for the critical mass flow rate calculation and blockage factor
-- [x] Do not use ordinary compute_cascade_exit function to determine the flow conditions at the throat
-- [ ] Have the option to calculate the state at the throat for the actual condition? Probably not
-- [x] Remove option of choking condition
-- [ ] Deviation model new structure:
- - [ ] low_speed_limit: aungier | ainley-methieson | zero-deviation
- - [ ] interpolation: aungier | ainley-methieson | borg
- - [ ] blending_parmaters:
-	n: 
-	m: 
-	slope:
-- [x] We improve slope so that it works when A_th=A_exit and in other cases (slope zero or one)
-- [ ] Improve initialization / initial guess generation
-- [x] Write down notes explaining the behavior of the deviation model when the area is the same and when it changes
-- [ ] Write down notes explaining the equations for the blending
-- [x] check the throat area calculation i nthe geometyr.py script and clean it
-- [ ] add option to calculate throat loasses without trailing edge losses 
-
-## TODO 16.01.2024
-- [ ] Improve solver to not converge for low residual, and converge for satisfactory residuals
-- [ ] Implement the blender such that it is only used for zero deviation
-- [ ] Improve initial guess function
-
-
-Guidelines/instructions for the deviation model
-
-Subsonic deviation according to ainley-mathieson or aungier, or zero-deviation
-critical mach number according to out calculation
-critical flow angle equal to metal_angle_out
-
-interpolation can be:
-- ainley-methieson (linear)
-- aungier (5th order polynomial for delta)
-- third order polynomial with controller end-point slope (parameter to have slope 1 or slope 0)
-
-slope should be zero for cases when Mach_crit_throat = Mach_crit_exit
-slope should be close to one for cases when Mach_crit_throat > Mach_crit_exit
-
-sigmoid blending between subsonic interpolated function and the supersonic solution (in the subsonic regime)
-we have to expend the supersonic branch into the subsonic to calculate the flow angle to ensure that we have a smooth flow angle transition
-
-
-
-## CI/CD
-
-[Get started with GitLab CI/CD](https://docs.gitlab.com/ee/ci/quick_start/index.html)
-
-
-## Improvements from Marta
-- [ ] Change loader and dumper to recognize numpy (custom tags)
-- [ ] Use pedantic for validation instead of custom function (JSON schema)
-- [ ] Follow up gitlab pages
-- [ ] Dynamic generation of function mapping dictionary
-- [ ] Add logger with errors, info and warnings
-- [ ] FASTApi - open API (swagger)
-
-
-
-
-## Installation
-
-
-To create virtual environment to isolate installation
-
-
-conda env create -f environment.yaml
-
-
-conda activate turboflow_env
-pip install poetry
-
-
-poetry init
-
-
-## Developer installation
-
-
-Clone the repository
-
-conda env create -f environment.yaml
-
-
-conda activate turboflow_env
-pip install poetry
-
-poetry install
-
-
-## Additional optimizers
-optionally install pygmo optimizers
-
-conda install pygmo pygmo_plugins_nonfree
-
-
-Additionally, in order to use snopt you have to define the environmental variables for the license file and the xx.
-
-You can do this in windows following this tutorial, or by addtion these lines to you bashrc file in linux or if using gitbash in windows
-
-instructions for snopt DLLs
+
+# TurboFlow: Axial Turbine Mean-line Modelling
+
+**Meanline Axial** is a tool for mean-line modelling of axial turbines. It provides a systematic approach to analyze and optimize axial turbines based on specified requirements.
+
+## Core Features
+- **Performance Analysis Mode**: 
+  - Evaluate single operating points
+  - Produce performance maps
+- **Design Optimization Mode**: 
+  - Single point optimization
+  - Multi-point optimization
+- **Problem formulation and solution**
+  - Equation-oriented problem formulation for performance analysis and design optimization
+  - Consistency between both calculation modes is guaranteed by design
+  - Efficient solution with gradient-based root-finding and optimization solvers
+  - Multi-start strategies or derivative-free optimizers for global optimization
+- **Fluid Property Analysis**:
+  - Use CoolProp to determine real gas fluid properties.
+- **Design Flexibility**: 
+  - Supports modelling with any number of turbine stages.
+  - Specify turbine geometry using main geometric parameters.
+- **Choking Calculations**:
+  - General computational strategy to evaluate cascade choking.
+  - Formulation autonomously identifies choked cascades for a set of operating conditions
+- **Loss Models**: 
+  - Kacker Okapuu model.
+  - Benner model.
+
+
+## Installation Instructions
+
+1. **Install Conda**:
+
+   Before proceeding, ensure you have [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/windows.html) installed on your system.
+
+2. **Create a virtual environment with dependencies**:
+
+   Run the following command in [Bash](https://gitforwindows.org/) to set up a new virtual environment with all the required dependencies:
+
+   ```bash
+   conda env create --file environment.yaml
+   ```
+
+   This command will create a virtual environment named `meanline_env` and will install all the packages specified in the `environment.yaml` file.
+
+3. **Activate the virtual environment**:
+
+   ```bash
+   conda activate meanline_env
+   ```
+
+4. **Installing additional packages (optional)**:
+
+   If you need any additional packages, they can be installed using:
+
+   ```bash
+   conda install <name of the package>
+   ```
+
+   Alternatively, you can add package names directly to the `environment.yaml` file and then update the environment:
+
+   ```bash
+   conda env update --file environment.yaml --prune
+   ```
+
+5. **Setting up for local development**
+
+   To ensure that you can import the package for local development, you need to add the package directory to the `PYTHONPATH` variable. We have provided a convenient script named `install_local.py` to do this for you.
+
+   Just run:
+
+   ```bash
+   python install_local.py
+   ```
+
+   This will append the current working directory (which should be the root of this repository) to your `PYTHONPATH` by adding a line to your `~/.bashrc` file.
+
+   **Note:** This is a temporary development solution. In the future the package will be installed via pip/conda.
+
+
+## To-do list
+- [x] Verify torque and efficiency deviation match
+- [x] Check if the correlation for incidence losses from Moustapha 1990 is better
+- [x] Check if baseline kacker-okapuu + moustapha 1990 is better
+- [x] Create clean dataset including torque/efficiency/flow angle for each pressure ration/speed
+- [x] Extend validation Kofskey1972 to exit flow angle
+- [x] Add x=y comparison plots for the validation
+- [ ] Verify the displacement thickness value for kofskey1974
+- [ ] Try to extract the shock loss at the inlet fromm the profile loss to have a more clear split of the losses
+- [x] Add better smoother for Mach number constraint
+- [x] Add generic smoothing functions for min/max/abs/piecewise
+- [ ] Replace all non-differentiable functions of the code (specially in the loss models)
+- [x] Improve initial guess computation for supersonic case (call the function `initialize()`)
+- [ ] Validate model with experimental data
+- [x] Add automatic testing (pytest) to ensure that new code pushes do not introduce breaking changes
+- [x] Make function to plot performance maps more general
+- [x] Add functionality to export/import all the parameters corresponding to a calculated operating point
+- [x] Make the plotting of performance maps more general and add functionality to save figures
+- [x] Add environment.yaml file to install all dependencies at once
+- [x] Add Sphinx documentation structure
+
+- [ ] Implement design optimization
+  - [ ] Single-point
+  - [ ] Multi-point
+- [ ] Add CI/CD pipeline to run tests and update documentation on new pushes to main branch
+- [ ] Think of a nice catchy name for the project
+  - MeanFlow?
+  - MeanStream?
+  - MeanTurbo
+  - MeanTurboPy
+  - TurboCompuCore
+  - TurboCore
+  - Meanpy
+  - Others?
+  - TurboFlow?
+
+
+## To-do list 10.11.2023
+- [x] Residuals and independent variables should be dictionaries
+- [x] Improve initial guess functionality (Lasse)
+  - [x] Move initial guess generation into the CascadesNonlinearSystemProblem() class
+  - [x] Move the solution scaling into the CascadesNonlinearSystemProblem() class [how to re-use scaling for optimization?]
+  - [x] Generation within CascadesNonlinearSystemProblem() class
+  - [x] Initial guess should be a dictionary with keys
+  - [x] Initial guess specification in YAML file (this will be used for the first operation point)
+  - [x] Improve initial guess calculation and extend to multistage
+  - [ ] 1D correlation for Ma_crit as a function of the loss coefficient of the cascade
+- [x] Geometry processing (Roberto)
+  - [x] Clean up code
+  - [x] Improve docstrings
+  - [x] Discuss and improve the throat area calculation (Roberto/Lasse)
+- [ ] Update PySolverView to solve root finding problems with the optimization solver.
+- [x] Add computation time in solver report
+- [ ] Improve robustness with random generation of initial guesses
+  - [ ] Specify the number of initial guesses that will be tried, e.g., 50
+  - [ ] Specify the ranges of variables to be sampled from (enthalpy frac distribution, efficiency ts and tt)
+  - [ ] Use a sampling techniques like latin hypercube, montecarlo, or orthogonal sampling'
+  - [ ] https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.LatinHypercube.html
+- [x] Add regression tests
+- [ ] 
+
+## TODO 17.11.2023
+- [x] Add validation of configuration file
+- [x] Add configuration file options to documentation (automated process)
+- [ ] Add logger object to the performance analysis and optimization functions
+- [ ] Log errors and warnings during model evaluation (print after convergence history?)
+- [ ] Improve geometry generation functionality to create full geometry from design optimization variables
+- [ ] Improve printing/logging with imporve control over verbosity
+- [ ] Solver improvements
+  - [ ] Unify optimization problem so that it can be solver with root finding and optimization methods
+  - [ ] Add pygmo solvers to the pysolverview interface (IPOPT/SNOPT)
+- [x] Improve deviation modeling to cover cases when:
+  - [x] Area changes from throat to exit
+  - [x] Radius changes from throat to exit
+  - [x] There is an additional blockage factor at the throat
+- [ ] Add plotting functionality
+  - [ ] Plot velocity triangles
+  - [ ] Plot blade to plade plane
+  - [ ] Plot meridional view
+  - [ ] Plot h-s or T-s diagrams of the expansion
+  - [ ] Nice validation plotting against experimental data
+  - [ ] 
+
+
+
+## TODO 06.12.2023
+- [x] Use gauge and and not opening-to-pitch in deviation model functions
+- [x] Make sure opening is only used for the critical mass flow rate calculation and blockage factor
+- [x] Do not use ordinary compute_cascade_exit function to determine the flow conditions at the throat
+- [ ] Have the option to calculate the state at the throat for the actual condition? Probably not
+- [x] Remove option of choking condition
+- [ ] Deviation model new structure:
+ - [ ] low_speed_limit: aungier | ainley-methieson | zero-deviation
+ - [ ] interpolation: aungier | ainley-methieson | borg
+ - [ ] blending_parmaters:
+	n: 
+	m: 
+	slope:
+- [x] We improve slope so that it works when A_th=A_exit and in other cases (slope zero or one)
+- [ ] Improve initialization / initial guess generation
+- [x] Write down notes explaining the behavior of the deviation model when the area is the same and when it changes
+- [ ] Write down notes explaining the equations for the blending
+- [x] check the throat area calculation i nthe geometyr.py script and clean it
+- [ ] add option to calculate throat loasses without trailing edge losses 
+
+## TODO 16.01.2024
+- [ ] Improve solver to not converge for low residual, and converge for satisfactory residuals
+- [ ] Implement the blender such that it is only used for zero deviation
+- [ ] Improve initial guess function
+
+
+Guidelines/instructions for the deviation model
+
+Subsonic deviation according to ainley-mathieson or aungier, or zero-deviation
+critical mach number according to out calculation
+critical flow angle equal to metal_angle_out
+
+interpolation can be:
+- ainley-methieson (linear)
+- aungier (5th order polynomial for delta)
+- third order polynomial with controller end-point slope (parameter to have slope 1 or slope 0)
+
+slope should be zero for cases when Mach_crit_throat = Mach_crit_exit
+slope should be close to one for cases when Mach_crit_throat > Mach_crit_exit
+
+sigmoid blending between subsonic interpolated function and the supersonic solution (in the subsonic regime)
+we have to expend the supersonic branch into the subsonic to calculate the flow angle to ensure that we have a smooth flow angle transition
+
+
+
+## CI/CD
+
+[Get started with GitLab CI/CD](https://docs.gitlab.com/ee/ci/quick_start/index.html)
+
+
+## Improvements from Marta
+- [ ] Change loader and dumper to recognize numpy (custom tags)
+- [ ] Use pedantic for validation instead of custom function (JSON schema)
+- [ ] Follow up gitlab pages
+- [ ] Dynamic generation of function mapping dictionary
+- [ ] Add logger with errors, info and warnings
+- [ ] FASTApi - open API (swagger)
+
+
+
+
+## Installation
+
+
+To create virtual environment to isolate installation
+
+
+conda env create -f environment.yaml
+
+
+conda activate turboflow_env
+pip install poetry
+
+
+poetry init
+
+
+## Developer installation
+
+
+Clone the repository
+
+conda env create -f environment.yaml
+
+
+conda activate turboflow_env
+pip install poetry
+
+poetry install
+
+
+## Additional optimizers
+optionally install pygmo optimizers
+
+conda install pygmo pygmo_plugins_nonfree
+
+
+Additionally, in order to use snopt you have to define the environmental variables for the license file and the xx.
+
+You can do this in windows following this tutorial, or by addtion these lines to you bashrc file in linux or if using gitbash in windows
+
+instructions for snopt DLLs
+
+
+
+
+
+
+## to create a new version and release to pypi
+
+bumpversion patch  # This increments the version, updates relevant files, commits the changes, and creates a new tag.
+git push origin main --tags  # This pushes the `main` branch and the new tag to the remote repository.
```

### Comparing `turboflow-0.1.2/turboflow/add_to_pythonpath.py` & `turboflow-0.1.3/turboflow/add_to_pythonpath.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,41 +1,41 @@
-# Import packages
-import os
-import sys
-
-# Define package name for re-usability
-package_name = "turboflow"
-
-# Define bash command to append cwd to PYTHONPATH
-# PACKAGE_PATH = os.getcwd()
-PACKAGE_PATH = os.path.dirname(os.getcwd())
-bashrc_header = f"# Append {package_name} package to PYTHONPATH"
-if sys.platform == "win32":  # Windows
-    bashrc_line = f'export PYTHONPATH=$PYTHONPATH\;"{PACKAGE_PATH}"'
-else:  # Linux or MacOS
-    bashrc_line = f'export PYTHONPATH=$PYTHONPATH:"{PACKAGE_PATH}"'
-
-# Locate the .bashrc file
-bashrc_path = os.path.expanduser("~/.bashrc")
-
-# Ask for user confirmation with default set to no
-response = input(
-    f"Do you want to add the {package_name} path to your .bashrc? [yes/NO]: "
-)
-if response.lower() in ["y", "yes"]:
-    try:
-        # Check if the line already exists in the .bashrc
-        with open(bashrc_path, "r") as file:
-            if bashrc_line in file.read():
-                print(f".bashrc already contains the {package_name} path.")
-            else:
-                with open(bashrc_path, "a") as file_append:
-                    file_append.write(f"\n{bashrc_header}")
-                    file_append.write(f"\n{bashrc_line}\n")
-                print(f"Path to {package_name} package added to .bashrc")
-                print(
-                    f"Restart the terminal or run 'source ~/.bashrc' for the changes to take effect."
-                )
-    except Exception as e:
-        print(f"An error occurred: {e}")
-else:
-    print("Operation aborted by user.")
+# Import packages
+import os
+import sys
+
+# Define package name for re-usability
+package_name = "turboflow"
+
+# Define bash command to append cwd to PYTHONPATH
+# PACKAGE_PATH = os.getcwd()
+PACKAGE_PATH = os.path.dirname(os.getcwd())
+bashrc_header = f"# Append {package_name} package to PYTHONPATH"
+if sys.platform == "win32":  # Windows
+    bashrc_line = f'export PYTHONPATH=$PYTHONPATH\;"{PACKAGE_PATH}"'
+else:  # Linux or MacOS
+    bashrc_line = f'export PYTHONPATH=$PYTHONPATH:"{PACKAGE_PATH}"'
+
+# Locate the .bashrc file
+bashrc_path = os.path.expanduser("~/.bashrc")
+
+# Ask for user confirmation with default set to no
+response = input(
+    f"Do you want to add the {package_name} path to your .bashrc? [yes/NO]: "
+)
+if response.lower() in ["y", "yes"]:
+    try:
+        # Check if the line already exists in the .bashrc
+        with open(bashrc_path, "r") as file:
+            if bashrc_line in file.read():
+                print(f".bashrc already contains the {package_name} path.")
+            else:
+                with open(bashrc_path, "a") as file_append:
+                    file_append.write(f"\n{bashrc_header}")
+                    file_append.write(f"\n{bashrc_line}\n")
+                print(f"Path to {package_name} package added to .bashrc")
+                print(
+                    f"Restart the terminal or run 'source ~/.bashrc' for the changes to take effect."
+                )
+    except Exception as e:
+        print(f"An error occurred: {e}")
+else:
+    print("Operation aborted by user.")
```

### Comparing `turboflow-0.1.2/turboflow/axial_turbine/choking_model.py` & `turboflow-0.1.3/turboflow/axial_turbine/choking_model.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,757 +1,757 @@
-from scipy.optimize._numdiff import approx_derivative
-from scipy import interpolate
-import numpy as np
-
-from .. import math
-from . import flow_model as fm
-from . import deviation_model as dm
-
-CHOKING_MODELS = [
-    "evaluate_cascade_critical",
-    "evaluate_cascade_throat",
-    "evaluate_cascade_isentropic_throat",
-]
-
-
-def evaluate_choking(
-    choking_input,
-    inlet_plane,
-    exit_plane,
-    fluid,
-    geometry,
-    angular_speed,
-    model_options,
-    reference_values,
-):
-    r"""
-
-    Calculate condition for choking and evaluate wheter or not the cascade is choked, based on selected choking model.
-
-    Parameters
-    ----------
-    choking_input : dict
-        Dictionary containing necessary input parameters required for selected choking model. See each models documentation for specifics.
-    inlet_plane : dict
-        Dictionary containing data on the inlet plane for the actual cascade operating condition.
-    exit_plane : dict
-        Dictionary containing data on the exit plane for the actual cascade operating condition.
-    fluid : object
-        A fluid object with methods for thermodynamic property calculations.
-    geometry : dict
-        Geometric parameters of the cascade.
-    angular_speed : float
-        Angular speed of the cascade.
-    model_options : dict
-        Options for the model used to evaluate choking.
-    reference_values : dict
-        Reference values used in the calculation, including the reference mass flow rate.
-
-    Returns
-    -------
-    dict
-        Dictionary containing the residuals of choking model
-    dict
-        Dictionary containing state information on the critical conditions.
-
-    Raises
-    ------
-    ValueError
-        If an invalid choking model is provided.
-
-    """
-
-    critical_cascade_functions = {
-        CHOKING_MODELS[0]: evaluate_cascade_critical,
-        CHOKING_MODELS[1]: evaluate_cascade_throat,
-        CHOKING_MODELS[2]: evaluate_cascade_isentropic_throat,
-    }
-
-    # Evaluate loss model
-    model = model_options["choking_model"]
-    if model in critical_cascade_functions:
-        return critical_cascade_functions[model](
-            choking_input,
-            inlet_plane,
-            exit_plane,
-            fluid,
-            geometry,
-            angular_speed,
-            model_options,
-            reference_values,
-        )
-    else:
-        options = ", ".join(f"'{k}'" for k in CHOKING_MODELS)
-        raise ValueError(
-            f"Invalid critical cascade model '{model}'. Available options: {options}"
-        )
-
-
-def evaluate_cascade_throat(
-    choking_input,
-    inlet_plane,
-    exit_plane,
-    fluid,
-    geometry,
-    angular_speed,
-    model_options,
-    reference_values,
-):
-    r"""
-
-    Calculate condition for choking and evaluate wheter or not the cascade is choked, based on the evaluate_cascade_throat choking model.
-
-    This choking model evaluates the cascade throat and checks if the throat mach number exceed the critical. The critical mach number is calculated
-    from a correlation depending on the throat loss coefficient. The exit flow angle is calculated by the selected deviation model at subsonic condition,
-    and by ensuring that the mach at the throat equals the critical at supercritical conditions.
-
-    Parameters
-    ----------
-    choking_input : dict
-        Dictionary containing scaled input parameters required for selected choking model. Required items are:
-
-        - `w*_throat` : throat velocity.
-        - `s*_throat` : throat entropy.
-        - `beta*_throat` : throat relative flow angle.
-    inlet_plane : dict
-        Dictionary containing data on the inlet plane for the actual cascade operating condition.
-    exit_plane : dict
-        Dictionary containing data on the exit plane for the actual cascade operating condition.
-    fluid : object
-        A fluid object with methods for thermodynamic property calculations.
-    geometry : dict
-        Geometric parameters of the cascade.
-    angular_speed : float
-        Angular speed of the cascade.
-    model_options : dict
-        Options for the model used to evaluate choking.
-    reference_values : dict
-        Reference values used in the calculation, including the reference mass flow rate.
-
-    Returns
-    -------
-    dict
-        Dictionary containing the residuals of choking model:
-
-        - `m*`: Mass flow rate residual.
-        - `Y*`: Loss error residual.
-        - `beta*`: Residual of the flow angle.
-        - `choking`: Choking residual.
-    dict
-        Dictionary containing relevant information on the critical state and throat plane.
-
-    """
-
-    # Rename variables
-    loss_model = model_options["loss_model"]
-    blockage = model_options["blockage_model"]
-    deviation_model = model_options["deviation_model"]
-    A_throat = geometry["A_throat"]
-    A_out = geometry["A_out"]
-    v0 = reference_values["v0"]
-    s_range = reference_values["s_range"]
-    s_min = reference_values["s_min"]
-    angle_range = reference_values["angle_range"]
-    angle_min = reference_values["angle_min"]
-
-    # Evaluate throat
-    cascade_throat_input = {
-        "w": choking_input["w*_throat"] * v0,
-        "s": choking_input["s*_throat"] * s_range + s_min,
-        "beta": choking_input["beta*_throat"] * angle_range + angle_min,
-        "rothalpy": inlet_plane["rothalpy"],
-    }
-    throat_plane, loss_dict = fm.evaluate_cascade_throat(
-        cascade_throat_input,
-        fluid,
-        geometry,
-        inlet_plane,
-        angular_speed,
-        blockage,
-        loss_model,
-    )
-
-    # Evaluate critical mach
-    Y_tot = loss_dict["loss_total"]
-    critical_mass_flux, critical_mach = interpolate_critical_state(
-        inlet_plane["p0_rel"], inlet_plane["T0_rel"], Y_tot
-    )
-
-    # Evaluate residual flow angle
-    beta_model = np.sign(throat_plane["beta"]) * dm.get_subsonic_deviation(
-        throat_plane["Ma_rel"], critical_mach, geometry, deviation_model
-    )
-    beta_residual = math.cosd(beta_model) - math.cosd(throat_plane["beta"])
-
-    # Evaluate if flow cascade is choked or not an add choking residual
-    if exit_plane["Ma_rel"] <= critical_mach:
-        beta_model = np.sign(exit_plane["beta"]) * dm.get_subsonic_deviation(
-            exit_plane["Ma_rel"], critical_mach, geometry, deviation_model
-        )
-        # Compute error of guessed beta and deviation model
-        choking_residual = math.cosd(beta_model) - math.cosd(exit_plane["beta"])
-    else:
-        choking_residual = throat_plane["Ma_rel"] - critical_mach
-
-    # Evaluate resiudals
-    residual_values = np.array(
-        [
-            (inlet_plane["mass_flow"] - throat_plane["mass_flow"])
-            / reference_values["mass_flow_ref"],
-            throat_plane["loss_error"],
-            beta_residual,
-            choking_residual,
-        ]
-    )
-    residual_keys = ["m*", "Y*", "beta*", "choking"]
-    residuals_critical = dict(zip(residual_keys, residual_values))
-
-    # Define output values
-    critical_state = {
-        "critical_mach": critical_mach,
-        "critical_mass_flow_rate": critical_mass_flux * A_throat,
-    }
-    throat_plane = {f"{key}_throat": val for key, val in throat_plane.items()}
-
-    return residuals_critical, {**critical_state, **throat_plane}
-
-
-def interpolate_critical_state(p0_in, T0_in, Y):
-    r"""
-    Compute the critical mass flux and mach number from a correlation depending on the cascade inlet stagnation state and loss coefficient.
-    The correlation is made from linear regression analysis, and the regression coefficients are prescribed in the function.
-
-    Parameters
-    ----------
-    p0_in : float
-        Cascade inlet stagnation pressure.
-    T0_in : float
-        Cascade inlet stagnation temperature.
-    Y : float
-        Cascade throat total loss coefficnet.
-
-    Returns
-    -------
-    float
-        Critical mass flux.
-    float
-        Critical mach number.
-
-    """
-
-    x = np.array(
-        [
-            1,
-            p0_in,
-            T0_in,
-            Y,
-            p0_in**2,
-            T0_in**2,
-            Y**2,
-            p0_in * T0_in,
-            p0_in * Y,
-            T0_in * Y,
-        ]
-    )
-
-    coeff_mach_crit = np.array(
-        [
-            9.97808878e-01,
-            -8.59556818e-09,
-            2.18283101e-05,
-            -3.38413836e-01,
-            -4.89469816e-14,
-            -5.99021408e-08,
-            9.93519991e-02,
-            7.71201115e-11,
-            -4.13346725e-09,
-            4.91317761e-06,
-        ]
-    )
-
-    coeff_phi_max = np.array(
-        [
-            9.81120337e01,
-            3.46299580e-03,
-            -6.34357717e-01,
-            -6.84234362e01,
-            1.05506996e-11,
-            1.04045797e-03,
-            3.36231786e01,
-            -3.81019918e-06,
-            -6.90348074e-04,
-            1.26692586e-01,
-        ]
-    )
-
-    mach_crit = sum(x * coeff_mach_crit)
-    phi_max = sum(x * coeff_phi_max)
-
-    return phi_max, mach_crit
-
-
-def evaluate_cascade_critical(
-    choking_input,
-    inlet_plane,
-    exit_plane,
-    fluid,
-    geometry,
-    angular_speed,
-    model_options,
-    reference_values,
-):
-    r"""
-
-    Calculate condition for choking and evaluate wheter or not the cascade is choked, based on the `evaluate_cascade_critical` choking model.
-
-    This choking model evaluate the critical state by optimizing the mass flow rate at the throat through the method of lagrange multipliers, and checks if the throat mach number exceed the critical. 
-    The exit flow angle is calculated by the selected devaition model at subsonic condition, and from the critical mass flow rate at supersonic conditions. 
-
-    Compute the gradient of the Lagrange function of the critical mass flow rate and the residuals of mass
-    conservation and loss computation equations at the throat.
-
-    The constrained optimization problem to maximize the mass flow rate can be converted to a set of equations through the method of lagrange multipliers. The method
-    utlize that at the optimal point, the gradient of the constraints are proportional to the gradient fo the objective function. Thus the solution to the optimization problem 
-    eaulas the solution as the following set of equations: 
-        
-    .. math::
-
-        \nabla L = \begin{bmatrix}
-        \frac{\partial f}{\partial x_1} + \lambda_1 \cdot \frac{\partial g_1}{\partial x_1} + \lambda_2 \cdot \frac{\partial g_2}{\partial x_1} \\
-        \frac{\partial f}{\partial x_2} + \lambda_1 \cdot \frac{\partial g_1}{\partial x_2} + \lambda_2 \cdot \frac{\partial g_2}{\partial x_2} \\
-        \frac{\partial f}{\partial x_3} + \lambda_1 \cdot \frac{\partial g_1}{\partial x_3} + \lambda_2 \cdot \frac{\partial g_2}{\partial x_3} \\
-        g_1(x_1, x_2, x_3) \\
-        g_2(x_1, x_2, x_3) \\
-        \end{bmatrix} = 0
-        
-    This function returns the value of the three last equations in the set above, while the two first equations are used to 
-    explicitly calculate the lagrange multipliers.
-    
-    .. math::
-        \lambda_1 = \frac{\frac{\partial g_2}{\partial x_2}\cdot\frac{-\partial f}{\partial x_1} - \frac{\partial g_2}{\partial x_1}\cdot-\frac{\partial f}{\partial x_2}} 
-                     {\frac{\partial g_1}{\partial x_1}\cdot\frac{\partial g_2}{\partial x_2} - \frac{\partial g_2}{\partial x_1}\cdot\frac{\partial g_1}{\partial x_2}} \\                     
-        \lambda_2 = \frac{\frac{\partial g_1}{\partial x_1}\cdot\frac{-\partial f}{\partial x_2} - \frac{\partial g_1}{\partial x_2}\cdot-\frac{\partial f}{\partial x_1}} 
-                     {\frac{\partial g_1}{\partial x_1}\cdot\frac{\partial g_2}{\partial x_2} - \frac{\partial g_2}{\partial x_1}\cdot\frac{\partial g_1}{\partial x_2}} 
-
-    By transforming the problem into a system of equations, this approach allows the evaluation of the critical
-    point without directly solving an optimization problem. One significant advantage of this
-    equation-oriented method is that it enables the coupling of these critical condition equations with the
-    other modeling equations. This integrated system of equations can then be efficiently solved using gradient-based
-    root finding algorithms (e.g., Newton-Raphson solvers).
-
-    Such a coupled solution strategy, as opposed to segregated approaches where nested systems are solved
-    sequentially and iteratively, offers superior computational efficiency. This method thus provides
-    a more direct and computationally effective way of determining the critical conditions in a cascade.
-
-    Parameters
-    ----------
-    critical_cascade_input : dict
-        Dictionary containing scaled input parameters required for selected choking model. Required items are:
-
-        - `v*_in` : inlet velocity at critical state.  
-        - `w*_throat` : throat velocity at critical state. 
-        - `s*_throat` : throat entropy at critical state.
-    inlet_plane : dict
-        Dictionary containing data on the inlet plane for the actual cascade operating condition.
-    exit_plane : dict
-        Dictionary containing data on the exit plane for the actual cascade operating condition.
-    fluid : object
-        A fluid object with methods for thermodynamic property calculations.
-    geometry : dict
-        Geometric parameters of the cascade.
-    angular_speed : float
-        Angular speed of the cascade.
-    model_options : dict
-        Options for the model used in the critical condition evaluation.
-    reference_values : dict
-        Reference values used in the calculation, including the reference mass flow rate.
-
-    Returns
-    -------
-    dict
-        Dictionary containing the residuals of choking model:
-        
-        - `m*`: Mass flow rate residual.
-        - `Y*`: Loss error residual.
-        - `L*`: Residual of the lagrange function.
-        - `choking`: Choking residual.
-    dict
-        Dictionary containing state information on the critical conditions at inlet and throat plane. 
-
-    """
-
-    # Load reference values
-    mass_flow_ref = reference_values["mass_flow_ref"]
-
-    # Load model options
-    loss_model = model_options["loss_model"]
-    blockage = model_options["blockage_model"]
-    deviation_model = model_options["deviation_model"]
-
-    # Define critical state dictionary to store information
-    critical_state = {}
-
-    # Define array for inputs in compute critical values
-    x_crit = np.array(
-        [
-            choking_input["v*_in"],
-            choking_input["w*_throat"],
-            choking_input["s*_throat"],
-        ]
-    )
-
-    # Evaluate the current cascade at critical conditions
-    f0 = compute_critical_values(
-        x_crit,
-        inlet_plane,
-        fluid,
-        geometry,
-        angular_speed,
-        critical_state,
-        model_options,
-        reference_values,
-    )
-
-    # Evaluate the Jacobian of the evaluate_critical_cascade function
-    J = compute_critical_jacobian(
-        x_crit,
-        inlet_plane,
-        fluid,
-        geometry,
-        angular_speed,
-        critical_state,
-        model_options,
-        reference_values,
-        f0,
-    )
-
-    # Rename gradients
-    a11, a12, a21, a22, b1, b2 = (
-        J[1, 0],
-        J[2, 0],
-        J[1, 1 + 1],
-        J[2, 1 + 1],
-        -1 * J[0, 0],
-        -1 * J[0, 1 + 1],
-    )
-
-    # Calculate the Lagrange multipliers explicitly
-    determinant = a11 * a22 - a12 * a21
-    l1_det = a22 * b1 - a12 * b2
-    l2_det = a11 * b2 - a21 * b1
-
-    # Evaluate the last equation
-    df, dg1, dg2 = J[0, 2 - 1], J[1, 2 - 1], J[2, 2 - 1]
-    grad = (determinant * df + l1_det * dg1 + l2_det * dg2) / mass_flow_ref
-
-    critical_mach = critical_state["throat_plane"]["Ma_rel"]
-    critical_mass_flow = critical_state["throat_plane"]["mass_flow"]
-    if exit_plane["Ma_rel"] <= critical_mach:
-        beta_model = np.sign(exit_plane["beta"]) * dm.get_subsonic_deviation(
-            exit_plane["Ma_rel"], critical_mach, geometry, deviation_model
-        )
-    else:
-        beta_model = np.sign(exit_plane["beta"]) * math.arccosd(
-            critical_mass_flow / exit_plane["d"] / exit_plane["w"] / geometry["A_out"]
-        )
-    choking_residual = math.cosd(beta_model) - math.cosd(exit_plane["beta"])
-
-    # Restructure critical state dictionary
-    inlet_plane = {
-        f"critical_{key}_in": val for key, val in critical_state["inlet_plane"].items()
-    }
-    throat_plane = {
-        f"critical_{key}_throat": val
-        for key, val in critical_state["throat_plane"].items()
-    }
-    critical_state = {**inlet_plane, **throat_plane}
-
-    # Return last 3 equations of the Lagrangian gradient (df/dx2+l1*dg1/dx2+l2*dg2/dx2 and g1, g2)
-    g = f0[1:]  # The two constraints
-    residual_values = np.concatenate((g, np.array([grad, choking_residual])))
-    residual_keys = ["m*", "Y*", "L*", "choking"]
-    residuals_critical = dict(zip(residual_keys, residual_values))
-
-    return residuals_critical, critical_state
-
-
-def compute_critical_values(
-    x_crit,
-    inlet_plane,
-    fluid,
-    geometry,
-    angular_speed,
-    critical_state,
-    model_options,
-    reference_values,
-):
-    """
-    Compute cascade performance at the critical conditions
-
-    This function evaluates the performance of a cascade at its critical operating point defined by:
-
-        1. Critical inlet absolute velocity,
-        2. Critical throat relative velocity,
-        3. Critical throat entropy.
-
-    Using these variables, the function calculates the critical mass flow rate and residuals of the mass balance and the loss model equations.
-
-    Parameters
-    ----------
-    x_crit : numpy.ndarray
-        Array containing scaled critical variables `[v_in*, w_throat*, s_throat*]`.
-    inlet_plane : dict
-        Dictionary containing data on the inlet plane for the actual cascade operating condition.
-    fluid : object
-        A fluid object with methods for thermodynamic property calculations.
-    geometry : dict
-        Geometric parameters of the cascade.
-    angular_speed : float
-        Angular speed of the cascade.
-    critical_state : dict
-        Dictionary to store the critical state information.
-    model_options : dict
-        Options for the model used in the critical condition evaluation.
-    reference_values : dict
-        Reference values used in the calculations, including mass flow reference and other parameters.
-
-    Returns
-    -------
-    numpy.ndarray
-        An array containing the computed mass flow at the throat plane at critical state and the residuals
-        for mass conservation and loss coefficient error.
-
-    """
-
-    # Define model options
-    loss_model = model_options["loss_model"]
-
-    # Load reference values
-    mass_flow_ref = reference_values["mass_flow_ref"]
-    v0 = reference_values["v0"]
-    s_range = reference_values["s_range"]
-    s_min = reference_values["s_min"]
-
-    # Load input for critical cascade
-    s_in = inlet_plane["s"]
-    h0_in = inlet_plane["h0"]
-    alpha_in = inlet_plane["alpha"]
-    v_in, w_throat, s_throat = (
-        x_crit[0] * v0,
-        x_crit[1] * v0,
-        x_crit[2] * s_range + s_min,
-    )
-
-    # Evaluate inlet plane
-    critical_inlet_input = {
-        "v": v_in,
-        "s": s_in,
-        "h0": h0_in,
-        "alpha": alpha_in,
-    }
-    critical_inlet_plane = fm.evaluate_cascade_inlet(
-        critical_inlet_input, fluid, geometry, angular_speed
-    )
-
-    # Evaluate throat plane
-    critical_throat_input = {
-        "w": w_throat,
-        "s": s_throat,
-        "beta": np.sign(geometry["gauging_angle"])
-        * math.arccosd(geometry["A_throat"] / geometry["A_out"]),
-        "rothalpy": critical_inlet_plane["rothalpy"],
-    }
-
-    critical_throat_plane, loss_dict = fm.evaluate_cascade_throat(
-        critical_throat_input,
-        fluid,
-        geometry,
-        critical_inlet_plane,
-        angular_speed,
-        model_options["blockage_model"],
-        loss_model,
-    )
-
-    # Add residuals
-    residuals = np.array(
-        [
-            (critical_inlet_plane["mass_flow"] - critical_throat_plane["mass_flow"])
-            / mass_flow_ref,
-            critical_throat_plane["loss_error"],
-        ]
-    )
-
-    # Update critical state dictionary
-    critical_state["inlet_plane"] = critical_inlet_plane
-    critical_state["throat_plane"] = critical_throat_plane
-
-    output = np.insert(residuals, 0, critical_throat_plane["mass_flow"])
-
-    return output
-
-
-def compute_critical_jacobian(
-    x,
-    inlet_plane,
-    fluid,
-    geometry,
-    angular_speed,
-    critical_state,
-    model_options,
-    reference_values,
-    f0,
-):
-    """
-    Compute the Jacobian matrix of the compute_critical_values function using finite differences.
-
-    This function approximates the Jacobian of a combined function that includes the mass flow rate value,
-    mass balance residual, and loss model evaluation residual at the critical point. It uses forward finite
-    difference to approximate the partial derivatives of the Jacobian matrix.
-
-    Parameters
-    ----------
-    x : numpy.ndarray
-        Array of input variables for the `compute_critical_values` function.
-    inlet_plane : dict
-        Dictionary containing data on the inlet plane for the actual cascade operating condition.
-    fluid : object
-        A fluid object with methods for thermodynamic property calculations.
-    geometry : dict
-        Geometric parameters of the cascade.
-    angular_speed : float
-        Angular speed of the cascade.
-    critical_state : dict
-        Dictionary to store the critical state information.
-    model_options : dict
-        Options for the model used in the critical condition evaluation.
-    reference_values : dict
-        Reference values used in the calculations, including mass flow reference and other parameters.
-    f0 : numpy.ndarray
-        The function value at x, used for finite difference approximation.
-
-    Returns
-    -------
-    numpy.ndarray
-        The approximated Jacobian matrix of the compute_critical_values function.
-
-    """
-
-    # Define finite difference relative step size
-    eps = model_options["rel_step_fd"] * x
-
-    # Approximate problem Jacobian by finite differences
-    jacobian = approx_derivative(
-        compute_critical_values,
-        x,
-        method="2-point",
-        f0=f0,
-        abs_step=eps,
-        args=(
-            inlet_plane,
-            fluid,
-            geometry,
-            angular_speed,
-            critical_state,
-            model_options,
-            reference_values,
-        ),
-    )
-
-    return jacobian
-
-
-def evaluate_cascade_isentropic_throat(
-    choking_input,
-    inlet_plane,
-    exit_plane,
-    fluid,
-    geometry,
-    angular_speed,
-    model_options,
-    reference_values,
-):
-    r"""
-
-    Calculate condition for choking and evaluate wheter or not the cascade is choked, based on the `evaluate_cascade_isentropic_throat` choking model.
-
-    This choking model evaluates the cascade throat and checks if the throat mach number exceed unity. The throat is isentropic from inlet to throat.
-    The exit flow angle is determined by ensuring that the mach at throat mach number equals the exit for subconic condition, and unity for supersonic conditions.
-
-    Parameters
-    ----------
-    choking_input : dict
-        Dictionary containing scaled input parameters required for selected choking model. Required items are:
-
-        - `w*_throat` : throat velocity.
-    inlet_plane : dict
-        Dictionary containing data on the inlet plane for the actual cascade operating condition.
-    exit_plane : dict
-        Dictionary containing data on the exit plane for the actual cascade operating condition.
-    fluid : object
-        A fluid object with methods for thermodynamic property calculations.
-    geometry : dict
-        Geometric parameters of the cascade.
-    angular_speed : float
-        Angular speed of the cascade.
-    model_options : dict
-        Options for the model used to evaluate choking.
-    reference_values : dict
-        Reference values used in the calculation, including the reference mass flow rate.
-
-    Returns
-    -------
-    dict
-        Dictionary containing the residuals of choking model:
-
-        - `m*`: Mass flow rate residual.
-        - `choking`: Choking residual.
-    dict
-        Dictionary satisfying the required elements for critical state dict.
-
-    """
-
-    # Rename variables
-    loss_model = model_options["loss_model"]
-    blockage = model_options["blockage_model"]
-    deviation_model = model_options["deviation_model"]
-    A_throat = geometry["A_throat"]
-    A_out = geometry["A_out"]
-    v0 = reference_values["v0"]
-    angle_range = reference_values["angle_range"]
-    angle_min = reference_values["angle_min"]
-
-    # Evaluate throat
-    cascade_throat_input = {
-        "w": choking_input["w*_throat"] * v0,
-        "s": inlet_plane["s"],
-        "beta": np.sign(exit_plane["beta"])
-        * math.arccosd(geometry["A_throat"] / geometry["A_out"]),
-        "rothalpy": inlet_plane["rothalpy"],
-    }
-    throat_plane, loss_dict = fm.evaluate_cascade_throat(
-        cascade_throat_input,
-        fluid,
-        geometry,
-        inlet_plane,
-        angular_speed,
-        blockage,
-        loss_model,
-    )
-
-    # Evaluate critical mach
-    choking_residual = throat_plane["Ma_rel"] - min(exit_plane["Ma_rel"], 1)
-
-    # Evaluate resiudals
-    residual_values = np.array(
-        [
-            (inlet_plane["mass_flow"] - throat_plane["mass_flow"])
-            / reference_values["mass_flow_ref"],
-            choking_residual,
-        ]
-    )
-    residual_keys = ["m*", "choking"]
-    residuals_critical = dict(zip(residual_keys, residual_values))
-
-    # Define output values
-    throat_plane = {f"{key}_throat": val for key, val in throat_plane.items()}
-
-    return residuals_critical, throat_plane
+from scipy.optimize._numdiff import approx_derivative
+from scipy import interpolate
+import numpy as np
+
+from .. import math
+from . import flow_model as fm
+from . import deviation_model as dm
+
+CHOKING_MODELS = [
+    "evaluate_cascade_critical",
+    "evaluate_cascade_throat",
+    "evaluate_cascade_isentropic_throat",
+]
+
+
+def evaluate_choking(
+    choking_input,
+    inlet_plane,
+    exit_plane,
+    fluid,
+    geometry,
+    angular_speed,
+    model_options,
+    reference_values,
+):
+    r"""
+
+    Calculate condition for choking and evaluate wheter or not the cascade is choked, based on selected choking model.
+
+    Parameters
+    ----------
+    choking_input : dict
+        Dictionary containing necessary input parameters required for selected choking model. See each models documentation for specifics.
+    inlet_plane : dict
+        Dictionary containing data on the inlet plane for the actual cascade operating condition.
+    exit_plane : dict
+        Dictionary containing data on the exit plane for the actual cascade operating condition.
+    fluid : object
+        A fluid object with methods for thermodynamic property calculations.
+    geometry : dict
+        Geometric parameters of the cascade.
+    angular_speed : float
+        Angular speed of the cascade.
+    model_options : dict
+        Options for the model used to evaluate choking.
+    reference_values : dict
+        Reference values used in the calculation, including the reference mass flow rate.
+
+    Returns
+    -------
+    dict
+        Dictionary containing the residuals of choking model
+    dict
+        Dictionary containing state information on the critical conditions.
+
+    Raises
+    ------
+    ValueError
+        If an invalid choking model is provided.
+
+    """
+
+    critical_cascade_functions = {
+        CHOKING_MODELS[0]: evaluate_cascade_critical,
+        CHOKING_MODELS[1]: evaluate_cascade_throat,
+        CHOKING_MODELS[2]: evaluate_cascade_isentropic_throat,
+    }
+
+    # Evaluate loss model
+    model = model_options["choking_model"]
+    if model in critical_cascade_functions:
+        return critical_cascade_functions[model](
+            choking_input,
+            inlet_plane,
+            exit_plane,
+            fluid,
+            geometry,
+            angular_speed,
+            model_options,
+            reference_values,
+        )
+    else:
+        options = ", ".join(f"'{k}'" for k in CHOKING_MODELS)
+        raise ValueError(
+            f"Invalid critical cascade model '{model}'. Available options: {options}"
+        )
+
+
+def evaluate_cascade_throat(
+    choking_input,
+    inlet_plane,
+    exit_plane,
+    fluid,
+    geometry,
+    angular_speed,
+    model_options,
+    reference_values,
+):
+    r"""
+
+    Calculate condition for choking and evaluate wheter or not the cascade is choked, based on the evaluate_cascade_throat choking model.
+
+    This choking model evaluates the cascade throat and checks if the throat mach number exceed the critical. The critical mach number is calculated
+    from a correlation depending on the throat loss coefficient. The exit flow angle is calculated by the selected deviation model at subsonic condition,
+    and by ensuring that the mach at the throat equals the critical at supercritical conditions.
+
+    Parameters
+    ----------
+    choking_input : dict
+        Dictionary containing scaled input parameters required for selected choking model. Required items are:
+
+        - `w*_throat` : throat velocity.
+        - `s*_throat` : throat entropy.
+        - `beta*_throat` : throat relative flow angle.
+    inlet_plane : dict
+        Dictionary containing data on the inlet plane for the actual cascade operating condition.
+    exit_plane : dict
+        Dictionary containing data on the exit plane for the actual cascade operating condition.
+    fluid : object
+        A fluid object with methods for thermodynamic property calculations.
+    geometry : dict
+        Geometric parameters of the cascade.
+    angular_speed : float
+        Angular speed of the cascade.
+    model_options : dict
+        Options for the model used to evaluate choking.
+    reference_values : dict
+        Reference values used in the calculation, including the reference mass flow rate.
+
+    Returns
+    -------
+    dict
+        Dictionary containing the residuals of choking model:
+
+        - `m*`: Mass flow rate residual.
+        - `Y*`: Loss error residual.
+        - `beta*`: Residual of the flow angle.
+        - `choking`: Choking residual.
+    dict
+        Dictionary containing relevant information on the critical state and throat plane.
+
+    """
+
+    # Rename variables
+    loss_model = model_options["loss_model"]
+    blockage = model_options["blockage_model"]
+    deviation_model = model_options["deviation_model"]
+    A_throat = geometry["A_throat"]
+    A_out = geometry["A_out"]
+    v0 = reference_values["v0"]
+    s_range = reference_values["s_range"]
+    s_min = reference_values["s_min"]
+    angle_range = reference_values["angle_range"]
+    angle_min = reference_values["angle_min"]
+
+    # Evaluate throat
+    cascade_throat_input = {
+        "w": choking_input["w*_throat"] * v0,
+        "s": choking_input["s*_throat"] * s_range + s_min,
+        "beta": choking_input["beta*_throat"] * angle_range + angle_min,
+        "rothalpy": inlet_plane["rothalpy"],
+    }
+    throat_plane, loss_dict = fm.evaluate_cascade_throat(
+        cascade_throat_input,
+        fluid,
+        geometry,
+        inlet_plane,
+        angular_speed,
+        blockage,
+        loss_model,
+    )
+
+    # Evaluate critical mach
+    Y_tot = loss_dict["loss_total"]
+    critical_mass_flux, critical_mach = interpolate_critical_state(
+        inlet_plane["p0_rel"], inlet_plane["T0_rel"], Y_tot
+    )
+
+    # Evaluate residual flow angle
+    beta_model = np.sign(throat_plane["beta"]) * dm.get_subsonic_deviation(
+        throat_plane["Ma_rel"], critical_mach, geometry, deviation_model
+    )
+    beta_residual = math.cosd(beta_model) - math.cosd(throat_plane["beta"])
+
+    # Evaluate if flow cascade is choked or not an add choking residual
+    if exit_plane["Ma_rel"] <= critical_mach:
+        beta_model = np.sign(exit_plane["beta"]) * dm.get_subsonic_deviation(
+            exit_plane["Ma_rel"], critical_mach, geometry, deviation_model
+        )
+        # Compute error of guessed beta and deviation model
+        choking_residual = math.cosd(beta_model) - math.cosd(exit_plane["beta"])
+    else:
+        choking_residual = throat_plane["Ma_rel"] - critical_mach
+
+    # Evaluate resiudals
+    residual_values = np.array(
+        [
+            (inlet_plane["mass_flow"] - throat_plane["mass_flow"])
+            / reference_values["mass_flow_ref"],
+            throat_plane["loss_error"],
+            beta_residual,
+            choking_residual,
+        ]
+    )
+    residual_keys = ["m*", "Y*", "beta*", "choking"]
+    residuals_critical = dict(zip(residual_keys, residual_values))
+
+    # Define output values
+    critical_state = {
+        "critical_mach": critical_mach,
+        "critical_mass_flow_rate": critical_mass_flux * A_throat,
+    }
+    throat_plane = {f"{key}_throat": val for key, val in throat_plane.items()}
+
+    return residuals_critical, {**critical_state, **throat_plane}
+
+
+def interpolate_critical_state(p0_in, T0_in, Y):
+    r"""
+    Compute the critical mass flux and mach number from a correlation depending on the cascade inlet stagnation state and loss coefficient.
+    The correlation is made from linear regression analysis, and the regression coefficients are prescribed in the function.
+
+    Parameters
+    ----------
+    p0_in : float
+        Cascade inlet stagnation pressure.
+    T0_in : float
+        Cascade inlet stagnation temperature.
+    Y : float
+        Cascade throat total loss coefficnet.
+
+    Returns
+    -------
+    float
+        Critical mass flux.
+    float
+        Critical mach number.
+
+    """
+
+    x = np.array(
+        [
+            1,
+            p0_in,
+            T0_in,
+            Y,
+            p0_in**2,
+            T0_in**2,
+            Y**2,
+            p0_in * T0_in,
+            p0_in * Y,
+            T0_in * Y,
+        ]
+    )
+
+    coeff_mach_crit = np.array(
+        [
+            9.97808878e-01,
+            -8.59556818e-09,
+            2.18283101e-05,
+            -3.38413836e-01,
+            -4.89469816e-14,
+            -5.99021408e-08,
+            9.93519991e-02,
+            7.71201115e-11,
+            -4.13346725e-09,
+            4.91317761e-06,
+        ]
+    )
+
+    coeff_phi_max = np.array(
+        [
+            9.81120337e01,
+            3.46299580e-03,
+            -6.34357717e-01,
+            -6.84234362e01,
+            1.05506996e-11,
+            1.04045797e-03,
+            3.36231786e01,
+            -3.81019918e-06,
+            -6.90348074e-04,
+            1.26692586e-01,
+        ]
+    )
+
+    mach_crit = sum(x * coeff_mach_crit)
+    phi_max = sum(x * coeff_phi_max)
+
+    return phi_max, mach_crit
+
+
+def evaluate_cascade_critical(
+    choking_input,
+    inlet_plane,
+    exit_plane,
+    fluid,
+    geometry,
+    angular_speed,
+    model_options,
+    reference_values,
+):
+    r"""
+
+    Calculate condition for choking and evaluate wheter or not the cascade is choked, based on the `evaluate_cascade_critical` choking model.
+
+    This choking model evaluate the critical state by optimizing the mass flow rate at the throat through the method of lagrange multipliers, and checks if the throat mach number exceed the critical. 
+    The exit flow angle is calculated by the selected devaition model at subsonic condition, and from the critical mass flow rate at supersonic conditions. 
+
+    Compute the gradient of the Lagrange function of the critical mass flow rate and the residuals of mass
+    conservation and loss computation equations at the throat.
+
+    The constrained optimization problem to maximize the mass flow rate can be converted to a set of equations through the method of lagrange multipliers. The method
+    utlize that at the optimal point, the gradient of the constraints are proportional to the gradient fo the objective function. Thus the solution to the optimization problem 
+    eaulas the solution as the following set of equations: 
+        
+    .. math::
+
+        \nabla L = \begin{bmatrix}
+        \frac{\partial f}{\partial x_1} + \lambda_1 \cdot \frac{\partial g_1}{\partial x_1} + \lambda_2 \cdot \frac{\partial g_2}{\partial x_1} \\
+        \frac{\partial f}{\partial x_2} + \lambda_1 \cdot \frac{\partial g_1}{\partial x_2} + \lambda_2 \cdot \frac{\partial g_2}{\partial x_2} \\
+        \frac{\partial f}{\partial x_3} + \lambda_1 \cdot \frac{\partial g_1}{\partial x_3} + \lambda_2 \cdot \frac{\partial g_2}{\partial x_3} \\
+        g_1(x_1, x_2, x_3) \\
+        g_2(x_1, x_2, x_3) \\
+        \end{bmatrix} = 0
+        
+    This function returns the value of the three last equations in the set above, while the two first equations are used to 
+    explicitly calculate the lagrange multipliers.
+    
+    .. math::
+        \lambda_1 = \frac{\frac{\partial g_2}{\partial x_2}\cdot\frac{-\partial f}{\partial x_1} - \frac{\partial g_2}{\partial x_1}\cdot-\frac{\partial f}{\partial x_2}} 
+                     {\frac{\partial g_1}{\partial x_1}\cdot\frac{\partial g_2}{\partial x_2} - \frac{\partial g_2}{\partial x_1}\cdot\frac{\partial g_1}{\partial x_2}} \\                     
+        \lambda_2 = \frac{\frac{\partial g_1}{\partial x_1}\cdot\frac{-\partial f}{\partial x_2} - \frac{\partial g_1}{\partial x_2}\cdot-\frac{\partial f}{\partial x_1}} 
+                     {\frac{\partial g_1}{\partial x_1}\cdot\frac{\partial g_2}{\partial x_2} - \frac{\partial g_2}{\partial x_1}\cdot\frac{\partial g_1}{\partial x_2}} 
+
+    By transforming the problem into a system of equations, this approach allows the evaluation of the critical
+    point without directly solving an optimization problem. One significant advantage of this
+    equation-oriented method is that it enables the coupling of these critical condition equations with the
+    other modeling equations. This integrated system of equations can then be efficiently solved using gradient-based
+    root finding algorithms (e.g., Newton-Raphson solvers).
+
+    Such a coupled solution strategy, as opposed to segregated approaches where nested systems are solved
+    sequentially and iteratively, offers superior computational efficiency. This method thus provides
+    a more direct and computationally effective way of determining the critical conditions in a cascade.
+
+    Parameters
+    ----------
+    critical_cascade_input : dict
+        Dictionary containing scaled input parameters required for selected choking model. Required items are:
+
+        - `v*_in` : inlet velocity at critical state.  
+        - `w*_throat` : throat velocity at critical state. 
+        - `s*_throat` : throat entropy at critical state.
+    inlet_plane : dict
+        Dictionary containing data on the inlet plane for the actual cascade operating condition.
+    exit_plane : dict
+        Dictionary containing data on the exit plane for the actual cascade operating condition.
+    fluid : object
+        A fluid object with methods for thermodynamic property calculations.
+    geometry : dict
+        Geometric parameters of the cascade.
+    angular_speed : float
+        Angular speed of the cascade.
+    model_options : dict
+        Options for the model used in the critical condition evaluation.
+    reference_values : dict
+        Reference values used in the calculation, including the reference mass flow rate.
+
+    Returns
+    -------
+    dict
+        Dictionary containing the residuals of choking model:
+        
+        - `m*`: Mass flow rate residual.
+        - `Y*`: Loss error residual.
+        - `L*`: Residual of the lagrange function.
+        - `choking`: Choking residual.
+    dict
+        Dictionary containing state information on the critical conditions at inlet and throat plane. 
+
+    """
+
+    # Load reference values
+    mass_flow_ref = reference_values["mass_flow_ref"]
+
+    # Load model options
+    loss_model = model_options["loss_model"]
+    blockage = model_options["blockage_model"]
+    deviation_model = model_options["deviation_model"]
+
+    # Define critical state dictionary to store information
+    critical_state = {}
+
+    # Define array for inputs in compute critical values
+    x_crit = np.array(
+        [
+            choking_input["v*_in"],
+            choking_input["w*_throat"],
+            choking_input["s*_throat"],
+        ]
+    )
+
+    # Evaluate the current cascade at critical conditions
+    f0 = compute_critical_values(
+        x_crit,
+        inlet_plane,
+        fluid,
+        geometry,
+        angular_speed,
+        critical_state,
+        model_options,
+        reference_values,
+    )
+
+    # Evaluate the Jacobian of the evaluate_critical_cascade function
+    J = compute_critical_jacobian(
+        x_crit,
+        inlet_plane,
+        fluid,
+        geometry,
+        angular_speed,
+        critical_state,
+        model_options,
+        reference_values,
+        f0,
+    )
+
+    # Rename gradients
+    a11, a12, a21, a22, b1, b2 = (
+        J[1, 0],
+        J[2, 0],
+        J[1, 1 + 1],
+        J[2, 1 + 1],
+        -1 * J[0, 0],
+        -1 * J[0, 1 + 1],
+    )
+
+    # Calculate the Lagrange multipliers explicitly
+    determinant = a11 * a22 - a12 * a21
+    l1_det = a22 * b1 - a12 * b2
+    l2_det = a11 * b2 - a21 * b1
+
+    # Evaluate the last equation
+    df, dg1, dg2 = J[0, 2 - 1], J[1, 2 - 1], J[2, 2 - 1]
+    grad = (determinant * df + l1_det * dg1 + l2_det * dg2) / mass_flow_ref
+
+    critical_mach = critical_state["throat_plane"]["Ma_rel"]
+    critical_mass_flow = critical_state["throat_plane"]["mass_flow"]
+    if exit_plane["Ma_rel"] <= critical_mach:
+        beta_model = np.sign(exit_plane["beta"]) * dm.get_subsonic_deviation(
+            exit_plane["Ma_rel"], critical_mach, geometry, deviation_model
+        )
+    else:
+        beta_model = np.sign(exit_plane["beta"]) * math.arccosd(
+            critical_mass_flow / exit_plane["d"] / exit_plane["w"] / geometry["A_out"]
+        )
+    choking_residual = math.cosd(beta_model) - math.cosd(exit_plane["beta"])
+
+    # Restructure critical state dictionary
+    inlet_plane = {
+        f"critical_{key}_in": val for key, val in critical_state["inlet_plane"].items()
+    }
+    throat_plane = {
+        f"critical_{key}_throat": val
+        for key, val in critical_state["throat_plane"].items()
+    }
+    critical_state = {**inlet_plane, **throat_plane}
+
+    # Return last 3 equations of the Lagrangian gradient (df/dx2+l1*dg1/dx2+l2*dg2/dx2 and g1, g2)
+    g = f0[1:]  # The two constraints
+    residual_values = np.concatenate((g, np.array([grad, choking_residual])))
+    residual_keys = ["m*", "Y*", "L*", "choking"]
+    residuals_critical = dict(zip(residual_keys, residual_values))
+
+    return residuals_critical, critical_state
+
+
+def compute_critical_values(
+    x_crit,
+    inlet_plane,
+    fluid,
+    geometry,
+    angular_speed,
+    critical_state,
+    model_options,
+    reference_values,
+):
+    """
+    Compute cascade performance at the critical conditions
+
+    This function evaluates the performance of a cascade at its critical operating point defined by:
+
+        1. Critical inlet absolute velocity,
+        2. Critical throat relative velocity,
+        3. Critical throat entropy.
+
+    Using these variables, the function calculates the critical mass flow rate and residuals of the mass balance and the loss model equations.
+
+    Parameters
+    ----------
+    x_crit : numpy.ndarray
+        Array containing scaled critical variables `[v_in*, w_throat*, s_throat*]`.
+    inlet_plane : dict
+        Dictionary containing data on the inlet plane for the actual cascade operating condition.
+    fluid : object
+        A fluid object with methods for thermodynamic property calculations.
+    geometry : dict
+        Geometric parameters of the cascade.
+    angular_speed : float
+        Angular speed of the cascade.
+    critical_state : dict
+        Dictionary to store the critical state information.
+    model_options : dict
+        Options for the model used in the critical condition evaluation.
+    reference_values : dict
+        Reference values used in the calculations, including mass flow reference and other parameters.
+
+    Returns
+    -------
+    numpy.ndarray
+        An array containing the computed mass flow at the throat plane at critical state and the residuals
+        for mass conservation and loss coefficient error.
+
+    """
+
+    # Define model options
+    loss_model = model_options["loss_model"]
+
+    # Load reference values
+    mass_flow_ref = reference_values["mass_flow_ref"]
+    v0 = reference_values["v0"]
+    s_range = reference_values["s_range"]
+    s_min = reference_values["s_min"]
+
+    # Load input for critical cascade
+    s_in = inlet_plane["s"]
+    h0_in = inlet_plane["h0"]
+    alpha_in = inlet_plane["alpha"]
+    v_in, w_throat, s_throat = (
+        x_crit[0] * v0,
+        x_crit[1] * v0,
+        x_crit[2] * s_range + s_min,
+    )
+
+    # Evaluate inlet plane
+    critical_inlet_input = {
+        "v": v_in,
+        "s": s_in,
+        "h0": h0_in,
+        "alpha": alpha_in,
+    }
+    critical_inlet_plane = fm.evaluate_cascade_inlet(
+        critical_inlet_input, fluid, geometry, angular_speed
+    )
+
+    # Evaluate throat plane
+    critical_throat_input = {
+        "w": w_throat,
+        "s": s_throat,
+        "beta": np.sign(geometry["gauging_angle"])
+        * math.arccosd(geometry["A_throat"] / geometry["A_out"]),
+        "rothalpy": critical_inlet_plane["rothalpy"],
+    }
+
+    critical_throat_plane, loss_dict = fm.evaluate_cascade_throat(
+        critical_throat_input,
+        fluid,
+        geometry,
+        critical_inlet_plane,
+        angular_speed,
+        model_options["blockage_model"],
+        loss_model,
+    )
+
+    # Add residuals
+    residuals = np.array(
+        [
+            (critical_inlet_plane["mass_flow"] - critical_throat_plane["mass_flow"])
+            / mass_flow_ref,
+            critical_throat_plane["loss_error"],
+        ]
+    )
+
+    # Update critical state dictionary
+    critical_state["inlet_plane"] = critical_inlet_plane
+    critical_state["throat_plane"] = critical_throat_plane
+
+    output = np.insert(residuals, 0, critical_throat_plane["mass_flow"])
+
+    return output
+
+
+def compute_critical_jacobian(
+    x,
+    inlet_plane,
+    fluid,
+    geometry,
+    angular_speed,
+    critical_state,
+    model_options,
+    reference_values,
+    f0,
+):
+    """
+    Compute the Jacobian matrix of the compute_critical_values function using finite differences.
+
+    This function approximates the Jacobian of a combined function that includes the mass flow rate value,
+    mass balance residual, and loss model evaluation residual at the critical point. It uses forward finite
+    difference to approximate the partial derivatives of the Jacobian matrix.
+
+    Parameters
+    ----------
+    x : numpy.ndarray
+        Array of input variables for the `compute_critical_values` function.
+    inlet_plane : dict
+        Dictionary containing data on the inlet plane for the actual cascade operating condition.
+    fluid : object
+        A fluid object with methods for thermodynamic property calculations.
+    geometry : dict
+        Geometric parameters of the cascade.
+    angular_speed : float
+        Angular speed of the cascade.
+    critical_state : dict
+        Dictionary to store the critical state information.
+    model_options : dict
+        Options for the model used in the critical condition evaluation.
+    reference_values : dict
+        Reference values used in the calculations, including mass flow reference and other parameters.
+    f0 : numpy.ndarray
+        The function value at x, used for finite difference approximation.
+
+    Returns
+    -------
+    numpy.ndarray
+        The approximated Jacobian matrix of the compute_critical_values function.
+
+    """
+
+    # Define finite difference relative step size
+    eps = model_options["rel_step_fd"] * x
+
+    # Approximate problem Jacobian by finite differences
+    jacobian = approx_derivative(
+        compute_critical_values,
+        x,
+        method="2-point",
+        f0=f0,
+        abs_step=eps,
+        args=(
+            inlet_plane,
+            fluid,
+            geometry,
+            angular_speed,
+            critical_state,
+            model_options,
+            reference_values,
+        ),
+    )
+
+    return jacobian
+
+
+def evaluate_cascade_isentropic_throat(
+    choking_input,
+    inlet_plane,
+    exit_plane,
+    fluid,
+    geometry,
+    angular_speed,
+    model_options,
+    reference_values,
+):
+    r"""
+
+    Calculate condition for choking and evaluate wheter or not the cascade is choked, based on the `evaluate_cascade_isentropic_throat` choking model.
+
+    This choking model evaluates the cascade throat and checks if the throat mach number exceed unity. The throat is isentropic from inlet to throat.
+    The exit flow angle is determined by ensuring that the mach at throat mach number equals the exit for subconic condition, and unity for supersonic conditions.
+
+    Parameters
+    ----------
+    choking_input : dict
+        Dictionary containing scaled input parameters required for selected choking model. Required items are:
+
+        - `w*_throat` : throat velocity.
+    inlet_plane : dict
+        Dictionary containing data on the inlet plane for the actual cascade operating condition.
+    exit_plane : dict
+        Dictionary containing data on the exit plane for the actual cascade operating condition.
+    fluid : object
+        A fluid object with methods for thermodynamic property calculations.
+    geometry : dict
+        Geometric parameters of the cascade.
+    angular_speed : float
+        Angular speed of the cascade.
+    model_options : dict
+        Options for the model used to evaluate choking.
+    reference_values : dict
+        Reference values used in the calculation, including the reference mass flow rate.
+
+    Returns
+    -------
+    dict
+        Dictionary containing the residuals of choking model:
+
+        - `m*`: Mass flow rate residual.
+        - `choking`: Choking residual.
+    dict
+        Dictionary satisfying the required elements for critical state dict.
+
+    """
+
+    # Rename variables
+    loss_model = model_options["loss_model"]
+    blockage = model_options["blockage_model"]
+    deviation_model = model_options["deviation_model"]
+    A_throat = geometry["A_throat"]
+    A_out = geometry["A_out"]
+    v0 = reference_values["v0"]
+    angle_range = reference_values["angle_range"]
+    angle_min = reference_values["angle_min"]
+
+    # Evaluate throat
+    cascade_throat_input = {
+        "w": choking_input["w*_throat"] * v0,
+        "s": inlet_plane["s"],
+        "beta": np.sign(exit_plane["beta"])
+        * math.arccosd(geometry["A_throat"] / geometry["A_out"]),
+        "rothalpy": inlet_plane["rothalpy"],
+    }
+    throat_plane, loss_dict = fm.evaluate_cascade_throat(
+        cascade_throat_input,
+        fluid,
+        geometry,
+        inlet_plane,
+        angular_speed,
+        blockage,
+        loss_model,
+    )
+
+    # Evaluate critical mach
+    choking_residual = throat_plane["Ma_rel"] - min(exit_plane["Ma_rel"], 1)
+
+    # Evaluate resiudals
+    residual_values = np.array(
+        [
+            (inlet_plane["mass_flow"] - throat_plane["mass_flow"])
+            / reference_values["mass_flow_ref"],
+            choking_residual,
+        ]
+    )
+    residual_keys = ["m*", "choking"]
+    residuals_critical = dict(zip(residual_keys, residual_values))
+
+    # Define output values
+    throat_plane = {f"{key}_throat": val for key, val in throat_plane.items()}
+
+    return residuals_critical, throat_plane
```

### Comparing `turboflow-0.1.2/turboflow/axial_turbine/design_optimization.py` & `turboflow-0.1.3/turboflow/axial_turbine/design_optimization.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,788 +1,788 @@
-import numpy as np
-import pandas as pd
-import CoolProp as cp
-import datetime
-import os
-import yaml
-from .. import pysolver_view as psv
-from .. import utilities as utils
-from . import geometry_model as geom
-from . import flow_model as flow
-from .. import properties as props
-from . import performance_analysis as pa
-
-
-CONSTRAINTS = ["mass_flow_rate", "interstage_flaring"]
-AVAILABLE_INEQ_CONSTRAINTS = ["mass_flow_rate", "interstage_flaring"]
-OBJECTIVE_FUNCTIONS = ["none", "efficiency_ts"]
-RADIUS_TYPE = ["constant_mean", "constant_hub", "constant_tip"]
-ANGLE_KEYS = ["leading_edge_angle", "gauging_angle"]
-INDEPENDENT_VARIABLES = [
-    "v_in",
-    "w_out",
-    "s_out",
-    "beta_out",
-    "v*_in",
-    "beta*_throat",
-    "w*_throat",
-    "s*_throat",
-]
-INDEXED_VARIABLES = [
-    "hub_tip_ratio_in",
-    "hub_tip_ratio_out",
-    "aspect_ratio",
-    "pitch_chord_ratio",
-    "trailing_edge_thickness_opening_ratio",
-    "leading_edge_angle",
-    "gauging_angle",
-    "throat_location_fraction",
-    "leading_edge_diameter",
-    "leading_edge_wedge_angle",
-    "tip_clearance",
-    "cascade_type",
-]
-VARIABLES = (
-    INDEXED_VARIABLES + ["specific_speed", "blade_jet_ratio"] + INDEPENDENT_VARIABLES
-)
-
-
-def compute_optimal_turbine(
-    config,
-    out_filename=None,
-    out_dir="output",
-    export_results=True,
-):
-    r"""
-    Calculate the optimal turbine configuration based on the specified optimization problem.
-
-    The function checks the configuration file before performing design optimization.
-
-    Parameters
-    ----------
-    config : dict
-        A dictionary containing necessary configuration options for computing optimal turbine.
-    initial_guess : None or dict
-        The initial guess of the design optimization. If None, a defualt initial guess is provided. If given, he initial guess must correspond
-        with the given set of design variables given in the configuration dictionary.
-
-    Returns
-    -------
-    solution : object
-        The solution object containing the results of the design optimization.
-    """
-
-    # Initialize problem object
-    problem = CascadesOptimizationProblem(config)
-
-    # Perform initial function call to initialize problem
-    # This populates the arrays of equality and inequality constraints
-    # TODO: it might be more intuitive to create a new method called initialize_problem() that generates the initial guess and evaluates the fitness() function with it
-    problem.fitness(problem.initial_guess)
-
-    # Load solver configuration
-    solver_config = config["design_optimization"]["solver_options"]
-
-    # Initialize solver object using keyword-argument dictionary unpacking
-    solver = psv.OptimizationSolver(problem, **solver_config)
-
-    # Solve optimization problem for initial guess x0
-    solver.solve(problem.initial_guess)
-
-    dfs = {
-        "operation point": pd.DataFrame(
-            {key: pd.Series(val) for key, val in problem.boundary_conditions.items()}
-        ),
-        "overall": pd.DataFrame(problem.results["overall"], index=[0]),
-        "plane": problem.results["plane"],
-        "cascade": problem.results["cascade"],
-        "stage": problem.results["stage"],
-        "geometry": pd.DataFrame(
-            {key: pd.Series(val) for key, val in problem.geometry.items()}
-        ),
-        "solver": pd.DataFrame(
-            {
-                "completed": pd.Series(True, index=[0]),
-                "success": pd.Series(solver.success, index=[0]),
-                "message": pd.Series(solver.message, index=[0]),
-                "grad_count": solver.convergence_history["grad_count"],
-                "func_count": solver.convergence_history["func_count"],
-                "func_count_total": solver.convergence_history["func_count_total"],
-                "objective_value": solver.convergence_history["objective_value"],
-                "constraint_violation": solver.convergence_history[
-                    "constraint_violation"
-                ],
-                "norm_step": solver.convergence_history["norm_step"],
-            },
-            index=range(len(solver.convergence_history["grad_count"])),
-        ),
-    }
-
-    if export_results:
-        # Create a directory to save simulation results
-        if not os.path.exists(out_dir):
-            os.makedirs(out_dir)
-
-        # Define filename with unique date-time identifier
-        if out_filename == None:
-            current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
-            out_filename = f"design_optimization_{current_time}"
-
-        # Export simulation configuration as YAML file
-        config_data = {k: v for k, v in config.items() if v}  # Filter empty entries
-        config_data = utils.convert_numpy_to_python(config_data, precision=12)
-        config_file = os.path.join(out_dir, f"{out_filename}.yaml")
-        with open(config_file, "w") as file:
-            yaml.dump(config_data, file, default_flow_style=False, sort_keys=False)
-
-        # Export optimal turbine in excel file
-        filepath = os.path.join(out_dir, f"{out_filename}.xlsx")
-        with pd.ExcelWriter(filepath, engine="openpyxl") as writer:
-            for sheet_name, df in dfs.items():
-                df.to_excel(writer, sheet_name=sheet_name, index=True)
-
-    return solver
-
-
-class CascadesOptimizationProblem(psv.OptimizationProblem):
-    """
-    A class representing a turbine design optimization problem.
-
-    This class is designed for solving design optimization problems of axial turbines. It is initialized by providing a
-    dictionary containing information on design variables, objective function, constraints and bounds.
-
-    Parameters
-    ----------
-    config : dictionary
-        A dictionary containing necessary configuration options for computing optimal turbine.
-
-    Attributes
-    ----------
-    fluid : FluidCoolProp_2Phase
-        An instance of the FluidCoolProp_2Phase class representing the fluid properties.
-    results : dict
-        A dictionary to store results.
-    boundary_conditions : dict
-        A dictionary containing boundary condition data.
-    geometry : dict
-        A dictionary containing geometry-related data.
-    model_options : dict
-        A dictionary containing options related to the analysis model.
-    reference_values : dict
-        A dictionary containing reference values for calculations.
-    design_variables_keys : list
-        A list of strings defining the selected design variables.
-    objective_function : str
-        A string defining the selected objective function for the design optimization problem.
-    eq_constraints : list
-        A list of strings containing the selected equaility constraints for the design optimization problem.
-    ineq_constraints : list
-        A list of strings containing the selected inequaility constraints for the design optimization problem.
-    bounds : list
-        A list of tuples on the form (lower bound, upper bound) defining the bounds for the design optimization problem.
-    radius_type : str
-        A string deciding what type of turbine geometry should be condidered (constant mean, hub or tip radius).
-    vars_scaled : dict
-        A dict containing the scaled flow design variables used to evaluate turbine performance.
-
-
-    Methods
-    -------
-    fitness(x)
-        Evaluate the objective function and constraints at a given point `x`.
-    get_bounds()
-        Provide the bounds for the design optimization problem.
-    get_n_eq()
-        Get the number of equality constraints.
-    get_n_ineq()
-        Get the number of oinequality constraints.
-    get_omega(specific_speed, mass_flow, h0_in, d_is, h_is)
-        Convert specific speed to actual angular speed
-    extend_variables(variables)
-        Convert a dict containing lists or arrays to a dictionaries of only one element.
-    get_given_bounds(design_variable)
-        Retrieves the lower and upper bounds for the given design variables.
-    get_constraints(self, constraints):
-        Converts a list of constraints to equality and inequality constraints.
-    update_boundary_conditions(design_point)
-        Update the boundary conditions of the turbine analysis problem with the design point of the design optimization problem.
-    convert_performance_analysis_results(performance_problem)
-        Generate a feasable set of initial guess from a solved performance analysis problem object.
-
-    """
-
-    def __init__(self, config):
-        r"""
-        Initialize a CascadesOptimizationProblem.
-
-        Parameters
-        ----------
-        config : dict
-            A dictionary containing case-specific data.
-
-        """
-
-        # Get list of design variables
-        self.obj_func = config["design_optimization"]["objective_function"]
-        self.radius_type = config["design_optimization"]["radius_type"]
-        self.eq_constraints, self.ineq_constraints = self.get_constraints(
-            config["design_optimization"]["constraints"]
-        )
-
-        # Update design point
-        self.update_boundary_conditions(config["operation_points"])
-
-        # Initialize model options
-        self.model_options = config["simulation_options"]
-
-        # Adjust given variables according to choking model (remove excess variables)
-        variables = config["design_optimization"]["variables"]
-        if self.model_options["choking_model"] == "evaluate_cascade_throat":
-            variables = {
-                key: var
-                for key, var in variables.items()
-                if not key.startswith("v*_in")
-            }
-        elif self.model_options["choking_model"] == "evaluate_cascade_critical":
-            variables = {
-                key: var
-                for key, var in variables.items()
-                if not key.startswith("beta*_throat")
-            }
-        elif (
-            self.model_options["choking_model"] == "evaluate_cascade_isentropic_throat"
-        ):
-            variables = {
-                key: var
-                for key, var in variables.items()
-                if not (
-                    key.startswith("v*_in")
-                    or key.startswith("s*_throat")
-                    or key.startswith("beta*_throat")
-                )
-            }
-
-        # Separate fixed variables and design variables
-        fixed_params = {}
-        design_variables = {}
-        for key, value in variables.items():
-            if value["lower_bound"] is not None:
-                design_variables[key] = value
-            else:
-                fixed_params[key] = value
-
-        # Initialize initial guess
-        initial_guess = self.extend_variables(design_variables)
-        self.design_variables_keys = initial_guess.keys()
-        self.initial_guess = np.array(list(initial_guess.values()))
-
-        # Extend fixed variables
-        self.fixed_params = self.extend_variables(fixed_params)
-
-        # Initialize bounds
-        lb, ub = self.get_given_bounds(design_variables)
-        self.bounds = (lb, ub)
-
-        # Adjust set of independent variables according to choking model
-        self.independent_variables = [
-            key
-            for key in initial_guess.keys()
-            if any(key.startswith(var) for var in INDEPENDENT_VARIABLES)
-        ]
-        if self.model_options["choking_model"] == "evaluate_cascade_throat":
-            self.independent_variables = [
-                var for var in self.independent_variables if not var.startswith("v*_in")
-            ]
-        elif self.model_options["choking_model"] == "evaluate_cascade_critical":
-            self.independent_variables = [
-                var
-                for var in self.independent_variables
-                if not var.startswith("beta*_throat")
-            ]
-        elif (
-            self.model_options["choking_model"] == "evaluate_cascade_isentropic_throat"
-        ):
-            self.independent_variables = [
-                var
-                for var in self.independent_variables
-                if not (
-                    var.startswith("v*_in")
-                    or var.startswith("s*_throat")
-                    or var.startswith("beta*_throat")
-                )
-            ]
-
-    def extend_variables(self, variables):
-        """
-        Index the design variables for each cascade for the default initial guess.
-
-        The design variables are given without indexing the cascades. For example, the aspect ratio is simply provided by aspect_ratio.
-        This function extend the design_variable dictionary such that for each cascade specific design variable, one item is given for each cascade.
-        for example `aspect_ratio_1` for first cascade and etc.
-
-        Parameters
-        ----------
-        variables : dict
-            A dictionary where each key maps to a dictionary containing at least a
-            "value" key. The "value" can be a list, numpy array, or float.
-
-        Returns
-        -------
-        dict
-            A dictionary where each list or numpy array in the original dictionary
-            is expanded into individual elements with keys formatted as "key_index".
-            Float values are kept as is.
-
-        """
-
-        variables_extended = {}
-        for key in variables.keys():
-            if isinstance(variables[key]["value"], (list, np.ndarray)):
-                for i in range(len(variables[key]["value"])):
-                    variables_extended[f"{key}_{i+1}"] = variables[key]["value"][i]
-            elif isinstance(variables[key]["value"], float):
-                variables_extended[key] = variables[key]["value"]
-
-        return variables_extended
-
-    def get_given_bounds(self, design_variables):
-        """
-        Retrieves the lower and upper bounds for the given design variables.
-
-        Parameters
-        ----------
-        design_variables : dict
-            A dictionary where each key maps to a dictionary containing a
-            "value" key, a "lower_bound" key, and an "upper_bound" key. The values
-            can be a list, numpy array, or float.
-
-        Returns
-        -------
-        lb : list
-            A list of lower bounds for the design variables.
-        ub : list
-            A list of upper bounds for the design variables.
-
-        """
-
-        lb = []
-        ub = []
-        for key in design_variables.keys():
-            if isinstance(design_variables[key]["value"], list):
-                lb += design_variables[key]["lower_bound"]
-                ub += design_variables[key]["upper_bound"]
-            elif isinstance(design_variables[key]["value"], np.ndarray):
-                lb += list(design_variables[key]["lower_bound"])
-                ub += list(design_variables[key]["upper_bound"])
-            elif isinstance(design_variables[key]["value"], float):
-                lb += [design_variables[key]["lower_bound"]]
-                ub += [design_variables[key]["upper_bound"]]
-
-        return lb, ub
-
-    def fitness(self, x):
-        r"""
-        Evaluate the objective function and constraints at a given pint `x`.
-
-        Parameters
-        ----------
-        x : array-like
-            Vector of design variables.
-
-        Returns
-        -------
-        numpy.ndarray
-            An array containg the value of the objective function and constraints.
-        """
-
-        # Rename reference values
-        h0_in = self.boundary_conditions["h0_in"]
-        mass_flow = self.reference_values["mass_flow_ref"]
-        h_is = self.reference_values["h_out_s"]
-        d_is = self.reference_values["d_out_s"]
-        v0 = self.reference_values["v0"]
-        angle_range = self.reference_values["angle_range"]
-        angle_min = self.reference_values["angle_min"]
-
-        # Structure design variables to dictionary (Assume set of design variables)
-        design_variables = dict(zip(self.design_variables_keys, x))
-
-        # Construct array with independent variables
-        self.vars_scaled = {
-            key: design_variables[key] for key in self.independent_variables
-        }
-
-        # Contruct variables
-        variables = {**design_variables, **self.fixed_params}
-
-        # Calculate angular speed
-        specific_speed = variables["specific_speed"]
-        self.boundary_conditions["omega"] = self.get_omega(
-            specific_speed, mass_flow, h0_in, d_is, h_is
-        )
-
-        # Calculate mean radius
-        blade_jet_ratio = variables["blade_jet_ratio"]
-        blade_speed = blade_jet_ratio * v0
-        self.geometry = {}
-        self.geometry["radius"] = blade_speed / self.boundary_conditions["omega"]
-
-        # Assign geometry design variables to geometry attribute
-        for key in INDEXED_VARIABLES:
-            self.geometry[key] = np.array(
-                [v for k, v in variables.items() if k.startswith(key)]
-            )
-            if key in ANGLE_KEYS:
-                self.geometry[key] = self.geometry[key] * angle_range + angle_min
-
-        self.geometry = geom.prepare_geometry(self.geometry, self.radius_type)
-        self.geometry = geom.calculate_full_geometry(self.geometry)
-
-        # Evaluate turbine model
-        self.results = flow.evaluate_axial_turbine(
-            self.vars_scaled,
-            self.boundary_conditions,
-            self.geometry,
-            self.fluid,
-            self.model_options,
-            self.reference_values,
-        )
-
-        # Evaluate available objecitve functions
-        objective_functions = {
-            "none": 0,
-            "efficiency_ts": -self.results["overall"]["efficiency_ts"].values[0] / 10,
-        }
-        self.f = objective_functions[self.obj_func]
-
-        # Evaluate available equality constraints
-        available_constraints = {
-            "mass_flow_rate": self.results["overall"]["mass_flow_rate"].values[0],
-            "interstage_flaring": self.geometry["A_in"][1:]
-            / self.geometry["A_out"][0:-1],
-            "flaring": self.geometry["flaring_angle"],
-        }
-        self.c_eq = np.array(list(self.results["residuals"].values()))
-        self.c_ineq = np.array([])
-        for constraint in self.eq_constraints:
-            self.c_eq = np.append(
-                self.c_eq,
-                (available_constraints[constraint["variable"]] - constraint["value"])
-                / constraint["scale"],
-            )
-
-        for constraint in self.ineq_constraints:
-            self.c_ineq = np.append(
-                self.c_ineq,
-                (constraint["value"] - available_constraints[constraint["variable"]])
-                / constraint["scale"],
-            )
-
-        objective_and_constraints = psv.combine_objective_and_constraints(
-            self.f, self.c_eq, self.c_ineq
-        )
-
-        return objective_and_constraints
-
-    def get_bounds(self):
-        r"""
-        Provide the bounds for the design optimization problem.
-
-        Returns
-        -------
-        list
-            List of toutples containg the lower and upper bound for each design variable.
-        """
-
-        return self.bounds
-
-    def get_nec(self):
-        r"""
-        Get the number of equality constraints.
-
-        Returns
-        -------
-        int
-            Number of equality constraints.
-
-        """
-
-        return psv.count_constraints(self.c_eq)
-
-    def get_nic(self):
-        r"""
-        Get the number of inequality constraints.
-
-        Returns
-        -------
-        int
-            Number of inequality constraints.
-
-        """
-        return psv.count_constraints(self.c_ineq)
-
-    def get_omega(self, specific_speed, mass_flow, h0_in, d_is, h_is):
-        r"""
-        Convert specific speed to actual angular speed
-
-        Parameters
-        ----------
-        specific_speed : float
-            Given specific speed.
-        mass_flow : float
-            Given mass flow rate.
-        h0_in : float
-            Turbine inlet stagnation enthalpy.
-        d_is : float
-            Turbine exit density for an isentropic expansion.
-        h_is : float
-            Turbine exit enthalpy for an isentropic expansion.
-
-        Returns
-        -------
-        float
-            Actual angular speed.
-
-        """
-
-        return specific_speed * (h0_in - h_is) ** (3 / 4) / ((mass_flow / d_is) ** 0.5)
-
-    def update_boundary_conditions(self, design_point):
-        """
-        Update the boundary conditions of the turbine analysis problem with the design point of the design optimization problem.
-
-        This method updates the boundary conditions attributes used to evaluate the turbine performance.
-        It also initializes a Fluid object using the 'fluid_name' specified in the operation point.
-        The method computes additional properties and reference values like stagnation properties at
-        the inlet, exit static properties, spouting velocity, and reference mass flow rate.
-        These are stored in the object's internal state for further use in calculations.
-
-        Parameters
-        ----------
-        design_point : dict
-            A dictionary containing the boundary conditions defining the operation point. It must include the following keys:
-
-            - `fluid_name` (str) : The name of the fluid to be used in the Fluid object.
-            - `T0_in` (float): The inlet temperature (in Kelvin).
-            - `p0_in` (float): The inlet pressure (in Pascals).
-            - `p_out` (float): The outlet pressure (in Pascals).
-            - `omega` (float): The rotational speed (in rad/s).
-            - `alpha_in` (float): The inlet flow angle (in degrees).
-
-        Returns
-        -------
-        None
-            This method does not return a value but updates the attributes of the object.
-
-        """
-
-        # Define current operating point
-        self.boundary_conditions = design_point
-
-        # Initialize fluid object
-        self.fluid = props.Fluid(design_point["fluid_name"])
-
-        # Rename variables
-        p0_in = design_point["p0_in"]
-        T0_in = design_point["T0_in"]
-        p_out = design_point["p_out"]
-
-        # Compute stagnation properties at inlet
-        state_in_stag = self.fluid.get_props(cp.PT_INPUTS, p0_in, T0_in)
-        h0_in = state_in_stag["h"]
-        s_in = state_in_stag["s"]
-
-        # Store the inlet stagnation (h,s) for the first stage
-        # TODO: Improve logic of implementation?
-        self.boundary_conditions["h0_in"] = h0_in
-        self.boundary_conditions["s_in"] = s_in
-
-        # Calculate exit static properties for a isentropic expansion
-        state_out_s = self.fluid.get_props(cp.PSmass_INPUTS, p_out, s_in)
-        h_isentropic = state_out_s["h"]
-        d_isentropic = state_out_s["d"]
-
-        # Calculate exit static properties for a isenthalpic expansion
-        state_out_h = self.fluid.get_props(cp.HmassP_INPUTS, h0_in, p_out)
-        s_isenthalpic = state_out_h["s"]
-        # Calculate spouting velocity
-        v0 = np.sqrt(2 * (h0_in - h_isentropic))
-
-        mass_flow_rate = None  # Initialize mass_flow_rate to None
-        # Try to find mass_flow_rate from eq_constraints
-        for constraint in self.eq_constraints:
-            if constraint.get("variable") == "mass_flow_rate":
-                mass_flow_rate = constraint.get("value")
-                break
-
-        # If mass_flow_rate not found, calculate using default formula
-        if mass_flow_rate is None:
-            mass_flow_rate = v0 * d_isentropic
-
-        # Define reference_values
-        self.reference_values = {
-            "s_range": s_isenthalpic - s_in,
-            "s_min": s_in,
-            "v0": v0,
-            "h_out_s": h_isentropic,
-            "d_out_s": d_isentropic,
-            "mass_flow_ref": mass_flow_rate,
-            "angle_range": 180,
-            "angle_min": -90,
-        }
-
-        return
-
-    def convert_performance_analysis_results(self, performance_problem):
-        r"""
-        Generate a feasable set of initial guess from a solved performance analysis problem object.
-
-        The function adopt the vars_scaled attribute from the solved performance analysis problem, the geometry and
-        angular speed to generate an initial guess of correct form to be given to the design optimization problem.
-
-        Returns
-        -------
-        dict
-            Feasable initial guess.
-        """
-
-        design_variables = self.design_variables_keys
-        radius_type = self.radius_type
-        mass_flow = self.reference_values["mass_flow_ref"]
-
-        geometry = performance_problem.geometry
-        overall = performance_problem.results["overall"]
-        vars_scaled = performance_problem.vars_scaled
-
-        angle_min = self.reference_values["angle_min"]
-        angle_range = self.reference_values["angle_range"]
-        d_out_s = self.reference_values["d_out_s"]
-        h0_in = self.boundary_conditions["h0_in"]
-        h_out_s = self.reference_values["h_out_s"]
-
-        initial_guess = {}
-        for design_variable in set(design_variables) - {
-            "specific_speed",
-            "blade_jet_ratio",
-        }:
-            if "angle" in design_variable:
-                added_dict = {
-                    design_variable
-                    + f"_{i+1}": (geometry[design_variable][i] - angle_min)
-                    / angle_range
-                    for i in range(len(geometry[design_variable]))
-                }
-            else:
-                added_dict = {
-                    design_variable + f"_{i+1}": geometry[design_variable][i]
-                    for i in range(len(geometry[design_variable]))
-                }
-            initial_guess = {**initial_guess, **added_dict}
-
-        if "blade_jet_ratio" in design_variables:
-            if radius_type == "constant_mean":
-                initial_guess["blade_jet_ratio"] = overall[
-                    "blade_jet_ratio_mean"
-                ].values[0]
-            elif radius_type == "constant_hub":
-                initial_guess["blade_jet_ratio"] = overall[
-                    "blade_jet_ratio_hub"
-                ].values[0]
-            elif radius_type == "constant_mean":
-                initial_guess["blade_jet_ratio"] = overall[
-                    "blade_jet_ratio_tip"
-                ].values[0]
-
-        if "specific_speed" in design_variables:
-            angular_speed = overall["angular_speed"].values[0]
-            initial_guess["specific_speed"] = (
-                angular_speed
-                * (mass_flow / d_out_s) ** 0.5
-                / ((h0_in - h_out_s) ** 0.75)
-            )
-
-        return {**vars_scaled, **initial_guess}
-
-    def get_constraints(self, constraints):
-        """
-        Converts a list of constraints to equality and inequality constraints.
-
-        Parameters:
-        -----------
-        constraints : list of dict
-            List of constraints where each constraint is represented as a dictionary
-            with keys "type", "variable", "value", and "normalize". "type" represents
-            the type of constraint ('=', '<', or '>'). "variable" is the variable
-            involved in the constraint. "value" is the value of the constraint. "normalize"
-            specifies whether to normalize the constraint value.
-
-        Returns:
-        --------
-        list of dict
-            List of equality constraints, each represented as a dictionary with keys
-            "variable", "value", and "scale". "variable" is the variable involved
-            in the constraint. "value" is the value of the constraint. "scale" is
-            the scaling factor applied to the value.
-        list of dict
-            List of inequality constraints, each represented as a dictionary with keys
-            "variable", "value", and "scale". "variable" is the variable involved
-            in the constraint. "value" is the value of the constraint. "scale" is
-            the scaling factor applied to the value.
-        """
-
-        eq_constraints = []
-        ineq_constraints = []
-        for constraint in constraints.keys():
-            if constraints[constraint]["type"] == "=":
-                if constraints[constraint]["normalize"]:
-                    eq_constraints += [
-                        {
-                            "variable": constraint,
-                            "value": constraints[constraint]["value"],
-                            "scale": constraints[constraint]["value"],
-                        }
-                    ]
-                else:
-                    eq_constraints += [
-                        {
-                            "variable": constraint,
-                            "value": constraints[constraint]["value"],
-                            "scale": 1,
-                        }
-                    ]
-
-            elif constraints[constraint]["type"] == "<":
-                if constraints[constraint]["normalize"]:
-                    ineq_constraints += [
-                        {
-                            "variable": constraint,
-                            "value": constraints[constraint]["value"],
-                            "scale": abs(constraints[constraint]["value"]),
-                        }
-                    ]
-                else:
-                    ineq_constraints += [
-                        {
-                            "variable": constraint,
-                            "value": constraints[constraint]["value"],
-                            "scale": 1,
-                        }
-                    ]
-
-            elif constraints[constraint]["type"] == ">":
-                if constraints[constraint]["normalize"]:
-                    ineq_constraints += [
-                        {
-                            "variable": constraint,
-                            "value": constraints[constraint]["value"],
-                            "scale": -1 * abs(constraints[constraint]["value"]),
-                        }
-                    ]
-                else:
-                    ineq_constraints += [
-                        {
-                            "variable": constraint,
-                            "value": constraints[constraint]["value"],
-                            "scale": -1,
-                        }
-                    ]
-        return eq_constraints, ineq_constraints
+import numpy as np
+import pandas as pd
+import CoolProp as cp
+import datetime
+import os
+import yaml
+from .. import pysolver_view as psv
+from .. import utilities as utils
+from . import geometry_model as geom
+from . import flow_model as flow
+from .. import properties as props
+from . import performance_analysis as pa
+
+
+CONSTRAINTS = ["mass_flow_rate", "interstage_flaring"]
+AVAILABLE_INEQ_CONSTRAINTS = ["mass_flow_rate", "interstage_flaring"]
+OBJECTIVE_FUNCTIONS = ["none", "efficiency_ts"]
+RADIUS_TYPE = ["constant_mean", "constant_hub", "constant_tip"]
+ANGLE_KEYS = ["leading_edge_angle", "gauging_angle"]
+INDEPENDENT_VARIABLES = [
+    "v_in",
+    "w_out",
+    "s_out",
+    "beta_out",
+    "v*_in",
+    "beta*_throat",
+    "w*_throat",
+    "s*_throat",
+]
+INDEXED_VARIABLES = [
+    "hub_tip_ratio_in",
+    "hub_tip_ratio_out",
+    "aspect_ratio",
+    "pitch_chord_ratio",
+    "trailing_edge_thickness_opening_ratio",
+    "leading_edge_angle",
+    "gauging_angle",
+    "throat_location_fraction",
+    "leading_edge_diameter",
+    "leading_edge_wedge_angle",
+    "tip_clearance",
+    "cascade_type",
+]
+VARIABLES = (
+    INDEXED_VARIABLES + ["specific_speed", "blade_jet_ratio"] + INDEPENDENT_VARIABLES
+)
+
+
+def compute_optimal_turbine(
+    config,
+    out_filename=None,
+    out_dir="output",
+    export_results=True,
+):
+    r"""
+    Calculate the optimal turbine configuration based on the specified optimization problem.
+
+    The function checks the configuration file before performing design optimization.
+
+    Parameters
+    ----------
+    config : dict
+        A dictionary containing necessary configuration options for computing optimal turbine.
+    initial_guess : None or dict
+        The initial guess of the design optimization. If None, a defualt initial guess is provided. If given, he initial guess must correspond
+        with the given set of design variables given in the configuration dictionary.
+
+    Returns
+    -------
+    solution : object
+        The solution object containing the results of the design optimization.
+    """
+
+    # Initialize problem object
+    problem = CascadesOptimizationProblem(config)
+
+    # Perform initial function call to initialize problem
+    # This populates the arrays of equality and inequality constraints
+    # TODO: it might be more intuitive to create a new method called initialize_problem() that generates the initial guess and evaluates the fitness() function with it
+    problem.fitness(problem.initial_guess)
+
+    # Load solver configuration
+    solver_config = config["design_optimization"]["solver_options"]
+
+    # Initialize solver object using keyword-argument dictionary unpacking
+    solver = psv.OptimizationSolver(problem, **solver_config)
+
+    # Solve optimization problem for initial guess x0
+    solver.solve(problem.initial_guess)
+
+    dfs = {
+        "operation point": pd.DataFrame(
+            {key: pd.Series(val) for key, val in problem.boundary_conditions.items()}
+        ),
+        "overall": pd.DataFrame(problem.results["overall"], index=[0]),
+        "plane": problem.results["plane"],
+        "cascade": problem.results["cascade"],
+        "stage": problem.results["stage"],
+        "geometry": pd.DataFrame(
+            {key: pd.Series(val) for key, val in problem.geometry.items()}
+        ),
+        "solver": pd.DataFrame(
+            {
+                "completed": pd.Series(True, index=[0]),
+                "success": pd.Series(solver.success, index=[0]),
+                "message": pd.Series(solver.message, index=[0]),
+                "grad_count": solver.convergence_history["grad_count"],
+                "func_count": solver.convergence_history["func_count"],
+                "func_count_total": solver.convergence_history["func_count_total"],
+                "objective_value": solver.convergence_history["objective_value"],
+                "constraint_violation": solver.convergence_history[
+                    "constraint_violation"
+                ],
+                "norm_step": solver.convergence_history["norm_step"],
+            },
+            index=range(len(solver.convergence_history["grad_count"])),
+        ),
+    }
+
+    if export_results:
+        # Create a directory to save simulation results
+        if not os.path.exists(out_dir):
+            os.makedirs(out_dir)
+
+        # Define filename with unique date-time identifier
+        if out_filename == None:
+            current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
+            out_filename = f"design_optimization_{current_time}"
+
+        # Export simulation configuration as YAML file
+        config_data = {k: v for k, v in config.items() if v}  # Filter empty entries
+        config_data = utils.convert_numpy_to_python(config_data, precision=12)
+        config_file = os.path.join(out_dir, f"{out_filename}.yaml")
+        with open(config_file, "w") as file:
+            yaml.dump(config_data, file, default_flow_style=False, sort_keys=False)
+
+        # Export optimal turbine in excel file
+        filepath = os.path.join(out_dir, f"{out_filename}.xlsx")
+        with pd.ExcelWriter(filepath, engine="openpyxl") as writer:
+            for sheet_name, df in dfs.items():
+                df.to_excel(writer, sheet_name=sheet_name, index=True)
+
+    return solver
+
+
+class CascadesOptimizationProblem(psv.OptimizationProblem):
+    """
+    A class representing a turbine design optimization problem.
+
+    This class is designed for solving design optimization problems of axial turbines. It is initialized by providing a
+    dictionary containing information on design variables, objective function, constraints and bounds.
+
+    Parameters
+    ----------
+    config : dictionary
+        A dictionary containing necessary configuration options for computing optimal turbine.
+
+    Attributes
+    ----------
+    fluid : FluidCoolProp_2Phase
+        An instance of the FluidCoolProp_2Phase class representing the fluid properties.
+    results : dict
+        A dictionary to store results.
+    boundary_conditions : dict
+        A dictionary containing boundary condition data.
+    geometry : dict
+        A dictionary containing geometry-related data.
+    model_options : dict
+        A dictionary containing options related to the analysis model.
+    reference_values : dict
+        A dictionary containing reference values for calculations.
+    design_variables_keys : list
+        A list of strings defining the selected design variables.
+    objective_function : str
+        A string defining the selected objective function for the design optimization problem.
+    eq_constraints : list
+        A list of strings containing the selected equaility constraints for the design optimization problem.
+    ineq_constraints : list
+        A list of strings containing the selected inequaility constraints for the design optimization problem.
+    bounds : list
+        A list of tuples on the form (lower bound, upper bound) defining the bounds for the design optimization problem.
+    radius_type : str
+        A string deciding what type of turbine geometry should be condidered (constant mean, hub or tip radius).
+    vars_scaled : dict
+        A dict containing the scaled flow design variables used to evaluate turbine performance.
+
+
+    Methods
+    -------
+    fitness(x)
+        Evaluate the objective function and constraints at a given point `x`.
+    get_bounds()
+        Provide the bounds for the design optimization problem.
+    get_n_eq()
+        Get the number of equality constraints.
+    get_n_ineq()
+        Get the number of oinequality constraints.
+    get_omega(specific_speed, mass_flow, h0_in, d_is, h_is)
+        Convert specific speed to actual angular speed
+    extend_variables(variables)
+        Convert a dict containing lists or arrays to a dictionaries of only one element.
+    get_given_bounds(design_variable)
+        Retrieves the lower and upper bounds for the given design variables.
+    get_constraints(self, constraints):
+        Converts a list of constraints to equality and inequality constraints.
+    update_boundary_conditions(design_point)
+        Update the boundary conditions of the turbine analysis problem with the design point of the design optimization problem.
+    convert_performance_analysis_results(performance_problem)
+        Generate a feasable set of initial guess from a solved performance analysis problem object.
+
+    """
+
+    def __init__(self, config):
+        r"""
+        Initialize a CascadesOptimizationProblem.
+
+        Parameters
+        ----------
+        config : dict
+            A dictionary containing case-specific data.
+
+        """
+
+        # Get list of design variables
+        self.obj_func = config["design_optimization"]["objective_function"]
+        self.radius_type = config["design_optimization"]["radius_type"]
+        self.eq_constraints, self.ineq_constraints = self.get_constraints(
+            config["design_optimization"]["constraints"]
+        )
+
+        # Update design point
+        self.update_boundary_conditions(config["operation_points"])
+
+        # Initialize model options
+        self.model_options = config["simulation_options"]
+
+        # Adjust given variables according to choking model (remove excess variables)
+        variables = config["design_optimization"]["variables"]
+        if self.model_options["choking_model"] == "evaluate_cascade_throat":
+            variables = {
+                key: var
+                for key, var in variables.items()
+                if not key.startswith("v*_in")
+            }
+        elif self.model_options["choking_model"] == "evaluate_cascade_critical":
+            variables = {
+                key: var
+                for key, var in variables.items()
+                if not key.startswith("beta*_throat")
+            }
+        elif (
+            self.model_options["choking_model"] == "evaluate_cascade_isentropic_throat"
+        ):
+            variables = {
+                key: var
+                for key, var in variables.items()
+                if not (
+                    key.startswith("v*_in")
+                    or key.startswith("s*_throat")
+                    or key.startswith("beta*_throat")
+                )
+            }
+
+        # Separate fixed variables and design variables
+        fixed_params = {}
+        design_variables = {}
+        for key, value in variables.items():
+            if value["lower_bound"] is not None:
+                design_variables[key] = value
+            else:
+                fixed_params[key] = value
+
+        # Initialize initial guess
+        initial_guess = self.extend_variables(design_variables)
+        self.design_variables_keys = initial_guess.keys()
+        self.initial_guess = np.array(list(initial_guess.values()))
+
+        # Extend fixed variables
+        self.fixed_params = self.extend_variables(fixed_params)
+
+        # Initialize bounds
+        lb, ub = self.get_given_bounds(design_variables)
+        self.bounds = (lb, ub)
+
+        # Adjust set of independent variables according to choking model
+        self.independent_variables = [
+            key
+            for key in initial_guess.keys()
+            if any(key.startswith(var) for var in INDEPENDENT_VARIABLES)
+        ]
+        if self.model_options["choking_model"] == "evaluate_cascade_throat":
+            self.independent_variables = [
+                var for var in self.independent_variables if not var.startswith("v*_in")
+            ]
+        elif self.model_options["choking_model"] == "evaluate_cascade_critical":
+            self.independent_variables = [
+                var
+                for var in self.independent_variables
+                if not var.startswith("beta*_throat")
+            ]
+        elif (
+            self.model_options["choking_model"] == "evaluate_cascade_isentropic_throat"
+        ):
+            self.independent_variables = [
+                var
+                for var in self.independent_variables
+                if not (
+                    var.startswith("v*_in")
+                    or var.startswith("s*_throat")
+                    or var.startswith("beta*_throat")
+                )
+            ]
+
+    def extend_variables(self, variables):
+        """
+        Index the design variables for each cascade for the default initial guess.
+
+        The design variables are given without indexing the cascades. For example, the aspect ratio is simply provided by aspect_ratio.
+        This function extend the design_variable dictionary such that for each cascade specific design variable, one item is given for each cascade.
+        for example `aspect_ratio_1` for first cascade and etc.
+
+        Parameters
+        ----------
+        variables : dict
+            A dictionary where each key maps to a dictionary containing at least a
+            "value" key. The "value" can be a list, numpy array, or float.
+
+        Returns
+        -------
+        dict
+            A dictionary where each list or numpy array in the original dictionary
+            is expanded into individual elements with keys formatted as "key_index".
+            Float values are kept as is.
+
+        """
+
+        variables_extended = {}
+        for key in variables.keys():
+            if isinstance(variables[key]["value"], (list, np.ndarray)):
+                for i in range(len(variables[key]["value"])):
+                    variables_extended[f"{key}_{i+1}"] = variables[key]["value"][i]
+            elif isinstance(variables[key]["value"], float):
+                variables_extended[key] = variables[key]["value"]
+
+        return variables_extended
+
+    def get_given_bounds(self, design_variables):
+        """
+        Retrieves the lower and upper bounds for the given design variables.
+
+        Parameters
+        ----------
+        design_variables : dict
+            A dictionary where each key maps to a dictionary containing a
+            "value" key, a "lower_bound" key, and an "upper_bound" key. The values
+            can be a list, numpy array, or float.
+
+        Returns
+        -------
+        lb : list
+            A list of lower bounds for the design variables.
+        ub : list
+            A list of upper bounds for the design variables.
+
+        """
+
+        lb = []
+        ub = []
+        for key in design_variables.keys():
+            if isinstance(design_variables[key]["value"], list):
+                lb += design_variables[key]["lower_bound"]
+                ub += design_variables[key]["upper_bound"]
+            elif isinstance(design_variables[key]["value"], np.ndarray):
+                lb += list(design_variables[key]["lower_bound"])
+                ub += list(design_variables[key]["upper_bound"])
+            elif isinstance(design_variables[key]["value"], float):
+                lb += [design_variables[key]["lower_bound"]]
+                ub += [design_variables[key]["upper_bound"]]
+
+        return lb, ub
+
+    def fitness(self, x):
+        r"""
+        Evaluate the objective function and constraints at a given pint `x`.
+
+        Parameters
+        ----------
+        x : array-like
+            Vector of design variables.
+
+        Returns
+        -------
+        numpy.ndarray
+            An array containg the value of the objective function and constraints.
+        """
+
+        # Rename reference values
+        h0_in = self.boundary_conditions["h0_in"]
+        mass_flow = self.reference_values["mass_flow_ref"]
+        h_is = self.reference_values["h_out_s"]
+        d_is = self.reference_values["d_out_s"]
+        v0 = self.reference_values["v0"]
+        angle_range = self.reference_values["angle_range"]
+        angle_min = self.reference_values["angle_min"]
+
+        # Structure design variables to dictionary (Assume set of design variables)
+        design_variables = dict(zip(self.design_variables_keys, x))
+
+        # Construct array with independent variables
+        self.vars_scaled = {
+            key: design_variables[key] for key in self.independent_variables
+        }
+
+        # Contruct variables
+        variables = {**design_variables, **self.fixed_params}
+
+        # Calculate angular speed
+        specific_speed = variables["specific_speed"]
+        self.boundary_conditions["omega"] = self.get_omega(
+            specific_speed, mass_flow, h0_in, d_is, h_is
+        )
+
+        # Calculate mean radius
+        blade_jet_ratio = variables["blade_jet_ratio"]
+        blade_speed = blade_jet_ratio * v0
+        self.geometry = {}
+        self.geometry["radius"] = blade_speed / self.boundary_conditions["omega"]
+
+        # Assign geometry design variables to geometry attribute
+        for key in INDEXED_VARIABLES:
+            self.geometry[key] = np.array(
+                [v for k, v in variables.items() if k.startswith(key)]
+            )
+            if key in ANGLE_KEYS:
+                self.geometry[key] = self.geometry[key] * angle_range + angle_min
+
+        self.geometry = geom.prepare_geometry(self.geometry, self.radius_type)
+        self.geometry = geom.calculate_full_geometry(self.geometry)
+
+        # Evaluate turbine model
+        self.results = flow.evaluate_axial_turbine(
+            self.vars_scaled,
+            self.boundary_conditions,
+            self.geometry,
+            self.fluid,
+            self.model_options,
+            self.reference_values,
+        )
+
+        # Evaluate available objecitve functions
+        objective_functions = {
+            "none": 0,
+            "efficiency_ts": -self.results["overall"]["efficiency_ts"].values[0] / 10,
+        }
+        self.f = objective_functions[self.obj_func]
+
+        # Evaluate available equality constraints
+        available_constraints = {
+            "mass_flow_rate": self.results["overall"]["mass_flow_rate"].values[0],
+            "interstage_flaring": self.geometry["A_in"][1:]
+            / self.geometry["A_out"][0:-1],
+            "flaring": self.geometry["flaring_angle"],
+        }
+        self.c_eq = np.array(list(self.results["residuals"].values()))
+        self.c_ineq = np.array([])
+        for constraint in self.eq_constraints:
+            self.c_eq = np.append(
+                self.c_eq,
+                (available_constraints[constraint["variable"]] - constraint["value"])
+                / constraint["scale"],
+            )
+
+        for constraint in self.ineq_constraints:
+            self.c_ineq = np.append(
+                self.c_ineq,
+                (constraint["value"] - available_constraints[constraint["variable"]])
+                / constraint["scale"],
+            )
+
+        objective_and_constraints = psv.combine_objective_and_constraints(
+            self.f, self.c_eq, self.c_ineq
+        )
+
+        return objective_and_constraints
+
+    def get_bounds(self):
+        r"""
+        Provide the bounds for the design optimization problem.
+
+        Returns
+        -------
+        list
+            List of toutples containg the lower and upper bound for each design variable.
+        """
+
+        return self.bounds
+
+    def get_nec(self):
+        r"""
+        Get the number of equality constraints.
+
+        Returns
+        -------
+        int
+            Number of equality constraints.
+
+        """
+
+        return psv.count_constraints(self.c_eq)
+
+    def get_nic(self):
+        r"""
+        Get the number of inequality constraints.
+
+        Returns
+        -------
+        int
+            Number of inequality constraints.
+
+        """
+        return psv.count_constraints(self.c_ineq)
+
+    def get_omega(self, specific_speed, mass_flow, h0_in, d_is, h_is):
+        r"""
+        Convert specific speed to actual angular speed
+
+        Parameters
+        ----------
+        specific_speed : float
+            Given specific speed.
+        mass_flow : float
+            Given mass flow rate.
+        h0_in : float
+            Turbine inlet stagnation enthalpy.
+        d_is : float
+            Turbine exit density for an isentropic expansion.
+        h_is : float
+            Turbine exit enthalpy for an isentropic expansion.
+
+        Returns
+        -------
+        float
+            Actual angular speed.
+
+        """
+
+        return specific_speed * (h0_in - h_is) ** (3 / 4) / ((mass_flow / d_is) ** 0.5)
+
+    def update_boundary_conditions(self, design_point):
+        """
+        Update the boundary conditions of the turbine analysis problem with the design point of the design optimization problem.
+
+        This method updates the boundary conditions attributes used to evaluate the turbine performance.
+        It also initializes a Fluid object using the 'fluid_name' specified in the operation point.
+        The method computes additional properties and reference values like stagnation properties at
+        the inlet, exit static properties, spouting velocity, and reference mass flow rate.
+        These are stored in the object's internal state for further use in calculations.
+
+        Parameters
+        ----------
+        design_point : dict
+            A dictionary containing the boundary conditions defining the operation point. It must include the following keys:
+
+            - `fluid_name` (str) : The name of the fluid to be used in the Fluid object.
+            - `T0_in` (float): The inlet temperature (in Kelvin).
+            - `p0_in` (float): The inlet pressure (in Pascals).
+            - `p_out` (float): The outlet pressure (in Pascals).
+            - `omega` (float): The rotational speed (in rad/s).
+            - `alpha_in` (float): The inlet flow angle (in degrees).
+
+        Returns
+        -------
+        None
+            This method does not return a value but updates the attributes of the object.
+
+        """
+
+        # Define current operating point
+        self.boundary_conditions = design_point
+
+        # Initialize fluid object
+        self.fluid = props.Fluid(design_point["fluid_name"])
+
+        # Rename variables
+        p0_in = design_point["p0_in"]
+        T0_in = design_point["T0_in"]
+        p_out = design_point["p_out"]
+
+        # Compute stagnation properties at inlet
+        state_in_stag = self.fluid.get_props(cp.PT_INPUTS, p0_in, T0_in)
+        h0_in = state_in_stag["h"]
+        s_in = state_in_stag["s"]
+
+        # Store the inlet stagnation (h,s) for the first stage
+        # TODO: Improve logic of implementation?
+        self.boundary_conditions["h0_in"] = h0_in
+        self.boundary_conditions["s_in"] = s_in
+
+        # Calculate exit static properties for a isentropic expansion
+        state_out_s = self.fluid.get_props(cp.PSmass_INPUTS, p_out, s_in)
+        h_isentropic = state_out_s["h"]
+        d_isentropic = state_out_s["d"]
+
+        # Calculate exit static properties for a isenthalpic expansion
+        state_out_h = self.fluid.get_props(cp.HmassP_INPUTS, h0_in, p_out)
+        s_isenthalpic = state_out_h["s"]
+        # Calculate spouting velocity
+        v0 = np.sqrt(2 * (h0_in - h_isentropic))
+
+        mass_flow_rate = None  # Initialize mass_flow_rate to None
+        # Try to find mass_flow_rate from eq_constraints
+        for constraint in self.eq_constraints:
+            if constraint.get("variable") == "mass_flow_rate":
+                mass_flow_rate = constraint.get("value")
+                break
+
+        # If mass_flow_rate not found, calculate using default formula
+        if mass_flow_rate is None:
+            mass_flow_rate = v0 * d_isentropic
+
+        # Define reference_values
+        self.reference_values = {
+            "s_range": s_isenthalpic - s_in,
+            "s_min": s_in,
+            "v0": v0,
+            "h_out_s": h_isentropic,
+            "d_out_s": d_isentropic,
+            "mass_flow_ref": mass_flow_rate,
+            "angle_range": 180,
+            "angle_min": -90,
+        }
+
+        return
+
+    def convert_performance_analysis_results(self, performance_problem):
+        r"""
+        Generate a feasable set of initial guess from a solved performance analysis problem object.
+
+        The function adopt the vars_scaled attribute from the solved performance analysis problem, the geometry and
+        angular speed to generate an initial guess of correct form to be given to the design optimization problem.
+
+        Returns
+        -------
+        dict
+            Feasable initial guess.
+        """
+
+        design_variables = self.design_variables_keys
+        radius_type = self.radius_type
+        mass_flow = self.reference_values["mass_flow_ref"]
+
+        geometry = performance_problem.geometry
+        overall = performance_problem.results["overall"]
+        vars_scaled = performance_problem.vars_scaled
+
+        angle_min = self.reference_values["angle_min"]
+        angle_range = self.reference_values["angle_range"]
+        d_out_s = self.reference_values["d_out_s"]
+        h0_in = self.boundary_conditions["h0_in"]
+        h_out_s = self.reference_values["h_out_s"]
+
+        initial_guess = {}
+        for design_variable in set(design_variables) - {
+            "specific_speed",
+            "blade_jet_ratio",
+        }:
+            if "angle" in design_variable:
+                added_dict = {
+                    design_variable
+                    + f"_{i+1}": (geometry[design_variable][i] - angle_min)
+                    / angle_range
+                    for i in range(len(geometry[design_variable]))
+                }
+            else:
+                added_dict = {
+                    design_variable + f"_{i+1}": geometry[design_variable][i]
+                    for i in range(len(geometry[design_variable]))
+                }
+            initial_guess = {**initial_guess, **added_dict}
+
+        if "blade_jet_ratio" in design_variables:
+            if radius_type == "constant_mean":
+                initial_guess["blade_jet_ratio"] = overall[
+                    "blade_jet_ratio_mean"
+                ].values[0]
+            elif radius_type == "constant_hub":
+                initial_guess["blade_jet_ratio"] = overall[
+                    "blade_jet_ratio_hub"
+                ].values[0]
+            elif radius_type == "constant_mean":
+                initial_guess["blade_jet_ratio"] = overall[
+                    "blade_jet_ratio_tip"
+                ].values[0]
+
+        if "specific_speed" in design_variables:
+            angular_speed = overall["angular_speed"].values[0]
+            initial_guess["specific_speed"] = (
+                angular_speed
+                * (mass_flow / d_out_s) ** 0.5
+                / ((h0_in - h_out_s) ** 0.75)
+            )
+
+        return {**vars_scaled, **initial_guess}
+
+    def get_constraints(self, constraints):
+        """
+        Converts a list of constraints to equality and inequality constraints.
+
+        Parameters:
+        -----------
+        constraints : list of dict
+            List of constraints where each constraint is represented as a dictionary
+            with keys "type", "variable", "value", and "normalize". "type" represents
+            the type of constraint ('=', '<', or '>'). "variable" is the variable
+            involved in the constraint. "value" is the value of the constraint. "normalize"
+            specifies whether to normalize the constraint value.
+
+        Returns:
+        --------
+        list of dict
+            List of equality constraints, each represented as a dictionary with keys
+            "variable", "value", and "scale". "variable" is the variable involved
+            in the constraint. "value" is the value of the constraint. "scale" is
+            the scaling factor applied to the value.
+        list of dict
+            List of inequality constraints, each represented as a dictionary with keys
+            "variable", "value", and "scale". "variable" is the variable involved
+            in the constraint. "value" is the value of the constraint. "scale" is
+            the scaling factor applied to the value.
+        """
+
+        eq_constraints = []
+        ineq_constraints = []
+        for constraint in constraints.keys():
+            if constraints[constraint]["type"] == "=":
+                if constraints[constraint]["normalize"]:
+                    eq_constraints += [
+                        {
+                            "variable": constraint,
+                            "value": constraints[constraint]["value"],
+                            "scale": constraints[constraint]["value"],
+                        }
+                    ]
+                else:
+                    eq_constraints += [
+                        {
+                            "variable": constraint,
+                            "value": constraints[constraint]["value"],
+                            "scale": 1,
+                        }
+                    ]
+
+            elif constraints[constraint]["type"] == "<":
+                if constraints[constraint]["normalize"]:
+                    ineq_constraints += [
+                        {
+                            "variable": constraint,
+                            "value": constraints[constraint]["value"],
+                            "scale": abs(constraints[constraint]["value"]),
+                        }
+                    ]
+                else:
+                    ineq_constraints += [
+                        {
+                            "variable": constraint,
+                            "value": constraints[constraint]["value"],
+                            "scale": 1,
+                        }
+                    ]
+
+            elif constraints[constraint]["type"] == ">":
+                if constraints[constraint]["normalize"]:
+                    ineq_constraints += [
+                        {
+                            "variable": constraint,
+                            "value": constraints[constraint]["value"],
+                            "scale": -1 * abs(constraints[constraint]["value"]),
+                        }
+                    ]
+                else:
+                    ineq_constraints += [
+                        {
+                            "variable": constraint,
+                            "value": constraints[constraint]["value"],
+                            "scale": -1,
+                        }
+                    ]
+        return eq_constraints, ineq_constraints
```

### Comparing `turboflow-0.1.2/turboflow/axial_turbine/flow_model.py` & `turboflow-0.1.3/turboflow/axial_turbine/flow_model.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,1125 +1,1125 @@
-import numpy as np
-import pandas as pd
-import CoolProp as cp
-from scipy.linalg import solve
-from scipy.optimize._numdiff import approx_derivative
-
-from .. import math
-from .. import utilities as utils
-from . import loss_model as lm
-from . import deviation_model as dm
-from . import choking_model as cm
-
-# List of valid options
-BLOCKAGE_MODELS = ["flat_plate_turbulent"]
-
-
-def evaluate_axial_turbine(
-    variables,
-    boundary_conditions,
-    geometry,
-    fluid,
-    model_options,
-    reference_values,
-):
-    """
-    Compute the performance of an axial turbine by evaluating a series of cascades.
-
-    The function iterates through each cascade, accumulating data on both performance and residuals used to evaluate the physicality of the flow.
-    Between each cascade, the space between the cascades is modelled to give the input to the next cascade.
-
-    The results are organized into a dictionary that includes:
-
-    - `cascade`: Contains data specific to each cascade in the series, including loss coefficients and critical conditions.
-    - `plane`: Contains data specific to each flow station, including thermodynamic properties and velocity triangles.
-    - `stage`: Contains data specific to each turbine stage, including degree of reaction.
-    - `geometry`: Contains data on the turbine geometry.
-    - `overall`: Summarizes the overall performance of the turbine, including mass flow rate, efficiency and power output.
-    - `reference_values`: Contain supplementary information.
-    - `residuals`: Summarizes the mismatch in the nonlinear system of equations used to model the turbine.
-
-    .. note::
-
-        The output of this function can only be regarded as a physically meaningful solution when the equations are fully closed (i.e., all residuals are close to zero).
-        Therefore, this function is intended to be used within a root-finding or optimization algorithm that systematically adjusts the input variables to drive the residuals to zero and close the system of equations.
-
-    Parameters
-    ----------
-    variables : dict
-        Dictionary containing flow variable for the cascades.
-    boundary_conditions : dict
-        Dictionary containing boundary conditions for the series of cascades.
-    geometry : dict
-        Dictionary containing geometric parameters of the series of cascades.
-    fluid : object
-        A fluid object with methods for thermodynamic property calculations.
-    model_options : dict
-        Dictionary containing various model options.
-    reference_values : dict
-        Dictionary containing reference values for scaling.
-
-    Returns
-    -------
-    results : dict
-        Dictionary containing the evaluated results, including planes, cascades, stage, overall,
-        and geometry information.
-
-    """
-
-    # Load variables
-    number_of_cascades = geometry["number_of_cascades"]
-    h0_in = boundary_conditions["h0_in"]
-    s_in = boundary_conditions["s_in"]
-    alpha_in = boundary_conditions["alpha_in"]
-    angular_speed = boundary_conditions["omega"]
-
-    # Load reference_values
-    v0 = reference_values["v0"]
-    s_range = reference_values["s_range"]
-    s_min = reference_values["s_min"]
-    angle_range = reference_values["angle_range"]
-    angle_min = reference_values["angle_min"]
-
-    # Initialize results structure
-    results = {
-        "plane": pd.DataFrame(),
-        "cascade": pd.DataFrame(),
-        "stage": {},
-        "overall": {},
-        "geometry": geometry,
-        "reference_values": reference_values,
-        "boundary_conditions": boundary_conditions,
-    }
-
-    # initialize residual arrays
-    residuals = {}
-
-    # Rename turbine inlet velocity
-    v_in = variables["v_in"] * v0
-
-    for i in range(number_of_cascades):
-        # Update angular speed
-        angular_speed_cascade = angular_speed * (i % 2)
-
-        # update geometry for current cascade
-        geometry_cascade = {
-            key: values[i]
-            for key, values in geometry.items()
-            if key not in ["number_of_cascades", "number_of_stages"]
-        }
-
-        # Rename variables
-        cascade = "_" + str(i + 1)
-        w_out = variables["w_out" + cascade] * v0
-        s_out = variables["s_out" + cascade] * s_range + s_min
-        beta_out = variables["beta_out" + cascade] * angle_range + angle_min
-
-        # Evaluate current cascade
-        cascade_inlet_input = {
-            "h0": h0_in,
-            "s": s_in,
-            "alpha": alpha_in,
-            "v": v_in,
-        }
-        cascade_exit_input = {
-            "w": w_out,
-            "beta": beta_out,
-            "s": s_out,
-        }
-
-        choking_input = {
-            key.replace(cascade, ""): val
-            for key, val in variables.items()
-            if ("*" and cascade in key) or key == "v*_in"
-        }
-
-        cascade_residuals = evaluate_cascade(
-            cascade_inlet_input,
-            cascade_exit_input,
-            choking_input,
-            fluid,
-            geometry_cascade,
-            angular_speed_cascade,
-            results,
-            model_options,
-            reference_values,
-        )
-
-        # Add cascade residuals to residual array
-        cascade_residuals = utils.add_string_to_keys(cascade_residuals, f"_{i+1}")
-        residuals.update(cascade_residuals)
-
-        # Calculate input of next cascade
-        if i != number_of_cascades - 1:
-            (
-                h0_in,
-                s_in,
-                alpha_in,
-                v_in,
-            ) = evaluate_cascade_interspace(
-                results["plane"]["h0"].values[-1],
-                results["plane"]["v_m"].values[-1],
-                results["plane"]["v_t"].values[-1],
-                results["plane"]["d"].values[-1],
-                geometry_cascade["radius_mean_out"],
-                geometry_cascade["A_out"],
-                results["plane"]["blockage"].values[-1],
-                geometry["radius_mean_in"][i + 1],
-                geometry["A_in"][i + 1],
-                fluid,
-            )
-
-    # Add exit pressure error to residuals
-    p_calc = results["plane"]["p"].values[-1]
-    p_error = (p_calc - boundary_conditions["p_out"]) / boundary_conditions["p0_in"]
-    residuals["p_out"] = p_error
-
-    # Additional calculations
-    loss_fractions = compute_efficiency_breakdown(results)
-    results["cascade"] = pd.concat([results["cascade"], loss_fractions], axis=1)
-
-    # Compute stage performance
-    results["stage"] = pd.DataFrame(compute_stage_performance(results))
-
-    # Compute overall perfrormance
-    results["overall"] = pd.DataFrame([compute_overall_performance(results, geometry)])
-
-    # Store all residuals for export
-    results["residuals"] = residuals
-
-    # Store the input variables
-    results["independent_variables"] = variables
-
-    # Retain only variables defined per cascade
-    geom_cascades = {
-        key: value
-        for key, value in geometry.items()
-        if len(utils.ensure_iterable(value)) == number_of_cascades
-    }
-    results["geometry"] = pd.DataFrame([geom_cascades])
-
-    return results
-
-
-def evaluate_cascade(
-    cascade_inlet_input,
-    cascade_exit_input,
-    choking_input,
-    fluid,
-    geometry,
-    angular_speed,
-    results,
-    model_options,
-    reference_values,
-):
-    """
-    Evaluate the performance of a cascade configuration.
-
-    This function evaluates the cascade performance by considering inlet, throat, and exit planes.
-    It also determines the condition for choking according to the selected choking model, and evaluates if the cascade is choked or not. The results are stored in a dictionary.
-    The function returns a dictionary of residuals that includes the mass balance error at both the inlet and exit, loss coefficient errors,
-    and the residuals related to the critical state and choking condition.
-
-    Parameters
-    ----------
-    cascade_inlet_input : dict
-        Input conditions at the cascade inlet.
-    cascade_throat_input : dict
-        Input conditions at the cascade throat.
-    cascade_exit_input : dict
-        Input conditions at the cascade exit.
-    critical_cascade_input : dict
-        Input conditions for critical state evaluation.
-    fluid : object
-        A fluid object with methods for thermodynamic property calculations.
-    geometry : dict
-        Dictionary containing geometric parameters of the cascade.
-    angular_speed : float
-        Angular speed of the cascade.
-    results : dict
-        Dictionary to store the evaluation results.
-    model_options : dict
-        Dictionary containing various model options.
-    reference_values : dict
-        Dictionary containing reference values for normalization.
-
-    Returns
-    -------
-    residuals : dict
-        A dictionary containing the residuals of the cascade evaluation.
-
-    """
-
-    # Define model options
-    # TODO Add warnings that some settings have been assumed. This is a dangerous silent behavior as it is now
-    # TODO It would make sense to define default values for these options at a higher level, for instance after processing the configuration file
-    # TODO an advantage of the approach above is that it is possible to print a warning that will not be shown for each iteration of the solvers
-    loss_model = model_options["loss_model"]
-    deviation_model = model_options["deviation_model"]
-
-    # Load reference values
-    mass_flow_reference = reference_values["mass_flow_ref"]
-
-    # Evaluate inlet plane
-    inlet_plane = evaluate_cascade_inlet(
-        cascade_inlet_input, fluid, geometry, angular_speed
-    )
-
-    # Evaluate exit plane
-    cascade_exit_input["rothalpy"] = inlet_plane["rothalpy"]
-    exit_plane, loss_dict = evaluate_cascade_exit(
-        cascade_exit_input,
-        fluid,
-        geometry,
-        inlet_plane,
-        angular_speed,
-        model_options["blockage_model"],
-        loss_model,
-    )
-
-    # Evaluate isentropic enthalpy change
-    props_is = fluid.get_props(cp.PSmass_INPUTS, exit_plane["p"], inlet_plane["s"])
-    dh_is = exit_plane["h"] - props_is["h"]
-
-    # Evaluate critical state
-    residuals_critical, critical_state = cm.evaluate_choking(
-        choking_input,
-        inlet_plane,
-        exit_plane,
-        fluid,
-        geometry,
-        angular_speed,
-        model_options,
-        reference_values,
-    )
-
-    # Create dictionary with equation residuals
-    mass_error_exit = inlet_plane["mass_flow"] - exit_plane["mass_flow"]
-    residuals = {
-        "loss_error_exit": exit_plane["loss_error"],
-        "mass_error_exit": mass_error_exit / mass_flow_reference,
-        **residuals_critical,
-    }
-
-    # Return plane data in dataframe
-    results["plane"].loc[len(results["plane"]), inlet_plane.keys()] = inlet_plane
-    results["plane"].loc[len(results["plane"]), exit_plane.keys()] = exit_plane
-
-    # Return cascade data in dataframe
-    cascade_data = {
-        **loss_dict,
-        **critical_state,
-        "dh_s": dh_is,
-        "incidence": inlet_plane["beta"] - geometry["leading_edge_angle"],
-    }
-
-    results["cascade"].loc[len(results["cascade"]), cascade_data.keys()] = cascade_data
-
-    return residuals
-
-
-def evaluate_cascade_inlet(cascade_inlet_input, fluid, geometry, angular_speed):
-    r"""
-    Evaluate the inlet plane of a cascade including velocity triangles,
-    thermodynamic properties, and flow characteristics.
-
-    This function calculates performance data at the inlet of a cascade based on the cascade geometry,
-    fluid, and flow conditions. It computes velocity triangles, static and stagnation properties,
-    Reynolds and Mach numbers, and the mass flow rate at the inlet.
-
-    Parameters
-    ----------
-    cascade_inlet_input : dict
-        Input parameters specific to the cascade inlet, including stagnation enthalpy (`h0`),
-        entropy (`s`), absolute velocity (`v`), and flow angle (`alpha`).
-    fluid : object
-        A fluid object with methods for thermodynamic property calculations.
-    geometry : dict
-        Geometric parameters of the cascade such as radius at the mean inlet (`radius_mean_in`),
-        chord length, and inlet area (`A_in`).
-    angular_speed : float
-        Angular speed of the cascade.
-
-    Returns
-    -------
-    dict
-        A dictionary of calculated parameters at the cascade inlet.
-    """
-    # Load cascade inlet input
-    h0 = cascade_inlet_input["h0"]
-    s = cascade_inlet_input["s"]
-    v = cascade_inlet_input["v"]
-    alpha = cascade_inlet_input["alpha"]
-
-    # Load geometry
-    radius = geometry["radius_mean_in"]
-    chord = geometry["chord"]
-    area = geometry["A_in"]
-
-    # Calculate velocity triangles
-    blade_speed = radius * angular_speed
-    velocity_triangle = evaluate_velocity_triangle_in(blade_speed, v, alpha)
-    w = velocity_triangle["w"]
-    w_m = velocity_triangle["w_m"]
-
-    # Calculate static properties
-    h = h0 - 0.5 * v**2
-    static_properties = fluid.get_props(cp.HmassSmass_INPUTS, h, s)
-    rho = static_properties["d"]
-    mu = static_properties["mu"]
-    a = static_properties["a"]
-
-    # Calculate stagnation properties
-    stagnation_properties = fluid.get_props(cp.HmassSmass_INPUTS, h0, s)
-    stagnation_properties = utils.add_string_to_keys(stagnation_properties, "0")
-
-    # Calculate relative stagnation properties
-    h0_rel = h + 0.5 * w**2
-    relative_stagnation_properties = fluid.get_props(cp.HmassSmass_INPUTS, h0_rel, s)
-    relative_stagnation_properties = utils.add_string_to_keys(
-        relative_stagnation_properties, "0_rel"
-    )
-
-    # Calculate mach, reynolds and mass flow rate for cascade inlet
-    Ma = v / a
-    Ma_rel = w / a
-    Re = rho * w * chord / mu
-    m = rho * w_m * area
-    rothalpy = h0_rel - 0.5 * blade_speed**2
-
-    # Store results in dictionary
-    plane = {
-        **velocity_triangle,
-        **static_properties,
-        **stagnation_properties,
-        **relative_stagnation_properties,
-        "Ma": Ma,
-        "Ma_rel": Ma_rel,
-        "Re": Re,
-        "mass_flow": m,
-        "rothalpy": rothalpy,
-    }
-
-    return plane
-
-
-def evaluate_cascade_exit(
-    cascade_exit_input,
-    fluid,
-    geometry,
-    inlet_plane,
-    angular_speed,
-    blockage,
-    loss_model,
-):
-    r"""
-
-    Evaluate the exit plane of a cascade including velocity triangles,
-    thermodynamic properties, flow characteristics and loss coefficients.
-
-    This function calculates performance data at the exit of a cascade based on the cascade geometry,
-    fluid, and flow conditions. It computes velocity triangles, static and stagnation properties,
-    Reynolds and Mach numbers, the mass flow rate and loss coefficients at the exit.
-
-    Parameters
-    ----------
-    cascade_exit_input : dict
-        Input parameters specific to the cascade exit, including relative velocity (`w`),
-        relative flow angle (`beta`), entropy (`s`), and rothalpy.
-    fluid : object
-        A fluid object with methods for thermodynamic property calculations.
-    geometry : dict
-        Geometric parameters of the cascade such as chord length, opening, and area.
-    inlet_plane : dict
-        performance data at the inlet plane of the cascade.
-    angular_speed : float
-        Angular speed of the cascade.
-    blockage : str or float or None
-        The method or value for determining the throat blockage. It can be
-        a string specifying a model name (`flat_plate_turbulent`), a numeric
-        value between 0 and 1 representing the blockage factor directly, or
-        None to use a default calculation method.
-    loss_model : str
-        The loss model used for calculating loss coefficients.
-
-    Returns
-    -------
-    dict
-        A dictionary of calculated parameters at the cascade exit.
-    dict
-        A dictionary of loss coefficients as calculated by the loss model.
-
-    """
-
-    # Load cascade exit variables
-    w = cascade_exit_input["w"]
-    beta = cascade_exit_input["beta"]
-    s = cascade_exit_input["s"]
-    rothalpy = cascade_exit_input["rothalpy"]
-
-    # Load geometry
-    chord = geometry["chord"]
-    opening = geometry["opening"]
-    area = geometry["A_out"]
-    radius = geometry["radius_mean_out"]
-
-    # Calculate velocity triangles
-    blade_speed = angular_speed * radius
-    velocity_triangle = evaluate_velocity_triangle_out(blade_speed, w, beta)
-    v = velocity_triangle["v"]
-    w_m = velocity_triangle["w_m"]
-
-    # Calculate static properties
-    h = rothalpy + 0.5 * blade_speed**2 - 0.5 * w**2
-    static_properties = fluid.get_props(cp.HmassSmass_INPUTS, h, s)
-    rho = static_properties["d"]
-    mu = static_properties["mu"]
-    a = static_properties["a"]
-
-    # Calculate stagnation properties
-    h0 = h + 0.5 * v**2
-    stagnation_properties = fluid.get_props(cp.HmassSmass_INPUTS, h0, s)
-    stagnation_properties = utils.add_string_to_keys(stagnation_properties, "0")
-
-    # Calculate relative stagnation properties
-    h0_rel = h + 0.5 * w**2
-    relative_stagnation_properties = fluid.get_props(cp.HmassSmass_INPUTS, h0_rel, s)
-    relative_stagnation_properties = utils.add_string_to_keys(
-        relative_stagnation_properties, "0_rel"
-    )
-
-    # Calculate mach, reynolds and mass flow rate for cascade inlet
-    Ma = v / a
-    Ma_rel = w / a
-    Re = rho * w * chord / mu
-    rothalpy = h0_rel - 0.5 * blade_speed**2
-
-    # Compute mass flow rate
-    blockage_factor = compute_blockage_boundary_layer(blockage, Re, chord, opening)
-    mass_flow = rho * w_m * area * (1 - blockage_factor)
-
-    # Evaluate loss coefficient
-    min_val = 1e-3
-    loss_model_input = {
-        "geometry": geometry,
-        "loss_model": loss_model,
-        "flow": {
-            "p0_rel_in": inlet_plane["p0_rel"],
-            "p0_rel_out": relative_stagnation_properties["p0_rel"],
-            "p_in": inlet_plane["p"],
-            "p_out": static_properties["p"],
-            "beta_out": beta,
-            "beta_in": inlet_plane["beta"],
-            "Ma_rel_in": max(min_val, inlet_plane["Ma_rel"]),
-            "Ma_rel_out": max(min_val, Ma_rel),
-            "Re_in": max(min_val, inlet_plane["Re"]),
-            "Re_out": max(min_val, Re),
-            "gamma_out": static_properties["gamma"],
-        },
-    }
-
-    # Compute loss coefficient from loss model
-    loss_dict = lm.evaluate_loss_model(loss_model, loss_model_input)
-
-    # Store results in dictionary
-    plane = {
-        **velocity_triangle,
-        **static_properties,
-        **stagnation_properties,
-        **relative_stagnation_properties,
-        **loss_dict,
-        "Ma": Ma,
-        "Ma_rel": Ma_rel,
-        "Re": Re,
-        "mass_flow": mass_flow,
-        "rothalpy": rothalpy,
-        "blockage": blockage_factor,
-    }
-    return plane, loss_dict
-
-
-def evaluate_cascade_throat(
-    cascade_throat_input,
-    fluid,
-    geometry,
-    inlet_plane,
-    angular_speed,
-    blockage,
-    loss_model,
-):
-    r"""
-    Evaluate the throat plane of a cascade including velocity triangles,
-    thermodynamic properties, flow characteristics and loss coefficients.
-
-    This function calculates performance data at the throat of a cascade based on the cascade geometry,
-    fluid, and flow conditions. It computes velocity triangles, static and stagnation properties,
-    Reynolds and Mach numbers, the mass flow rate and loss coefficients at the throat.
-
-    Parameters
-    ----------
-    cascade_throat_input : dict
-        Input parameters specific to the cascade throat, including relative velocity (`w`),
-        relative flow angle (`beta`), entropy (`s`), and rothalpy.
-    fluid : object
-        A fluid object with methods for thermodynamic property calculations.
-    geometry : dict
-        Geometric parameters of the cascade such as chord length, opening, and area.
-    inlet_plane : dict
-        performance data at the inlet plane of the cascade.
-    angular_speed : float
-        Angular speed of the cascade.
-    blockage : str or float or None
-        The method or value for determining the throat blockage. It can be
-        a string specifying a model name (`flat_plate_turbulent`), a numeric
-        value between 0 and 1 representing the blockage factor directly, or
-        None to use a default calculation method.
-    loss_model : str
-        The loss model used for calculating loss coefficients.
-
-    Returns
-    -------
-    dict
-        A dictionary of calculated parameters at the cascade throat.
-    dict
-        A dictionary of loss coefficients as calculated by the loss model.
-
-    """
-
-    # Load cascade exit variables
-    w = cascade_throat_input["w"]
-    beta = cascade_throat_input["beta"]
-    s = cascade_throat_input["s"]
-    rothalpy = cascade_throat_input["rothalpy"]
-
-    # Load geometry
-    chord = geometry["chord"]
-    opening = geometry["opening"]
-    area = geometry["A_throat"]
-    radius = geometry["radius_mean_throat"]
-
-    # Calculate velocity triangles
-    blade_speed = angular_speed * radius
-    velocity_triangle = evaluate_velocity_triangle_out(blade_speed, w, beta)
-    v = velocity_triangle["v"]
-
-    # Calculate static properties
-    h = rothalpy + 0.5 * blade_speed**2 - 0.5 * w**2
-    static_properties = fluid.get_props(cp.HmassSmass_INPUTS, h, s)
-    rho = static_properties["d"]
-    mu = static_properties["mu"]
-    a = static_properties["a"]
-
-    # Calculate stagnation properties
-    h0 = h + 0.5 * v**2
-    stagnation_properties = fluid.get_props(cp.HmassSmass_INPUTS, h0, s)
-    stagnation_properties = utils.add_string_to_keys(stagnation_properties, "0")
-
-    # Calculate relative stagnation properties
-    h0_rel = h + 0.5 * w**2
-    relative_stagnation_properties = fluid.get_props(cp.HmassSmass_INPUTS, h0_rel, s)
-    relative_stagnation_properties = utils.add_string_to_keys(
-        relative_stagnation_properties, "0_rel"
-    )
-
-    # Calculate mach, reynolds and mass flow rate for cascade inlet
-    Ma = v / a
-    Ma_rel = w / a
-    Re = rho * w * chord / mu
-    rothalpy = h0_rel - 0.5 * blade_speed**2
-
-    # Compute mass flow rate
-    blockage_factor = compute_blockage_boundary_layer(blockage, Re, chord, opening)
-    mass_flow = rho * w * area * (1 - blockage_factor)
-
-    # Evaluate loss coefficient
-    min_val = 1e-3
-    loss_model_input = {
-        "geometry": geometry,
-        "loss_model": loss_model,
-        "flow": {
-            "p0_rel_in": inlet_plane["p0_rel"],
-            "p0_rel_out": relative_stagnation_properties["p0_rel"],
-            "p_in": inlet_plane["p"],
-            "p_out": static_properties["p"],
-            "beta_out": beta,
-            "beta_in": inlet_plane["beta"],
-            "Ma_rel_in": max(min_val, inlet_plane["Ma_rel"]),
-            "Ma_rel_out": max(min_val, Ma_rel),
-            "Re_in": max(min_val, inlet_plane["Re"]),
-            "Re_out": max(min_val, Re),
-            "gamma_out": static_properties["gamma"],
-        },
-    }
-
-    # Compute loss coefficient from loss model
-    loss_dict = lm.evaluate_loss_model(loss_model, loss_model_input)
-
-    # Store results in dictionary
-    plane = {
-        **velocity_triangle,
-        **static_properties,
-        **stagnation_properties,
-        **relative_stagnation_properties,
-        **loss_dict,
-        "Ma": Ma,
-        "Ma_rel": Ma_rel,
-        "Re": Re,
-        "mass_flow": mass_flow,
-        "rothalpy": rothalpy,
-        "blockage": blockage_factor,
-    }
-    return plane, loss_dict
-
-
-def evaluate_cascade_interspace(
-    h0_exit,
-    v_m_exit,
-    v_t_exit,
-    rho_exit,
-    radius_exit,
-    area_exit,
-    blockage_exit,
-    radius_inlet,
-    area_inlet,
-    fluid,
-):
-    """
-    Calculate the inlet conditions for the next cascade based on the exit conditions of the previous cascade.
-
-    This function computes the inlet thermodynamic and velocity conditions for the next cascade using the exit conditions
-    from the previous cascade and the flow equations for the interspace between cascades.
-
-    Assumptions:
-
-    - Conservation of stagnation enthalpy
-    - Conservation of angular momentum
-    - Negligible change in density
-
-
-    Parameters
-    ----------
-    h0_exit : float
-        Stagnation enthalpy at the exit of the previous cascade.
-    v_m_exit : float
-        Meridional component of velocity at the exit of the previous cascade.
-    v_t_exit : float
-        Tangential component of velocity at the exit of the previous cascade.
-    rho_exit : float
-        Fluid density at the exit of the previous cascade.
-    radius_exit : float
-        Radius at the exit of the previous cascade.
-    area_exit : float
-        Flow area at the exit of the previous cascade.
-    radius_inlet : float
-        Radius at the inlet of the next cascade.
-    area_inlet : float
-        Flow area at the inlet of the next cascade.
-    fluid : object
-        A fluid object with methods for thermodynamic property calculations.
-
-    Returns
-    -------
-    float
-        Stagnation enthalpy at the inlet of the next cascade.
-    float
-        Entropy at the inlet of the next cascade.
-    float
-        Flow angle at the inlet of the next cascade (in degrees).
-    float
-        Velocity at the inlet of the next cascade.
-
-    Warnings
-    --------
-    The assumption of constant density leads to a small inconsistency in the thermodynamic state,
-    manifesting as a slight variation in entropy across the interspace. In future versions of the
-    code, it is recommended to evaluate the interspace with a 1D model for the flow in annular ducts
-    to improve accuracy and consistency in the analysis.
-    """
-
-    # Assume no heat transfer
-    h0_in = h0_exit
-
-    # Assume no friction (angular momentum is conserved)
-    v_t_in = v_t_exit * radius_exit / radius_inlet
-
-    # Assume density variation is negligible
-    v_m_in = v_m_exit * area_exit / area_inlet * (1 - blockage_exit)
-
-    # Compute velocity vector
-    v_in = np.sqrt(v_t_in**2 + v_m_in**2)
-    alpha_in = math.arctand(v_t_in / v_m_in)
-
-    # Compute thermodynamic state
-    h_in = h0_in - 0.5 * v_in**2
-    rho_in = rho_exit
-    stagnation_properties = fluid.get_props(cp.DmassHmass_INPUTS, rho_in, h_in)
-    s_in = stagnation_properties["s"]
-
-    return h0_in, s_in, alpha_in, v_in
-
-
-def evaluate_velocity_triangle_in(blade_speed, v, alpha):
-    """
-    Compute the velocity triangle at the inlet of the cascade.
-
-    This function calculates the components of the velocity triangle at the
-    inlet of a cascade, based on the blade speed, absolute velocity, and
-    absolute flow angle. It computes both the absolute and relative velocity
-    components in the meridional and tangential directions, as well as the
-    relative flow angle.
-
-    Parameters
-    ----------
-    blade_speed : float
-        Blade speed.
-    v : float
-        Absolute velocity.
-    alpha : float
-        Absolute flow angle in radians.
-
-    Returns
-    -------
-    dict
-        A dictionary containing the following properties:
-
-        - `blade_speed` (float): Blade velocity.
-        - `v` (float): Absolute velocity.
-        - `v_m` (float): Meridional component of absolute velocity.
-        - `v_t` (float): Tangential component of absolute velocity.
-        - `alpha` (float): Absolute flow angle in radians.
-        - `w` (float): Relative velocity magnitude.
-        - `w_m` (float): Meridional component of relative velocity.
-        - `w_t` (float): Tangential component of relative velocity.
-        - `beta` (float): Relative flow angle in radians.
-    """
-
-    # Absolute velocities
-    v_t = v * math.sind(alpha)
-    v_m = v * math.cosd(alpha)
-
-    # Relative velocities
-    w_t = v_t - blade_speed
-    w_m = v_m
-    w = np.sqrt(w_t**2 + w_m**2)
-
-    # Relative flow angle
-    beta = math.arctand(w_t / w_m)
-
-    # Store in dict
-    vel_in = {
-        "blade_speed": blade_speed,
-        "v": v,
-        "v_m": v_m,
-        "v_t": v_t,
-        "alpha": alpha,
-        "w": w,
-        "w_m": w_m,
-        "w_t": w_t,
-        "beta": beta,
-    }
-
-    return vel_in
-
-
-def evaluate_velocity_triangle_out(blade_speed, w, beta):
-    """
-    Compute the velocity triangle at the outlet of the cascade.
-
-    This function calculates the components of the velocity triangle at the
-    outlet of a cascade, based on the blade speed, relative velocity, and
-    relative flow angle. It computes both the absolute and relative velocity
-    components in the meridional and tangential directions, as well as the
-    absolute flow angle.
-
-    Parameters
-    ----------
-    blade_speed : float
-        Blade speed.
-    w : float
-        Relative velocity.
-    beta : float
-        Relative flow angle in radians.
-
-    Returns
-    -------
-    dict
-        A dictionary containing the following properties:
-
-        - `blade_speed` (float): Blade velocity.
-        - `v` (float): Absolute velocity.
-        - `v_m` (float): Meridional component of absolute velocity.
-        - `v_t` (float): Tangential component of absolute velocity.
-        - `alpha` (float): Absolute flow angle in radians.
-        - `w` (float): Relative velocity magnitude.
-        - `w_m` (float): Meridional component of relative velocity.
-        - `w_t` (float): Tangential component of relative velocity.
-        - `beta` (float): Relative flow angle in radians.
-    """
-
-    # Relative velocities
-    w_t = w * math.sind(beta)
-    w_m = w * math.cosd(beta)
-
-    # Absolute velocities
-    v_t = w_t + blade_speed
-    v_m = w_m
-    v = np.sqrt(v_t**2 + v_m**2)
-
-    # Absolute flow angle
-    alpha = math.arctand(v_t / v_m)
-
-    # Store in dict
-    vel_out = {
-        "blade_speed": blade_speed,
-        "v": v,
-        "v_m": v_m,
-        "v_t": v_t,
-        "alpha": alpha,
-        "w": w,
-        "w_m": w_m,
-        "w_t": w_t,
-        "beta": beta,
-    }
-
-    return vel_out
-
-
-def compute_blockage_boundary_layer(blockage_model, Re, chord, opening):
-    r"""
-    Calculate the blockage factor due to boundary layer displacement thickness.
-
-    This function computes the blockage factor caused by the boundary layer
-    displacement thickness at a given flow station. The blockage factor affects
-    mass flow rate calculations effectively reducing the flow area.
-
-    The blockage factor can be determined through various methods, including:
-
-        1. Calculation based on a correlation for the displacement thickness of turbulent boundary layer over a flat plate with zero pressure gradient.
-        2. Using a numerical value specified directly by the user.
-
-    The correlation for turbulent boundary layer displacement thickness over a flat plate is given by :cite:`cengel_fluid_2014`:
-
-    .. math::
-        \delta^* = \frac{0.048}{Re^{1/5}} \times 0.9 \times \text{chord}
-
-    From this the blockage factor is calculated as
-
-    .. math::
-        \text{blockage_factor} = 2 \times \frac{\delta^*}{\text{opening}}
-
-    Parameters
-    ----------
-    blockage_model : str or float or None
-        The method or value for determining the blockage factor. It can be
-        a string specifying a model name (`flat_plate_turbulent`), a numeric
-        value between 0 and 1 representing the blockage factor directly, or
-        None to use a default calculation method.
-    Re : float
-        Reynolds number, used if `blockage_model` is `flat_plate_turbulent`.
-    chord : float
-        Chord length, used if `blockage_model` is `flat_plate_turbulent`.
-    opening : float
-        Throat opening, used if `blockage_model` is `flat_plate_turbulent`.
-
-    Returns
-    -------
-    float
-        The calculated blockage factor, a value between 0 and 1, where 1
-        indicates full blockage and 0 indicates no blockage.
-
-    Raises
-    ------
-    ValueError
-        If `blockage_model` is an invalid option, or required parameters
-        for the chosen method are missing.
-    """
-
-    if blockage_model == BLOCKAGE_MODELS[0]:
-        displacement_thickness = 0.048 / Re ** (1 / 5) * 0.9 * chord
-        blockage_factor = 2 * displacement_thickness / opening
-
-    elif isinstance(blockage_model, (float, int)) and 0 <= blockage_model <= 1:
-        blockage_factor = blockage_model
-
-    elif blockage_model is None:
-        blockage_factor = 0.00
-
-    else:
-        raise ValueError(
-            f"Invalid throat blockage option: '{blockage_model}'. "
-            "Valid options are 'flat_plate_turbulent', a numeric value between 0 and 1, or None."
-        )
-
-    return blockage_factor
-
-
-def compute_efficiency_breakdown(results):
-    """
-    Compute the loss of total-to-static efficiency due to each various loss component in cascades.
-
-    This function calculates the fraction of total-to-static efficiency drop attributed to each loss component
-    in a turbine cascade. A correction factor for the re-heating effect is applied to align the sum of individual
-    cascade losses with the total observed change in enthalpy and ensure consistency with
-    the overall total-to-static efficiency.
-
-    Parameters
-    ----------
-    results : dict
-        The data for the cascades, including plane and cascade specific parameters.
-
-    Returns
-    -------
-    pd.DataFrame
-        A DataFrame containing efficiency drop fractions for each loss type in each cascade.
-        Columns are named as `efficiency_drop_{loss_type}`, where `loss_type` includes:
-
-        - `profile`
-        - `incidence`
-        - `secondary`
-        - `clearance`
-        - `trailing`
-
-    """
-    # Load parameters
-    h_out_s = results["reference_values"]["h_out_s"]
-    h0_in = results["plane"]["h0"].values[0]
-    h_out = results["plane"]["h"].values[-1]
-    cascade = results["cascade"]
-    number_of_cascades = results["geometry"]["number_of_cascades"]
-
-    # Compute a correction factor due to re-heating effect
-    dhs_total = h_out - h_out_s
-    dhs_sum = cascade["dh_s"].sum()
-    correction = dhs_total / dhs_sum
-
-    # Initialize DataFrame
-    loss_types = ["profile", "incidence", "secondary", "clearance", "trailing"]
-    breakdown = pd.DataFrame(columns=[f"efficiency_drop_{type}" for type in loss_types])
-
-    # Loss breakdown in each cascade
-    for i in range(number_of_cascades):
-        # Construct column names dynamically based on loss_types
-        col_names = [f"loss_{type}" for type in loss_types]
-
-        # Retrieve values for each specified column and compute the fractions
-        fracs = cascade.loc[i, col_names] / cascade.loc[i, "loss_total"]
-        dh_s = cascade["dh_s"][i]
-        efficiency_drop = correction * dh_s / (h0_in - h_out_s)
-
-        # Append efficiency drop breakdown
-        breakdown.loc[len(breakdown)] = (fracs * efficiency_drop).values
-
-    return breakdown
-
-
-def compute_stage_performance(results):
-    r"""
-    Calculate the stage performance metrics of the turbine.
-
-    This function extracts necessary values from the `results` dictionary, performs calculations to determine
-    stage performance metrics, and returns these in a dictionary.
-
-    Parameters
-    ----------
-    results : dict
-        A dictionary containing all necessary information, such as geometry and flow characteristics.
-
-    Returns
-    -------
-    dict
-        A dictionary with calculated stage parameters.
-
-    """
-
-    # Only proceed if there are stages
-    number_of_stages = results["geometry"]["number_of_stages"]
-    if number_of_stages == 0:
-        return {}
-
-    # Calculate the degree of reaction for each stage using list comprehension
-    h = results["plane"]["h"].values
-    R = np.array(
-        [
-            (h[i * 4 + 1] - h[i * 4 + 3]) / (h[i * 4] - h[i * 4 + 3])
-            for i in range(number_of_stages)
-        ]
-    )
-
-    # Store all variables in dictionary
-    stages = {"reaction": R}
-
-    return stages
-
-
-def compute_overall_performance(results, geometry):
-    """
-    Calculate the overall performance metrics of the turbine.
-
-    This function extracts necessary values from the `results` dictionary, performs calculations to determine
-    overall performance metrics, and returns these in a dictionary.
-
-    Parameters
-    ----------
-    results : dict
-        A dictionary containing all necessary information, such as geometry and flow characteristics.
-
-    Returns
-    -------
-    dict
-        An dictionary containing the calculated performance metrics.
-    """
-
-    angular_speed = results["boundary_conditions"]["omega"]
-    v0 = results["reference_values"]["v0"]
-    h_out_s = results["reference_values"]["h_out_s"]
-    d_out_s = results["reference_values"]["d_out_s"]
-
-    # Calculation of overall performance
-    p = results["plane"]["p"].values
-    p0 = results["plane"]["p0"].values
-    h0 = results["plane"]["h0"].values
-    v_out = results["plane"]["v"].values[-1]
-    u_out = results["plane"]["blade_speed"].values[-1]
-    mass_flow = results["plane"]["mass_flow"].values[-1]
-    exit_flow_angle = results["plane"]["alpha"].values[-1]
-    PR_tt = p0[0] / p0[-1]
-    PR_ts = p0[0] / p[-1]
-    h0_in = h0[0]
-    h0_out = h0[-1]
-    efficiency_tt = (h0_in - h0_out) / (h0_in - h_out_s - 0.5 * v_out**2) * 100
-    efficiency_ts = (h0_in - h0_out) / (h0_in - h_out_s) * 100
-    efficiency_ts_drop_kinetic = 0.5 * v_out**2 / (h0_in - h_out_s)
-    efficiency_ts_drop_losses = 1.0 - efficiency_ts - efficiency_ts_drop_kinetic
-    power = mass_flow * (h0_in - h0_out)
-    torque = power / angular_speed
-    specific_speed = (
-        angular_speed * (mass_flow / d_out_s) ** 0.5 / ((h0_in - h_out_s) ** 0.75)
-    )
-
-    # Store all variables in dictionary
-    overall = {
-        "PR_tt": PR_tt,
-        "PR_ts": PR_ts,
-        "mass_flow_rate": mass_flow,
-        "efficiency_tt": efficiency_tt,
-        "efficiency_ts": efficiency_ts,
-        "efficiency_ts_drop_kinetic": efficiency_ts_drop_kinetic,
-        "efficiency_ts_drop_losses": efficiency_ts_drop_losses,
-        "power": power,
-        "torque": torque,
-        "angular_speed": angular_speed,
-        "exit_flow_angle": exit_flow_angle,
-        "exit_velocity": v_out,
-        "spouting_velocity": v0,
-        "last_blade_velocity": u_out,
-        "blade_jet_ratio": u_out / v0,
-        "h0_in": h0_in,
-        "h0_out": h0_out,
-        "h_out_s": h_out_s,
-        "specific_speed": specific_speed,
-        "blade_jet_ratio_hub": angular_speed * geometry["radius_hub_out"][-1] / v0,
-        "blade_jet_ratio_mean": angular_speed * geometry["radius_mean_out"][-1] / v0,
-        "blade_jet_ratio_tip": angular_speed * geometry["radius_tip_out"][-1] / v0,
-    }
-
-    return overall
+import numpy as np
+import pandas as pd
+import CoolProp as cp
+from scipy.linalg import solve
+from scipy.optimize._numdiff import approx_derivative
+
+from .. import math
+from .. import utilities as utils
+from . import loss_model as lm
+from . import deviation_model as dm
+from . import choking_model as cm
+
+# List of valid options
+BLOCKAGE_MODELS = ["flat_plate_turbulent"]
+
+
+def evaluate_axial_turbine(
+    variables,
+    boundary_conditions,
+    geometry,
+    fluid,
+    model_options,
+    reference_values,
+):
+    """
+    Compute the performance of an axial turbine by evaluating a series of cascades.
+
+    The function iterates through each cascade, accumulating data on both performance and residuals used to evaluate the physicality of the flow.
+    Between each cascade, the space between the cascades is modelled to give the input to the next cascade.
+
+    The results are organized into a dictionary that includes:
+
+    - `cascade`: Contains data specific to each cascade in the series, including loss coefficients and critical conditions.
+    - `plane`: Contains data specific to each flow station, including thermodynamic properties and velocity triangles.
+    - `stage`: Contains data specific to each turbine stage, including degree of reaction.
+    - `geometry`: Contains data on the turbine geometry.
+    - `overall`: Summarizes the overall performance of the turbine, including mass flow rate, efficiency and power output.
+    - `reference_values`: Contain supplementary information.
+    - `residuals`: Summarizes the mismatch in the nonlinear system of equations used to model the turbine.
+
+    .. note::
+
+        The output of this function can only be regarded as a physically meaningful solution when the equations are fully closed (i.e., all residuals are close to zero).
+        Therefore, this function is intended to be used within a root-finding or optimization algorithm that systematically adjusts the input variables to drive the residuals to zero and close the system of equations.
+
+    Parameters
+    ----------
+    variables : dict
+        Dictionary containing flow variable for the cascades.
+    boundary_conditions : dict
+        Dictionary containing boundary conditions for the series of cascades.
+    geometry : dict
+        Dictionary containing geometric parameters of the series of cascades.
+    fluid : object
+        A fluid object with methods for thermodynamic property calculations.
+    model_options : dict
+        Dictionary containing various model options.
+    reference_values : dict
+        Dictionary containing reference values for scaling.
+
+    Returns
+    -------
+    results : dict
+        Dictionary containing the evaluated results, including planes, cascades, stage, overall,
+        and geometry information.
+
+    """
+
+    # Load variables
+    number_of_cascades = geometry["number_of_cascades"]
+    h0_in = boundary_conditions["h0_in"]
+    s_in = boundary_conditions["s_in"]
+    alpha_in = boundary_conditions["alpha_in"]
+    angular_speed = boundary_conditions["omega"]
+
+    # Load reference_values
+    v0 = reference_values["v0"]
+    s_range = reference_values["s_range"]
+    s_min = reference_values["s_min"]
+    angle_range = reference_values["angle_range"]
+    angle_min = reference_values["angle_min"]
+
+    # Initialize results structure
+    results = {
+        "plane": pd.DataFrame(),
+        "cascade": pd.DataFrame(),
+        "stage": {},
+        "overall": {},
+        "geometry": geometry,
+        "reference_values": reference_values,
+        "boundary_conditions": boundary_conditions,
+    }
+
+    # initialize residual arrays
+    residuals = {}
+
+    # Rename turbine inlet velocity
+    v_in = variables["v_in"] * v0
+
+    for i in range(number_of_cascades):
+        # Update angular speed
+        angular_speed_cascade = angular_speed * (i % 2)
+
+        # update geometry for current cascade
+        geometry_cascade = {
+            key: values[i]
+            for key, values in geometry.items()
+            if key not in ["number_of_cascades", "number_of_stages"]
+        }
+
+        # Rename variables
+        cascade = "_" + str(i + 1)
+        w_out = variables["w_out" + cascade] * v0
+        s_out = variables["s_out" + cascade] * s_range + s_min
+        beta_out = variables["beta_out" + cascade] * angle_range + angle_min
+
+        # Evaluate current cascade
+        cascade_inlet_input = {
+            "h0": h0_in,
+            "s": s_in,
+            "alpha": alpha_in,
+            "v": v_in,
+        }
+        cascade_exit_input = {
+            "w": w_out,
+            "beta": beta_out,
+            "s": s_out,
+        }
+
+        choking_input = {
+            key.replace(cascade, ""): val
+            for key, val in variables.items()
+            if ("*" and cascade in key) or key == "v*_in"
+        }
+
+        cascade_residuals = evaluate_cascade(
+            cascade_inlet_input,
+            cascade_exit_input,
+            choking_input,
+            fluid,
+            geometry_cascade,
+            angular_speed_cascade,
+            results,
+            model_options,
+            reference_values,
+        )
+
+        # Add cascade residuals to residual array
+        cascade_residuals = utils.add_string_to_keys(cascade_residuals, f"_{i+1}")
+        residuals.update(cascade_residuals)
+
+        # Calculate input of next cascade
+        if i != number_of_cascades - 1:
+            (
+                h0_in,
+                s_in,
+                alpha_in,
+                v_in,
+            ) = evaluate_cascade_interspace(
+                results["plane"]["h0"].values[-1],
+                results["plane"]["v_m"].values[-1],
+                results["plane"]["v_t"].values[-1],
+                results["plane"]["d"].values[-1],
+                geometry_cascade["radius_mean_out"],
+                geometry_cascade["A_out"],
+                results["plane"]["blockage"].values[-1],
+                geometry["radius_mean_in"][i + 1],
+                geometry["A_in"][i + 1],
+                fluid,
+            )
+
+    # Add exit pressure error to residuals
+    p_calc = results["plane"]["p"].values[-1]
+    p_error = (p_calc - boundary_conditions["p_out"]) / boundary_conditions["p0_in"]
+    residuals["p_out"] = p_error
+
+    # Additional calculations
+    loss_fractions = compute_efficiency_breakdown(results)
+    results["cascade"] = pd.concat([results["cascade"], loss_fractions], axis=1)
+
+    # Compute stage performance
+    results["stage"] = pd.DataFrame(compute_stage_performance(results))
+
+    # Compute overall perfrormance
+    results["overall"] = pd.DataFrame([compute_overall_performance(results, geometry)])
+
+    # Store all residuals for export
+    results["residuals"] = residuals
+
+    # Store the input variables
+    results["independent_variables"] = variables
+
+    # Retain only variables defined per cascade
+    geom_cascades = {
+        key: value
+        for key, value in geometry.items()
+        if len(utils.ensure_iterable(value)) == number_of_cascades
+    }
+    results["geometry"] = pd.DataFrame([geom_cascades])
+
+    return results
+
+
+def evaluate_cascade(
+    cascade_inlet_input,
+    cascade_exit_input,
+    choking_input,
+    fluid,
+    geometry,
+    angular_speed,
+    results,
+    model_options,
+    reference_values,
+):
+    """
+    Evaluate the performance of a cascade configuration.
+
+    This function evaluates the cascade performance by considering inlet, throat, and exit planes.
+    It also determines the condition for choking according to the selected choking model, and evaluates if the cascade is choked or not. The results are stored in a dictionary.
+    The function returns a dictionary of residuals that includes the mass balance error at both the inlet and exit, loss coefficient errors,
+    and the residuals related to the critical state and choking condition.
+
+    Parameters
+    ----------
+    cascade_inlet_input : dict
+        Input conditions at the cascade inlet.
+    cascade_throat_input : dict
+        Input conditions at the cascade throat.
+    cascade_exit_input : dict
+        Input conditions at the cascade exit.
+    critical_cascade_input : dict
+        Input conditions for critical state evaluation.
+    fluid : object
+        A fluid object with methods for thermodynamic property calculations.
+    geometry : dict
+        Dictionary containing geometric parameters of the cascade.
+    angular_speed : float
+        Angular speed of the cascade.
+    results : dict
+        Dictionary to store the evaluation results.
+    model_options : dict
+        Dictionary containing various model options.
+    reference_values : dict
+        Dictionary containing reference values for normalization.
+
+    Returns
+    -------
+    residuals : dict
+        A dictionary containing the residuals of the cascade evaluation.
+
+    """
+
+    # Define model options
+    # TODO Add warnings that some settings have been assumed. This is a dangerous silent behavior as it is now
+    # TODO It would make sense to define default values for these options at a higher level, for instance after processing the configuration file
+    # TODO an advantage of the approach above is that it is possible to print a warning that will not be shown for each iteration of the solvers
+    loss_model = model_options["loss_model"]
+    deviation_model = model_options["deviation_model"]
+
+    # Load reference values
+    mass_flow_reference = reference_values["mass_flow_ref"]
+
+    # Evaluate inlet plane
+    inlet_plane = evaluate_cascade_inlet(
+        cascade_inlet_input, fluid, geometry, angular_speed
+    )
+
+    # Evaluate exit plane
+    cascade_exit_input["rothalpy"] = inlet_plane["rothalpy"]
+    exit_plane, loss_dict = evaluate_cascade_exit(
+        cascade_exit_input,
+        fluid,
+        geometry,
+        inlet_plane,
+        angular_speed,
+        model_options["blockage_model"],
+        loss_model,
+    )
+
+    # Evaluate isentropic enthalpy change
+    props_is = fluid.get_props(cp.PSmass_INPUTS, exit_plane["p"], inlet_plane["s"])
+    dh_is = exit_plane["h"] - props_is["h"]
+
+    # Evaluate critical state
+    residuals_critical, critical_state = cm.evaluate_choking(
+        choking_input,
+        inlet_plane,
+        exit_plane,
+        fluid,
+        geometry,
+        angular_speed,
+        model_options,
+        reference_values,
+    )
+
+    # Create dictionary with equation residuals
+    mass_error_exit = inlet_plane["mass_flow"] - exit_plane["mass_flow"]
+    residuals = {
+        "loss_error_exit": exit_plane["loss_error"],
+        "mass_error_exit": mass_error_exit / mass_flow_reference,
+        **residuals_critical,
+    }
+
+    # Return plane data in dataframe
+    results["plane"].loc[len(results["plane"]), inlet_plane.keys()] = inlet_plane
+    results["plane"].loc[len(results["plane"]), exit_plane.keys()] = exit_plane
+
+    # Return cascade data in dataframe
+    cascade_data = {
+        **loss_dict,
+        **critical_state,
+        "dh_s": dh_is,
+        "incidence": inlet_plane["beta"] - geometry["leading_edge_angle"],
+    }
+
+    results["cascade"].loc[len(results["cascade"]), cascade_data.keys()] = cascade_data
+
+    return residuals
+
+
+def evaluate_cascade_inlet(cascade_inlet_input, fluid, geometry, angular_speed):
+    r"""
+    Evaluate the inlet plane of a cascade including velocity triangles,
+    thermodynamic properties, and flow characteristics.
+
+    This function calculates performance data at the inlet of a cascade based on the cascade geometry,
+    fluid, and flow conditions. It computes velocity triangles, static and stagnation properties,
+    Reynolds and Mach numbers, and the mass flow rate at the inlet.
+
+    Parameters
+    ----------
+    cascade_inlet_input : dict
+        Input parameters specific to the cascade inlet, including stagnation enthalpy (`h0`),
+        entropy (`s`), absolute velocity (`v`), and flow angle (`alpha`).
+    fluid : object
+        A fluid object with methods for thermodynamic property calculations.
+    geometry : dict
+        Geometric parameters of the cascade such as radius at the mean inlet (`radius_mean_in`),
+        chord length, and inlet area (`A_in`).
+    angular_speed : float
+        Angular speed of the cascade.
+
+    Returns
+    -------
+    dict
+        A dictionary of calculated parameters at the cascade inlet.
+    """
+    # Load cascade inlet input
+    h0 = cascade_inlet_input["h0"]
+    s = cascade_inlet_input["s"]
+    v = cascade_inlet_input["v"]
+    alpha = cascade_inlet_input["alpha"]
+
+    # Load geometry
+    radius = geometry["radius_mean_in"]
+    chord = geometry["chord"]
+    area = geometry["A_in"]
+
+    # Calculate velocity triangles
+    blade_speed = radius * angular_speed
+    velocity_triangle = evaluate_velocity_triangle_in(blade_speed, v, alpha)
+    w = velocity_triangle["w"]
+    w_m = velocity_triangle["w_m"]
+
+    # Calculate static properties
+    h = h0 - 0.5 * v**2
+    static_properties = fluid.get_props(cp.HmassSmass_INPUTS, h, s)
+    rho = static_properties["d"]
+    mu = static_properties["mu"]
+    a = static_properties["a"]
+
+    # Calculate stagnation properties
+    stagnation_properties = fluid.get_props(cp.HmassSmass_INPUTS, h0, s)
+    stagnation_properties = utils.add_string_to_keys(stagnation_properties, "0")
+
+    # Calculate relative stagnation properties
+    h0_rel = h + 0.5 * w**2
+    relative_stagnation_properties = fluid.get_props(cp.HmassSmass_INPUTS, h0_rel, s)
+    relative_stagnation_properties = utils.add_string_to_keys(
+        relative_stagnation_properties, "0_rel"
+    )
+
+    # Calculate mach, reynolds and mass flow rate for cascade inlet
+    Ma = v / a
+    Ma_rel = w / a
+    Re = rho * w * chord / mu
+    m = rho * w_m * area
+    rothalpy = h0_rel - 0.5 * blade_speed**2
+
+    # Store results in dictionary
+    plane = {
+        **velocity_triangle,
+        **static_properties,
+        **stagnation_properties,
+        **relative_stagnation_properties,
+        "Ma": Ma,
+        "Ma_rel": Ma_rel,
+        "Re": Re,
+        "mass_flow": m,
+        "rothalpy": rothalpy,
+    }
+
+    return plane
+
+
+def evaluate_cascade_exit(
+    cascade_exit_input,
+    fluid,
+    geometry,
+    inlet_plane,
+    angular_speed,
+    blockage,
+    loss_model,
+):
+    r"""
+
+    Evaluate the exit plane of a cascade including velocity triangles,
+    thermodynamic properties, flow characteristics and loss coefficients.
+
+    This function calculates performance data at the exit of a cascade based on the cascade geometry,
+    fluid, and flow conditions. It computes velocity triangles, static and stagnation properties,
+    Reynolds and Mach numbers, the mass flow rate and loss coefficients at the exit.
+
+    Parameters
+    ----------
+    cascade_exit_input : dict
+        Input parameters specific to the cascade exit, including relative velocity (`w`),
+        relative flow angle (`beta`), entropy (`s`), and rothalpy.
+    fluid : object
+        A fluid object with methods for thermodynamic property calculations.
+    geometry : dict
+        Geometric parameters of the cascade such as chord length, opening, and area.
+    inlet_plane : dict
+        performance data at the inlet plane of the cascade.
+    angular_speed : float
+        Angular speed of the cascade.
+    blockage : str or float or None
+        The method or value for determining the throat blockage. It can be
+        a string specifying a model name (`flat_plate_turbulent`), a numeric
+        value between 0 and 1 representing the blockage factor directly, or
+        None to use a default calculation method.
+    loss_model : str
+        The loss model used for calculating loss coefficients.
+
+    Returns
+    -------
+    dict
+        A dictionary of calculated parameters at the cascade exit.
+    dict
+        A dictionary of loss coefficients as calculated by the loss model.
+
+    """
+
+    # Load cascade exit variables
+    w = cascade_exit_input["w"]
+    beta = cascade_exit_input["beta"]
+    s = cascade_exit_input["s"]
+    rothalpy = cascade_exit_input["rothalpy"]
+
+    # Load geometry
+    chord = geometry["chord"]
+    opening = geometry["opening"]
+    area = geometry["A_out"]
+    radius = geometry["radius_mean_out"]
+
+    # Calculate velocity triangles
+    blade_speed = angular_speed * radius
+    velocity_triangle = evaluate_velocity_triangle_out(blade_speed, w, beta)
+    v = velocity_triangle["v"]
+    w_m = velocity_triangle["w_m"]
+
+    # Calculate static properties
+    h = rothalpy + 0.5 * blade_speed**2 - 0.5 * w**2
+    static_properties = fluid.get_props(cp.HmassSmass_INPUTS, h, s)
+    rho = static_properties["d"]
+    mu = static_properties["mu"]
+    a = static_properties["a"]
+
+    # Calculate stagnation properties
+    h0 = h + 0.5 * v**2
+    stagnation_properties = fluid.get_props(cp.HmassSmass_INPUTS, h0, s)
+    stagnation_properties = utils.add_string_to_keys(stagnation_properties, "0")
+
+    # Calculate relative stagnation properties
+    h0_rel = h + 0.5 * w**2
+    relative_stagnation_properties = fluid.get_props(cp.HmassSmass_INPUTS, h0_rel, s)
+    relative_stagnation_properties = utils.add_string_to_keys(
+        relative_stagnation_properties, "0_rel"
+    )
+
+    # Calculate mach, reynolds and mass flow rate for cascade inlet
+    Ma = v / a
+    Ma_rel = w / a
+    Re = rho * w * chord / mu
+    rothalpy = h0_rel - 0.5 * blade_speed**2
+
+    # Compute mass flow rate
+    blockage_factor = compute_blockage_boundary_layer(blockage, Re, chord, opening)
+    mass_flow = rho * w_m * area * (1 - blockage_factor)
+
+    # Evaluate loss coefficient
+    min_val = 1e-3
+    loss_model_input = {
+        "geometry": geometry,
+        "loss_model": loss_model,
+        "flow": {
+            "p0_rel_in": inlet_plane["p0_rel"],
+            "p0_rel_out": relative_stagnation_properties["p0_rel"],
+            "p_in": inlet_plane["p"],
+            "p_out": static_properties["p"],
+            "beta_out": beta,
+            "beta_in": inlet_plane["beta"],
+            "Ma_rel_in": max(min_val, inlet_plane["Ma_rel"]),
+            "Ma_rel_out": max(min_val, Ma_rel),
+            "Re_in": max(min_val, inlet_plane["Re"]),
+            "Re_out": max(min_val, Re),
+            "gamma_out": static_properties["gamma"],
+        },
+    }
+
+    # Compute loss coefficient from loss model
+    loss_dict = lm.evaluate_loss_model(loss_model, loss_model_input)
+
+    # Store results in dictionary
+    plane = {
+        **velocity_triangle,
+        **static_properties,
+        **stagnation_properties,
+        **relative_stagnation_properties,
+        **loss_dict,
+        "Ma": Ma,
+        "Ma_rel": Ma_rel,
+        "Re": Re,
+        "mass_flow": mass_flow,
+        "rothalpy": rothalpy,
+        "blockage": blockage_factor,
+    }
+    return plane, loss_dict
+
+
+def evaluate_cascade_throat(
+    cascade_throat_input,
+    fluid,
+    geometry,
+    inlet_plane,
+    angular_speed,
+    blockage,
+    loss_model,
+):
+    r"""
+    Evaluate the throat plane of a cascade including velocity triangles,
+    thermodynamic properties, flow characteristics and loss coefficients.
+
+    This function calculates performance data at the throat of a cascade based on the cascade geometry,
+    fluid, and flow conditions. It computes velocity triangles, static and stagnation properties,
+    Reynolds and Mach numbers, the mass flow rate and loss coefficients at the throat.
+
+    Parameters
+    ----------
+    cascade_throat_input : dict
+        Input parameters specific to the cascade throat, including relative velocity (`w`),
+        relative flow angle (`beta`), entropy (`s`), and rothalpy.
+    fluid : object
+        A fluid object with methods for thermodynamic property calculations.
+    geometry : dict
+        Geometric parameters of the cascade such as chord length, opening, and area.
+    inlet_plane : dict
+        performance data at the inlet plane of the cascade.
+    angular_speed : float
+        Angular speed of the cascade.
+    blockage : str or float or None
+        The method or value for determining the throat blockage. It can be
+        a string specifying a model name (`flat_plate_turbulent`), a numeric
+        value between 0 and 1 representing the blockage factor directly, or
+        None to use a default calculation method.
+    loss_model : str
+        The loss model used for calculating loss coefficients.
+
+    Returns
+    -------
+    dict
+        A dictionary of calculated parameters at the cascade throat.
+    dict
+        A dictionary of loss coefficients as calculated by the loss model.
+
+    """
+
+    # Load cascade exit variables
+    w = cascade_throat_input["w"]
+    beta = cascade_throat_input["beta"]
+    s = cascade_throat_input["s"]
+    rothalpy = cascade_throat_input["rothalpy"]
+
+    # Load geometry
+    chord = geometry["chord"]
+    opening = geometry["opening"]
+    area = geometry["A_throat"]
+    radius = geometry["radius_mean_throat"]
+
+    # Calculate velocity triangles
+    blade_speed = angular_speed * radius
+    velocity_triangle = evaluate_velocity_triangle_out(blade_speed, w, beta)
+    v = velocity_triangle["v"]
+
+    # Calculate static properties
+    h = rothalpy + 0.5 * blade_speed**2 - 0.5 * w**2
+    static_properties = fluid.get_props(cp.HmassSmass_INPUTS, h, s)
+    rho = static_properties["d"]
+    mu = static_properties["mu"]
+    a = static_properties["a"]
+
+    # Calculate stagnation properties
+    h0 = h + 0.5 * v**2
+    stagnation_properties = fluid.get_props(cp.HmassSmass_INPUTS, h0, s)
+    stagnation_properties = utils.add_string_to_keys(stagnation_properties, "0")
+
+    # Calculate relative stagnation properties
+    h0_rel = h + 0.5 * w**2
+    relative_stagnation_properties = fluid.get_props(cp.HmassSmass_INPUTS, h0_rel, s)
+    relative_stagnation_properties = utils.add_string_to_keys(
+        relative_stagnation_properties, "0_rel"
+    )
+
+    # Calculate mach, reynolds and mass flow rate for cascade inlet
+    Ma = v / a
+    Ma_rel = w / a
+    Re = rho * w * chord / mu
+    rothalpy = h0_rel - 0.5 * blade_speed**2
+
+    # Compute mass flow rate
+    blockage_factor = compute_blockage_boundary_layer(blockage, Re, chord, opening)
+    mass_flow = rho * w * area * (1 - blockage_factor)
+
+    # Evaluate loss coefficient
+    min_val = 1e-3
+    loss_model_input = {
+        "geometry": geometry,
+        "loss_model": loss_model,
+        "flow": {
+            "p0_rel_in": inlet_plane["p0_rel"],
+            "p0_rel_out": relative_stagnation_properties["p0_rel"],
+            "p_in": inlet_plane["p"],
+            "p_out": static_properties["p"],
+            "beta_out": beta,
+            "beta_in": inlet_plane["beta"],
+            "Ma_rel_in": max(min_val, inlet_plane["Ma_rel"]),
+            "Ma_rel_out": max(min_val, Ma_rel),
+            "Re_in": max(min_val, inlet_plane["Re"]),
+            "Re_out": max(min_val, Re),
+            "gamma_out": static_properties["gamma"],
+        },
+    }
+
+    # Compute loss coefficient from loss model
+    loss_dict = lm.evaluate_loss_model(loss_model, loss_model_input)
+
+    # Store results in dictionary
+    plane = {
+        **velocity_triangle,
+        **static_properties,
+        **stagnation_properties,
+        **relative_stagnation_properties,
+        **loss_dict,
+        "Ma": Ma,
+        "Ma_rel": Ma_rel,
+        "Re": Re,
+        "mass_flow": mass_flow,
+        "rothalpy": rothalpy,
+        "blockage": blockage_factor,
+    }
+    return plane, loss_dict
+
+
+def evaluate_cascade_interspace(
+    h0_exit,
+    v_m_exit,
+    v_t_exit,
+    rho_exit,
+    radius_exit,
+    area_exit,
+    blockage_exit,
+    radius_inlet,
+    area_inlet,
+    fluid,
+):
+    """
+    Calculate the inlet conditions for the next cascade based on the exit conditions of the previous cascade.
+
+    This function computes the inlet thermodynamic and velocity conditions for the next cascade using the exit conditions
+    from the previous cascade and the flow equations for the interspace between cascades.
+
+    Assumptions:
+
+    - Conservation of stagnation enthalpy
+    - Conservation of angular momentum
+    - Negligible change in density
+
+
+    Parameters
+    ----------
+    h0_exit : float
+        Stagnation enthalpy at the exit of the previous cascade.
+    v_m_exit : float
+        Meridional component of velocity at the exit of the previous cascade.
+    v_t_exit : float
+        Tangential component of velocity at the exit of the previous cascade.
+    rho_exit : float
+        Fluid density at the exit of the previous cascade.
+    radius_exit : float
+        Radius at the exit of the previous cascade.
+    area_exit : float
+        Flow area at the exit of the previous cascade.
+    radius_inlet : float
+        Radius at the inlet of the next cascade.
+    area_inlet : float
+        Flow area at the inlet of the next cascade.
+    fluid : object
+        A fluid object with methods for thermodynamic property calculations.
+
+    Returns
+    -------
+    float
+        Stagnation enthalpy at the inlet of the next cascade.
+    float
+        Entropy at the inlet of the next cascade.
+    float
+        Flow angle at the inlet of the next cascade (in degrees).
+    float
+        Velocity at the inlet of the next cascade.
+
+    Warnings
+    --------
+    The assumption of constant density leads to a small inconsistency in the thermodynamic state,
+    manifesting as a slight variation in entropy across the interspace. In future versions of the
+    code, it is recommended to evaluate the interspace with a 1D model for the flow in annular ducts
+    to improve accuracy and consistency in the analysis.
+    """
+
+    # Assume no heat transfer
+    h0_in = h0_exit
+
+    # Assume no friction (angular momentum is conserved)
+    v_t_in = v_t_exit * radius_exit / radius_inlet
+
+    # Assume density variation is negligible
+    v_m_in = v_m_exit * area_exit / area_inlet * (1 - blockage_exit)
+
+    # Compute velocity vector
+    v_in = np.sqrt(v_t_in**2 + v_m_in**2)
+    alpha_in = math.arctand(v_t_in / v_m_in)
+
+    # Compute thermodynamic state
+    h_in = h0_in - 0.5 * v_in**2
+    rho_in = rho_exit
+    stagnation_properties = fluid.get_props(cp.DmassHmass_INPUTS, rho_in, h_in)
+    s_in = stagnation_properties["s"]
+
+    return h0_in, s_in, alpha_in, v_in
+
+
+def evaluate_velocity_triangle_in(blade_speed, v, alpha):
+    """
+    Compute the velocity triangle at the inlet of the cascade.
+
+    This function calculates the components of the velocity triangle at the
+    inlet of a cascade, based on the blade speed, absolute velocity, and
+    absolute flow angle. It computes both the absolute and relative velocity
+    components in the meridional and tangential directions, as well as the
+    relative flow angle.
+
+    Parameters
+    ----------
+    blade_speed : float
+        Blade speed.
+    v : float
+        Absolute velocity.
+    alpha : float
+        Absolute flow angle in radians.
+
+    Returns
+    -------
+    dict
+        A dictionary containing the following properties:
+
+        - `blade_speed` (float): Blade velocity.
+        - `v` (float): Absolute velocity.
+        - `v_m` (float): Meridional component of absolute velocity.
+        - `v_t` (float): Tangential component of absolute velocity.
+        - `alpha` (float): Absolute flow angle in radians.
+        - `w` (float): Relative velocity magnitude.
+        - `w_m` (float): Meridional component of relative velocity.
+        - `w_t` (float): Tangential component of relative velocity.
+        - `beta` (float): Relative flow angle in radians.
+    """
+
+    # Absolute velocities
+    v_t = v * math.sind(alpha)
+    v_m = v * math.cosd(alpha)
+
+    # Relative velocities
+    w_t = v_t - blade_speed
+    w_m = v_m
+    w = np.sqrt(w_t**2 + w_m**2)
+
+    # Relative flow angle
+    beta = math.arctand(w_t / w_m)
+
+    # Store in dict
+    vel_in = {
+        "blade_speed": blade_speed,
+        "v": v,
+        "v_m": v_m,
+        "v_t": v_t,
+        "alpha": alpha,
+        "w": w,
+        "w_m": w_m,
+        "w_t": w_t,
+        "beta": beta,
+    }
+
+    return vel_in
+
+
+def evaluate_velocity_triangle_out(blade_speed, w, beta):
+    """
+    Compute the velocity triangle at the outlet of the cascade.
+
+    This function calculates the components of the velocity triangle at the
+    outlet of a cascade, based on the blade speed, relative velocity, and
+    relative flow angle. It computes both the absolute and relative velocity
+    components in the meridional and tangential directions, as well as the
+    absolute flow angle.
+
+    Parameters
+    ----------
+    blade_speed : float
+        Blade speed.
+    w : float
+        Relative velocity.
+    beta : float
+        Relative flow angle in radians.
+
+    Returns
+    -------
+    dict
+        A dictionary containing the following properties:
+
+        - `blade_speed` (float): Blade velocity.
+        - `v` (float): Absolute velocity.
+        - `v_m` (float): Meridional component of absolute velocity.
+        - `v_t` (float): Tangential component of absolute velocity.
+        - `alpha` (float): Absolute flow angle in radians.
+        - `w` (float): Relative velocity magnitude.
+        - `w_m` (float): Meridional component of relative velocity.
+        - `w_t` (float): Tangential component of relative velocity.
+        - `beta` (float): Relative flow angle in radians.
+    """
+
+    # Relative velocities
+    w_t = w * math.sind(beta)
+    w_m = w * math.cosd(beta)
+
+    # Absolute velocities
+    v_t = w_t + blade_speed
+    v_m = w_m
+    v = np.sqrt(v_t**2 + v_m**2)
+
+    # Absolute flow angle
+    alpha = math.arctand(v_t / v_m)
+
+    # Store in dict
+    vel_out = {
+        "blade_speed": blade_speed,
+        "v": v,
+        "v_m": v_m,
+        "v_t": v_t,
+        "alpha": alpha,
+        "w": w,
+        "w_m": w_m,
+        "w_t": w_t,
+        "beta": beta,
+    }
+
+    return vel_out
+
+
+def compute_blockage_boundary_layer(blockage_model, Re, chord, opening):
+    r"""
+    Calculate the blockage factor due to boundary layer displacement thickness.
+
+    This function computes the blockage factor caused by the boundary layer
+    displacement thickness at a given flow station. The blockage factor affects
+    mass flow rate calculations effectively reducing the flow area.
+
+    The blockage factor can be determined through various methods, including:
+
+        1. Calculation based on a correlation for the displacement thickness of turbulent boundary layer over a flat plate with zero pressure gradient.
+        2. Using a numerical value specified directly by the user.
+
+    The correlation for turbulent boundary layer displacement thickness over a flat plate is given by :cite:`cengel_fluid_2014`:
+
+    .. math::
+        \delta^* = \frac{0.048}{Re^{1/5}} \times 0.9 \times \text{chord}
+
+    From this the blockage factor is calculated as
+
+    .. math::
+        \text{blockage_factor} = 2 \times \frac{\delta^*}{\text{opening}}
+
+    Parameters
+    ----------
+    blockage_model : str or float or None
+        The method or value for determining the blockage factor. It can be
+        a string specifying a model name (`flat_plate_turbulent`), a numeric
+        value between 0 and 1 representing the blockage factor directly, or
+        None to use a default calculation method.
+    Re : float
+        Reynolds number, used if `blockage_model` is `flat_plate_turbulent`.
+    chord : float
+        Chord length, used if `blockage_model` is `flat_plate_turbulent`.
+    opening : float
+        Throat opening, used if `blockage_model` is `flat_plate_turbulent`.
+
+    Returns
+    -------
+    float
+        The calculated blockage factor, a value between 0 and 1, where 1
+        indicates full blockage and 0 indicates no blockage.
+
+    Raises
+    ------
+    ValueError
+        If `blockage_model` is an invalid option, or required parameters
+        for the chosen method are missing.
+    """
+
+    if blockage_model == BLOCKAGE_MODELS[0]:
+        displacement_thickness = 0.048 / Re ** (1 / 5) * 0.9 * chord
+        blockage_factor = 2 * displacement_thickness / opening
+
+    elif isinstance(blockage_model, (float, int)) and 0 <= blockage_model <= 1:
+        blockage_factor = blockage_model
+
+    elif blockage_model is None:
+        blockage_factor = 0.00
+
+    else:
+        raise ValueError(
+            f"Invalid throat blockage option: '{blockage_model}'. "
+            "Valid options are 'flat_plate_turbulent', a numeric value between 0 and 1, or None."
+        )
+
+    return blockage_factor
+
+
+def compute_efficiency_breakdown(results):
+    """
+    Compute the loss of total-to-static efficiency due to each various loss component in cascades.
+
+    This function calculates the fraction of total-to-static efficiency drop attributed to each loss component
+    in a turbine cascade. A correction factor for the re-heating effect is applied to align the sum of individual
+    cascade losses with the total observed change in enthalpy and ensure consistency with
+    the overall total-to-static efficiency.
+
+    Parameters
+    ----------
+    results : dict
+        The data for the cascades, including plane and cascade specific parameters.
+
+    Returns
+    -------
+    pd.DataFrame
+        A DataFrame containing efficiency drop fractions for each loss type in each cascade.
+        Columns are named as `efficiency_drop_{loss_type}`, where `loss_type` includes:
+
+        - `profile`
+        - `incidence`
+        - `secondary`
+        - `clearance`
+        - `trailing`
+
+    """
+    # Load parameters
+    h_out_s = results["reference_values"]["h_out_s"]
+    h0_in = results["plane"]["h0"].values[0]
+    h_out = results["plane"]["h"].values[-1]
+    cascade = results["cascade"]
+    number_of_cascades = results["geometry"]["number_of_cascades"]
+
+    # Compute a correction factor due to re-heating effect
+    dhs_total = h_out - h_out_s
+    dhs_sum = cascade["dh_s"].sum()
+    correction = dhs_total / dhs_sum
+
+    # Initialize DataFrame
+    loss_types = ["profile", "incidence", "secondary", "clearance", "trailing"]
+    breakdown = pd.DataFrame(columns=[f"efficiency_drop_{type}" for type in loss_types])
+
+    # Loss breakdown in each cascade
+    for i in range(number_of_cascades):
+        # Construct column names dynamically based on loss_types
+        col_names = [f"loss_{type}" for type in loss_types]
+
+        # Retrieve values for each specified column and compute the fractions
+        fracs = cascade.loc[i, col_names] / cascade.loc[i, "loss_total"]
+        dh_s = cascade["dh_s"][i]
+        efficiency_drop = correction * dh_s / (h0_in - h_out_s)
+
+        # Append efficiency drop breakdown
+        breakdown.loc[len(breakdown)] = (fracs * efficiency_drop).values
+
+    return breakdown
+
+
+def compute_stage_performance(results):
+    r"""
+    Calculate the stage performance metrics of the turbine.
+
+    This function extracts necessary values from the `results` dictionary, performs calculations to determine
+    stage performance metrics, and returns these in a dictionary.
+
+    Parameters
+    ----------
+    results : dict
+        A dictionary containing all necessary information, such as geometry and flow characteristics.
+
+    Returns
+    -------
+    dict
+        A dictionary with calculated stage parameters.
+
+    """
+
+    # Only proceed if there are stages
+    number_of_stages = results["geometry"]["number_of_stages"]
+    if number_of_stages == 0:
+        return {}
+
+    # Calculate the degree of reaction for each stage using list comprehension
+    h = results["plane"]["h"].values
+    R = np.array(
+        [
+            (h[i * 4 + 1] - h[i * 4 + 3]) / (h[i * 4] - h[i * 4 + 3])
+            for i in range(number_of_stages)
+        ]
+    )
+
+    # Store all variables in dictionary
+    stages = {"reaction": R}
+
+    return stages
+
+
+def compute_overall_performance(results, geometry):
+    """
+    Calculate the overall performance metrics of the turbine.
+
+    This function extracts necessary values from the `results` dictionary, performs calculations to determine
+    overall performance metrics, and returns these in a dictionary.
+
+    Parameters
+    ----------
+    results : dict
+        A dictionary containing all necessary information, such as geometry and flow characteristics.
+
+    Returns
+    -------
+    dict
+        An dictionary containing the calculated performance metrics.
+    """
+
+    angular_speed = results["boundary_conditions"]["omega"]
+    v0 = results["reference_values"]["v0"]
+    h_out_s = results["reference_values"]["h_out_s"]
+    d_out_s = results["reference_values"]["d_out_s"]
+
+    # Calculation of overall performance
+    p = results["plane"]["p"].values
+    p0 = results["plane"]["p0"].values
+    h0 = results["plane"]["h0"].values
+    v_out = results["plane"]["v"].values[-1]
+    u_out = results["plane"]["blade_speed"].values[-1]
+    mass_flow = results["plane"]["mass_flow"].values[-1]
+    exit_flow_angle = results["plane"]["alpha"].values[-1]
+    PR_tt = p0[0] / p0[-1]
+    PR_ts = p0[0] / p[-1]
+    h0_in = h0[0]
+    h0_out = h0[-1]
+    efficiency_tt = (h0_in - h0_out) / (h0_in - h_out_s - 0.5 * v_out**2) * 100
+    efficiency_ts = (h0_in - h0_out) / (h0_in - h_out_s) * 100
+    efficiency_ts_drop_kinetic = 0.5 * v_out**2 / (h0_in - h_out_s)
+    efficiency_ts_drop_losses = 1.0 - efficiency_ts - efficiency_ts_drop_kinetic
+    power = mass_flow * (h0_in - h0_out)
+    torque = power / angular_speed
+    specific_speed = (
+        angular_speed * (mass_flow / d_out_s) ** 0.5 / ((h0_in - h_out_s) ** 0.75)
+    )
+
+    # Store all variables in dictionary
+    overall = {
+        "PR_tt": PR_tt,
+        "PR_ts": PR_ts,
+        "mass_flow_rate": mass_flow,
+        "efficiency_tt": efficiency_tt,
+        "efficiency_ts": efficiency_ts,
+        "efficiency_ts_drop_kinetic": efficiency_ts_drop_kinetic,
+        "efficiency_ts_drop_losses": efficiency_ts_drop_losses,
+        "power": power,
+        "torque": torque,
+        "angular_speed": angular_speed,
+        "exit_flow_angle": exit_flow_angle,
+        "exit_velocity": v_out,
+        "spouting_velocity": v0,
+        "last_blade_velocity": u_out,
+        "blade_jet_ratio": u_out / v0,
+        "h0_in": h0_in,
+        "h0_out": h0_out,
+        "h_out_s": h_out_s,
+        "specific_speed": specific_speed,
+        "blade_jet_ratio_hub": angular_speed * geometry["radius_hub_out"][-1] / v0,
+        "blade_jet_ratio_mean": angular_speed * geometry["radius_mean_out"][-1] / v0,
+        "blade_jet_ratio_tip": angular_speed * geometry["radius_tip_out"][-1] / v0,
+    }
+
+    return overall
```

### Comparing `turboflow-0.1.2/turboflow/axial_turbine/geometry_model.py` & `turboflow-0.1.3/turboflow/axial_turbine/geometry_model.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,594 +1,594 @@
-import numpy as np
-from .. import math
-from .. import utilities as utils
-
-
-def validate_turbine_geometry(geom, display=False):
-    """
-    Performs validation of an axial turbine's geometry configuration.
-
-    - Ensures all required geometry parameters are specified in 'geom'.
-    - Verifies that no parameters beyond the required set are included.
-    - Checks 'number_of_cascades' is an integer and correctly dictates sizes of other parameters.
-    - Validates that all parameter arrays match the size of 'number_of_cascades'.
-    - Asserts that all geometry parameter values are numeric (integers or floats).
-    - Ensures non-angle parameters contain only non-negative values.
-    - Angle parameters ('leading_edge_angle', 'stagger_angle') can be negative.
-
-    This function is intended as a preliminary check before in-depth geometry analysis.
-
-    Parameters
-    ----------
-    geom : dict
-        The geometry configuration parameters for the turbine as a dictionary.
-
-    Returns
-    -------
-    bool
-        True if all validations pass, indicating a correctly structured geometry configuration.
-    """
-
-    # Define the list of keys that must be defined in the configuration file
-    required_keys = {
-        "cascade_type",
-        "radius_hub_in",
-        "radius_hub_out",
-        "radius_tip_in",
-        "radius_tip_out",
-        "pitch",
-        "chord",
-        "stagger_angle",
-        "opening",
-        "leading_edge_diameter",
-        "leading_edge_wedge_angle",
-        "leading_edge_angle",
-        "trailing_edge_thickness",
-        "tip_clearance",
-        "maximum_thickness",
-        "throat_location_fraction",
-    }
-
-    # Angular variables need not be non-negative
-    angle_keys = ["leading_edge_angle", "stagger_angle"]
-
-    # Check that there are no missing or extra variables
-    utils.validate_keys(geom, required_keys, required_keys)
-
-    # Get the number of cascades
-    number_of_cascades = len(geom["cascade_type"])
-
-    # Check if cascade_type is correctly defined
-    valid_types = np.array(["stator", "rotor"])
-    invalid_types = geom["cascade_type"][~np.isin(geom["cascade_type"], valid_types)]
-    if invalid_types.size > 0:
-        invalid_types_str = ", ".join(invalid_types)
-        raise ValueError(
-            f"Invalid types in 'cascade_type': {invalid_types_str}. Only 'stator' and 'rotor' are allowed."
-        )
-
-    # Check array size and values
-    for key, value in geom.items():
-        # Skip the cascade_type in the loop
-        if key in ["cascade_type", "geometry_type"]:
-            continue
-
-        # Check that all variables are numpy arrays
-        if not isinstance(value, np.ndarray):
-            raise TypeError(f"Parameter '{key}' must be a NumPy array.")
-
-        else:
-            if value.size != number_of_cascades:
-                raise ValueError(
-                    f"Size of '{key}' must be equal to number of cascades. Current value: {value}"
-                )
-
-        # Check that values of parameters are numeric
-        if not math.all_numeric(value):
-            raise ValueError(
-                f"Parameter '{key}' must be an array of numeric types. Current value: {value}"
-            )
-
-        # Check that non-angle variables are not negative
-        if key not in angle_keys and not math.all_non_negative(value):
-            raise ValueError(
-                f"Parameter '{key}' must be an array of non-negative numbers. Current value: {value}"
-            )
-
-    msg = "The structure of the turbine geometry parameters is valid"
-    if display:
-        print(msg)
-
-    return msg
-
-
-def calculate_throat_radius(radius_in, radius_out, throat_location_fraction):
-    """
-    Calculate the throat radius as a weighted average of the inlet and outlet radii.
-
-    This approach to estimate the area at the throat is inspired by the method
-    described in Eq. (3) of :cite:`ainley_method_1951`. The throat radius is computed
-    as a linear combination of the inlet and outlet radii, weighted by the location of
-    the throat expressed as a fraction of the axial length of the cascade.
-    For instance, a `throat_location_fraction` of 5/6 would return the throat radius as:
-    (1/6)*radius_in + (5/6)*radius out.
-
-    Parameters
-    ----------
-    radius_in : np.array
-        The radius values at the inlet sections.
-    radius_out : np.array
-        The radius values at the outlet sections.
-    throat_location_fraction : float
-        The fraction used to weight the inlet and outlet radii in the calculation.
-
-    Returns
-    -------
-    np.array
-        The calculated throat radius values, representing the weighted average of the
-        inlet and outlet radii based on the specified throat location.
-    """
-    return (
-        1 - throat_location_fraction
-    ) * radius_in + throat_location_fraction * radius_out
-
-
-def prepare_geometry(geometry, radius_type):
-    """
-
-    Convert the geometrical design variables to parameters suitable for turbine evaluation.
-
-    For design optimization, the geometrical design variables are defined as ratios, e.g. hub-to-tip radius and aspect ratio.
-    This function convert the set of geometrical design variables to a set which is suited for turbine evaluation.
-    The function utilize `radius_type` to decide if the given radius is constant for hub, mean or tip throughout the turbine.
-
-    Parameters
-    ----------
-    geometry : dict
-        A dictionary with all necessary geometrical parameters.
-    radius_type : str
-        A string deciding if hub, mean or tip radius is constant throughout the turbine.
-
-    Returns
-    -------
-    dict
-        Set of geometry suited for turbine evaluation.
-
-    """
-    # Create a copy of the input dictionary to avoid mutating the original
-    geom = geometry.copy()
-
-    # Get number of cascades
-    number_of_cascades = len(geom["cascade_type"])
-
-    # Get number of stages
-    if number_of_cascades > 1:
-        if math.is_even(number_of_cascades):
-            number_of_stages = int(number_of_cascades / 2)
-        else:
-            number_of_stages = int((number_of_cascades - 1) / 2)
-    else:
-        number_of_stages = 0
-
-    # Extract initial
-    radius = geom["radius"]
-    hub_to_tip_in = geom["hub_tip_ratio_in"]
-    hub_to_tip_out = geom["hub_tip_ratio_out"]
-
-    # Compute radius at hub and tip
-    if radius_type == "constant_mean":
-        radius_mean = radius
-        radius_tip_in = 2 * radius_mean / (1 + hub_to_tip_in)
-        radius_tip_out = 2 * radius_mean / (1 + hub_to_tip_out)
-        radius_hub_in = hub_to_tip_in * radius_tip_in
-        radius_hub_out = hub_to_tip_out * radius_tip_out
-        radius_mean_in = np.full_like(hub_to_tip_in, radius_mean)
-        radius_mean_out = np.full_like(hub_to_tip_out, radius_mean)
-    elif radius_type == "constant_hub":
-        radius_hub = radius
-        radius_tip_in = radius_hub / hub_to_tip_in
-        radius_tip_out = radius_hub / hub_to_tip_out
-        radius_mean_in = radius_tip_in * (1 + hub_to_tip_in) / 2
-        radius_mean_out = radius_tip_out * (1 + hub_to_tip_out) / 2
-        radius_hub_in = np.full_like(hub_to_tip_in, radius_hub)
-        radius_hub_out = np.full_like(hub_to_tip_out, radius_hub)
-    elif radius_type == "constant_tip":
-        radius_tip = radius
-        radius_hub_in = hub_to_tip_in * radius_tip
-        radius_hub_out = hub_to_tip_out * radius_tip
-        radius_tip_in = np.full_like(hub_to_tip_in, radius_tip)
-        radius_tip_out = np.full_like(hub_to_tip_out, radius_tip)
-        radius_mean_in = radius_tip_in * (1 + hub_to_tip_in) / 2
-        radius_mean_out = radius_tip_out * (1 + hub_to_tip_out) / 2
-
-    # Compute throat radius
-    radius_mean_throat = calculate_throat_radius(
-        radius_mean_in, radius_mean_out, geom["throat_location_fraction"]
-    )
-    radius_hub_throat = calculate_throat_radius(
-        radius_hub_in, radius_hub_out, geom["throat_location_fraction"]
-    )
-    radius_tip_throat = calculate_throat_radius(
-        radius_tip_in, radius_tip_out, geom["throat_location_fraction"]
-    )
-
-    # Compute blade heights
-    height_in = radius_tip_in - radius_hub_in
-    height_throat = radius_tip_throat - radius_hub_throat
-    height_out = radius_tip_out - radius_hub_out
-    height = (height_in + height_out) / 2
-
-    # Compute chord
-    chord = height / geom["aspect_ratio"]
-
-    # Compute pitch
-    pitch = geom["pitch_chord_ratio"] * chord
-
-    # Compute maximum thickness
-    gauging_angle = geom["gauging_angle"]
-    blade_camber = abs(geom["leading_edge_angle"] - gauging_angle)
-    maximum_thickness = np.array(
-        [
-            (
-                0.15 * (blade_camber[i] <= 40)
-                + (0.15 + 1.25e-3 * (blade_camber[i] - 40))
-                * (40 < blade_camber[i] <= 120)
-                + 0.25 * (blade_camber[i] > 120)
-            )
-            * chord[i]
-            for i in range(len(blade_camber))
-        ]
-    )
-    # Compute areas
-    A_in = np.pi * (radius_tip_in**2 - radius_hub_in**2)
-    A_out = np.pi * (radius_tip_out**2 - radius_hub_out**2)
-    A_throat = A_out * math.cosd(gauging_angle)
-
-    # Compute opening
-    opening = A_throat * pitch / (2 * np.pi * radius_mean_throat * height_throat)
-
-    # Compute trailing edge thickness
-    trailing_edge_thickness = geom["trailing_edge_thickness_opening_ratio"] * opening
-
-    # Compute stagger angle
-    stagger_angle = 0.5 * (geom["leading_edge_angle"] + gauging_angle)
-
-    # Define new geometry dictionary
-    new_geometry = {
-        "radius_hub_in": radius_hub_in,
-        "radius_tip_in": radius_tip_in,
-        "radius_hub_out": radius_hub_out,
-        "radius_tip_out": radius_tip_out,
-        "pitch": pitch,
-        "chord": chord,
-        "opening": opening,
-        "maximum_thickness": maximum_thickness,
-        "cascade_type": geom["cascade_type"],
-        "leading_edge_angle": geom["leading_edge_angle"],
-        "gauging_angle": gauging_angle,
-        "stagger_angle": stagger_angle,
-        "leading_edge_diameter": geom["leading_edge_diameter"],
-        "leading_edge_wedge_angle": geom["leading_edge_wedge_angle"],
-        "trailing_edge_thickness": trailing_edge_thickness,
-        "tip_clearance": geom["tip_clearance"],
-        "throat_location_fraction": geom["throat_location_fraction"],
-    }
-
-    return new_geometry
-
-
-def calculate_full_geometry(geometry):
-    """
-    Computes the complete geometry of an axial turbine based on input geometric parameters.
-
-    Parameters
-    ----------
-    geometry : dict
-        A dictionary containing the input geometry parameters
-
-    Returns
-    -------
-    dict
-        A dictionary with both the original and newly computed geometry parameters.
-    """
-
-    # Create a copy of the input dictionary to avoid mutating the original
-    geom = geometry.copy()
-
-    # Get number of cascades
-    number_of_cascades = len(geom["cascade_type"])
-
-    # Get number of stages
-    if number_of_cascades > 1:
-        if math.is_even(number_of_cascades):
-            number_of_stages = int(number_of_cascades / 2)
-        else:
-            number_of_stages = int((number_of_cascades - 1) / 2)
-    else:
-        number_of_stages = 0
-
-    # Get the location of the throat as a fraction of meridional length
-    throat_frac = geom["throat_location_fraction"]
-
-    # Extract initial radii and compute throat values
-    radius_hub_in = geom["radius_hub_in"]
-    radius_hub_out = geom["radius_hub_out"]
-    radius_tip_in = geom["radius_tip_in"]
-    radius_tip_out = geom["radius_tip_out"]
-    radius_hub_throat = calculate_throat_radius(
-        radius_hub_in, radius_hub_out, throat_frac
-    )
-    radius_tip_throat = calculate_throat_radius(
-        radius_tip_in, radius_tip_out, throat_frac
-    )
-
-    # Calculate shroud radius by adding clearance to tip radius
-    radius_shroud_in = radius_tip_in + geom["tip_clearance"]
-    radius_shroud_out = radius_tip_out + geom["tip_clearance"]
-    radius_shroud_throat = calculate_throat_radius(
-        radius_shroud_in, radius_shroud_out, throat_frac
-    )
-
-    # Calculate the inner and outer radii for the mean section
-    radius_mean_in = (radius_tip_in + radius_hub_in) / 2
-    radius_mean_out = (radius_tip_out + radius_hub_out) / 2
-    radius_mean_throat = calculate_throat_radius(
-        radius_mean_in, radius_mean_out, throat_frac
-    )
-
-    # Compute hub to tip radius ratio
-    hub_tip_ratio_in = radius_hub_in / radius_tip_in
-    hub_tip_ratio_out = radius_hub_out / radius_tip_out
-    hub_tip_ratio_throat = radius_hub_throat / radius_tip_throat
-
-    # Compute blade height at different sections
-    height_in = radius_tip_in - radius_hub_in
-    height_out = radius_tip_out - radius_hub_out
-    height_throat = radius_tip_throat - radius_hub_throat
-    height = (height_in + height_out) / 2
-
-    # Compute areas for the full, in, out, and throat sections
-    A_in = np.pi * (radius_tip_in**2 - radius_hub_in**2)
-    A_out = np.pi * (radius_tip_out**2 - radius_hub_out**2)
-    A_throat = (
-        2 * np.pi * radius_mean_throat * height_throat * geom["opening"] / geom["pitch"]
-    )
-
-    # Gauging angle
-    gauging_angle = math.arccosd(A_throat / A_out) * np.array(
-        [(-1) ** i for i in range(number_of_cascades)]
-    )
-
-    # Compute axial chord
-    axial_chord = geom["chord"] * math.cosd(geom["stagger_angle"])
-
-    # Compute flaring angle
-    flaring_angle = np.arctan((height_out - height_in) / axial_chord / 2)
-
-    # Compute geometric ratios
-    aspect_ratio = height / geom["chord"]
-    pitch_chord_ratio = geom["pitch"] / geom["chord"]
-    solidity = 1.0 / pitch_chord_ratio
-    maximum_thickness_chord_ratio = geom["maximum_thickness"] / geom["chord"]
-    trailing_edge_thickness_opening_ratio = (
-        geom["trailing_edge_thickness"] / geom["opening"]
-    )
-    tip_clearance_height = geom["tip_clearance"] / height
-    leading_edge_diameter_chord_ratio = geom["leading_edge_diameter"] / geom["chord"]
-
-    # Create a dictionary with the newly computed parameters
-    new_parameters = {
-        "number_of_stages": number_of_stages,
-        "number_of_cascades": number_of_cascades,
-        "axial_chord": axial_chord,
-        "radius_mean_in": radius_mean_in,
-        "radius_mean_out": radius_mean_out,
-        "radius_mean_throat": radius_mean_throat,
-        "radius_hub_in": radius_hub_in,
-        "radius_hub_out": radius_hub_out,
-        "radius_hub_throat": radius_hub_throat,
-        "radius_tip_in": radius_tip_in,
-        "radius_tip_out": radius_tip_out,
-        "radius_tip_throat": radius_tip_throat,
-        "radius_shroud_in": radius_shroud_in,
-        "radius_shroud_out": radius_shroud_out,
-        "radius_shroud_throat": radius_shroud_throat,
-        "hub_tip_ratio_in": hub_tip_ratio_in,
-        "hub_tip_ratio_out": hub_tip_ratio_out,
-        "hub_tip_ratio_throat": hub_tip_ratio_throat,
-        "A_in": A_in,
-        "A_out": A_out,
-        "A_throat": A_throat,
-        "height": height,
-        "height_in": height_in,
-        "height_throat": height_throat,
-        "height_out": height_out,
-        "flaring_angle": flaring_angle,
-        "aspect_ratio": aspect_ratio,
-        "pitch_chord_ratio": pitch_chord_ratio,
-        "solidity": solidity,
-        "maximum_thickness_chord_ratio": maximum_thickness_chord_ratio,
-        "trailing_edge_thickness_opening_ratio": trailing_edge_thickness_opening_ratio,
-        "tip_clearance_height_ratio": tip_clearance_height,
-        "leading_edge_diameter_chord_ratio": leading_edge_diameter_chord_ratio,
-        "gauging_angle": gauging_angle,
-    }
-
-    return {**geometry, **new_parameters}
-
-
-def check_turbine_geometry(geometry, display=True):
-    """
-    Checks if the geometrical parameters are within the predefined recommended ranges.
-
-
-    .. table:: Recommended Parameter Ranges and References
-
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Variable                 | Lower Limit  | Upper Limit  | Reference(s)                                     |
-        +==========================+==============+==============+==================================================+
-        | Blade chord              | 5.0 mm       | inf          | Manufacturing                                    |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Blade height             | 5.0 mm       | inf          | Manufacturing                                    |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Blade maximum thickness  | 1.0 mm       | inf          | Manufacturing                                    |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Blade trailing edge      | 0.5 mm       | inf          | Manufacturing                                    |
-        | thickness                |              |              |                                                  |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Tip clearance            | 0.2 mm       | inf          | Manufacturing                                    |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Hub to tip radius ratio  | 0.50         | 0.95         | Figure (6) of :cite:`kacker_mean_1982`           |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Flaring angle            | -25 deg      | +25 deg      | Section (7) of :cite:`ainley_examination_1951`   |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Aspect ratio             | 0.80         | 5.00         | Section (7.3) of :cite:`saravanamuttoo_gas_2008` |
-        |                          |              |              | Figure (13) of :cite:`kacker_mean_1982`          |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Pitch to chord ratio     | 0.30         | 1.10         | Figure (4) of :cite:`ainley_method_1951`         |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Stagger angle            | -10 deg      | +70 deg      | Figure (5) of :cite:`kacker_mean_1982`           |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Leading edge metal angle | -60 deg      | +25 deg      | Figure (5) of :cite:`kacker_mean_1982`           |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Trailing edge metal angle| +40 deg      | +80 deg      | Figure (4) of :cite:`ainley_method_1951`         |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Leading wedge angle      | +10 deg      | +60 deg      | Figure (2) from :cite:`benner_influence_1997`    |
-        |                          |              |              | Figure (10) from :cite:`pritchard_eleven_1985`   |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Leading edge diameter to | 0.03         | 0.30         | Table (1) :cite:`moustapha_improved_1990`        |
-        | chord ratio              |              |              |                                                  |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Maximum thickness to     | 0.05         | 0.30         | Figure (4) of :cite:`kacker_mean_1982`           |
-        | chord ratio              |              |              |                                                  |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Trailing edge thickness  | 0.00         | 0.40         | Figure (14) of :cite:`kacker_mean_1982`          |
-        | to opening ratio         |              |              |                                                  |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Tip clearance to height  | 0.00         | 0.05         | Figure (7) of :cite:`dunham_improvements_1970`   |
-        | ratio                    |              |              |                                                  |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-        | Throat location fraction | 0.50         | 1.00         | Equation (3) of :cite:`ainley_method_1951`       |
-        +--------------------------+--------------+--------------+--------------------------------------------------+
-
-    The ranges of metal angles and stagger angles are reversed for rotor cascades
-
-    Parameters
-    ----------
-    geometry : dict
-        A dictionary containing the computed geometry of the turbine.
-
-    Returns
-    -------
-    list
-        A list of messages with a summary of the checks.
-    """
-
-    # Define good practice ranges for each parameter
-    recommended_ranges = {
-        "chord": {"min": 5e-3, "max": np.inf},
-        "height": {"min": 5e-3, "max": np.inf},
-        "maximum_thickness": {"min": 1e-3, "max": np.inf},
-        "trailing_edge_thickness": {"min": 5e-4, "max": np.inf},
-        "tip_clearance": {"min": 2e-4, "max": np.inf},
-        "hub_tip_ratio": {"min": 0.50, "max": 0.95},
-        "aspect_ratio": {"min": 0.8, "max": 5.0},
-        "pitch_chord_ratio": {"min": 0.3, "max": 1.1},
-        "stagger_angle": {"min": -10, "max": +70},
-        "leading_edge_angle": {"min": -60, "max": +25},
-        "leading_edge_wedge_angle": {"min": 10, "max": 60},
-        "leading_edge_diameter_chord_ratio": {"min": 0.03, "max": 0.30},
-        "maximum_thickness_chord_ratio": {"min": 0.05, "max": 0.30},
-        "trailing_edge_thickness_opening_ratio": {"min": 0.00, "max": 0.40},
-        "tip_clearance_height_ratio": {"min": 0.0, "max": 0.05},
-        "throat_location_fraction": {"min": 0.5, "max": 1.0},
-    }
-
-    special_angles = ["leading_edge_angle", "stagger_angle"]
-
-    # Get the the type of cascade
-    cascade_types = geometry["cascade_type"]
-
-    # Define report width
-    report_width = 80
-
-    # Define report title
-    msgs = []
-    msgs.append("-" * report_width)  # Horizontal line
-    msgs.append("Axial turbine geometry report".center(report_width))
-    msgs.append("-" * report_width)  # Horizontal line
-
-    # Define table header with four columns
-    table_header = f" {'Parameter':<34}{'Value':>8}{'Range':>20}{'In range?':>14}"
-    msgs.append(table_header)
-    msgs.append("-" * report_width)  # Horizontal line
-
-    # Initialize a list to store variables outside the recommended range
-    vars_outside_range = []
-
-    # Iterate over each good practice parameter and perform checks
-    for parameter, limits in recommended_ranges.items():
-        if parameter == "cascade_type":
-            continue  # Skip the cascade_type entry
-
-        lb, ub = limits["min"], limits["max"]
-        value_array = geometry[parameter]
-
-        for index, value in enumerate(np.atleast_1d(value_array)):
-            # Determine cascade type for special cases
-            blade_type = cascade_types[index % len(cascade_types)]
-
-            if parameter in special_angles and blade_type == "rotor":
-                lb, ub = -ub, -lb  # Reverse limits for rotor blades
-
-            if parameter == "tip_clearance":
-                if blade_type == "stator":
-                    lb = 0.0
-                elif blade_type == "rotor":
-                    lb = limits["min"]
-
-            # Check if the value is within the recommended range
-            in_range = lb <= value <= ub
-
-            # Create a formatted message in table format using f-strings
-            bounds = f"({lb:+0.4f}, {ub:+0.4f})"
-            formatted_message = f" {parameter+'_'+str(index+1):<32}{value:>+8.4f}{bounds:>24}{str(in_range):>12}"
-            msgs.append(formatted_message)
-
-            # Append variables outside the recommended range to the list
-            if not in_range:
-                vars_outside_range.append(f"{parameter}_{index+1}")
-
-    # Footer for the geometry report
-    msgs.append("-" * report_width)  # Horizontal line
-
-    if not vars_outside_range:
-        msgs.append(
-            " Geometry report summary: All parameters are within recommended ranges."
-        )
-    else:
-        msgs.append(
-            " Geometry report summary: Some parameters are outside recommended ranges."
-        )
-        for warning in vars_outside_range:
-            msgs.append(f"     - {warning}")
-
-    # Footer for the geometry report
-    msgs.append("-" * report_width)  # Horizontal line
-
-    # Concatenate lines into single string
-    msg = "\n".join(msgs)
-
-    # Display the report
-    if display:
-        print(msg)
-
-    return msg
-
-
-# TODO: Check that this function behaves as intended
-# Make table of correct values and give reference to motivation
-# TODO angles are correct for rotor/statos
-# Tip clearance can be zero for rotor
-# TODO angles in degree
-# TODO flaring angle calculation
-# TODO throat area calculation correct in cascade_series
+import numpy as np
+from .. import math
+from .. import utilities as utils
+
+
+def validate_turbine_geometry(geom, display=False):
+    """
+    Performs validation of an axial turbine's geometry configuration.
+
+    - Ensures all required geometry parameters are specified in 'geom'.
+    - Verifies that no parameters beyond the required set are included.
+    - Checks 'number_of_cascades' is an integer and correctly dictates sizes of other parameters.
+    - Validates that all parameter arrays match the size of 'number_of_cascades'.
+    - Asserts that all geometry parameter values are numeric (integers or floats).
+    - Ensures non-angle parameters contain only non-negative values.
+    - Angle parameters ('leading_edge_angle', 'stagger_angle') can be negative.
+
+    This function is intended as a preliminary check before in-depth geometry analysis.
+
+    Parameters
+    ----------
+    geom : dict
+        The geometry configuration parameters for the turbine as a dictionary.
+
+    Returns
+    -------
+    bool
+        True if all validations pass, indicating a correctly structured geometry configuration.
+    """
+
+    # Define the list of keys that must be defined in the configuration file
+    required_keys = {
+        "cascade_type",
+        "radius_hub_in",
+        "radius_hub_out",
+        "radius_tip_in",
+        "radius_tip_out",
+        "pitch",
+        "chord",
+        "stagger_angle",
+        "opening",
+        "leading_edge_diameter",
+        "leading_edge_wedge_angle",
+        "leading_edge_angle",
+        "trailing_edge_thickness",
+        "tip_clearance",
+        "maximum_thickness",
+        "throat_location_fraction",
+    }
+
+    # Angular variables need not be non-negative
+    angle_keys = ["leading_edge_angle", "stagger_angle"]
+
+    # Check that there are no missing or extra variables
+    utils.validate_keys(geom, required_keys, required_keys)
+
+    # Get the number of cascades
+    number_of_cascades = len(geom["cascade_type"])
+
+    # Check if cascade_type is correctly defined
+    valid_types = np.array(["stator", "rotor"])
+    invalid_types = geom["cascade_type"][~np.isin(geom["cascade_type"], valid_types)]
+    if invalid_types.size > 0:
+        invalid_types_str = ", ".join(invalid_types)
+        raise ValueError(
+            f"Invalid types in 'cascade_type': {invalid_types_str}. Only 'stator' and 'rotor' are allowed."
+        )
+
+    # Check array size and values
+    for key, value in geom.items():
+        # Skip the cascade_type in the loop
+        if key in ["cascade_type", "geometry_type"]:
+            continue
+
+        # Check that all variables are numpy arrays
+        if not isinstance(value, np.ndarray):
+            raise TypeError(f"Parameter '{key}' must be a NumPy array.")
+
+        else:
+            if value.size != number_of_cascades:
+                raise ValueError(
+                    f"Size of '{key}' must be equal to number of cascades. Current value: {value}"
+                )
+
+        # Check that values of parameters are numeric
+        if not math.all_numeric(value):
+            raise ValueError(
+                f"Parameter '{key}' must be an array of numeric types. Current value: {value}"
+            )
+
+        # Check that non-angle variables are not negative
+        if key not in angle_keys and not math.all_non_negative(value):
+            raise ValueError(
+                f"Parameter '{key}' must be an array of non-negative numbers. Current value: {value}"
+            )
+
+    msg = "The structure of the turbine geometry parameters is valid"
+    if display:
+        print(msg)
+
+    return msg
+
+
+def calculate_throat_radius(radius_in, radius_out, throat_location_fraction):
+    """
+    Calculate the throat radius as a weighted average of the inlet and outlet radii.
+
+    This approach to estimate the area at the throat is inspired by the method
+    described in Eq. (3) of :cite:`ainley_method_1951`. The throat radius is computed
+    as a linear combination of the inlet and outlet radii, weighted by the location of
+    the throat expressed as a fraction of the axial length of the cascade.
+    For instance, a `throat_location_fraction` of 5/6 would return the throat radius as:
+    (1/6)*radius_in + (5/6)*radius out.
+
+    Parameters
+    ----------
+    radius_in : np.array
+        The radius values at the inlet sections.
+    radius_out : np.array
+        The radius values at the outlet sections.
+    throat_location_fraction : float
+        The fraction used to weight the inlet and outlet radii in the calculation.
+
+    Returns
+    -------
+    np.array
+        The calculated throat radius values, representing the weighted average of the
+        inlet and outlet radii based on the specified throat location.
+    """
+    return (
+        1 - throat_location_fraction
+    ) * radius_in + throat_location_fraction * radius_out
+
+
+def prepare_geometry(geometry, radius_type):
+    """
+
+    Convert the geometrical design variables to parameters suitable for turbine evaluation.
+
+    For design optimization, the geometrical design variables are defined as ratios, e.g. hub-to-tip radius and aspect ratio.
+    This function convert the set of geometrical design variables to a set which is suited for turbine evaluation.
+    The function utilize `radius_type` to decide if the given radius is constant for hub, mean or tip throughout the turbine.
+
+    Parameters
+    ----------
+    geometry : dict
+        A dictionary with all necessary geometrical parameters.
+    radius_type : str
+        A string deciding if hub, mean or tip radius is constant throughout the turbine.
+
+    Returns
+    -------
+    dict
+        Set of geometry suited for turbine evaluation.
+
+    """
+    # Create a copy of the input dictionary to avoid mutating the original
+    geom = geometry.copy()
+
+    # Get number of cascades
+    number_of_cascades = len(geom["cascade_type"])
+
+    # Get number of stages
+    if number_of_cascades > 1:
+        if math.is_even(number_of_cascades):
+            number_of_stages = int(number_of_cascades / 2)
+        else:
+            number_of_stages = int((number_of_cascades - 1) / 2)
+    else:
+        number_of_stages = 0
+
+    # Extract initial
+    radius = geom["radius"]
+    hub_to_tip_in = geom["hub_tip_ratio_in"]
+    hub_to_tip_out = geom["hub_tip_ratio_out"]
+
+    # Compute radius at hub and tip
+    if radius_type == "constant_mean":
+        radius_mean = radius
+        radius_tip_in = 2 * radius_mean / (1 + hub_to_tip_in)
+        radius_tip_out = 2 * radius_mean / (1 + hub_to_tip_out)
+        radius_hub_in = hub_to_tip_in * radius_tip_in
+        radius_hub_out = hub_to_tip_out * radius_tip_out
+        radius_mean_in = np.full_like(hub_to_tip_in, radius_mean)
+        radius_mean_out = np.full_like(hub_to_tip_out, radius_mean)
+    elif radius_type == "constant_hub":
+        radius_hub = radius
+        radius_tip_in = radius_hub / hub_to_tip_in
+        radius_tip_out = radius_hub / hub_to_tip_out
+        radius_mean_in = radius_tip_in * (1 + hub_to_tip_in) / 2
+        radius_mean_out = radius_tip_out * (1 + hub_to_tip_out) / 2
+        radius_hub_in = np.full_like(hub_to_tip_in, radius_hub)
+        radius_hub_out = np.full_like(hub_to_tip_out, radius_hub)
+    elif radius_type == "constant_tip":
+        radius_tip = radius
+        radius_hub_in = hub_to_tip_in * radius_tip
+        radius_hub_out = hub_to_tip_out * radius_tip
+        radius_tip_in = np.full_like(hub_to_tip_in, radius_tip)
+        radius_tip_out = np.full_like(hub_to_tip_out, radius_tip)
+        radius_mean_in = radius_tip_in * (1 + hub_to_tip_in) / 2
+        radius_mean_out = radius_tip_out * (1 + hub_to_tip_out) / 2
+
+    # Compute throat radius
+    radius_mean_throat = calculate_throat_radius(
+        radius_mean_in, radius_mean_out, geom["throat_location_fraction"]
+    )
+    radius_hub_throat = calculate_throat_radius(
+        radius_hub_in, radius_hub_out, geom["throat_location_fraction"]
+    )
+    radius_tip_throat = calculate_throat_radius(
+        radius_tip_in, radius_tip_out, geom["throat_location_fraction"]
+    )
+
+    # Compute blade heights
+    height_in = radius_tip_in - radius_hub_in
+    height_throat = radius_tip_throat - radius_hub_throat
+    height_out = radius_tip_out - radius_hub_out
+    height = (height_in + height_out) / 2
+
+    # Compute chord
+    chord = height / geom["aspect_ratio"]
+
+    # Compute pitch
+    pitch = geom["pitch_chord_ratio"] * chord
+
+    # Compute maximum thickness
+    gauging_angle = geom["gauging_angle"]
+    blade_camber = abs(geom["leading_edge_angle"] - gauging_angle)
+    maximum_thickness = np.array(
+        [
+            (
+                0.15 * (blade_camber[i] <= 40)
+                + (0.15 + 1.25e-3 * (blade_camber[i] - 40))
+                * (40 < blade_camber[i] <= 120)
+                + 0.25 * (blade_camber[i] > 120)
+            )
+            * chord[i]
+            for i in range(len(blade_camber))
+        ]
+    )
+    # Compute areas
+    A_in = np.pi * (radius_tip_in**2 - radius_hub_in**2)
+    A_out = np.pi * (radius_tip_out**2 - radius_hub_out**2)
+    A_throat = A_out * math.cosd(gauging_angle)
+
+    # Compute opening
+    opening = A_throat * pitch / (2 * np.pi * radius_mean_throat * height_throat)
+
+    # Compute trailing edge thickness
+    trailing_edge_thickness = geom["trailing_edge_thickness_opening_ratio"] * opening
+
+    # Compute stagger angle
+    stagger_angle = 0.5 * (geom["leading_edge_angle"] + gauging_angle)
+
+    # Define new geometry dictionary
+    new_geometry = {
+        "radius_hub_in": radius_hub_in,
+        "radius_tip_in": radius_tip_in,
+        "radius_hub_out": radius_hub_out,
+        "radius_tip_out": radius_tip_out,
+        "pitch": pitch,
+        "chord": chord,
+        "opening": opening,
+        "maximum_thickness": maximum_thickness,
+        "cascade_type": geom["cascade_type"],
+        "leading_edge_angle": geom["leading_edge_angle"],
+        "gauging_angle": gauging_angle,
+        "stagger_angle": stagger_angle,
+        "leading_edge_diameter": geom["leading_edge_diameter"],
+        "leading_edge_wedge_angle": geom["leading_edge_wedge_angle"],
+        "trailing_edge_thickness": trailing_edge_thickness,
+        "tip_clearance": geom["tip_clearance"],
+        "throat_location_fraction": geom["throat_location_fraction"],
+    }
+
+    return new_geometry
+
+
+def calculate_full_geometry(geometry):
+    """
+    Computes the complete geometry of an axial turbine based on input geometric parameters.
+
+    Parameters
+    ----------
+    geometry : dict
+        A dictionary containing the input geometry parameters
+
+    Returns
+    -------
+    dict
+        A dictionary with both the original and newly computed geometry parameters.
+    """
+
+    # Create a copy of the input dictionary to avoid mutating the original
+    geom = geometry.copy()
+
+    # Get number of cascades
+    number_of_cascades = len(geom["cascade_type"])
+
+    # Get number of stages
+    if number_of_cascades > 1:
+        if math.is_even(number_of_cascades):
+            number_of_stages = int(number_of_cascades / 2)
+        else:
+            number_of_stages = int((number_of_cascades - 1) / 2)
+    else:
+        number_of_stages = 0
+
+    # Get the location of the throat as a fraction of meridional length
+    throat_frac = geom["throat_location_fraction"]
+
+    # Extract initial radii and compute throat values
+    radius_hub_in = geom["radius_hub_in"]
+    radius_hub_out = geom["radius_hub_out"]
+    radius_tip_in = geom["radius_tip_in"]
+    radius_tip_out = geom["radius_tip_out"]
+    radius_hub_throat = calculate_throat_radius(
+        radius_hub_in, radius_hub_out, throat_frac
+    )
+    radius_tip_throat = calculate_throat_radius(
+        radius_tip_in, radius_tip_out, throat_frac
+    )
+
+    # Calculate shroud radius by adding clearance to tip radius
+    radius_shroud_in = radius_tip_in + geom["tip_clearance"]
+    radius_shroud_out = radius_tip_out + geom["tip_clearance"]
+    radius_shroud_throat = calculate_throat_radius(
+        radius_shroud_in, radius_shroud_out, throat_frac
+    )
+
+    # Calculate the inner and outer radii for the mean section
+    radius_mean_in = (radius_tip_in + radius_hub_in) / 2
+    radius_mean_out = (radius_tip_out + radius_hub_out) / 2
+    radius_mean_throat = calculate_throat_radius(
+        radius_mean_in, radius_mean_out, throat_frac
+    )
+
+    # Compute hub to tip radius ratio
+    hub_tip_ratio_in = radius_hub_in / radius_tip_in
+    hub_tip_ratio_out = radius_hub_out / radius_tip_out
+    hub_tip_ratio_throat = radius_hub_throat / radius_tip_throat
+
+    # Compute blade height at different sections
+    height_in = radius_tip_in - radius_hub_in
+    height_out = radius_tip_out - radius_hub_out
+    height_throat = radius_tip_throat - radius_hub_throat
+    height = (height_in + height_out) / 2
+
+    # Compute areas for the full, in, out, and throat sections
+    A_in = np.pi * (radius_tip_in**2 - radius_hub_in**2)
+    A_out = np.pi * (radius_tip_out**2 - radius_hub_out**2)
+    A_throat = (
+        2 * np.pi * radius_mean_throat * height_throat * geom["opening"] / geom["pitch"]
+    )
+
+    # Gauging angle
+    gauging_angle = math.arccosd(A_throat / A_out) * np.array(
+        [(-1) ** i for i in range(number_of_cascades)]
+    )
+
+    # Compute axial chord
+    axial_chord = geom["chord"] * math.cosd(geom["stagger_angle"])
+
+    # Compute flaring angle
+    flaring_angle = np.arctan((height_out - height_in) / axial_chord / 2)
+
+    # Compute geometric ratios
+    aspect_ratio = height / geom["chord"]
+    pitch_chord_ratio = geom["pitch"] / geom["chord"]
+    solidity = 1.0 / pitch_chord_ratio
+    maximum_thickness_chord_ratio = geom["maximum_thickness"] / geom["chord"]
+    trailing_edge_thickness_opening_ratio = (
+        geom["trailing_edge_thickness"] / geom["opening"]
+    )
+    tip_clearance_height = geom["tip_clearance"] / height
+    leading_edge_diameter_chord_ratio = geom["leading_edge_diameter"] / geom["chord"]
+
+    # Create a dictionary with the newly computed parameters
+    new_parameters = {
+        "number_of_stages": number_of_stages,
+        "number_of_cascades": number_of_cascades,
+        "axial_chord": axial_chord,
+        "radius_mean_in": radius_mean_in,
+        "radius_mean_out": radius_mean_out,
+        "radius_mean_throat": radius_mean_throat,
+        "radius_hub_in": radius_hub_in,
+        "radius_hub_out": radius_hub_out,
+        "radius_hub_throat": radius_hub_throat,
+        "radius_tip_in": radius_tip_in,
+        "radius_tip_out": radius_tip_out,
+        "radius_tip_throat": radius_tip_throat,
+        "radius_shroud_in": radius_shroud_in,
+        "radius_shroud_out": radius_shroud_out,
+        "radius_shroud_throat": radius_shroud_throat,
+        "hub_tip_ratio_in": hub_tip_ratio_in,
+        "hub_tip_ratio_out": hub_tip_ratio_out,
+        "hub_tip_ratio_throat": hub_tip_ratio_throat,
+        "A_in": A_in,
+        "A_out": A_out,
+        "A_throat": A_throat,
+        "height": height,
+        "height_in": height_in,
+        "height_throat": height_throat,
+        "height_out": height_out,
+        "flaring_angle": flaring_angle,
+        "aspect_ratio": aspect_ratio,
+        "pitch_chord_ratio": pitch_chord_ratio,
+        "solidity": solidity,
+        "maximum_thickness_chord_ratio": maximum_thickness_chord_ratio,
+        "trailing_edge_thickness_opening_ratio": trailing_edge_thickness_opening_ratio,
+        "tip_clearance_height_ratio": tip_clearance_height,
+        "leading_edge_diameter_chord_ratio": leading_edge_diameter_chord_ratio,
+        "gauging_angle": gauging_angle,
+    }
+
+    return {**geometry, **new_parameters}
+
+
+def check_turbine_geometry(geometry, display=True):
+    """
+    Checks if the geometrical parameters are within the predefined recommended ranges.
+
+
+    .. table:: Recommended Parameter Ranges and References
+
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Variable                 | Lower Limit  | Upper Limit  | Reference(s)                                     |
+        +==========================+==============+==============+==================================================+
+        | Blade chord              | 5.0 mm       | inf          | Manufacturing                                    |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Blade height             | 5.0 mm       | inf          | Manufacturing                                    |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Blade maximum thickness  | 1.0 mm       | inf          | Manufacturing                                    |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Blade trailing edge      | 0.5 mm       | inf          | Manufacturing                                    |
+        | thickness                |              |              |                                                  |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Tip clearance            | 0.2 mm       | inf          | Manufacturing                                    |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Hub to tip radius ratio  | 0.50         | 0.95         | Figure (6) of :cite:`kacker_mean_1982`           |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Flaring angle            | -25 deg      | +25 deg      | Section (7) of :cite:`ainley_examination_1951`   |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Aspect ratio             | 0.80         | 5.00         | Section (7.3) of :cite:`saravanamuttoo_gas_2008` |
+        |                          |              |              | Figure (13) of :cite:`kacker_mean_1982`          |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Pitch to chord ratio     | 0.30         | 1.10         | Figure (4) of :cite:`ainley_method_1951`         |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Stagger angle            | -10 deg      | +70 deg      | Figure (5) of :cite:`kacker_mean_1982`           |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Leading edge metal angle | -60 deg      | +25 deg      | Figure (5) of :cite:`kacker_mean_1982`           |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Trailing edge metal angle| +40 deg      | +80 deg      | Figure (4) of :cite:`ainley_method_1951`         |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Leading wedge angle      | +10 deg      | +60 deg      | Figure (2) from :cite:`benner_influence_1997`    |
+        |                          |              |              | Figure (10) from :cite:`pritchard_eleven_1985`   |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Leading edge diameter to | 0.03         | 0.30         | Table (1) :cite:`moustapha_improved_1990`        |
+        | chord ratio              |              |              |                                                  |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Maximum thickness to     | 0.05         | 0.30         | Figure (4) of :cite:`kacker_mean_1982`           |
+        | chord ratio              |              |              |                                                  |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Trailing edge thickness  | 0.00         | 0.40         | Figure (14) of :cite:`kacker_mean_1982`          |
+        | to opening ratio         |              |              |                                                  |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Tip clearance to height  | 0.00         | 0.05         | Figure (7) of :cite:`dunham_improvements_1970`   |
+        | ratio                    |              |              |                                                  |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+        | Throat location fraction | 0.50         | 1.00         | Equation (3) of :cite:`ainley_method_1951`       |
+        +--------------------------+--------------+--------------+--------------------------------------------------+
+
+    The ranges of metal angles and stagger angles are reversed for rotor cascades
+
+    Parameters
+    ----------
+    geometry : dict
+        A dictionary containing the computed geometry of the turbine.
+
+    Returns
+    -------
+    list
+        A list of messages with a summary of the checks.
+    """
+
+    # Define good practice ranges for each parameter
+    recommended_ranges = {
+        "chord": {"min": 5e-3, "max": np.inf},
+        "height": {"min": 5e-3, "max": np.inf},
+        "maximum_thickness": {"min": 1e-3, "max": np.inf},
+        "trailing_edge_thickness": {"min": 5e-4, "max": np.inf},
+        "tip_clearance": {"min": 2e-4, "max": np.inf},
+        "hub_tip_ratio": {"min": 0.50, "max": 0.95},
+        "aspect_ratio": {"min": 0.8, "max": 5.0},
+        "pitch_chord_ratio": {"min": 0.3, "max": 1.1},
+        "stagger_angle": {"min": -10, "max": +70},
+        "leading_edge_angle": {"min": -60, "max": +25},
+        "leading_edge_wedge_angle": {"min": 10, "max": 60},
+        "leading_edge_diameter_chord_ratio": {"min": 0.03, "max": 0.30},
+        "maximum_thickness_chord_ratio": {"min": 0.05, "max": 0.30},
+        "trailing_edge_thickness_opening_ratio": {"min": 0.00, "max": 0.40},
+        "tip_clearance_height_ratio": {"min": 0.0, "max": 0.05},
+        "throat_location_fraction": {"min": 0.5, "max": 1.0},
+    }
+
+    special_angles = ["leading_edge_angle", "stagger_angle"]
+
+    # Get the the type of cascade
+    cascade_types = geometry["cascade_type"]
+
+    # Define report width
+    report_width = 80
+
+    # Define report title
+    msgs = []
+    msgs.append("-" * report_width)  # Horizontal line
+    msgs.append("Axial turbine geometry report".center(report_width))
+    msgs.append("-" * report_width)  # Horizontal line
+
+    # Define table header with four columns
+    table_header = f" {'Parameter':<34}{'Value':>8}{'Range':>20}{'In range?':>14}"
+    msgs.append(table_header)
+    msgs.append("-" * report_width)  # Horizontal line
+
+    # Initialize a list to store variables outside the recommended range
+    vars_outside_range = []
+
+    # Iterate over each good practice parameter and perform checks
+    for parameter, limits in recommended_ranges.items():
+        if parameter == "cascade_type":
+            continue  # Skip the cascade_type entry
+
+        lb, ub = limits["min"], limits["max"]
+        value_array = geometry[parameter]
+
+        for index, value in enumerate(np.atleast_1d(value_array)):
+            # Determine cascade type for special cases
+            blade_type = cascade_types[index % len(cascade_types)]
+
+            if parameter in special_angles and blade_type == "rotor":
+                lb, ub = -ub, -lb  # Reverse limits for rotor blades
+
+            if parameter == "tip_clearance":
+                if blade_type == "stator":
+                    lb = 0.0
+                elif blade_type == "rotor":
+                    lb = limits["min"]
+
+            # Check if the value is within the recommended range
+            in_range = lb <= value <= ub
+
+            # Create a formatted message in table format using f-strings
+            bounds = f"({lb:+0.4f}, {ub:+0.4f})"
+            formatted_message = f" {parameter+'_'+str(index+1):<32}{value:>+8.4f}{bounds:>24}{str(in_range):>12}"
+            msgs.append(formatted_message)
+
+            # Append variables outside the recommended range to the list
+            if not in_range:
+                vars_outside_range.append(f"{parameter}_{index+1}")
+
+    # Footer for the geometry report
+    msgs.append("-" * report_width)  # Horizontal line
+
+    if not vars_outside_range:
+        msgs.append(
+            " Geometry report summary: All parameters are within recommended ranges."
+        )
+    else:
+        msgs.append(
+            " Geometry report summary: Some parameters are outside recommended ranges."
+        )
+        for warning in vars_outside_range:
+            msgs.append(f"     - {warning}")
+
+    # Footer for the geometry report
+    msgs.append("-" * report_width)  # Horizontal line
+
+    # Concatenate lines into single string
+    msg = "\n".join(msgs)
+
+    # Display the report
+    if display:
+        print(msg)
+
+    return msg
+
+
+# TODO: Check that this function behaves as intended
+# Make table of correct values and give reference to motivation
+# TODO angles are correct for rotor/statos
+# Tip clearance can be zero for rotor
+# TODO angles in degree
+# TODO flaring angle calculation
+# TODO throat area calculation correct in cascade_series
```

### Comparing `turboflow-0.1.2/turboflow/axial_turbine/loss_model.py` & `turboflow-0.1.3/turboflow/axial_turbine/loss_model.py`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,141 +1,141 @@
-from . import loss_model_benner as br
-from . import loss_model_kacker_okapuu as ko
-from . import loss_model_moustapha as mo
-from .. import utilities as utils
-
-# List of valid options
-LOSS_MODELS = [
-    "kacker_okapuu",
-    "moustapha",
-    "benner",
-    "isentropic",
-    "custom",
-]
-"""
-Available loss models.
-"""
-
-LOSS_COEFFICIENTS = [
-    "stagnation_pressure",
-]
-"""
-Available loss coefficients.
-"""
-
-# Keys that the output loss dictionary must have
-KEYS_LOSSES = [
-    "loss_definition",
-    "loss_error",
-    "loss_total",
-    "loss_profile",
-    "loss_clearance",
-    "loss_secondary",
-    "loss_trailing",
-    "loss_incidence",
-]
-
-
-def evaluate_loss_model(loss_model_options, input_parameters):
-    """
-    Calculate loss coefficient based on the selected loss model.
-
-    The avaialble loss models are:
-
-    - `kacker_okapuu` : loss model according to :cite:`kacker_mean_1982`.
-    - `moustapha` : loss model according to :cite:`moustapha_improved_1990`.
-    - `benner` : loss model according to :cite:`benner_influence_1997`, :cite:`benner_empirical_2006-1` and :cite:`benner_empirical_2006`.
-    - `isentropic` : Loss coefficient set to zero (isentropic).
-    - `custom` :  Constant loss coefficent according to user. Require `loss_model_options["custom_value"]`.
-
-    Parameters
-    ----------
-    loss_model_options : dict
-        Options for the loss calculation.
-    input_parameters : dict
-        Input parameters required for loss model calculation.
-
-    Returns:
-    dict
-        A dictionary containing loss components.
-    """
-
-    # Function mappings for each loss model
-    loss_model_functions = {
-        LOSS_MODELS[0]: ko.compute_losses,
-        LOSS_MODELS[1]: mo.compute_losses,
-        LOSS_MODELS[2]: br.compute_losses,
-        LOSS_MODELS[3]: lambda _: {
-            "loss_profile": 0.0,
-            "loss_incidence": 0.0,
-            "loss_trailing": 0.0,
-            "loss_secondary": 0.0,
-            "loss_clearance": 0.0,
-            "loss_total": 0.0,
-        },
-        LOSS_MODELS[4]: lambda input_parameters: {
-            "loss_profile": 0.0,
-            "loss_incidence": 0.0,
-            "loss_trailing": 0.0,
-            "loss_secondary": 0.0,
-            "loss_clearance": 0.0,
-            "loss_total": loss_model_options["custom_value"],
-        },
-    }
-
-    # Evaluate loss model
-    model = loss_model_options["model"]
-    if model in loss_model_functions:
-        loss_dict = loss_model_functions[model](input_parameters)
-    else:
-        options = ", ".join(f"'{k}'" for k in LOSS_MODELS)
-        raise ValueError(f"Invalid loss model '{model}'. Available options: {options}")
-
-    # Apply tuning factors (empty dict if not provided)
-    tuning_factors = loss_model_options.get("tuning_factors", {})
-    tuning_factors = {f"loss_{key}": value for key, value in tuning_factors.items()}
-    apply_tuning_factors(loss_dict, tuning_factors)
-
-    # Compute the loss coefficient according to definition
-    loss_coeff = loss_model_options["loss_coefficient"]
-    if loss_coeff == "stagnation_pressure":
-        # TODO add other loss coefficient definitions
-        p0rel_in = input_parameters["flow"]["p0_rel_in"]
-        p0rel_out = input_parameters["flow"]["p0_rel_out"]
-        p_out = input_parameters["flow"]["p_out"]
-        Y_definition = (p0rel_in - p0rel_out) / (p0rel_out - p_out)
-
-    else:
-        options = ", ".join(f"'{k}'" for k in LOSS_COEFFICIENTS)
-        raise ValueError(
-            f"Invalid loss coefficient '{loss_coeff}'. Available options: {options}"
-        )
-
-    # Compute loss coefficient error
-    loss_dict["loss_error"] = Y_definition - loss_dict["loss_total"]
-
-    return loss_dict
-
-
-def apply_tuning_factors(loss_dict, tuning_factors):
-    """
-    Apply tuning factors to the loss model
-
-    The tuning factors are multiplied with their associated loss component.
-
-    Parameters
-    ----------
-    loss_dict : dict
-        A dictionary containing loss components.
-    tuning_factors : dict
-        A dictionary containing the multiplicative tuning factors.
-
-    Raises:
-        KeyError: If a key from `tuning_factors` is not found in `loss_dict`.
-    """
-
-    for key, factor in tuning_factors.items():
-        if key not in loss_dict:
-            raise KeyError(f"Tuning factor key '{key}' not found in loss dictionary.")
-        loss_dict[key] *= factor
-
-    return loss_dict
+from . import loss_model_benner as br
+from . import loss_model_kacker_okapuu as ko
+from . import loss_model_moustapha as mo
+from .. import utilities as utils
+
+# List of valid options
+LOSS_MODELS = [
+    "kacker_okapuu",
+    "moustapha",
+    "benner",
+    "isentropic",
+    "custom",
+]
+"""
+Available loss models.
+"""
+
+LOSS_COEFFICIENTS = [
+    "stagnation_pressure",
+]
+"""
+Available loss coefficients.
+"""
+
+# Keys that the output loss dictionary must have
+KEYS_LOSSES = [
+    "loss_definition",
+    "loss_error",
+    "loss_total",
+    "loss_profile",
+    "loss_clearance",
+    "loss_secondary",
+    "loss_trailing",
+    "loss_incidence",
+]
+
+
+def evaluate_loss_model(loss_model_options, input_parameters):
+    """
+    Calculate loss coefficient based on the selected loss model.
+
+    The avaialble loss models are:
+
+    - `kacker_okapuu` : loss model according to :cite:`kacker_mean_1982`.
+    - `moustapha` : loss model according to :cite:`moustapha_improved_1990`.
+    - `benner` : loss model according to :cite:`benner_influence_1997`, :cite:`benner_empirical_2006-1` and :cite:`benner_empirical_2006`.
+    - `isentropic` : Loss coefficient set to zero (isentropic).
+    - `custom` :  Constant loss coefficent according to user. Require `loss_model_options["custom_value"]`.
+
+    Parameters
+    ----------
+    loss_model_options : dict
+        Options for the loss calculation.
+    input_parameters : dict
+        Input parameters required for loss model calculation.
+
+    Returns:
+    dict
+        A dictionary containing loss components.
+    """
+
+    # Function mappings for each loss model
+    loss_model_functions = {
+        LOSS_MODELS[0]: ko.compute_losses,
+        LOSS_MODELS[1]: mo.compute_losses,
+        LOSS_MODELS[2]: br.compute_losses,
+        LOSS_MODELS[3]: lambda _: {
+            "loss_profile": 0.0,
+            "loss_incidence": 0.0,
+            "loss_trailing": 0.0,
+            "loss_secondary": 0.0,
+            "loss_clearance": 0.0,
+            "loss_total": 0.0,
+        },
+        LOSS_MODELS[4]: lambda input_parameters: {
+            "loss_profile": 0.0,
+            "loss_incidence": 0.0,
+            "loss_trailing": 0.0,
+            "loss_secondary": 0.0,
+            "loss_clearance": 0.0,
+            "loss_total": loss_model_options["custom_value"],
+        },
+    }
+
+    # Evaluate loss model
+    model = loss_model_options["model"]
+    if model in loss_model_functions:
+        loss_dict = loss_model_functions[model](input_parameters)
+    else:
+        options = ", ".join(f"'{k}'" for k in LOSS_MODELS)
+        raise ValueError(f"Invalid loss model '{model}'. Available options: {options}")
+
+    # Apply tuning factors (empty dict if not provided)
+    tuning_factors = loss_model_options.get("tuning_factors", {})
+    tuning_factors = {f"loss_{key}": value for key, value in tuning_factors.items()}
+    apply_tuning_factors(loss_dict, tuning_factors)
+
+    # Compute the loss coefficient according to definition
+    loss_coeff = loss_model_options["loss_coefficient"]
+    if loss_coeff == "stagnation_pressure":
+        # TODO add other loss coefficient definitions
+        p0rel_in = input_parameters["flow"]["p0_rel_in"]
+        p0rel_out = input_parameters["flow"]["p0_rel_out"]
+        p_out = input_parameters["flow"]["p_out"]
+        Y_definition = (p0rel_in - p0rel_out) / (p0rel_out - p_out)
+
+    else:
+        options = ", ".join(f"'{k}'" for k in LOSS_COEFFICIENTS)
+        raise ValueError(
+            f"Invalid loss coefficient '{loss_coeff}'. Available options: {options}"
+        )
+
+    # Compute loss coefficient error
+    loss_dict["loss_error"] = Y_definition - loss_dict["loss_total"]
+
+    return loss_dict
+
+
+def apply_tuning_factors(loss_dict, tuning_factors):
+    """
+    Apply tuning factors to the loss model
+
+    The tuning factors are multiplied with their associated loss component.
+
+    Parameters
+    ----------
+    loss_dict : dict
+        A dictionary containing loss components.
+    tuning_factors : dict
+        A dictionary containing the multiplicative tuning factors.
+
+    Raises:
+        KeyError: If a key from `tuning_factors` is not found in `loss_dict`.
+    """
+
+    for key, factor in tuning_factors.items():
+        if key not in loss_dict:
+            raise KeyError(f"Tuning factor key '{key}' not found in loss dictionary.")
+        loss_dict[key] *= factor
+
+    return loss_dict
```

### Comparing `turboflow-0.1.2/turboflow/axial_turbine/loss_model_benner.py` & `turboflow-0.1.3/turboflow/axial_turbine/loss_model_moustapha.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,1014 +1,850 @@
-import numpy as np
-from .. import math
-
-# TODO: Add smoothing to the min/max/abs/piecewise functions
-
-# TODO: Create test scripts to plot how the different loss model functions behave when smoothing is applied / verify that the behavior is reasonable beyond their intended range
-# TODO: Raise warninings to a log file to be aware when the loss model is outside its range of validity and because of which variables
-# TODO: the log object can be passed as an additional variable to the functions with a default parameter Logger=None
-# TODO: Whenever something is out of range we can do "if Logger: " to check if the logger was actually passed as an input and log an descriptive message
-
-# TODO: Write detailed docstring in .rst format explaning the equations of the loss model?
-# TODO: This could be prepared in latex and then transformed into .rst with chatGPT
-
-# TODO: For all functions:
-# TODO: add docstring explaning the equations and the original paper
-# TODO: explain smoothing/blending tricks
-# We can sit together one day and prepare a draft of the explanation of each function with help of chatGPT
-# In this way I also get the chance to understand how the loss model works in detail
-
-
-def compute_losses(input_parameters):
-    r"""
-
-    Evaluate loss coefficient according to :cite:`benner_influence_1997`, :cite:`benner_empirical_2006-1` and :cite:`benner_empirical_2006`.
-
-    This model split the total loss coefficient into profile, secondary, trailing edge, tip clearance and incidence losses.
-    The loss coefficient are combined through the following relation:
-
-    .. math::
-
-        \mathrm{Y_{tot}} = (\mathrm{Y_{p}} + \mathrm{Y_{te}} + \mathrm{Y_{inc}})(1-Z/H) + \mathrm{Y_{s}} + \mathrm{Y_{cl}}
-
-    where :math:`Z` is the penetration depth of the passage vortex separation line in spanwise direction, and :math:`H` is the mean blade height. This quantity depend on the displacement thickness of the
-    inlet endwall bounary layer, which is approximated from a reference value:
-
-    .. math::
-
-        \frac{\delta}{H} = \frac{\delta}{H}_\mathrm{ref} \frac{\mathrm{Re_{in}}}{3\cdot10^5}^{-1/7}
-
-    The reference value may be specified by the user.
-
-    The function calls a function for each loss component:
-
-        - :math:`\mathrm{Y_{p}}` : `get_profile_loss`
-        - :math:`\mathrm{Y_{te}}` : `get_trailing_edge_loss`
-        - :math:`\mathrm{Y_{s}}` : `get_secondary_loss`
-        - :math:`\mathrm{Y_{cl}}` : `get_tip_clearance_loss`
-        - :math:`\mathrm{Y_{inc}}` : `get_incidence_loss`
-        - :math:`Z/H` : `get_penetration_depth`
-
-
-    Parameters
-    ----------
-    input_parameters : dict
-        A dictionary containing containing the keys, `flow`, `geometry` and `options`.
-
-    Returns
-    -------
-    dict
-        A dictionary with the loss components.
-
-    """
-
-    # Extract input parameters
-    flow_parameters = input_parameters["flow"]
-    geometry = input_parameters["geometry"]
-    options = input_parameters["loss_model"]
-
-    # Assume angle of minimum incidence loss is equal to metal angle
-    # TODO: Digitize Figure 2 from :cite:`moustapha_improved_1990`?
-    beta_des = geometry["leading_edge_angle"]
-
-    # Calculate inlet displacement thickness to height ratio
-    delta_height = options["inlet_displacement_thickness_height_ratio"]
-    delta_height = delta_height * (flow_parameters["Re_in"] / 3e5) ** (-1 / 7)
-
-    # Profile loss coefficient
-    Y_p = get_profile_loss(flow_parameters, geometry)
-
-    # Trailing edge coefficient
-    Y_te = get_trailing_edge_loss(flow_parameters, geometry)
-
-    # Secondary loss coefficient
-    Y_s = get_secondary_loss(flow_parameters, geometry, delta_height)
-
-    # Tip clearance loss coefficient
-    Y_cl = get_tip_clearance_loss(flow_parameters, geometry)
-
-    # Incidence loss coefficient
-    Y_inc = get_incidence_loss(flow_parameters, geometry, beta_des)
-
-    # Penetration depth to blade height ratio
-    ZTE = get_penetration_depth(flow_parameters, geometry, delta_height)
-
-    # Correct profile losses according to penetration depth
-    Y_p *= 1 - ZTE
-    Y_te *= 1 - ZTE
-    Y_inc *= 1 - ZTE
-
-    # Return a dictionary of loss components
-    losses = {
-        "loss_profile": Y_p,
-        "loss_incidence": Y_inc,
-        "loss_trailing": Y_te,
-        "loss_secondary": Y_s,
-        "loss_clearance": Y_cl,
-        "loss_total": Y_p + Y_te + Y_inc + Y_s + Y_cl,
-    }
-
-    return losses
-
-
-def get_profile_loss(flow_parameters, geometry):
-    r"""
-    Calculate the profile loss coefficient for the current cascade using the Kacker and Okapuu loss model.
-    The equation for :math:`\mathrm{Y_p}` is given by:
-
-    .. math::
-
-        \mathrm{Y_p} = \mathrm{Y_{reaction}} - \left|\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right| \cdot
-        \left(\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right) \cdot (\mathrm{Y_{impulse}} - \mathrm{Y_{reaction}})
-
-    where:
-
-        - :math:`\mathrm{Y_{reaction}}` is the reaction loss coefficient computed using Aungier correlation.
-        - :math:`\mathrm{Y_{impulse}}` is the impulse loss coefficient computed using Aungier correlation.
-        - :math:`\theta_\mathrm{in}` is the inlet metal angle.
-        - :math:`\beta_\mathrm{out}` is the exit flow angle.
-
-    The function also applies various corrections based on flow parameters and geometry factors.
-
-    Parameters
-    ----------
-    flow_parameters : dict
-        Dictionary containing flow-related parameters.
-    geometry : dict
-        Dictionary with geometric parameters.
-
-    Returns
-    -------
-    float
-        Profile loss coefficient.
-
-    """
-
-    # TODO: explain smoothing/blending tricks
-
-    # Load data
-    Re = flow_parameters["Re_out"]
-    Ma_rel_out = flow_parameters["Ma_rel_out"]
-    Ma_rel_in = flow_parameters["Ma_rel_in"]
-    p0rel_in = flow_parameters["p0_rel_in"]
-    p_in = flow_parameters["p_in"]
-    p0rel_out = flow_parameters["p0_rel_out"]
-    p_out = flow_parameters["p_out"]
-    beta_out = flow_parameters["beta_out"]
-
-    r_ht_in = geometry["hub_tip_ratio_in"]
-    s = geometry["pitch"]
-    c = geometry["chord"]
-    theta_in = geometry["leading_edge_angle"]
-    t_max = geometry["maximum_thickness"]
-    cascade_type = geometry["cascade_type"]
-
-    # Reynolds number correction factor
-    f_Re = (
-        (Re / 2e5) ** (-0.4) * (Re < 2e5)
-        + 1 * (Re >= 2e5 and Re <= 1e6)
-        + (Re / 1e6) ** (-0.2) * (Re > 1e6)
-    )
-
-    # Mach number correction factor
-    f_Ma = 1 + 60 * (Ma_rel_out - 1) ** 2 * (
-        Ma_rel_out > 1
-    )  ## TODO smoothing / mach crit
-
-    # Compute losses related to shock effects at the inlet of the cascade
-    f_hub = get_hub_to_mean_mach_ratio(r_ht_in, cascade_type)
-    a = max(0, f_hub * Ma_rel_in - 0.4)  # TODO: smoothing
-    Y_shock = 0.75 * a**1.75 * r_ht_in * (p0rel_in - p_in) / (p0rel_out - p_out)
-    Y_shock = max(0, Y_shock)  # TODO: smoothing
-
-    # Compute compressible flow correction factors
-    Kp, K2, K1 = get_compressible_correction_factors(Ma_rel_in, Ma_rel_out)
-
-    # Yp_reaction and Yp_impulse according to Aungier correlation
-    # These formulas are valid for 40<abs(angle_out)<80
-    # Extrapolating outside of this limits might give completely wrong results
-    # If the optimization algorithm has upper and lower bounds for the outlet
-    # angle there is no need to worry about this problem
-    # angle_out_bis keeps the 40deg-losses for outlet angles lower than 40deg
-    angle_out_bis = max(abs(beta_out), 40)
-    Yp_reaction = nozzle_blades(s / c, angle_out_bis)
-    Yp_impulse = impulse_blades(s / c, angle_out_bis)
-
-    # Formula according to Kacker-Okapuu
-    Y_p = Yp_reaction - abs(theta_in / beta_out) * (theta_in / beta_out) * (
-        Yp_impulse - Yp_reaction
-    )
-
-    # Limit the extrapolation of the profile loss to avoid negative values for
-    # blade profiles with little deflection
-    # Low limit to 80% of the axial entry nozzle profile loss
-    # This value is completely arbitrary
-    Y_p = max(Y_p, 0.8 * Yp_reaction)  # TODO: smoothing
-
-    # Avoid unphysical effect on the thickness by defining the variable aa
-    aa = max(0, -theta_in / beta_out)  # TODO: smoothing
-    Y_p = Y_p * ((t_max / c) / 0.2) ** aa
-    Y_p = 0.914 * (2 / 3 * Y_p * Kp + Y_shock)
-
-    # Corrected profile loss coefficient
-    Y_p = f_Re * f_Ma * Y_p
-
-    return Y_p
-
-
-def get_trailing_edge_loss(flow_parameters, geometry):
-    r"""
-    Calculate the trailing edge loss coefficient using the Kacker-Okapuu model.
-    The main equation for the kinetic-energy coefficient is given by:
-
-    .. math::
-
-        d_{\phi^2} = d_{\phi^2_\mathrm{reaction}} - \left|\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right| \cdot
-        \left(\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right) \cdot (d_{\phi^2_\mathrm{impulse}} - d_{\phi^2_\mathrm{reaction}})
-
-    The kinetic-energy coefficient is converted to the total pressure loss coefficient by:
-
-    .. math::
-
-        \mathrm{Y_{te}} = \frac{1}{{1 - \phi^2}} - 1
-
-    where:
-
-        - :math:`d_{\phi^2_\mathrm{reaction}}` and :math:`d_{\phi^2_\mathrm{impulse}}` are coefficients related to kinetic energy loss for reaction and impulse blades respectively, and are interpolated based on trailing edge to throat opening ratio.
-        - :math:`\beta_\mathrm{out}` is the exit flow angle.
-        - :math:`\theta_\mathrm{in}` is the inlet metal angle.
-
-    The function also applies various interpolations and computations based on flow parameters and geometry factors.
-
-    Parameters
-    ----------
-    flow_parameters : dict
-        Dictionary containing flow-related parameters.
-    geometry : dict
-        Dictionary with geometric parameters.
-
-    Returns
-    -------
-    float
-        Trailing edge loss coefficient.
-
-    """
-
-    t_te = geometry["trailing_edge_thickness"]
-    o = geometry["opening"]
-    angle_in = geometry["leading_edge_angle"]
-    angle_out = flow_parameters["beta_out"]
-
-    # Range of trailing edge to throat opening ratio
-    r_to_data = [0, 0.2, 0.4]
-
-    # Reacting blading
-    phi_data_reaction = [0, 0.045, 0.15]
-
-    # Impulse blading
-    phi_data_impulse = [0, 0.025, 0.075]
-
-    # Numerical trick to avoid too big r_to's
-    r_to = min(0.4, t_te / o)  # TODO: smoothing
-
-    # Interpolate data
-    d_phi2_reaction = np.interp(r_to, r_to_data, phi_data_reaction)
-    d_phi2_impulse = np.interp(r_to, r_to_data, phi_data_impulse)
-
-    # Compute kinetic energy loss coefficient
-    d_phi2 = d_phi2_reaction - abs(angle_in / angle_out) * (angle_in / angle_out) * (
-        d_phi2_impulse - d_phi2_reaction
-    )
-
-    # Limit the extrapolation of the trailing edge loss
-    d_phi2 = max(d_phi2, d_phi2_impulse / 2)  # TODO: smoothing
-    Y_te = 1 / (1 - d_phi2) - 1
-
-    return Y_te
-
-
-def get_secondary_loss(flow_parameters, geometry, delta_height):
-    r"""
-    Calculate the secondary loss coefficient.
-
-    The correlation takes two different forms depending on the aspect ratio:
-
-    If aspect ratio :math:`\leq 2.0`
-
-    .. math::
-
-        \mathrm{Y_{s}} = \frac{0.038 + 0.41 \tanh(1.20\delta^*/H)}{\sqrt{\cos(\xi)}CR(H/c)^{0.55}\frac{\cos(\beta_\mathrm{out})}{\cos(\xi)}^{0.55}}
-
-    wheras if aspect ratio :math:`>2.0`
-
-    .. math::
-
-        \mathrm{Y_{s}} = \frac{0.052 + 0.56 \tanh(1.20\delta^*/H)}{\sqrt{\cos(\xi)}CR(H/c)\frac{\cos(\beta_\mathrm{out})}{\cos(\xi)}^{0.55}}
-
-    where:
-
-        - :math:`CR = \frac{\cos(\beta_\mathrm{in})}{\cos(\beta_\mathrm{out})}` is the convergence ratio.
-        - :math:`H` is the mean height.
-        - :math:`c` is the chord.
-        - :math:`\delta^*` is the inlet endwall boundary layer displacement thickness.
-        - :math:`\xi` is the stagger angle.
-        - :math:`\beta_\mathrm{out}` is the exit relative flow angle.
-
-    Parameters
-    ----------
-    flow_parameters : dict
-        Dictionary containing flow-related parameters.
-    geometry : dict
-        Dictionary with geometric parameters.
-
-    Returns
-    -------
-    float
-        Secondary loss coefficient.
-
-    """
-
-    beta_in = flow_parameters["beta_in"]
-    beta_out = flow_parameters["beta_out"]
-    height = geometry["height"]
-    chord = geometry["chord"]
-    stagger = geometry["stagger_angle"]
-
-    AR = height / chord
-    CR = math.cosd(beta_in) / math.cosd(beta_out)
-    if AR <= 2:  # TODO: sigmoid blending to convert to smooth piecewise function
-        denom = (
-            np.sqrt(math.cosd(stagger))
-            * CR
-            * AR**0.55
-            * (math.cosd(beta_out) / (math.cosd(stagger))) ** 0.55
-        )
-        Y_sec = (0.038 + 0.41 * np.tanh(1.2 * delta_height)) / denom
-    else:
-        denom = (
-            np.sqrt(math.cosd(stagger))
-            * CR
-            * AR
-            * (math.cosd(beta_out) / (math.cosd(stagger))) ** 0.55
-        )
-        Y_sec = (0.052 + 0.56 * np.tanh(1.2 * delta_height)) / denom
-
-    return Y_sec
-
-
-def get_tip_clearance_loss(flow_parameters, geometry):
-    r"""
-    Calculate the tip clearance loss coefficient for the current cascade using the Kacker and Okapuu loss model.
-    The equation for the tip clearance loss coefficent is given by:
-
-    .. math::
-
-        \mathrm{Y_{cl}} = B \cdot Z \cdot \frac{c}{H} \cdot \left(\frac{t_\mathrm{cl}}{H}\right)^{0.78}
-
-    where:
-
-        - :math:`B` is an empirical parameter that depends on the type of cascade (0 for stator, 0.37 for shrouded rotor).
-        - :math:`Z` is a blade loading parameter
-        - :math:`c` is the chord.
-        - :math:`H` is the mean blade height.
-        - :math:`t_\mathrm{cl}` is the tip clearance.
-
-
-    Parameters
-    ----------
-    flow_parameters : dict
-        Dictionary containing flow-related parameters.
-    geometry : dict
-        Dictionary with geometric parameters.
-
-    Returns
-    -------
-    float
-        Tip clearance loss coefficient.
-    """
-
-    beta_out = flow_parameters["beta_out"]
-    beta_in = flow_parameters["beta_in"]
-
-    H = geometry["height"]
-    c = geometry["chord"]
-    t_cl = geometry["tip_clearance"]
-    cascade_type = geometry["cascade_type"]
-
-    # Calculate blade loading parameter Z
-    angle_m = math.arctand((math.tand(beta_in) + math.tand(beta_out)) / 2)
-    Z = (
-        4
-        * (math.tand(beta_in) - math.tand(beta_out)) ** 2
-        * math.cosd(beta_out) ** 2
-        / math.cosd(angle_m)
-    )
-
-    # Empirical parameter (0 for stator, 0.37 for shrouded rotor)
-    if cascade_type == "stator":
-        B = 0
-    elif cascade_type == "rotor":
-        B = 0.37
-    else:
-        print("Specify the type of cascade")
-
-    # Tip clearance loss coefficient
-    Y_cl = B * Z * c / H * (t_cl / H) ** 0.78
-
-    return Y_cl
-
-
-def get_incidence_loss(flow_parameters, geometry, beta_des):
-    r"""
-    Calculate the incidence loss coefficient according to the correlation proposed by :cite:`benner_influence_1997`.
-
-    This model calculates the incidence loss parameter through the function `get_incidence_parameter`. The kinetic energu coefficient
-    can be obtained from `get_incidence_profile_loss_increment`, which is a function of the incidence parameter. The kinetic energy loss coefficient is converted
-    to the total pressure loss coefficient through `convert_kinetic_energy_coefficient`.
-
-    Parameters
-    ----------
-    flow_parameters : dict
-        Dictionary containing flow-related parameters.
-    geometry : dict
-        Dictionary with geometric parameters.
-    beta_des : float
-        Inlet flow angle for zero incidence losses (design).
-
-    Returns
-    -------
-    float
-        Incidence loss coefficient.
-    """
-
-    beta_in = flow_parameters["beta_in"]
-    gamma = flow_parameters["gamma_out"]
-    Ma_rel_out = flow_parameters["Ma_rel_out"]
-
-    le = geometry["leading_edge_diameter"]
-    s = geometry["pitch"]
-    We = geometry["leading_edge_wedge_angle"]
-    theta_in = geometry["leading_edge_angle"]
-    theta_out = math.arccosd(geometry["A_throat"] / geometry["A_out"])
-
-    chi = get_incidence_parameter(le, s, We, theta_in, theta_out, beta_in, beta_des)
-
-    dPhip = get_incidence_profile_loss_increment(chi)
-
-    Y_inc = convert_kinetic_energy_coefficient(dPhip, gamma, Ma_rel_out)
-
-    return Y_inc
-
-
-def get_hub_to_mean_mach_ratio(r_ht, cascade_type):
-    r"""
-    Compute the ratio between Mach at the hub and mean span at the inlet of the current cascade.
-
-    Due to radial variation in gas conditions, Mach at the hub will always be higher than at the mean.
-    Thus, shock losses at the hub could occur even when the Mach is subsonic at the mean blade span.
-
-    Parameters
-    ----------
-    r_ht : float
-        Hub to tip ratio at the inlet of the current cascade.
-    cascade_type : str
-        Type of the current cascade, either 'stator' or 'rotor'.
-
-    Returns
-    -------
-    float
-        Ratio between Mach at the hub and mean span at the inlet of the current cascade.
-
-    """
-
-    if r_ht < 0.5:  # TODO: add smoothing, this is essentially a max(r_ht, 0.5)
-        r_ht = 0.5  # Numerical trick to prevent extrapolation
-
-    r_ht_data = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
-
-    # Stator curve
-    f_data_S = [1.4, 1.18, 1.05, 1.0, 1.0, 1.0]
-
-    # Rotor curve
-    f_data_R = [2.15, 1.7, 1.35, 1.12, 1.0, 1.0]
-
-    if cascade_type == "stator":
-        f = np.interp(r_ht, r_ht_data, f_data_S)
-    elif cascade_type == "rotor":
-        f = np.interp(r_ht, r_ht_data, f_data_R)
-    else:
-        print("Specify the type of cascade")
-
-    return f
-
-
-def get_compressible_correction_factors(Ma_rel_in, Ma_rel_out):
-    r"""
-
-    Compute compressible flow correction factor according to Kacker and Okapuu loss model :cite:`kacker_mean_1982`.
-
-    The correction factors :math:`\mathrm{K_1}`, :math:`\mathrm{K_2}` and :math:`\mathrm{K_p}` was introduced by :cite:`kacker_mean_1982` to correct previous correlation (:cite:`ainley_method_1951`)
-    for effect of higher mach number and channel acceleration. The correction factors reduces the losses at higher mach number. Their definition follows: 
-
-    .. math::
-
-        &\mathrm{K_1} = \begin{cases}
-                1 & \text{if } \mathrm{Ma_{out}} < 0.2 \\
-                1 - 1.25(\mathrm{Ma_{out}} - 0.2) & \text{if } \mathrm{Ma_{out}} \geq 0.2
-              \end{cases} \\
-        &\mathrm{K_2} = \left(\frac{\mathrm{Ma_{in}}}{\mathrm{Ma_{out}}}\right) \\
-        &\mathrm{K_p} = 1 - \mathrm{K_2}(1-\mathrm{K_1})
-
-    where:
-
-        - :math:`\mathrm{Ma_{in}}` is the relative mach number at the cascade inlet.
-        -  :math:`\mathrm{Ma_{out}}` is the relative mach number at the cascade exit.
-    
-    Parameters
-    ----------
-    Ma_rel_in : float
-        Inlet relative mach number.
-    Ma_rel_out : float
-        Exit relative mach number.
-
-    Returns
-    -------
-    float
-        Correction factor :math:`\mathrm{K_p}`.
-    float
-        Correction factor :math:`\mathrm{K_2}`.
-    float
-        Correction factor :math:`\mathrm{K_1}`.
-    """
-
-    K1 = 1 * (Ma_rel_out < 0.2) + (1 - 1.25 * (Ma_rel_out - 0.2)) * (
-        Ma_rel_out > 0.2 and Ma_rel_out < 1.00
-    )  # TODO: this can be converted to a smooth piecewise function (sigmoid blending)
-    K2 = (Ma_rel_in / Ma_rel_out) ** 2
-    Kp = 1 - K2 * (1 - K1)
-    Kp = max(0.1, Kp)  # TODO: smoothing
-    return [Kp, K2, K1]
-
-
-def nozzle_blades(r_sc, angle_out):
-    r"""
-    Use Aungier correlation to compute the pressure loss coefficient for nozzle blades :cite:`aungier_turbine_2006`.
-
-    This correlation is a formula that reproduces the figures from the Ainley and Mathieson original figures :cite:`ainley_method_1951`,
-    and is a function of the pitch-to-chord ratio and exit relative flow angle. 
-
-    The correlation uses the following equations:
-
-    .. math::
-
-        & \beta_\mathrm{tan} = 90 - \beta_\mathrm{ax} \\
-        & \left(\frac{s}{c}\right)_\mathrm{min} = \begin{cases}
-                                                    0.46 + \frac{\beta_\mathrm{tan}}{77} && \text{if } \beta_\mathrm{tan} < 30 \\
-                                                    0.614 + \frac{\beta_\mathrm{tan}}{130} && \text{if } \beta_\mathrm{tan} \geq 30
-                                                \end{cases} \\
-        & X = \left(\frac{s}{c}\right) - \left(\frac{s}{c}\right)_\mathrm{min} \\
-        & A = \begin{cases}
-                0.025 + \frac{27 - \beta_\mathrm{tan}}{530} && \text{if } \beta_\mathrm{tan} < 27 \\
-                0.025 + \frac{27 - \beta_\mathrm{tan}}{3085} && \text{if } \beta_\mathrm{tan} \geq 27
-              \end{cases} \\
-        & B = 0.1583 - \frac{\beta_\mathrm{tan}}{1640} \\
-        & C = 0.08\left(\frac{\beta_\mathrm{tan}}{30}\right)^2 -1 \\
-        & n = 1 + \frac{\beta_\mathrm{tan}}{30} \\
-        & \mathrm{Y_{p,reaction}} = \begin{cases}
-                A + BX^2 + CX^3 && \text{if } \beta_\mathrm{tan} < 30 \\
-                A + B |X|^n && \text{if } \beta_\mathrm{tan} \geq 30
-              \end{cases}   
-
-    where:
-
-        - :math:`s` is the pitch
-        - :math:`c` is the chord
-        - :math:`\beta_\mathrm{tan}` and :math:`\beta_\mathrm{ax}` is the exit relative flow angle with respect to tangential and axial direction. 
-        
-    Parameters
-    ----------
-    r_sc : float
-        Pitch-to-chord ratio.
-    angle_out : float
-        Exit relative flow angle (in degrees).
-
-    Returns
-    -------
-    float
-        Pressure loss coefficient for impulse blades.
-
-    """
-
-    phi = 90 - angle_out
-    r_sc_min = (0.46 + phi / 77) * (phi < 30) + (0.614 + phi / 130) * (
-        phi >= 30
-    )  # TODO: sigmoid blending to convert to smooth piecewise function
-    X = r_sc - r_sc_min
-    A = (0.025 + (27 - phi) / 530) * (phi < 27) + (0.025 + (27 - phi) / 3085) * (
-        phi >= 27
-    )  # TODO: sigmoid blending to convert to smooth piecewise function
-    B = 0.1583 - phi / 1640
-    C = 0.08 * ((phi / 30) ** 2 - 1)
-    n = 1 + phi / 30
-    Yp_reaction = (A + B * X**2 + C * X**3) * (phi < 30) + (A + B * abs(X) ** n) * (
-        phi >= 30
-    )  # TODO: sigmoid blending to convert to smooth piecewise function
-    return Yp_reaction
-
-
-def impulse_blades(r_sc, angle_out):
-    r"""
-    Use Aungier correlation to compute the pressure loss coefficient for impulse blades :cite:`aungier_turbine_2006`.
-
-    This correlation is a formula that reproduces the figures from the Ainley and Mathieson original figures :cite:`ainley_method_1951`,
-    and is a function of the pitch-to-chord ratio and exit relative flow angle. 
-
-    The correlation uses the following equations:
-
-    .. math::
-
-        & \beta_\mathrm{tan} = 90 - \beta_\mathrm{ax} \\
-        & \left(\frac{s}{c}\right)_\mathrm{min} = 0.224 + 1.575\left(\frac{\beta_\mathrm{tan}}{90}\right) - \left(\frac{\beta_\mathrm{tan}}{90}\right)^2 \\
-        & X = \left(\frac{s}{c}\right) - \left(\frac{s}{c}\right)_\mathrm{min} \\
-        & A = 0.242 - \frac{\beta_\mathrm{tan}}{151} + \left(\frac{\beta_\mathrm{tan}}{127}\right)^2 \\
-        & B = \begin{cases}
-                0.3 + \frac{30 - \beta_\mathrm{tan}}{50} && \text{if } \beta_\mathrm{tan} < 30 \\
-                0.3 + \frac{30 - \beta_\mathrm{tan}}{275} && \text{if } \beta_\mathrm{tan} \geq 30
-              \end{cases} \\
-        & C = 0.88 - \frac{\beta_\mathrm{tan}}{42.4} + \left(\frac{\beta_\mathrm{tan}}{72.8}\right)^2 \\
-        & \mathrm{Y_{p,impulse}} = A + BX^2 - CX^3 
-
-    where:
-
-        - :math:`s` is the pitch
-        - :math:`c` is the chord
-        - :math:`\beta_\mathrm{tan}` and :math:`\beta_\mathrm{ax}` is the exit relative flow angle with respect to tangential and axial direction. 
-        
-    Parameters
-    ----------
-    r_sc : float
-        Pitch-to-chord ratio.
-    angle_out : float
-        Exit relative flow angle (in degrees).
-
-    Returns
-    -------
-    float
-        Pressure loss coefficient for impulse blades.
-
-    """
-
-    phi = 90 - angle_out
-    r_sc_min = 0.224 + 1.575 * (phi / 90) - (phi / 90) ** 2
-    X = r_sc - r_sc_min
-    A = 0.242 - phi / 151 + (phi / 127) ** 2
-    B = (0.3 + (30 - phi) / 50) * (phi < 30) + (0.3 + (30 - phi) / 275) * (
-        phi >= 30
-    )  # TODO: sigmoid blending to convert to smooth piecewise function
-    C = 0.88 - phi / 42.4 + (phi / 72.8) ** 2
-    Yp_impulse = A + B * X**2 - C * X**3
-    return Yp_impulse
-
-
-def get_penetration_depth(flow_parameters, geometry, delta_height):
-    r"""
-    Calculated the penetration depth of the passage vortex separation line relative to the blade span.
-
-    The endwall inlet boundary layer generate a vortex which propagtes through the cascade section,
-    and the spanwise penetration of this vortex affect the magnutude of the secondary loss coefficient.
-    This function approximate the vortex penetration depth to the blade span by the correlation developed by :cite:`benner_empirical_2006-1`.
-
-    The quantity is calculated as:
-
-    .. math::
-
-        \frac{\mathrm{Z_{te}}}{H} = \frac{0.10F_t^{0.79}}{\sqrt{CR}\left(\frac{H}{c}\right)^{0.55}} + 32.70\frac{\delta^*}{H}^2
-
-    where:
-
-        - :math:`CR = \frac{\cos(\beta_\mathrm{in})}{\cos(\beta_\mathrm{out})}` is the convergence ratio.
-        - :math:`H` is the mean height.
-        - :math:`c` is the chord.
-        - :math:`\delta^*` is the inlet endwall boundary layer displacement thickness.
-        - :math:`\beta_\mathrm{out}` is the exit relative flow angle.
-        - :math:`F_t` is the tangential loading coefficient.
-
-    The tangiential loading coefficient is calculated by a separate function (`F_t`).
-
-    Parameters
-    ----------
-    flow_parameters : dict
-        Dictionary containing flow-related parameters.
-    geometry : dict
-        Dictionary with geometric parameters.
-    delta_height : float
-        The inlet endwall boundary layer displacement thickness relative to the mean blade height.
-
-    Returns
-    -------
-    float
-        Spanwise penetration depth of the passage vortex relative to the mean blade height.
-
-    """
-
-    # TODO: explain smoothing/blending tricks
-
-    beta_in = flow_parameters["beta_in"]
-    beta_out = flow_parameters["beta_out"]
-    axial_chord = geometry["axial_chord"]
-    pitch = geometry["pitch"]
-    chord = geometry["chord"]
-    height = geometry["height"]
-
-    CR = math.cosd(beta_in) / math.cosd(
-        beta_out
-    )  # Convergence ratio from Benner et al.[2006]
-
-    BSx = axial_chord / pitch  # Axial blade solidity
-    delta_height = (
-        delta_height  # Boundary layer displacement thickness relative to blade height
-    )
-    AR = height / chord  # Aspect ratio
-
-    Ft = F_t(BSx, beta_in, beta_out)
-
-    Z_TE = 0.10 * Ft**0.79 / np.sqrt(CR) / (AR) ** 0.55 + 32.70 * delta_height**2
-
-    Z_TE = min(Z_TE, 0.99)  # TODO: smoothing
-
-    return Z_TE
-
-
-def convert_kinetic_energy_coefficient(dPhi, gamma, Ma_rel_out):
-    r"""
-
-    Convert the kinetic energy coefficient increment due to incidence to the total pressure loss coefficient according to the following correlation:
-
-    .. math::
-
-        \mathrm{Y} = \frac{\left(1-\frac{\gamma -1}{2}\mathrm{Ma_{out}}^2(\frac{1}{(1-\Delta\phi^2_p)}-1)\right)^\frac{-\gamma}{\gamma - 1}-1}{1-\left(1 + \frac{\gamma - 1}{2}\mathrm{Ma_{out}}^2\right)^\frac{-\gamma}{\gamma - 1}}
-
-    where:
-
-        - :math:`\gamma` is the specific heat ratio.
-        - :math:`\mathrm{Ma_{out}}` is the cascade exit relative mach number.
-        - :math:`\Delta\phi^2_p` is the kinetic energy loss coefficient increment due to incidence.
-
-    Parameters
-    ----------
-    dPhi : float
-        Kinetic energy coefficient increment.
-    gamma : float
-        Heat capacity ratio.
-    Ma_rel_out : float
-        The cascade exit relative mach number.
-
-    Returns
-    -------
-    float
-        The total pressure loss coefficient.
-
-    Warnings
-    --------
-    This conversion assumes that the fluid is a perfect gas.
-
-    """
-
-    denom = 1 - (1 + (gamma - 1) / 2 * Ma_rel_out**2) ** (-gamma / (gamma - 1))
-    numer = (1 - (gamma - 1) / 2 * Ma_rel_out**2 * (1 / (1 - dPhi) - 1)) ** (
-        -gamma / (gamma - 1)
-    ) - 1
-
-    Y = numer / denom
-
-    return Y
-
-
-def get_incidence_profile_loss_increment(chi, chi_extrapolation=5, loss_limit=0.5):
-    r"""
-    Computes the increment in profile losses due to the effect of incidence according to the correlation proposed by :cite:`benner_influence_1997`.
-
-    The increment in profile losses due to incidence is based on the incidence parameter :math:`\chi`.
-    The formula used to compute :math:`\chi` is given by:
-
-    .. math::
-
-       \chi = We^{-0.2}\left(\frac{d}{s}\right)^{-0.05}  \left(\frac{\cos{\beta_1}}{\cos{\beta_2}} \right)^{-1.4}  \left(\alpha_1 - \alpha_{1,\mathrm{des}}\right)
-
-    Depending on the value of :math:`\chi`, two equations are used for computing the increment in profile losses:
-
-    1. For :math:`\chi \geq 0`:
-
-    .. math::
-
-        \Delta \phi_{p}^2 = \sum_{i=1}^{8} a_i \, \chi^i
-
-    with coefficients:
-    
-    .. math::
-    
-        a_1 = -6.149 \times 10^{-5}  \\
-        a_2 = +1.327 \times 10^{-3}  \\
-        a_3 = -2.506 \times 10^{-4}  \\
-        a_4 = -1.542 \times 10^{-4}  \\
-        a_5 = +9.017 \times 10^{-5}  \\
-        a_6 = +1.106 \times 10^{-5}  \\
-        a_7 = -5.318 \times 10^{-6}  \\
-        a_8 = +3.711 \times 10^{-7}
-
-    2. For :math:`\chi < 0`:
-
-    .. math::
-
-        \Delta \phi_{p}^2 = \sum_{i=1}^{2} b_i \, \chi^i
-
-    with coefficients:
-    
-    .. math::
-    
-        b_1 = -8.720e-4 \times 10^{-4}  \\
-        b_2 = +1.358e-4 \times 10^{-4}
-    
-    This function also implements two safeguard methods to prevent unrealistic values of the pressure loss increment
-    when the value of :math:`\chi` is outside the range of validity of the correlation
-
-    1. Linear extrapolation beyond a certain :math:`\chi`:
-        Beyond a certain value of the distance parameter :math:`\hat{\chi}` the function linearly extrapolates based on the slope of the polynomial.
-
-    2. Loss Limiting:
-        The profile loss increment is limited to a maximum value :math:`\Delta \hat{\phi}_{p}^2` to prevent potential over-predictions. The limiting mechanism is smooth to ensure that the function is differentiable.
-
-    The final expression for the loss coefficient increment can be summarized as:
-
-    .. math::
-
-        \begin{align}
-        \Delta \phi_{p}^2 = 
-        \begin{cases}
-        \sum_{i=1}^{2} b_i \, \chi^i & \text{for } \chi < 0 \\
-        \sum_{i=1}^{8} a_i \, \chi^i & \text{for } 0 \leq \chi  \leq \hat{\chi} \\
-        \Delta\phi_{p}^2(\hat{\chi}) + \mathrm{slope}(\hat{\chi}) \cdot(\chi - \hat{\chi}) & \text{for } \chi > \hat{\chi}
-        \end{cases}
-        \end{align}
-
-    .. math::
-        \Delta \phi_{p}^2 = \mathrm{smooth\,min}(\Delta \phi_{p}^2, \Delta \hat{\phi}_{p}^2)
-
-
-    Parameters
-    ----------
-    chi : array-like
-        The distance parameter, :math:`\chi`, used to compute the profile losses increment.
-    chi_extrapolation : float, optional
-        The value of :math:`\chi` beyond which linear extrapolation is applied. Default is 5.
-    loss_limit : float, optional
-        The upper limit to which the profile loss increment is smoothly constrained. Default is 0.5.
-
-    Returns
-    -------
-    array-like
-        Increment in profile losses due to the effect of incidence.
-    """
-
-    # Define the polynomial function for positive values of chi
-    coeffs_1 = np.array(
-        [
-            +3.711e-7,
-            -5.318e-6,
-            +1.106e-5,
-            +9.017e-5,
-            -1.542e-4,
-            -2.506e-4,
-            +1.327e-3,
-            -6.149e-5,
-        ]
-    )
-    func_1 = lambda x: np.sum(
-        np.asarray([a * x**i for (i, a) in enumerate(coeffs_1[::-1], start=1)]),
-        axis=0,
-    )
-
-    # Define the polynomial function for negative values of chi
-    coeffs_2 = np.array([1.358e-4, -8.72e-4])
-    func_2 = lambda x: np.sum(
-        np.asarray([b * x**i for (i, b) in enumerate(coeffs_2[::-1], start=1)]),
-        axis=0,
-    )
-
-    # Calculate the profile losses increment based on polynomials
-    loss_poly = np.where(chi >= 0, func_1(chi), func_2(chi))
-
-    # Apply linear extrapolation beyond the threshold for chi
-    _chi = chi_extrapolation
-    loss = func_1(_chi)
-    coeffs_slope = np.array([i * a for i, a in enumerate(coeffs_1[::-1], start=1)])
-    slope = np.sum([a * _chi ** (i - 1) for i, a in enumerate(coeffs_slope, start=1)])
-    loss_extrap = loss + slope * (chi - _chi)
-    loss_increment = np.where(chi > _chi, loss_extrap, loss_poly)
-
-    # Limit the loss to the loss_limit
-    if loss_limit is not None:
-        loss_increment = np.asarray(
-            [loss_increment, np.full_like(loss_increment, loss_limit)]
-        )
-        loss_increment = math.smooth_min(
-            loss_increment, method="logsumexp", alpha=25, axis=0
-        )
-
-    return loss_increment
-
-
-def get_incidence_parameter(le, s, We, theta_in, theta_out, beta_in, beta_des):
-    r"""
-    Calculate the incidence parameter according to the correlation proposed by :cite:`benner_influence_1997`.
-
-    The incidence parameter is used to calculate the increment in profile losses due to the effect of incidence according to function `get_incidence_profile_loss_increment`.
-
-    The quantity is calculated as:
-
-    .. math::
-
-        \chi = \frac{\mathrm{d_{le}}}{s}^{-0.05}\mathrm{We_{le}}^{-0.2}\frac{\cos\theta_\mathrm{in}}{\cos\theta_\mathrm{out}}^{-1.4}(\beta_\mathrm{in} - \beta_\mathrm{des})
-
-    where:
-
-        - :math:`\mathrm{d_{le}}` is the leading edge diameter.
-        - :math:`s` is the pitch.
-        - :math:`\mathrm{We_{le}}` is the leading edge wedge angle.
-        - :math:`\theta_\mathrm{in}` and :math:`\theta_\mathrm{out}` is the blade metal angle at the inlet and outlet respectively.
-        - :math:`\beta_\mathrm{in}` and :math:`\beta_\mathrm{des}` is the inlet relative flow angle at given and design conditions respectively.
-
-    Parameters
-    ----------
-    le : float
-        Leading edge diameter.
-    s : float
-        Pitch.
-    We : float
-        Leading edge wedge angle (in degrees).
-    theta_in : float
-        Leading edge metal angle (in degrees).
-    theta_out : float
-        Trailing edge metal angle (in degrees).
-    beta_in : float
-        Inlet relative flow angle (in degrees).
-    beta_des : float
-        Inlet relative flow angle at design condition (in degrees).
-
-    Returns
-    -------
-    float
-        Incidence parameter.
-    """
-
-    chi = (
-        (le / s) ** (-0.05)
-        * (We) ** (-0.2)
-        * (math.cosd(theta_in) / math.cosd(theta_out)) ** (-1.4)
-        * (abs(beta_in) - abs(beta_des))
-    )
-    return chi
-
-
-def F_t(BSx, beta_in, beta_out):
-    r"""
-
-    Calculate the tangential loading coefficient according to the correlation proposed by :cite:`benner_empirical_2006-1`.
-
-    .. math::
-
-        F_t = 2\frac{s}{c_\mathrm{ax}}\cos^2(\beta_m)(\tan(\beta_\mathrm{in} - \beta_\mathrm{out}))
-
-    where:
-
-        - :math:`s` is the pitch.
-        - :math:`c_\mathrm{ax}` is the axial chord.
-        - :math:`\beta_m = \tan^{-1}(0.5(\tan(\beta_\mathrm{in}) + \tan(\beta_\mathrm{out})))` is the mean vector angle.
-        - :math:`\beta_\mathrm{in}` and :math:`\beta_\mathrm{out}` is the inlet and outlet relative flow angle.
-
-    Parameters
-    ----------
-    Bsx : float
-        Blade solidity based on axial chord.
-    beta_in : float
-        Inlet relative flow angle (in degrees).
-    beta_out : float
-        Exit relative flow angle (in degrees).
-
-    Returns
-    -------
-    float
-        Tangetnial loading parameter.
-
-    """
-
-    a_m = math.arctand(0.5 * (math.tand(beta_in) + math.tand(beta_out)))
-
-    F_t = (
-        2
-        * 1
-        / BSx
-        * math.cosd(a_m) ** 2
-        * (abs(math.tand(beta_in)) + abs(math.tand(beta_out)))
-    )
-
-    return F_t
+import numpy as np
+from .. import math
+
+
+def compute_losses(input_parameters):
+    r"""
+
+    Evaluate loss coefficient according to :cite:`kacker_mean_1982` and :cite:`moustapha_improved_1990`.
+
+    This model split the total loss coefficient into profile, secondary, trailing edge, tip clearance and incidence.
+    The model adopt the incidence loss coefficient proposed by  :cite:`moustapha_improved_1990`, but is otherwise similar to the
+    model proposed by :cite:`kacker_mean_1982`.
+    The loss coefficient are combined through the following relation:
+
+    .. math::
+
+        \mathrm{Y_{tot}} = \mathrm{Y_{p}} + \mathrm{Y_{te}} + \mathrm{Y_{corr}}\mathrm{Y_{s}} + \mathrm{Y_{cl}} + \mathrm{Y_{inc}}
+
+    The function calls a function for each loss component:
+
+        - :math:`\mathrm{Y_{p}}` : `get_profile_loss`
+        - :math:`\mathrm{Y_{te}}` : `get_trailing_edge_loss`
+        - :math:`\mathrm{Y_{s}}` : `get_secondary_loss`
+        - :math:`\mathrm{Y_{cl}}` : `get_tip_clearance_loss`
+        - :math:`\mathrm{Y_{inc}}` : `get_incidence_loss`
+        - :math:`\mathrm{Y_{corr}}` : `get_secondary_loss_correction_factor`
+
+    Parameters
+    ----------
+    input_parameters : dict
+        A dictionary containing containing the keys, `flow`, `geometry` and `options`.
+
+    Returns
+    -------
+    dict
+        A dictionary with the loss components.
+
+    """
+
+    # Load data
+    flow_parameters = input_parameters["flow"]
+    geometry = input_parameters["geometry"]
+    beta_des = geometry["leading_edge_angle"]
+
+    # Profile loss coefficient
+    Y_p = get_profile_loss(flow_parameters, geometry)
+
+    # Secondary loss coefficient
+    Y_s = get_secondary_loss(flow_parameters, geometry)
+
+    # Tip clearance loss coefficient
+    Y_cl = get_tip_clearance_loss(flow_parameters, geometry)
+
+    # Trailing edge loss coefficienct
+    Y_te = get_trailing_edge_loss(flow_parameters, geometry)
+
+    # Incidence loss for profile loss
+    Y_inc = get_incidence_loss(flow_parameters, geometry, beta_des)
+
+    # Incidence correction factor for secondary loss coefficient
+    Y_corr = get_secondary_loss_correction_factor(flow_parameters, geometry)
+    Y_s *= Y_corr
+
+    # Calculate total pressure loss coefficient
+    Y = Y_p + Y_s + Y_cl + Y_te + Y_inc
+
+    # Return a dictionary of loss components
+    losses = {
+        "loss_profile": Y_p,
+        "loss_incidence": Y_inc,
+        "loss_trailing": Y_te,
+        "loss_secondary": Y_s,
+        "loss_clearance": Y_cl,
+        "loss_total": Y_p + Y_te + Y_inc + Y_s + Y_cl,
+    }
+
+    return losses
+
+
+def get_profile_loss(flow_parameters, geometry):
+    r"""
+    Calculate the profile loss coefficient for the current cascade using the Kacker and Okapuu loss model.
+    The equation for :math:`\mathrm{Y_p}` is given by:
+
+    .. math::
+
+        \mathrm{Y_p} = \mathrm{Y_{reaction}} - \left|\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right| \cdot
+        \left(\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right) \cdot (\mathrm{Y_{impulse}} - \mathrm{Y_{reaction}})
+
+    where:
+
+        - :math:`\mathrm{Y_{reaction}}` is the reaction loss coefficient computed using Aungier correlation.
+        - :math:`\mathrm{Y_{impulse}}` is the impulse loss coefficient computed using Aungier correlation.
+        - :math:`\theta_\mathrm{in}` is the inlet metal angle.
+        - :math:`\beta_\mathrm{out}` is the exit flow angle.
+
+    The function also applies various corrections based on flow parameters and geometry factors.
+
+    Parameters
+    ----------
+    flow_parameters : dict
+        Dictionary containing flow-related parameters.
+    geometry : dict
+        Dictionary with geometric parameters.
+
+    Returns
+    -------
+    float
+        Profile loss coefficient.
+
+    """
+
+    # Load data
+    Re = flow_parameters["Re_out"]
+    Ma_rel_out = flow_parameters["Ma_rel_out"]
+    Ma_rel_in = flow_parameters["Ma_rel_in"]
+    p0rel_in = flow_parameters["p0_rel_in"]
+    p_in = flow_parameters["p_in"]
+    p0rel_out = flow_parameters["p0_rel_out"]
+    p_out = flow_parameters["p_out"]
+    beta_out = flow_parameters["beta_out"]
+
+    r_ht_in = geometry["hub_tip_ratio_in"]
+    s = geometry["pitch"]
+    c = geometry["chord"]
+    theta_in = geometry["leading_edge_angle"]
+    t_max = geometry["maximum_thickness"]
+    cascade_type = geometry["cascade_type"]
+
+    # Reynolds number correction factor
+    f_Re = (
+        (Re / 2e5) ** (-0.4) * (Re < 2e5)
+        + 1 * (Re >= 2e5 and Re <= 1e6)
+        + (Re / 1e6) ** (-0.2) * (Re > 1e6)
+    )
+
+    # Mach number correction factor
+    f_Ma = 1 + 60 * (Ma_rel_out - 1) ** 2 * (Ma_rel_out > 1)
+
+    # Compute losses related to shock effects at the inlet of the cascade
+    f_hub = get_hub_to_mean_mach_ratio(r_ht_in, cascade_type)
+    a = max(0, f_hub * Ma_rel_in - 0.4)
+    Y_shock = 0.75 * a**1.75 * r_ht_in * (p0rel_in - p_in) / (p0rel_out - p_out)
+    Y_shock = max(0, Y_shock)
+
+    # Compute compressible flow correction factors
+    Kp, K2, K1 = get_compressible_correction_factors(Ma_rel_in, Ma_rel_out)
+
+    # Yp_reaction and Yp_impulse according to Aungier correlation
+    # These formulas are valid for 40<abs(angle_out)<80
+    # Extrapolating outside of this limits might give completely wrong results
+    # If the optimization algorithm has upper and lower bounds for the outlet
+    # angle there is no need to worry about this problem
+    # angle_out_bis keeps the 40deg-losses for outlet angles lower than 40deg
+    angle_out_bis = max(abs(beta_out), 40)
+    Yp_reaction = nozzle_blades(s / c, angle_out_bis)
+    Yp_impulse = impulse_blades(s / c, angle_out_bis)
+
+    # Formula according to Kacker-Okapuu
+    Y_p = Yp_reaction - abs(theta_in / beta_out) * (theta_in / beta_out) * (
+        Yp_impulse - Yp_reaction
+    )
+
+    # Limit the extrapolation of the profile loss to avoid negative values for
+    # blade profiles with little deflection
+    # Low limit to 80% of the axial entry nozzle profile loss
+    # This value is completely arbitrary
+    Y_p = max(Y_p, 0.8 * Yp_reaction)
+
+    # Avoid unphysical effect on the thickness by defining the variable aa
+    aa = max(0, -theta_in / beta_out)
+    Y_p = Y_p * ((t_max / c) / 0.2) ** aa
+    Y_p = 0.914 * (2 / 3 * Y_p * Kp + Y_shock)
+
+    # Corrected profile loss coefficient
+    Y_p = f_Re * f_Ma * Y_p
+
+    return Y_p
+
+
+def get_secondary_loss(flow_parameters, geometry):
+    r"""
+    The function calculates the secondary loss coefficient using the Kacker-Okapuu model.
+    The main equation for :math:`\mathrm{Y_s}` is given by:
+
+    .. math::
+
+        \mathrm{Y_s} = 1.2 \cdot \mathrm{K_s} \cdot 0.0334 \cdot far \cdot Z \cdot \frac{\cos(\beta_{out})}{\cos(\theta_{in})}
+
+    where:
+
+        - :math:`K_s` is a correction factor accounting for compressible flow effects.
+        - :math:`far` is a factor that account for the aspect ratio of the current cascade.
+        - :math:`Z` is a blade loading parameter.
+        - :math:`\beta_\mathrm{out}` is the exit flow angle.
+        - :math:`\theta_\mathrm{in}` is the inlet metal angle.
+
+    The correction factor :math:`\mathrm{K_s}` from a separate correction factor given from `get_compressible_correction_factors`.
+    `get_compressible_correction_factors` returns :math:`\mathrm{K_s}`, which is used to calculate :math:`\mathrm{K_s}`:
+
+    .. math::
+
+        \mathrm{K_s} = 1 - \left(\frac{b}{H}\right)^2 (1-\mathrm{K_p})
+
+    where:
+
+        - :math:`b` is the axial chord.
+        - :math:`H` is the mean blade height.
+
+    The aspect ratio factor is calculated from the following correlation:
+
+    .. math:: 
+
+        far = \begin{cases}
+                1 - \frac{0.25 \sqrt{|2-H/c|}}{H/c} & \text{if } H/c < 2.0 \\
+                c/H & \text{if } H/c \geq 2.0
+                \end{cases}
+
+    where :math:`c` is the blade chord.
+
+    The loading parameters is caclualated from the following correlation:
+
+    .. math::
+
+        Z = 4 (\tan(\beta_\mathrm{in}) - \tan(\beta_\mathrm{out}))^2\frac{\cos^2(\beta_\mathrm{out})}{\cos(\beta_m)} 
+
+    where:
+
+        - :math:`\beta_\mathrm{in}` and :math:`\beta_\mathrm{out}` is the inlet and exit relative flow angle
+        - :math:`\beta_m = \tan^{-1}(0.5(\tan(\beta_\mathrm{in})-\tan(\beta_\mathrm{out})))` is the mean gas angle.
+           
+    Parameters
+    ----------
+    flow_parameters : dict
+        Dictionary containing flow-related parameters.
+
+    geometry : dict
+        Dictionary with geometric parameters.
+
+    Returns
+    -------
+    float
+        Secondary loss coefficient.
+
+    """
+
+    Ma_rel_out = flow_parameters["Ma_rel_out"]
+    Ma_rel_in = flow_parameters["Ma_rel_in"]
+    beta_out = flow_parameters["beta_out"]
+    beta_in = flow_parameters["beta_in"]
+
+    b = geometry["axial_chord"]
+    H = geometry["height"]
+    c = geometry["chord"]
+    theta_in = geometry["leading_edge_angle"]
+
+    # Compute compressible flow correction factors
+    Kp, K2, K1 = get_compressible_correction_factors(Ma_rel_in, Ma_rel_out)
+
+    # Secondary loss coefficient
+    K3 = (b / H) ** 2
+    Ks = 1 - K3 * (1 - Kp)
+    Ks = max(0.1, Ks)
+    angle_m = math.arctand((math.tand(beta_in) + math.tand(beta_out)) / 2)
+    Z = (
+        4
+        * (math.tand(beta_in) - math.tand(beta_out)) ** 2
+        * math.cosd(beta_out) ** 2
+        / math.cosd(angle_m)
+    )
+    far = (1 - 0.25 * np.sqrt(abs(2 - H / c))) / (H / c) * (H / c < 2) + 1 / (H / c) * (
+        H / c >= 2
+    )
+    Y_s = 1.2 * Ks * 0.0334 * far * Z * math.cosd(beta_out) / math.cosd(theta_in)
+
+    return Y_s
+
+
+def get_trailing_edge_loss(flow_parameters, geometry):
+    r"""
+    Calculate the trailing edge loss coefficient using the Kacker-Okapuu model.
+    The main equation for the kinetic-energy coefficient is given by:
+
+    .. math::
+
+        d_{\phi^2} = d_{\phi^2_\mathrm{reaction}} - \left|\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right| \cdot
+        \left(\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right) \cdot (d_{\phi^2_\mathrm{impulse}} - d_{\phi^2_\mathrm{reaction}})
+
+    The kinetic-energy coefficient is converted to the total pressure loss coefficient by:
+
+    .. math::
+
+        \mathrm{Y_{te}} = \frac{1}{{1 - \phi^2}} - 1
+
+    where:
+
+        - :math:`d_{\phi^2_\mathrm{reaction}}` and :math:`d_{\phi^2_\mathrm{impulse}}` are coefficients related to kinetic energy loss for reaction and impulse blades respectively, and are interpolated based on trailing edge to throat opening ratio.
+        - :math:`\beta_\mathrm{out}` is the exit flow angle.
+        - :math:`\theta_\mathrm{in}` is the inlet metal angle.
+
+    The function also applies various interpolations and computations based on flow parameters and geometry factors.
+
+    Parameters
+    ----------
+    flow_parameters : dict
+        Dictionary containing flow-related parameters.
+    geometry : dict
+        Dictionary with geometric parameters.
+
+    Returns
+    -------
+    float
+        Trailing edge loss coefficient.
+
+    """
+
+    t_te = geometry["trailing_edge_thickness"]
+    o = geometry["opening"]
+    angle_in = geometry["leading_edge_angle"]
+    angle_out = flow_parameters["beta_out"]
+
+    # Range of trailing edge to throat opening ratio
+    r_to_data = [0, 0.2, 0.4]
+
+    # Reacting blading
+    phi_data_reaction = [0, 0.045, 0.15]
+
+    # Impulse blading
+    phi_data_impulse = [0, 0.025, 0.075]
+
+    # Numerical trick to avoid too big r_to's
+    r_to = min(0.4, t_te / o)
+
+    # Interpolate data
+    d_phi2_reaction = np.interp(r_to, r_to_data, phi_data_reaction)
+    d_phi2_impulse = np.interp(r_to, r_to_data, phi_data_impulse)
+
+    # Compute kinetic energy loss coefficient
+    d_phi2 = d_phi2_reaction - abs(angle_in / angle_out) * (angle_in / angle_out) * (
+        d_phi2_impulse - d_phi2_reaction
+    )
+
+    # Limit the extrapolation of the trailing edge loss
+    d_phi2 = max(d_phi2, d_phi2_impulse / 2)
+    Y_te = 1 / (1 - d_phi2) - 1
+
+    return Y_te
+
+
+def get_tip_clearance_loss(flow_parameters, geometry):
+    r"""
+    Calculate the tip clearance loss coefficient for the current cascade using the Kacker and Okapuu loss model.
+    The equation for the tip clearance loss coefficent is given by:
+
+    .. math::
+
+        \mathrm{Y_{cl}} = B \cdot Z \cdot \frac{c}{H} \cdot \left(\frac{t_\mathrm{cl}}{H}\right)^{0.78}
+
+    where:
+
+        - :math:`B` is an empirical parameter that depends on the type of cascade (0 for stator, 0.37 for shrouded rotor).
+        - :math:`Z` is a blade loading parameter
+        - :math:`c` is the chord.
+        - :math:`H` is the mean blade height.
+        - :math:`t_\mathrm{cl}` is the tip clearance.
+
+
+    Parameters
+    ----------
+    flow_parameters : dict
+        Dictionary containing flow-related parameters.
+    geometry : dict
+        Dictionary with geometric parameters.
+
+    Returns
+    -------
+    float
+        Tip clearance loss coefficient.
+    """
+
+    beta_out = flow_parameters["beta_out"]
+    beta_in = flow_parameters["beta_in"]
+
+    H = geometry["height"]
+    c = geometry["chord"]
+    t_cl = geometry["tip_clearance"]
+    cascade_type = geometry["cascade_type"]
+
+    # Calculate blade loading parameter Z
+    angle_m = math.arctand((math.tand(beta_in) + math.tand(beta_out)) / 2)
+    Z = (
+        4
+        * (math.tand(beta_in) - math.tand(beta_out)) ** 2
+        * math.cosd(beta_out) ** 2
+        / math.cosd(angle_m)
+    )
+
+    # Empirical parameter (0 for stator, 0.37 for shrouded rotor)
+    if cascade_type == "stator":
+        B = 0
+    elif cascade_type == "rotor":
+        B = 0.37
+    else:
+        print("Specify the type of cascade")
+
+    # Tip clearance loss coefficient
+    Y_cl = B * Z * c / H * (t_cl / H) ** 0.78
+
+    return Y_cl
+
+
+def get_incidence_loss(flow_parameters, geometry, beta_des):
+    r"""
+    Calculate the incidence loss coefficient according to the correlation proposed by :cite:`moustapha_improved_1990`.
+    
+    The model first computes the incidence parameter, :math:`\chi`. Based on this parameter, the kinetic-energy incidence 
+    loss coefficient is calculated to match experimental data.
+
+    .. math::
+
+        \Delta\phi^2_p = \begin{cases}
+                        -5.1734e^{-6}\chi + 7.6902e^{-9}\chi^2 & \text{if } -800 \leq \chi \leq 0 \\
+                        0.778e^{-5}\chi + 0.56e^{-7}\chi^2 + 0.4e^{-10}\chi^3 + 2.054e^{-19}\chi^6 & \text{if } 0 \leq \chi \leq 800
+                        \end{cases}
+
+    The kinetic-energy coefficient is converted to total pressure loss coefficient through the `convert_kinetic_energy_coefficient` function. 
+
+    Parameters
+    ----------
+    flow_parameters : dict
+        Dictionary containing flow-related parameters.
+    geometry : dict
+        Dictionary with geometric parameters.
+    beta_des : float
+        Inlet flow angle for zero incidence losses (design).
+
+    Returns
+    -------
+    float
+        Incidence loss coefficient.
+
+    Raises
+    ------
+    Warning
+        If incidence parameter (:math:`chi`) is out of range.
+
+    """
+
+    Ma_rel_out = flow_parameters["Ma_rel_out"]  # Exit relative mach number
+    beta_in = flow_parameters["beta_in"]  # Inlet relativ flow angle
+    gamma = flow_parameters["gamma_out"]  # Specific heat ratio
+
+    s = geometry["pitch"]  # Pitch
+    le = geometry["leading_edge_diameter"]  # Leading edge diameter
+    theta_in = geometry["leading_edge_angle"]  # Inlet metal angle
+    theta_out = geometry["gauging_angle"]  # Exit metal angle
+
+    # Compute incidence parameter
+    chi = get_incidence_parameter(le, s, theta_in, theta_out, beta_in, beta_des)
+
+    # Check if incidence parameter is within the range of the experimental data
+    if abs(chi) > 800:
+        raise Warning("Incidence parameter out of range: chi = {chi}")
+
+    # Calculate kinetic-energy loss coefficient
+    if chi >= 0:
+        dPhi = 0.778e-5 * chi + 0.56e-7 * chi**2 + 0.4e-10 * chi**3 + 2.054e-19 * chi**6
+    elif chi < 0:
+        dPhi = -5.1734e-6 * chi + 7.6902e-9 * chi**2
+
+    # Convert kinetic-energy loss coefficient to total pressure loss coeffcient
+    Y_inc = convert_kinetic_energy_coefficient(dPhi, gamma, Ma_rel_out)
+
+    return Y_inc
+
+
+def get_secondary_loss_correction_factor(flow_parameters, geometry):
+    r"""
+    Calculate the correction factor for the secondary loss coefficient to account for incidence losses.
+
+    The correction factor is calculated based on the secondary flow incidence parameter (:math:`\chi`), which is determined using
+    geometrical and flow parameters.
+
+    The correction factor (:math:`Y_\mathrm{corr}`) is given by:
+
+    .. math::
+
+        Y_\mathrm{corr} = \begin{cases}
+        e^{0.9\chi} + 13\chi^2 + 400\chi^4 & \text{if } \chi \geq 0 \\
+        e^{0.9\chi} & \text{if } \chi < 0
+        \end{cases}
+
+    and the incidence paramter is calculated as:
+
+    .. math::
+
+        \chi = \frac{\beta_\mathrm{in} - \theta_\mathrm{in}}{180 - (\theta_\mathrm{in} + \theta_\mathrm{out})}\left(\frac{\cos(\theta_\mathrm{in})}{\cos(\theta_\mathrm{out})}\right)^{-1.5}\left(\frac{\mathrm{d_{le}}}{c}\right)^{-0.3}
+
+    where:
+
+        - :math:`\chi` is the secondary flow incidence parameter.
+        - :math:`c` is the chord length.
+        - :math:`\mathrm{d_{le}}` is the leading edge diameter.
+        - :math:`\theta_\mathrm{in}` is the inlet metal angle.
+        - :math:`\theta_\mathrm{out}` is the exit metal angle.
+        - :math:`\beta_\mathrm{in}` is the inlet relative flow angle.
+
+    The function also checks if the secondary incidence parameter falls within the range of the experimental data used to 
+    determine the correction factor.
+
+    Parameters
+    ----------
+    flow_parameters : dict
+        Dictionary containing flow-related parameters.
+    geometry : dict
+        Dictionary with geometric parameters.
+
+    Returns
+    -------
+    float
+        Secondary loss correction factor.
+
+    """
+
+    beta_in = flow_parameters["beta_in"]
+
+    c = geometry["chord"]
+    le = geometry["leading_edge_diameter"]
+    theta_in = geometry["leading_edge_angle"]
+    theta_out = geometry["gauging_angle"]
+
+    chi = (
+        (abs(beta_in) - abs(theta_in))
+        / (180 - (theta_in + theta_out))
+        * (math.cosd(theta_in) / math.cosd(theta_out)) ** -1.5
+        * (le / c) ** -0.3
+    )
+
+    if not (-0.4 < chi < 0.3):
+        raise Warning("Secondary incidence parameter out of range: chi = {chi}")
+
+    if chi >= 0:
+        Y_corr = np.exp(0.9 * chi) + 13 * chi**2 + 400 * chi**4
+    else:
+        Y_corr = np.exp(0.9 * chi)
+
+    return Y_corr
+
+
+def nozzle_blades(r_sc, angle_out):
+    r"""
+    Use Aungier correlation to compute the pressure loss coefficient for nozzle blades :cite:`aungier_turbine_2006`.
+
+    This correlation is a formula that reproduces the figures from the Ainley and Mathieson original figures :cite:`ainley_method_1951`,
+    and is a function of the pitch-to-chord ratio and exit relative flow angle. 
+
+    The correlation uses the following equations:
+
+    .. math::
+
+        & \beta_\mathrm{tan} = 90 - \beta_\mathrm{ax} \\
+        & \left(\frac{s}{c}\right)_\mathrm{min} = \begin{cases}
+                                                    0.46 + \frac{\beta_\mathrm{tan}}{77} && \text{if } \beta_\mathrm{tan} < 30 \\
+                                                    0.614 + \frac{\beta_\mathrm{tan}}{130} && \text{if } \beta_\mathrm{tan} \geq 30
+                                                \end{cases} \\
+        & X = \left(\frac{s}{c}\right) - \left(\frac{s}{c}\right)_\mathrm{min} \\
+        & A = \begin{cases}
+                0.025 + \frac{27 - \beta_\mathrm{tan}}{530} && \text{if } \beta_\mathrm{tan} < 27 \\
+                0.025 + \frac{27 - \beta_\mathrm{tan}}{3085} && \text{if } \beta_\mathrm{tan} \geq 27
+              \end{cases} \\
+        & B = 0.1583 - \frac{\beta_\mathrm{tan}}{1640} \\
+        & C = 0.08\left(\frac{\beta_\mathrm{tan}}{30}\right)^2 -1 \\
+        & n = 1 + \frac{\beta_\mathrm{tan}}{30} \\
+        & \mathrm{Y_{p,reaction}} = \begin{cases}
+                A + BX^2 + CX^3 && \text{if } \beta_\mathrm{tan} < 30 \\
+                A + B |X|^n && \text{if } \beta_\mathrm{tan} \geq 30
+              \end{cases}   
+
+    where:
+
+        - :math:`s` is the pitch
+        - :math:`c` is the chord
+        - :math:`\beta_\mathrm{tan}` and :math:`\beta_\mathrm{ax}` is the exit relative flow angle with respect to tangential and axial direction. 
+        
+    Parameters
+    ----------
+    r_sc : float
+        Pitch-to-chord ratio.
+    angle_out : float
+        Exit relative flow angle (in degrees).
+
+    Returns
+    -------
+    float
+        Pressure loss coefficient for impulse blades.
+
+    """
+
+    phi = 90 - angle_out
+    r_sc_min = (0.46 + phi / 77) * (phi < 30) + (0.614 + phi / 130) * (phi >= 30)
+    X = r_sc - r_sc_min
+    A = (0.025 + (27 - phi) / 530) * (phi < 27) + (0.025 + (27 - phi) / 3085) * (
+        phi >= 27
+    )
+    B = 0.1583 - phi / 1640
+    C = 0.08 * ((phi / 30) ** 2 - 1)
+    n = 1 + phi / 30
+    Yp_reaction = (A + B * X**2 + C * X**3) * (phi < 30) + (A + B * abs(X) ** n) * (
+        phi >= 30
+    )
+
+    return Yp_reaction
+
+
+def impulse_blades(r_sc, angle_out):
+    r"""
+    Use Aungier correlation to compute the pressure loss coefficient for impulse blades :cite:`aungier_turbine_2006`.
+
+    This correlation is a formula that reproduces the figures from the Ainley and Mathieson original figures :cite:`ainley_method_1951`,
+    and is a function of the pitch-to-chord ratio and exit relative flow angle. 
+
+    The correlation uses the following equations:
+
+    .. math::
+
+        & \beta_\mathrm{tan} = 90 - \beta_\mathrm{ax} \\
+        & \left(\frac{s}{c}\right)_\mathrm{min} = 0.224 + 1.575\left(\frac{\beta_\mathrm{tan}}{90}\right) - \left(\frac{\beta_\mathrm{tan}}{90}\right)^2 \\
+        & X = \left(\frac{s}{c}\right) - \left(\frac{s}{c}\right)_\mathrm{min} \\
+        & A = 0.242 - \frac{\beta_\mathrm{tan}}{151} + \left(\frac{\beta_\mathrm{tan}}{127}\right)^2 \\
+        & B = \begin{cases}
+                0.3 + \frac{30 - \beta_\mathrm{tan}}{50} && \text{if } \beta_\mathrm{tan} < 30 \\
+                0.3 + \frac{30 - \beta_\mathrm{tan}}{275} && \text{if } \beta_\mathrm{tan} \geq 30
+              \end{cases} \\
+        & C = 0.88 - \frac{\beta_\mathrm{tan}}{42.4} + \left(\frac{\beta_\mathrm{tan}}{72.8}\right)^2 \\
+        & \mathrm{Y_{p,impulse}} = A + BX^2 - CX^3 
+
+    where:
+
+        - :math:`s` is the pitch
+        - :math:`c` is the chord
+        - :math:`\beta_\mathrm{tan}` and :math:`\beta_\mathrm{ax}` is the exit relative flow angle with respect to tangential and axial direction. 
+        
+    Parameters
+    ----------
+    r_sc : float
+        Pitch-to-chord ratio.
+    angle_out : float
+        Exit relative flow angle (in degrees).
+
+    Returns
+    -------
+    float
+        Pressure loss coefficient for impulse blades.
+
+    """
+
+    phi = 90 - angle_out
+    r_sc_min = 0.224 + 1.575 * (phi / 90) - (phi / 90) ** 2
+    X = r_sc - r_sc_min
+    A = 0.242 - phi / 151 + (phi / 127) ** 2
+    B = (0.3 + (30 - phi) / 50) * (phi < 30) + (0.3 + (30 - phi) / 275) * (phi >= 30)
+    C = 0.88 - phi / 42.4 + (phi / 72.8) ** 2
+    Yp_impulse = A + B * X**2 - C * X**3
+
+    return Yp_impulse
+
+
+def get_compressible_correction_factors(Ma_rel_in, Ma_rel_out):
+    r"""
+
+    Compute compressible flow correction factor according to Kacker and Okapuu loss model :cite:`kacker_mean_1982`.
+
+    The correction factors :math:`\mathrm{K_1}`, :math:`\mathrm{K_2}` and :math:`\mathrm{K_p}` was introduced by :cite:`kacker_mean_1982` to correct previous correlation (:cite:`ainley_method_1951`)
+    for effect of higher mach number and channel acceleration. The correction factors reduces the losses at higher mach number. Their definition follows: 
+
+    .. math::
+
+        &\mathrm{K_1} = \begin{cases}
+                1 & \text{if } \mathrm{Ma_{out}} < 0.2 \\
+                1 - 1.25(\mathrm{Ma_{out}} - 0.2) & \text{if } \mathrm{Ma_{out}} \geq 0.2
+              \end{cases} \\
+        &\mathrm{K_2} = \left(\frac{\mathrm{Ma_{in}}}{\mathrm{Ma_{out}}}\right) \\
+        &\mathrm{K_p} = 1 - \mathrm{K_2}(1-\mathrm{K_1})
+
+    where:
+
+        - :math:`\mathrm{Ma_{in}}` is the relative mach number at the cascade inlet.
+        -  :math:`\mathrm{Ma_{out}}` is the relative mach number at the cascade exit.
+    
+    Parameters
+    ----------
+    Ma_rel_in : float
+        Inlet relative mach number.
+    Ma_rel_out : float
+        Exit relative mach number.
+
+    Returns
+    -------
+    float
+        Correction factor :math:`\mathrm{K_p}`.
+    float
+        Correction factor :math:`\mathrm{K_2}`.
+    float
+        Correction factor :math:`\mathrm{K_1}`.
+    """
+
+    K1 = 1 * (Ma_rel_out < 0.2) + (1 - 1.25 * (Ma_rel_out - 0.2)) * (
+        Ma_rel_out > 0.2 and Ma_rel_out < 1.00
+    )
+    K2 = (Ma_rel_in / Ma_rel_out) ** 2
+    Kp = 1 - K2 * (1 - K1)
+    Kp = max(0.1, Kp)
+    return [Kp, K2, K1]
+
+
+def get_hub_to_mean_mach_ratio(r_ht, cascade_type):
+    r"""
+    Compute the ratio between Mach at the hub and mean span at the inlet of the current cascade.
+
+    Due to radial variation in gas conditions, Mach at the hub will always be higher than at the mean.
+    Thus, shock losses at the hub could occur even when the Mach is subsonic at the mean blade span.
+
+    Parameters
+    ----------
+    r_ht : float
+        Hub to tip ratio at the inlet of the current cascade.
+    cascade_type : str
+        Type of the current cascade, either 'stator' or 'rotor'.
+
+    Returns
+    -------
+    float
+        Ratio between Mach at the hub and mean span at the inlet of the current cascade.
+
+    """
+
+    if r_ht < 0.5:
+        r_ht = 0.5  # Numerical trick to prevent extrapolation
+
+    r_ht_data = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
+
+    # Stator curve
+    f_data_S = [1.4, 1.18, 1.05, 1.0, 1.0, 1.0]
+
+    # Rotor curve
+    f_data_R = [2.15, 1.7, 1.35, 1.12, 1.0, 1.0]
+
+    if cascade_type == "stator":
+        f = np.interp(r_ht, r_ht_data, f_data_S)
+    elif cascade_type == "rotor":
+        f = np.interp(r_ht, r_ht_data, f_data_R)
+    else:
+        print("Specify the type of cascade")
+
+    return f
+
+
+def get_incidence_parameter(le, s, theta_in, theta_out, beta_in, beta_des):
+    r"""
+    Calculate the incidence parameter according to the correlation proposed by :cite:`moustapha_improved_1990`.
+
+    The incidence parameter is used to calculate the increment in profile losses due to the effect of incidence according to function `get_incidence_loss`.
+
+    The quantity is calculated as:
+
+    .. math::
+
+        \chi = \left(\frac{\mathrm{d_{le}}}{s}\right)^{-1.6}\left(\frac{\cos{\theta_\mathrm{in}}}{\cos{\theta_\mathrm{out}}}\right)^{-2}(\beta_\mathrm{in} - \beta_\mathrm{des})
+
+    where:
+
+        - :math:`\mathrm{d_{le}}` is the leading edge diameter.
+        - :math:`s` is the pitch.
+        - :math:`\theta_\mathrm{in}` and :math:`\theta_\mathrm{out}` is the blade metal angle at the inlet and outlet respectively.
+        - :math:`\beta_\mathrm{in}` and :math:`\beta_\mathrm{des}` is the inlet relative flow angle at given and design conditions respectively.
+
+    Parameters
+    ----------
+    le : float
+        Leading edge diameter.
+    s : float
+        Pitch.
+    theta_in : float
+        Leading edge metal angle (in degrees).
+    theta_out : float
+        Trailing edge metal angle (in degrees).
+    beta_in : float
+        Inlet relative flow angle (in degrees).
+    beta_des : float
+        Inlet relative flow angle at design condition (in degrees).
+
+    Returns
+    -------
+    float
+        Incidence parameter.
+    """
+
+    # TODO: explain smoothing/blending tricks
+
+    chi = (
+        (le / s) ** (-1.6)
+        * (math.cosd(theta_in) / math.cosd(theta_out)) ** (-2)
+        * (abs(beta_in) - abs(beta_des))
+    )
+    return chi
+
+
+def convert_kinetic_energy_coefficient(dPhi, gamma, Ma_rel_out):
+    r"""
+
+    Convert the kinetic energy coefficient increment due to incidence to the total pressure loss coefficient according to the following correlation:
+
+    .. math::
+
+        \mathrm{Y} = \frac{\left(1-\frac{\gamma -1}{2}\mathrm{Ma_{out}}^2(\frac{1}{(1-\Delta\phi^2_p)}-1)\right)^\frac{-\gamma}{\gamma - 1}-1}{1-\left(1 + \frac{\gamma - 1}{2}\mathrm{Ma_{out}}^2\right)^\frac{-\gamma}{\gamma - 1}}
+
+    where:
+
+        - :math:`\gamma` is the specific heat ratio.
+        - :math:`\mathrm{Ma_{out}}` is the cascade exit relative mach number.
+        - :math:`\Delta\phi^2_p` is the kinetic energy loss coefficient increment due to incidence.
+
+    Parameters
+    ----------
+    dPhi : float
+        Kinetic energy coefficient increment.
+    gamma : float
+        Heat capacity ratio.
+    Ma_rel_out : float
+        The cascade exit relative mach number.
+
+    Returns
+    -------
+    float
+        The total pressure loss coefficient.
+
+    Warnings
+    --------
+    This conversion assumes that the fluid is a perfect gas.
+
+    """
+
+    denom = 1 - (1 + (gamma - 1) / 2 * Ma_rel_out**2) ** (-gamma / (gamma - 1))
+    numer = (1 - (gamma - 1) / 2 * Ma_rel_out**2 * (1 / (1 - dPhi) - 1)) ** (
+        -gamma / (gamma - 1)
+    ) - 1
+
+    Y = numer / denom
+
+    return Y
```

### Comparing `turboflow-0.1.2/turboflow/axial_turbine/loss_model_kacker_okapuu.py` & `turboflow-0.1.3/turboflow/axial_turbine/loss_model_kacker_okapuu.py`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,608 +1,608 @@
-import numpy as np
-from .. import math
-
-
-def compute_losses(input_parameters):
-    r"""
-
-    Evaluate loss coefficient according to :cite:`kacker_mean_1982`.
-
-    This model split the total loss coefficient into profile, secondary, trailing edge and tip clearance.
-    The loss coefficient are combined through the following relation:
-
-    .. math::
-
-        \mathrm{Y_{tot}} = \mathrm{Y_{p}} + \mathrm{Y_{te}} + \mathrm{Y_{s}} + \mathrm{Y_{cl}}
-
-    The function calls a function for each loss component:
-
-        - :math:`\mathrm{Y_{p}}` : `get_profile_loss`
-        - :math:`\mathrm{Y_{te}}` : `get_trailing_edge_loss`
-        - :math:`\mathrm{Y_{s}}` : `get_secondary_loss`
-        - :math:`\mathrm{Y_{cl}}` : `get_tip_clearance_loss`
-
-    Parameters
-    ----------
-    input_parameters : dict
-        A dictionary containing containing the keys, `flow`, `geometry` and `options`.
-
-    Returns
-    -------
-    dict
-        A dictionary with the loss components.
-
-    """
-
-    # Load data
-    flow_parameters = input_parameters["flow"]
-    geometry = input_parameters["geometry"]
-
-    # Profile loss coefficient
-    Y_p = get_profile_loss(flow_parameters, geometry)
-
-    # Secondary loss coefficient
-    Y_s = get_secondary_loss(flow_parameters, geometry)
-
-    # Tip clearance loss coefficient
-    Y_cl = get_tip_clearance_loss(flow_parameters, geometry)
-
-    # Trailing edge loss coefficienct
-    Y_te = get_trailing_edge_loss(flow_parameters, geometry)
-
-    # Return a dictionary of loss components
-    losses = {
-        "loss_profile": Y_p,
-        "loss_incidence": 0.0,
-        "loss_trailing": Y_te,
-        "loss_secondary": Y_s,
-        "loss_clearance": Y_cl,
-        "loss_total": Y_p + Y_te + Y_s + Y_cl,
-    }
-
-    return losses
-
-
-def get_profile_loss(flow_parameters, geometry):
-    r"""
-    Calculate the profile loss coefficient for the current cascade using the Kacker and Okapuu loss model.
-    The equation for :math:`\mathrm{Y_p}` is given by:
-
-    .. math::
-
-        \mathrm{Y_p} = \mathrm{Y_{reaction}} - \left|\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right| \cdot
-        \left(\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right) \cdot (\mathrm{Y_{impulse}} - \mathrm{Y_{reaction}})
-
-    where:
-
-        - :math:`\mathrm{Y_{reaction}}` is the reaction loss coefficient computed using Aungier correlation.
-        - :math:`\mathrm{Y_{impulse}}` is the impulse loss coefficient computed using Aungier correlation.
-        - :math:`\theta_\mathrm{in}` is the inlet metal angle.
-        - :math:`\beta_\mathrm{out}` is the exit flow angle.
-
-    The function also applies various corrections based on flow parameters and geometry factors.
-
-    Parameters
-    ----------
-    flow_parameters : dict
-        Dictionary containing flow-related parameters.
-    geometry : dict
-        Dictionary with geometric parameters.
-
-    Returns
-    -------
-    float
-        Profile loss coefficient.
-
-    """
-
-    # TODO: explain smoothing/blending tricks
-
-    # Load data
-    Re = flow_parameters["Re_out"]
-    Ma_rel_out = flow_parameters["Ma_rel_out"]
-    Ma_rel_in = flow_parameters["Ma_rel_in"]
-    p0rel_in = flow_parameters["p0_rel_in"]
-    p_in = flow_parameters["p_in"]
-    p0rel_out = flow_parameters["p0_rel_out"]
-    p_out = flow_parameters["p_out"]
-    beta_out = flow_parameters["beta_out"]
-
-    r_ht_in = geometry["hub_tip_ratio_in"]
-    s = geometry["pitch"]
-    c = geometry["chord"]
-    theta_in = geometry["leading_edge_angle"]
-    t_max = geometry["maximum_thickness"]
-    cascade_type = geometry["cascade_type"]
-
-    # Reynolds number correction factor
-    f_Re = (
-        (Re / 2e5) ** (-0.4) * (Re < 2e5)
-        + 1 * (Re >= 2e5 and Re <= 1e6)
-        + (Re / 1e6) ** (-0.2) * (Re > 1e6)
-    )
-
-    # Mach number correction factor
-    f_Ma = 1 + 60 * (Ma_rel_out - 1) ** 2 * (
-        Ma_rel_out > 1
-    )  ## TODO smoothing / mach crit
-
-    # Compute losses related to shock effects at the inlet of the cascade
-    f_hub = get_hub_to_mean_mach_ratio(r_ht_in, cascade_type)
-    a = max(0, f_hub * Ma_rel_in - 0.4)  # TODO: smoothing
-    Y_shock = 0.75 * a**1.75 * r_ht_in * (p0rel_in - p_in) / (p0rel_out - p_out)
-    Y_shock = max(0, Y_shock)  # TODO: smoothing
-
-    # Compute compressible flow correction factors
-    Kp, K2, K1 = get_compressible_correction_factors(Ma_rel_in, Ma_rel_out)
-
-    # Yp_reaction and Yp_impulse according to Aungier correlation
-    # These formulas are valid for 40<abs(angle_out)<80
-    # Extrapolating outside of this limits might give completely wrong results
-    # If the optimization algorithm has upper and lower bounds for the outlet
-    # angle there is no need to worry about this problem
-    # angle_out_bis keeps the 40deg-losses for outlet angles lower than 40deg
-    angle_out_bis = max(abs(beta_out), 40)
-    Yp_reaction = nozzle_blades(s / c, angle_out_bis)
-    Yp_impulse = impulse_blades(s / c, angle_out_bis)
-
-    # Formula according to Kacker-Okapuu
-    Y_p = Yp_reaction - abs(theta_in / beta_out) * (theta_in / beta_out) * (
-        Yp_impulse - Yp_reaction
-    )
-
-    # Limit the extrapolation of the profile loss to avoid negative values for
-    # blade profiles with little deflection
-    # Low limit to 80% of the axial entry nozzle profile loss
-    # This value is completely arbitrary
-    Y_p = max(Y_p, 0.8 * Yp_reaction)  # TODO: smoothing
-
-    # Avoid unphysical effect on the thickness by defining the variable aa
-    aa = max(0, -theta_in / beta_out)  # TODO: smoothing
-    Y_p = Y_p * ((t_max / c) / 0.2) ** aa
-    Y_p = 0.914 * (2 / 3 * Y_p * Kp + Y_shock)
-
-    # Corrected profile loss coefficient
-    Y_p = f_Re * f_Ma * Y_p
-
-    return Y_p
-
-
-def get_secondary_loss(flow_parameters, geometry):
-    r"""
-    The function calculates the secondary loss coefficient using the Kacker-Okapuu model.
-    The main equation for :math:`\mathrm{Y_s}` is given by:
-
-    .. math::
-
-        \mathrm{Y_s} = 1.2 \cdot \mathrm{K_s} \cdot 0.0334 \cdot far \cdot Z \cdot \frac{\cos(\beta_{out})}{\cos(\theta_{in})}
-
-    where:
-
-        - :math:`K_s` is a correction factor accounting for compressible flow effects.
-        - :math:`far` is a factor that account for the aspect ratio of the current cascade.
-        - :math:`Z` is a blade loading parameter.
-        - :math:`\beta_\mathrm{out}` is the exit flow angle.
-        - :math:`\theta_\mathrm{in}` is the inlet metal angle.
-
-    The correction factor :math:`\mathrm{K_s}` from a separate correction factor given from `get_compressible_correction_factors`.
-    `get_compressible_correction_factors` returns :math:`\mathrm{K_s}`, which is used to calculate :math:`\mathrm{K_s}`:
-
-    .. math::
-
-        \mathrm{K_s} = 1 - \left(\frac{b}{H}\right)^2 (1-\mathrm{K_p})
-
-    where:
-
-        - :math:`b` is the axial chord.
-        - :math:`H` is the mean blade height.
-
-    The aspect ratio factor is calculated from the following correlation:
-
-    .. math:: 
-
-        far = \begin{cases}
-                1 - \frac{0.25 \sqrt{|2-H/c|}}{H/c} & \text{if } H/c < 2.0 \\
-                c/H & \text{if } H/c \geq 2.0
-                \end{cases}
-
-    where :math:`c` is the blade chord.
-
-    The loading parameters is caclualated from the following correlation:
-
-    .. math::
-
-        Z = 4 (\tan(\beta_\mathrm{in}) - \tan(\beta_\mathrm{out}))^2\frac{\cos^2(\beta_\mathrm{out})}{\cos(\beta_m)} 
-
-    where:
-
-        - :math:`\beta_\mathrm{in}` and :math:`\beta_\mathrm{out}` is the inlet and exit relative flow angle
-        - :math:`\beta_m = \tan^{-1}(0.5(\tan(\beta_\mathrm{in})-\tan(\beta_\mathrm{out})))` is the mean gas angle.
-           
-    Parameters
-    ----------
-    flow_parameters : dict
-        Dictionary containing flow-related parameters.
-
-    geometry : dict
-        Dictionary with geometric parameters.
-
-    Returns
-    -------
-    float
-        Secondary loss coefficient.
-
-    """
-
-    Ma_rel_out = flow_parameters["Ma_rel_out"]
-    Ma_rel_in = flow_parameters["Ma_rel_in"]
-    beta_out = flow_parameters["beta_out"]
-    beta_in = flow_parameters["beta_in"]
-
-    b = geometry["axial_chord"]
-    H = geometry["height"]
-    c = geometry["chord"]
-    theta_in = geometry["leading_edge_angle"]
-
-    # Compute compressible flow correction factors
-    Kp, K2, K1 = get_compressible_correction_factors(Ma_rel_in, Ma_rel_out)
-
-    # Secondary loss coefficient
-    K3 = (b / H) ** 2
-    Ks = 1 - K3 * (1 - Kp)
-    Ks = max(0.1, Ks)
-    angle_m = math.arctand((math.tand(beta_in) + math.tand(beta_out)) / 2)
-    Z = (
-        4
-        * (math.tand(beta_in) - math.tand(beta_out)) ** 2
-        * math.cosd(beta_out) ** 2
-        / math.cosd(angle_m)
-    )
-    far = (1 - 0.25 * np.sqrt(abs(2 - H / c))) / (H / c) * (H / c < 2) + 1 / (H / c) * (
-        H / c >= 2
-    )
-    Y_s = 1.2 * Ks * 0.0334 * far * Z * math.cosd(beta_out) / math.cosd(theta_in)
-
-    return Y_s
-
-
-def get_trailing_edge_loss(flow_parameters, geometry):
-    r"""
-    Calculate the trailing edge loss coefficient using the Kacker-Okapuu model.
-    The main equation for the kinetic-energy coefficient is given by:
-
-    .. math::
-
-        d_{\phi^2} = d_{\phi^2_\mathrm{reaction}} - \left|\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right| \cdot
-        \left(\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right) \cdot (d_{\phi^2_\mathrm{impulse}} - d_{\phi^2_\mathrm{reaction}})
-
-    The kinetic-energy coefficient is converted to the total pressure loss coefficient by:
-
-    .. math::
-
-        \mathrm{Y_{te}} = \frac{1}{{1 - \phi^2}} - 1
-
-    where:
-
-        - :math:`d_{\phi^2_\mathrm{reaction}}` and :math:`d_{\phi^2_\mathrm{impulse}}` are coefficients related to kinetic energy loss for reaction and impulse blades respectively, and are interpolated based on trailing edge to throat opening ratio.
-        - :math:`\beta_\mathrm{out}` is the exit flow angle.
-        - :math:`\theta_\mathrm{in}` is the inlet metal angle.
-
-    The function also applies various interpolations and computations based on flow parameters and geometry factors.
-
-    Parameters
-    ----------
-    flow_parameters : dict
-        Dictionary containing flow-related parameters.
-    geometry : dict
-        Dictionary with geometric parameters.
-
-    Returns
-    -------
-    float
-        Trailing edge loss coefficient.
-
-    """
-
-    t_te = geometry["trailing_edge_thickness"]
-    o = geometry["opening"]
-    angle_in = geometry["leading_edge_angle"]
-    angle_out = flow_parameters["beta_out"]
-
-    # Range of trailing edge to throat opening ratio
-    r_to_data = [0, 0.2, 0.4]
-
-    # Reacting blading
-    phi_data_reaction = [0, 0.045, 0.15]
-
-    # Impulse blading
-    phi_data_impulse = [0, 0.025, 0.075]
-
-    # Numerical trick to avoid too big r_to's
-    r_to = min(0.4, t_te / o)  # TODO: smoothing
-
-    # Interpolate data
-    d_phi2_reaction = np.interp(r_to, r_to_data, phi_data_reaction)
-    d_phi2_impulse = np.interp(r_to, r_to_data, phi_data_impulse)
-
-    # Compute kinetic energy loss coefficient
-    d_phi2 = d_phi2_reaction - abs(angle_in / angle_out) * (angle_in / angle_out) * (
-        d_phi2_impulse - d_phi2_reaction
-    )
-
-    # Limit the extrapolation of the trailing edge loss
-    d_phi2 = max(d_phi2, d_phi2_impulse / 2)  # TODO: smoothing
-    Y_te = 1 / (1 - d_phi2) - 1
-
-    return Y_te
-
-
-def get_tip_clearance_loss(flow_parameters, geometry):
-    r"""
-    Calculate the tip clearance loss coefficient for the current cascade using the Kacker and Okapuu loss model.
-    The equation for the tip clearance loss coefficent is given by:
-
-    .. math::
-
-        \mathrm{Y_{cl}} = B \cdot Z \cdot \frac{c}{H} \cdot \left(\frac{t_\mathrm{cl}}{H}\right)^{0.78}
-
-    where:
-
-        - :math:`B` is an empirical parameter that depends on the type of cascade (0 for stator, 0.37 for shrouded rotor).
-        - :math:`Z` is a blade loading parameter
-        - :math:`c` is the chord.
-        - :math:`H` is the mean blade height.
-        - :math:`t_\mathrm{cl}` is the tip clearance.
-
-
-    Parameters
-    ----------
-    flow_parameters : dict
-        Dictionary containing flow-related parameters.
-    geometry : dict
-        Dictionary with geometric parameters.
-
-    Returns
-    -------
-    float
-        Tip clearance loss coefficient.
-    """
-
-    beta_out = flow_parameters["beta_out"]
-    beta_in = flow_parameters["beta_in"]
-
-    H = geometry["height"]
-    c = geometry["chord"]
-    t_cl = geometry["tip_clearance"]
-    cascade_type = geometry["cascade_type"]
-
-    # Calculate blade loading parameter Z
-    angle_m = math.arctand((math.tand(beta_in) + math.tand(beta_out)) / 2)
-    Z = (
-        4
-        * (math.tand(beta_in) - math.tand(beta_out)) ** 2
-        * math.cosd(beta_out) ** 2
-        / math.cosd(angle_m)
-    )
-
-    # Empirical parameter (0 for stator, 0.37 for shrouded rotor)
-    if cascade_type == "stator":
-        B = 0
-    elif cascade_type == "rotor":
-        B = 0.37
-    else:
-        print("Specify the type of cascade")
-
-    # Tip clearance loss coefficient
-    Y_cl = B * Z * c / H * (t_cl / H) ** 0.78
-
-    return Y_cl
-
-
-def nozzle_blades(r_sc, angle_out):
-    r"""
-    Use Aungier correlation to compute the pressure loss coefficient for nozzle blades :cite:`aungier_turbine_2006`.
-
-    This correlation is a formula that reproduces the figures from the Ainley and Mathieson original figures :cite:`ainley_method_1951`,
-    and is a function of the pitch-to-chord ratio and exit relative flow angle. 
-
-    The correlation uses the following equations:
-
-    .. math::
-
-        & \beta_\mathrm{tan} = 90 - \beta_\mathrm{ax} \\
-        & \left(\frac{s}{c}\right)_\mathrm{min} = \begin{cases}
-                                                    0.46 + \frac{\beta_\mathrm{tan}}{77} && \text{if } \beta_\mathrm{tan} < 30 \\
-                                                    0.614 + \frac{\beta_\mathrm{tan}}{130} && \text{if } \beta_\mathrm{tan} \geq 30
-                                                \end{cases} \\
-        & X = \left(\frac{s}{c}\right) - \left(\frac{s}{c}\right)_\mathrm{min} \\
-        & A = \begin{cases}
-                0.025 + \frac{27 - \beta_\mathrm{tan}}{530} && \text{if } \beta_\mathrm{tan} < 27 \\
-                0.025 + \frac{27 - \beta_\mathrm{tan}}{3085} && \text{if } \beta_\mathrm{tan} \geq 27
-              \end{cases} \\
-        & B = 0.1583 - \frac{\beta_\mathrm{tan}}{1640} \\
-        & C = 0.08\left(\frac{\beta_\mathrm{tan}}{30}\right)^2 -1 \\
-        & n = 1 + \frac{\beta_\mathrm{tan}}{30} \\
-        & \mathrm{Y_{p,reaction}} = \begin{cases}
-                A + BX^2 + CX^3 && \text{if } \beta_\mathrm{tan} < 30 \\
-                A + B |X|^n && \text{if } \beta_\mathrm{tan} \geq 30
-              \end{cases}   
-
-    where:
-
-        - :math:`s` is the pitch
-        - :math:`c` is the chord
-        - :math:`\beta_\mathrm{tan}` and :math:`\beta_\mathrm{ax}` is the exit relative flow angle with respect to tangential and axial direction. 
-        
-    Parameters
-    ----------
-    r_sc : float
-        Pitch-to-chord ratio.
-    angle_out : float
-        Exit relative flow angle (in degrees).
-
-    Returns
-    -------
-    float
-        Pressure loss coefficient for impulse blades.
-
-    """
-
-    phi = 90 - angle_out
-    r_sc_min = (0.46 + phi / 77) * (phi < 30) + (0.614 + phi / 130) * (phi >= 30)
-    X = r_sc - r_sc_min
-    A = (0.025 + (27 - phi) / 530) * (phi < 27) + (0.025 + (27 - phi) / 3085) * (
-        phi >= 27
-    )
-    B = 0.1583 - phi / 1640
-    C = 0.08 * ((phi / 30) ** 2 - 1)
-    n = 1 + phi / 30
-    Yp_reaction = (A + B * X**2 + C * X**3) * (phi < 30) + (A + B * abs(X) ** n) * (
-        phi >= 30
-    )
-
-    return Yp_reaction
-
-
-def impulse_blades(r_sc, angle_out):
-    r"""
-    Use Aungier correlation to compute the pressure loss coefficient for impulse blades :cite:`aungier_turbine_2006`.
-
-    This correlation is a formula that reproduces the figures from the Ainley and Mathieson original figures :cite:`ainley_method_1951`,
-    and is a function of the pitch-to-chord ratio and exit relative flow angle. 
-
-    The correlation uses the following equations:
-
-    .. math::
-
-        & \beta_\mathrm{tan} = 90 - \beta_\mathrm{ax} \\
-        & \left(\frac{s}{c}\right)_\mathrm{min} = 0.224 + 1.575\left(\frac{\beta_\mathrm{tan}}{90}\right) - \left(\frac{\beta_\mathrm{tan}}{90}\right)^2 \\
-        & X = \left(\frac{s}{c}\right) - \left(\frac{s}{c}\right)_\mathrm{min} \\
-        & A = 0.242 - \frac{\beta_\mathrm{tan}}{151} + \left(\frac{\beta_\mathrm{tan}}{127}\right)^2 \\
-        & B = \begin{cases}
-                0.3 + \frac{30 - \beta_\mathrm{tan}}{50} && \text{if } \beta_\mathrm{tan} < 30 \\
-                0.3 + \frac{30 - \beta_\mathrm{tan}}{275} && \text{if } \beta_\mathrm{tan} \geq 30
-              \end{cases} \\
-        & C = 0.88 - \frac{\beta_\mathrm{tan}}{42.4} + \left(\frac{\beta_\mathrm{tan}}{72.8}\right)^2 \\
-        & \mathrm{Y_{p,impulse}} = A + BX^2 - CX^3 
-
-    where:
-
-        - :math:`s` is the pitch
-        - :math:`c` is the chord
-        - :math:`\beta_\mathrm{tan}` and :math:`\beta_\mathrm{ax}` is the exit relative flow angle with respect to tangential and axial direction. 
-        
-    Parameters
-    ----------
-    r_sc : float
-        Pitch-to-chord ratio.
-    angle_out : float
-        Exit relative flow angle (in degrees).
-
-    Returns
-    -------
-    float
-        Pressure loss coefficient for impulse blades.
-
-    """
-
-    phi = 90 - angle_out
-    r_sc_min = 0.224 + 1.575 * (phi / 90) - (phi / 90) ** 2
-    X = r_sc - r_sc_min
-    A = 0.242 - phi / 151 + (phi / 127) ** 2
-    B = (0.3 + (30 - phi) / 50) * (phi < 30) + (0.3 + (30 - phi) / 275) * (phi >= 30)
-    C = 0.88 - phi / 42.4 + (phi / 72.8) ** 2
-    Yp_impulse = A + B * X**2 - C * X**3
-
-    return Yp_impulse
-
-
-def get_compressible_correction_factors(Ma_rel_in, Ma_rel_out):
-    r"""
-
-    Compute compressible flow correction factor according to Kacker and Okapuu loss model :cite:`kacker_mean_1982`.
-
-    The correction factors :math:`\mathrm{K_1}`, :math:`\mathrm{K_2}` and :math:`\mathrm{K_p}` was introduced by :cite:`kacker_mean_1982` to correct previous correlation (:cite:`ainley_method_1951`)
-    for effect of higher mach number and channel acceleration. The correction factors reduces the losses at higher mach number. Their definition follows: 
-
-    .. math::
-
-        &\mathrm{K_1} = \begin{cases}
-                1 & \text{if } \mathrm{Ma_{out}} < 0.2 \\
-                1 - 1.25(\mathrm{Ma_{out}} - 0.2) & \text{if } \mathrm{Ma_{out}} \geq 0.2
-              \end{cases} \\
-        &\mathrm{K_2} = \left(\frac{\mathrm{Ma_{in}}}{\mathrm{Ma_{out}}}\right) \\
-        &\mathrm{K_p} = 1 - \mathrm{K_2}(1-\mathrm{K_1})
-
-    where:
-
-        - :math:`\mathrm{Ma_{in}}` is the relative mach number at the cascade inlet.
-        -  :math:`\mathrm{Ma_{out}}` is the relative mach number at the cascade exit.
-    
-    Parameters
-    ----------
-    Ma_rel_in : float
-        Inlet relative mach number.
-    Ma_rel_out : float
-        Exit relative mach number.
-
-    Returns
-    -------
-    float
-        Correction factor :math:`\mathrm{K_p}`.
-    float
-        Correction factor :math:`\mathrm{K_2}`.
-    float
-        Correction factor :math:`\mathrm{K_1}`.
-    """
-
-    # TODO: explain smoothing/blending tricks
-
-    K1 = 1 * (Ma_rel_out < 0.2) + (1 - 1.25 * (Ma_rel_out - 0.2)) * (
-        Ma_rel_out > 0.2 and Ma_rel_out < 1.00
-    )  # TODO: this can be converted to a smooth piecewise function (sigmoid blending)
-    K2 = (Ma_rel_in / Ma_rel_out) ** 2
-    Kp = 1 - K2 * (1 - K1)
-    Kp = max(0.1, Kp)  # TODO: smoothing
-    return [Kp, K2, K1]
-
-
-def get_hub_to_mean_mach_ratio(r_ht, cascade_type):
-    r"""
-    Compute the ratio between Mach at the hub and mean span at the inlet of the current cascade.
-
-    Due to radial variation in gas conditions, Mach at the hub will always be higher than at the mean.
-    Thus, shock losses at the hub could occur even when the Mach is subsonic at the mean blade span.
-
-    Parameters
-    ----------
-    r_ht : float
-        Hub to tip ratio at the inlet of the current cascade.
-    cascade_type : str
-        Type of the current cascade, either 'stator' or 'rotor'.
-
-    Returns
-    -------
-    float
-        Ratio between Mach at the hub and mean span at the inlet of the current cascade.
-
-    """
-
-    if r_ht < 0.5:  # TODO: add smoothing, this is essentially a max(r_ht, 0.5)
-        r_ht = 0.5  # Numerical trick to prevent extrapolation
-
-    r_ht_data = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
-
-    # Stator curve
-    f_data_S = [1.4, 1.18, 1.05, 1.0, 1.0, 1.0]
-
-    # Rotor curve
-    f_data_R = [2.15, 1.7, 1.35, 1.12, 1.0, 1.0]
-
-    if cascade_type == "stator":
-        f = np.interp(r_ht, r_ht_data, f_data_S)
-    elif cascade_type == "rotor":
-        f = np.interp(r_ht, r_ht_data, f_data_R)
-    else:
-        print("Specify the type of cascade")
-
-    return f
+import numpy as np
+from .. import math
+
+
+def compute_losses(input_parameters):
+    r"""
+
+    Evaluate loss coefficient according to :cite:`kacker_mean_1982`.
+
+    This model split the total loss coefficient into profile, secondary, trailing edge and tip clearance.
+    The loss coefficient are combined through the following relation:
+
+    .. math::
+
+        \mathrm{Y_{tot}} = \mathrm{Y_{p}} + \mathrm{Y_{te}} + \mathrm{Y_{s}} + \mathrm{Y_{cl}}
+
+    The function calls a function for each loss component:
+
+        - :math:`\mathrm{Y_{p}}` : `get_profile_loss`
+        - :math:`\mathrm{Y_{te}}` : `get_trailing_edge_loss`
+        - :math:`\mathrm{Y_{s}}` : `get_secondary_loss`
+        - :math:`\mathrm{Y_{cl}}` : `get_tip_clearance_loss`
+
+    Parameters
+    ----------
+    input_parameters : dict
+        A dictionary containing containing the keys, `flow`, `geometry` and `options`.
+
+    Returns
+    -------
+    dict
+        A dictionary with the loss components.
+
+    """
+
+    # Load data
+    flow_parameters = input_parameters["flow"]
+    geometry = input_parameters["geometry"]
+
+    # Profile loss coefficient
+    Y_p = get_profile_loss(flow_parameters, geometry)
+
+    # Secondary loss coefficient
+    Y_s = get_secondary_loss(flow_parameters, geometry)
+
+    # Tip clearance loss coefficient
+    Y_cl = get_tip_clearance_loss(flow_parameters, geometry)
+
+    # Trailing edge loss coefficienct
+    Y_te = get_trailing_edge_loss(flow_parameters, geometry)
+
+    # Return a dictionary of loss components
+    losses = {
+        "loss_profile": Y_p,
+        "loss_incidence": 0.0,
+        "loss_trailing": Y_te,
+        "loss_secondary": Y_s,
+        "loss_clearance": Y_cl,
+        "loss_total": Y_p + Y_te + Y_s + Y_cl,
+    }
+
+    return losses
+
+
+def get_profile_loss(flow_parameters, geometry):
+    r"""
+    Calculate the profile loss coefficient for the current cascade using the Kacker and Okapuu loss model.
+    The equation for :math:`\mathrm{Y_p}` is given by:
+
+    .. math::
+
+        \mathrm{Y_p} = \mathrm{Y_{reaction}} - \left|\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right| \cdot
+        \left(\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right) \cdot (\mathrm{Y_{impulse}} - \mathrm{Y_{reaction}})
+
+    where:
+
+        - :math:`\mathrm{Y_{reaction}}` is the reaction loss coefficient computed using Aungier correlation.
+        - :math:`\mathrm{Y_{impulse}}` is the impulse loss coefficient computed using Aungier correlation.
+        - :math:`\theta_\mathrm{in}` is the inlet metal angle.
+        - :math:`\beta_\mathrm{out}` is the exit flow angle.
+
+    The function also applies various corrections based on flow parameters and geometry factors.
+
+    Parameters
+    ----------
+    flow_parameters : dict
+        Dictionary containing flow-related parameters.
+    geometry : dict
+        Dictionary with geometric parameters.
+
+    Returns
+    -------
+    float
+        Profile loss coefficient.
+
+    """
+
+    # TODO: explain smoothing/blending tricks
+
+    # Load data
+    Re = flow_parameters["Re_out"]
+    Ma_rel_out = flow_parameters["Ma_rel_out"]
+    Ma_rel_in = flow_parameters["Ma_rel_in"]
+    p0rel_in = flow_parameters["p0_rel_in"]
+    p_in = flow_parameters["p_in"]
+    p0rel_out = flow_parameters["p0_rel_out"]
+    p_out = flow_parameters["p_out"]
+    beta_out = flow_parameters["beta_out"]
+
+    r_ht_in = geometry["hub_tip_ratio_in"]
+    s = geometry["pitch"]
+    c = geometry["chord"]
+    theta_in = geometry["leading_edge_angle"]
+    t_max = geometry["maximum_thickness"]
+    cascade_type = geometry["cascade_type"]
+
+    # Reynolds number correction factor
+    f_Re = (
+        (Re / 2e5) ** (-0.4) * (Re < 2e5)
+        + 1 * (Re >= 2e5 and Re <= 1e6)
+        + (Re / 1e6) ** (-0.2) * (Re > 1e6)
+    )
+
+    # Mach number correction factor
+    f_Ma = 1 + 60 * (Ma_rel_out - 1) ** 2 * (
+        Ma_rel_out > 1
+    )  ## TODO smoothing / mach crit
+
+    # Compute losses related to shock effects at the inlet of the cascade
+    f_hub = get_hub_to_mean_mach_ratio(r_ht_in, cascade_type)
+    a = max(0, f_hub * Ma_rel_in - 0.4)  # TODO: smoothing
+    Y_shock = 0.75 * a**1.75 * r_ht_in * (p0rel_in - p_in) / (p0rel_out - p_out)
+    Y_shock = max(0, Y_shock)  # TODO: smoothing
+
+    # Compute compressible flow correction factors
+    Kp, K2, K1 = get_compressible_correction_factors(Ma_rel_in, Ma_rel_out)
+
+    # Yp_reaction and Yp_impulse according to Aungier correlation
+    # These formulas are valid for 40<abs(angle_out)<80
+    # Extrapolating outside of this limits might give completely wrong results
+    # If the optimization algorithm has upper and lower bounds for the outlet
+    # angle there is no need to worry about this problem
+    # angle_out_bis keeps the 40deg-losses for outlet angles lower than 40deg
+    angle_out_bis = max(abs(beta_out), 40)
+    Yp_reaction = nozzle_blades(s / c, angle_out_bis)
+    Yp_impulse = impulse_blades(s / c, angle_out_bis)
+
+    # Formula according to Kacker-Okapuu
+    Y_p = Yp_reaction - abs(theta_in / beta_out) * (theta_in / beta_out) * (
+        Yp_impulse - Yp_reaction
+    )
+
+    # Limit the extrapolation of the profile loss to avoid negative values for
+    # blade profiles with little deflection
+    # Low limit to 80% of the axial entry nozzle profile loss
+    # This value is completely arbitrary
+    Y_p = max(Y_p, 0.8 * Yp_reaction)  # TODO: smoothing
+
+    # Avoid unphysical effect on the thickness by defining the variable aa
+    aa = max(0, -theta_in / beta_out)  # TODO: smoothing
+    Y_p = Y_p * ((t_max / c) / 0.2) ** aa
+    Y_p = 0.914 * (2 / 3 * Y_p * Kp + Y_shock)
+
+    # Corrected profile loss coefficient
+    Y_p = f_Re * f_Ma * Y_p
+
+    return Y_p
+
+
+def get_secondary_loss(flow_parameters, geometry):
+    r"""
+    The function calculates the secondary loss coefficient using the Kacker-Okapuu model.
+    The main equation for :math:`\mathrm{Y_s}` is given by:
+
+    .. math::
+
+        \mathrm{Y_s} = 1.2 \cdot \mathrm{K_s} \cdot 0.0334 \cdot far \cdot Z \cdot \frac{\cos(\beta_{out})}{\cos(\theta_{in})}
+
+    where:
+
+        - :math:`K_s` is a correction factor accounting for compressible flow effects.
+        - :math:`far` is a factor that account for the aspect ratio of the current cascade.
+        - :math:`Z` is a blade loading parameter.
+        - :math:`\beta_\mathrm{out}` is the exit flow angle.
+        - :math:`\theta_\mathrm{in}` is the inlet metal angle.
+
+    The correction factor :math:`\mathrm{K_s}` from a separate correction factor given from `get_compressible_correction_factors`.
+    `get_compressible_correction_factors` returns :math:`\mathrm{K_s}`, which is used to calculate :math:`\mathrm{K_s}`:
+
+    .. math::
+
+        \mathrm{K_s} = 1 - \left(\frac{b}{H}\right)^2 (1-\mathrm{K_p})
+
+    where:
+
+        - :math:`b` is the axial chord.
+        - :math:`H` is the mean blade height.
+
+    The aspect ratio factor is calculated from the following correlation:
+
+    .. math:: 
+
+        far = \begin{cases}
+                1 - \frac{0.25 \sqrt{|2-H/c|}}{H/c} & \text{if } H/c < 2.0 \\
+                c/H & \text{if } H/c \geq 2.0
+                \end{cases}
+
+    where :math:`c` is the blade chord.
+
+    The loading parameters is caclualated from the following correlation:
+
+    .. math::
+
+        Z = 4 (\tan(\beta_\mathrm{in}) - \tan(\beta_\mathrm{out}))^2\frac{\cos^2(\beta_\mathrm{out})}{\cos(\beta_m)} 
+
+    where:
+
+        - :math:`\beta_\mathrm{in}` and :math:`\beta_\mathrm{out}` is the inlet and exit relative flow angle
+        - :math:`\beta_m = \tan^{-1}(0.5(\tan(\beta_\mathrm{in})-\tan(\beta_\mathrm{out})))` is the mean gas angle.
+           
+    Parameters
+    ----------
+    flow_parameters : dict
+        Dictionary containing flow-related parameters.
+
+    geometry : dict
+        Dictionary with geometric parameters.
+
+    Returns
+    -------
+    float
+        Secondary loss coefficient.
+
+    """
+
+    Ma_rel_out = flow_parameters["Ma_rel_out"]
+    Ma_rel_in = flow_parameters["Ma_rel_in"]
+    beta_out = flow_parameters["beta_out"]
+    beta_in = flow_parameters["beta_in"]
+
+    b = geometry["axial_chord"]
+    H = geometry["height"]
+    c = geometry["chord"]
+    theta_in = geometry["leading_edge_angle"]
+
+    # Compute compressible flow correction factors
+    Kp, K2, K1 = get_compressible_correction_factors(Ma_rel_in, Ma_rel_out)
+
+    # Secondary loss coefficient
+    K3 = (b / H) ** 2
+    Ks = 1 - K3 * (1 - Kp)
+    Ks = max(0.1, Ks)
+    angle_m = math.arctand((math.tand(beta_in) + math.tand(beta_out)) / 2)
+    Z = (
+        4
+        * (math.tand(beta_in) - math.tand(beta_out)) ** 2
+        * math.cosd(beta_out) ** 2
+        / math.cosd(angle_m)
+    )
+    far = (1 - 0.25 * np.sqrt(abs(2 - H / c))) / (H / c) * (H / c < 2) + 1 / (H / c) * (
+        H / c >= 2
+    )
+    Y_s = 1.2 * Ks * 0.0334 * far * Z * math.cosd(beta_out) / math.cosd(theta_in)
+
+    return Y_s
+
+
+def get_trailing_edge_loss(flow_parameters, geometry):
+    r"""
+    Calculate the trailing edge loss coefficient using the Kacker-Okapuu model.
+    The main equation for the kinetic-energy coefficient is given by:
+
+    .. math::
+
+        d_{\phi^2} = d_{\phi^2_\mathrm{reaction}} - \left|\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right| \cdot
+        \left(\frac{\theta_\mathrm{in}}{\beta_\mathrm{out}}\right) \cdot (d_{\phi^2_\mathrm{impulse}} - d_{\phi^2_\mathrm{reaction}})
+
+    The kinetic-energy coefficient is converted to the total pressure loss coefficient by:
+
+    .. math::
+
+        \mathrm{Y_{te}} = \frac{1}{{1 - \phi^2}} - 1
+
+    where:
+
+        - :math:`d_{\phi^2_\mathrm{reaction}}` and :math:`d_{\phi^2_\mathrm{impulse}}` are coefficients related to kinetic energy loss for reaction and impulse blades respectively, and are interpolated based on trailing edge to throat opening ratio.
+        - :math:`\beta_\mathrm{out}` is the exit flow angle.
+        - :math:`\theta_\mathrm{in}` is the inlet metal angle.
+
+    The function also applies various interpolations and computations based on flow parameters and geometry factors.
+
+    Parameters
+    ----------
+    flow_parameters : dict
+        Dictionary containing flow-related parameters.
+    geometry : dict
+        Dictionary with geometric parameters.
+
+    Returns
+    -------
+    float
+        Trailing edge loss coefficient.
+
+    """
+
+    t_te = geometry["trailing_edge_thickness"]
+    o = geometry["opening"]
+    angle_in = geometry["leading_edge_angle"]
+    angle_out = flow_parameters["beta_out"]
+
+    # Range of trailing edge to throat opening ratio
+    r_to_data = [0, 0.2, 0.4]
+
+    # Reacting blading
+    phi_data_reaction = [0, 0.045, 0.15]
+
+    # Impulse blading
+    phi_data_impulse = [0, 0.025, 0.075]
+
+    # Numerical trick to avoid too big r_to's
+    r_to = min(0.4, t_te / o)  # TODO: smoothing
+
+    # Interpolate data
+    d_phi2_reaction = np.interp(r_to, r_to_data, phi_data_reaction)
+    d_phi2_impulse = np.interp(r_to, r_to_data, phi_data_impulse)
+
+    # Compute kinetic energy loss coefficient
+    d_phi2 = d_phi2_reaction - abs(angle_in / angle_out) * (angle_in / angle_out) * (
+        d_phi2_impulse - d_phi2_reaction
+    )
+
+    # Limit the extrapolation of the trailing edge loss
+    d_phi2 = max(d_phi2, d_phi2_impulse / 2)  # TODO: smoothing
+    Y_te = 1 / (1 - d_phi2) - 1
+
+    return Y_te
+
+
+def get_tip_clearance_loss(flow_parameters, geometry):
+    r"""
+    Calculate the tip clearance loss coefficient for the current cascade using the Kacker and Okapuu loss model.
+    The equation for the tip clearance loss coefficent is given by:
+
+    .. math::
+
+        \mathrm{Y_{cl}} = B \cdot Z \cdot \frac{c}{H} \cdot \left(\frac{t_\mathrm{cl}}{H}\right)^{0.78}
+
+    where:
+
+        - :math:`B` is an empirical parameter that depends on the type of cascade (0 for stator, 0.37 for shrouded rotor).
+        - :math:`Z` is a blade loading parameter
+        - :math:`c` is the chord.
+        - :math:`H` is the mean blade height.
+        - :math:`t_\mathrm{cl}` is the tip clearance.
+
+
+    Parameters
+    ----------
+    flow_parameters : dict
+        Dictionary containing flow-related parameters.
+    geometry : dict
+        Dictionary with geometric parameters.
+
+    Returns
+    -------
+    float
+        Tip clearance loss coefficient.
+    """
+
+    beta_out = flow_parameters["beta_out"]
+    beta_in = flow_parameters["beta_in"]
+
+    H = geometry["height"]
+    c = geometry["chord"]
+    t_cl = geometry["tip_clearance"]
+    cascade_type = geometry["cascade_type"]
+
+    # Calculate blade loading parameter Z
+    angle_m = math.arctand((math.tand(beta_in) + math.tand(beta_out)) / 2)
+    Z = (
+        4
+        * (math.tand(beta_in) - math.tand(beta_out)) ** 2
+        * math.cosd(beta_out) ** 2
+        / math.cosd(angle_m)
+    )
+
+    # Empirical parameter (0 for stator, 0.37 for shrouded rotor)
+    if cascade_type == "stator":
+        B = 0
+    elif cascade_type == "rotor":
+        B = 0.37
+    else:
+        print("Specify the type of cascade")
+
+    # Tip clearance loss coefficient
+    Y_cl = B * Z * c / H * (t_cl / H) ** 0.78
+
+    return Y_cl
+
+
+def nozzle_blades(r_sc, angle_out):
+    r"""
+    Use Aungier correlation to compute the pressure loss coefficient for nozzle blades :cite:`aungier_turbine_2006`.
+
+    This correlation is a formula that reproduces the figures from the Ainley and Mathieson original figures :cite:`ainley_method_1951`,
+    and is a function of the pitch-to-chord ratio and exit relative flow angle. 
+
+    The correlation uses the following equations:
+
+    .. math::
+
+        & \beta_\mathrm{tan} = 90 - \beta_\mathrm{ax} \\
+        & \left(\frac{s}{c}\right)_\mathrm{min} = \begin{cases}
+                                                    0.46 + \frac{\beta_\mathrm{tan}}{77} && \text{if } \beta_\mathrm{tan} < 30 \\
+                                                    0.614 + \frac{\beta_\mathrm{tan}}{130} && \text{if } \beta_\mathrm{tan} \geq 30
+                                                \end{cases} \\
+        & X = \left(\frac{s}{c}\right) - \left(\frac{s}{c}\right)_\mathrm{min} \\
+        & A = \begin{cases}
+                0.025 + \frac{27 - \beta_\mathrm{tan}}{530} && \text{if } \beta_\mathrm{tan} < 27 \\
+                0.025 + \frac{27 - \beta_\mathrm{tan}}{3085} && \text{if } \beta_\mathrm{tan} \geq 27
+              \end{cases} \\
+        & B = 0.1583 - \frac{\beta_\mathrm{tan}}{1640} \\
+        & C = 0.08\left(\frac{\beta_\mathrm{tan}}{30}\right)^2 -1 \\
+        & n = 1 + \frac{\beta_\mathrm{tan}}{30} \\
+        & \mathrm{Y_{p,reaction}} = \begin{cases}
+                A + BX^2 + CX^3 && \text{if } \beta_\mathrm{tan} < 30 \\
+                A + B |X|^n && \text{if } \beta_\mathrm{tan} \geq 30
+              \end{cases}   
+
+    where:
+
+        - :math:`s` is the pitch
+        - :math:`c` is the chord
+        - :math:`\beta_\mathrm{tan}` and :math:`\beta_\mathrm{ax}` is the exit relative flow angle with respect to tangential and axial direction. 
+        
+    Parameters
+    ----------
+    r_sc : float
+        Pitch-to-chord ratio.
+    angle_out : float
+        Exit relative flow angle (in degrees).
+
+    Returns
+    -------
+    float
+        Pressure loss coefficient for impulse blades.
+
+    """
+
+    phi = 90 - angle_out
+    r_sc_min = (0.46 + phi / 77) * (phi < 30) + (0.614 + phi / 130) * (phi >= 30)
+    X = r_sc - r_sc_min
+    A = (0.025 + (27 - phi) / 530) * (phi < 27) + (0.025 + (27 - phi) / 3085) * (
+        phi >= 27
+    )
+    B = 0.1583 - phi / 1640
+    C = 0.08 * ((phi / 30) ** 2 - 1)
+    n = 1 + phi / 30
+    Yp_reaction = (A + B * X**2 + C * X**3) * (phi < 30) + (A + B * abs(X) ** n) * (
+        phi >= 30
+    )
+
+    return Yp_reaction
+
+
+def impulse_blades(r_sc, angle_out):
+    r"""
+    Use Aungier correlation to compute the pressure loss coefficient for impulse blades :cite:`aungier_turbine_2006`.
+
+    This correlation is a formula that reproduces the figures from the Ainley and Mathieson original figures :cite:`ainley_method_1951`,
+    and is a function of the pitch-to-chord ratio and exit relative flow angle. 
+
+    The correlation uses the following equations:
+
+    .. math::
+
+        & \beta_\mathrm{tan} = 90 - \beta_\mathrm{ax} \\
+        & \left(\frac{s}{c}\right)_\mathrm{min} = 0.224 + 1.575\left(\frac{\beta_\mathrm{tan}}{90}\right) - \left(\frac{\beta_\mathrm{tan}}{90}\right)^2 \\
+        & X = \left(\frac{s}{c}\right) - \left(\frac{s}{c}\right)_\mathrm{min} \\
+        & A = 0.242 - \frac{\beta_\mathrm{tan}}{151} + \left(\frac{\beta_\mathrm{tan}}{127}\right)^2 \\
+        & B = \begin{cases}
+                0.3 + \frac{30 - \beta_\mathrm{tan}}{50} && \text{if } \beta_\mathrm{tan} < 30 \\
+                0.3 + \frac{30 - \beta_\mathrm{tan}}{275} && \text{if } \beta_\mathrm{tan} \geq 30
+              \end{cases} \\
+        & C = 0.88 - \frac{\beta_\mathrm{tan}}{42.4} + \left(\frac{\beta_\mathrm{tan}}{72.8}\right)^2 \\
+        & \mathrm{Y_{p,impulse}} = A + BX^2 - CX^3 
+
+    where:
+
+        - :math:`s` is the pitch
+        - :math:`c` is the chord
+        - :math:`\beta_\mathrm{tan}` and :math:`\beta_\mathrm{ax}` is the exit relative flow angle with respect to tangential and axial direction. 
+        
+    Parameters
+    ----------
+    r_sc : float
+        Pitch-to-chord ratio.
+    angle_out : float
+        Exit relative flow angle (in degrees).
+
+    Returns
+    -------
+    float
+        Pressure loss coefficient for impulse blades.
+
+    """
+
+    phi = 90 - angle_out
+    r_sc_min = 0.224 + 1.575 * (phi / 90) - (phi / 90) ** 2
+    X = r_sc - r_sc_min
+    A = 0.242 - phi / 151 + (phi / 127) ** 2
+    B = (0.3 + (30 - phi) / 50) * (phi < 30) + (0.3 + (30 - phi) / 275) * (phi >= 30)
+    C = 0.88 - phi / 42.4 + (phi / 72.8) ** 2
+    Yp_impulse = A + B * X**2 - C * X**3
+
+    return Yp_impulse
+
+
+def get_compressible_correction_factors(Ma_rel_in, Ma_rel_out):
+    r"""
+
+    Compute compressible flow correction factor according to Kacker and Okapuu loss model :cite:`kacker_mean_1982`.
+
+    The correction factors :math:`\mathrm{K_1}`, :math:`\mathrm{K_2}` and :math:`\mathrm{K_p}` was introduced by :cite:`kacker_mean_1982` to correct previous correlation (:cite:`ainley_method_1951`)
+    for effect of higher mach number and channel acceleration. The correction factors reduces the losses at higher mach number. Their definition follows: 
+
+    .. math::
+
+        &\mathrm{K_1} = \begin{cases}
+                1 & \text{if } \mathrm{Ma_{out}} < 0.2 \\
+                1 - 1.25(\mathrm{Ma_{out}} - 0.2) & \text{if } \mathrm{Ma_{out}} \geq 0.2
+              \end{cases} \\
+        &\mathrm{K_2} = \left(\frac{\mathrm{Ma_{in}}}{\mathrm{Ma_{out}}}\right) \\
+        &\mathrm{K_p} = 1 - \mathrm{K_2}(1-\mathrm{K_1})
+
+    where:
+
+        - :math:`\mathrm{Ma_{in}}` is the relative mach number at the cascade inlet.
+        -  :math:`\mathrm{Ma_{out}}` is the relative mach number at the cascade exit.
+    
+    Parameters
+    ----------
+    Ma_rel_in : float
+        Inlet relative mach number.
+    Ma_rel_out : float
+        Exit relative mach number.
+
+    Returns
+    -------
+    float
+        Correction factor :math:`\mathrm{K_p}`.
+    float
+        Correction factor :math:`\mathrm{K_2}`.
+    float
+        Correction factor :math:`\mathrm{K_1}`.
+    """
+
+    # TODO: explain smoothing/blending tricks
+
+    K1 = 1 * (Ma_rel_out < 0.2) + (1 - 1.25 * (Ma_rel_out - 0.2)) * (
+        Ma_rel_out > 0.2 and Ma_rel_out < 1.00
+    )  # TODO: this can be converted to a smooth piecewise function (sigmoid blending)
+    K2 = (Ma_rel_in / Ma_rel_out) ** 2
+    Kp = 1 - K2 * (1 - K1)
+    Kp = max(0.1, Kp)  # TODO: smoothing
+    return [Kp, K2, K1]
+
+
+def get_hub_to_mean_mach_ratio(r_ht, cascade_type):
+    r"""
+    Compute the ratio between Mach at the hub and mean span at the inlet of the current cascade.
+
+    Due to radial variation in gas conditions, Mach at the hub will always be higher than at the mean.
+    Thus, shock losses at the hub could occur even when the Mach is subsonic at the mean blade span.
+
+    Parameters
+    ----------
+    r_ht : float
+        Hub to tip ratio at the inlet of the current cascade.
+    cascade_type : str
+        Type of the current cascade, either 'stator' or 'rotor'.
+
+    Returns
+    -------
+    float
+        Ratio between Mach at the hub and mean span at the inlet of the current cascade.
+
+    """
+
+    if r_ht < 0.5:  # TODO: add smoothing, this is essentially a max(r_ht, 0.5)
+        r_ht = 0.5  # Numerical trick to prevent extrapolation
+
+    r_ht_data = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
+
+    # Stator curve
+    f_data_S = [1.4, 1.18, 1.05, 1.0, 1.0, 1.0]
+
+    # Rotor curve
+    f_data_R = [2.15, 1.7, 1.35, 1.12, 1.0, 1.0]
+
+    if cascade_type == "stator":
+        f = np.interp(r_ht, r_ht_data, f_data_S)
+    elif cascade_type == "rotor":
+        f = np.interp(r_ht, r_ht_data, f_data_R)
+    else:
+        print("Specify the type of cascade")
+
+    return f
```

### Comparing `turboflow-0.1.2/turboflow/axial_turbine/performance_analysis.py` & `turboflow-0.1.3/turboflow/axial_turbine/performance_analysis.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,1363 +1,1363 @@
-import os
-import yaml
-import copy
-import time
-import datetime
-import itertools
-import numpy as np
-import pandas as pd
-import CoolProp as cp
-import matplotlib.pyplot as plt
-
-from .. import math
-from .. import pysolver_view as psv
-from .. import utilities as utils
-from .. import properties as props
-from . import geometry_model as geom
-from . import flow_model as flow
-
-SOLVER_MAP = {"lm": "Lavenberg-Marquardt", "hybr": "Powell's hybrid"}
-"""
-Available solvers for performance analysis.
-"""
-
-
-def get_heuristic_guess_input(n):
-    r"""
-
-    Generate a list of `n` number of different sets of dictionaries used to generate initial guesses.
-
-    Total-to-total efficiency, total-to-static efficiency, enthalpy loss fractions for each cascade and critical mach can be used to generate
-    an initial guess for performance analysis. This function generate a list of such different sets used to generate a full initial guess through the function `compute_heuristic_initial_guess`.
-
-    The total-to-static efficiency, varies between  0.6, 0.7, or 0.8, while the total-to-total efficiency can be 0.7, 0.8 or 0.9. The enthalpy loss fractions can either split equally between the cascade,
-    or with equal increments for neighbouring values (see `utils.fill_array_with_increment` for more information). The critical mach is assumed to be 0.95.
-
-    Parameters
-     ----------
-     n : int
-         Number of cascades.
-
-     Returns
-     -------
-     list
-         List of sets of variables used to generate an initial guess.
-
-    """
-
-    eta_ts_vec = [0.8, 0.7, 0.6]
-
-    array = utils.fill_array_with_increment(n)
-    enthalpy_distributions = []
-    enthalpy_distributions.append(np.ones(n) * 1 / n)
-    enthalpy_distributions.append(array)
-    enthalpy_distributions.append(np.flip(array))
-
-    initial_guesses = []
-    for eta_ts in eta_ts_vec:
-        for enthalpy_distribution in enthalpy_distributions:
-            initial_guesses.append(
-                {
-                    "enthalpy_loss_fractions": enthalpy_distribution,
-                    "eta_ts": eta_ts,
-                    "eta_tt": eta_ts + 0.1,
-                    "Ma_crit": 0.95,
-                }
-            )
-
-    return initial_guesses
-
-
-def compute_performance(
-    operation_points,
-    config,
-    initial_guess=None,
-    out_filename=None,
-    out_dir="output",
-    stop_on_failure=False,
-    export_results=True,
-):
-    r"""
-    Compute and export the performance of each specified operation point to an Excel file.
-
-    This function handles two types of input for operation points:
-
-        1. An explicit list of dictionaries, each detailing a specific operation point.
-        2. A dictionary where each key has a range of values, representing the cross-product of all possible operation points. It generates the Cartesian product of these ranges internally.
-
-    For each operation point, it computes performance based on the provided case data and compiles
-    the results into an Excel workbook with multiple sheets for various data sections.
-
-    The function validates the input operation points, and if they are given as ranges, it generates
-    all possible combinations. Performance is computed for each operation point, and the results are
-    then stored in a structured Excel file with separate sheets for each aspect of the data (e.g.,
-    overall, plane, cascade, stage, solver, and solution data).
-
-    The initial guess variable is used for the first operation point. If given, it must be a dictionary with the following keys:
-
-        - `enthalpy_loss_fractions`, which is a list containing the assumed fractions of enthalpy loss that occurs for each cascade.
-        - `eta_ts`, which is the assumed total-to-static efficiency.
-        - `eta_tt`, which is the assumed total-to-total efficiency.
-        - `Ma_crit`, which is the assumed critical mash number.
-
-    It can also be a dictionary containing the full set of initial guess that is provided directly to the solver. This
-    require care as the user must have a complete knowledge of the different variables, and setup, of the initial guess that must be given that
-    corresponds with the rest of the configuration file. If the initial guess is not given, it is set to a default value.
-    For subsequent operation points, the function employs a strategy to use the closest previously computed operation point's solution
-    as the initial guess. This approach is based on the heuristic that similar operation points have similar
-    performance characteristics, which can improve convergence speed and robustness of the solution process.
-    If the solution fails to converge, a set of initial guesses is provided to try other guesses (see `get_heuristic_guess_input`).
-
-    The function returns a list of solver object for each operation point. This contain information on both solver related performance (see psv.NonlinearSystemSolver)
-    and the object of the performance analysis problem (see CascadesNonlinearSystemProblem).
-
-    Parameters
-    ----------
-    operation_points : list of dict or dict
-        A list of operation points where each is a dictionary of parameters, or a dictionary of parameter
-        ranges from which operation points will be generated.
-    config : dict
-        A dictionary containing necessary configuration options for computing performance at each operation point.
-    initial_guess : optional
-        A dictionary with the required elements to generate an initial guess (see description above).
-    out_file : str, optional
-        The name for the output Excel file. If not provided, a default name with a timestamp is generated.
-    out_dir : str, optional
-        The directory where the Excel file will be saved. Defaults to 'output'.
-    stop_on_failure: bool, optional
-        If true, the analysis stops if the solution fails to converge for an operating point.
-    export_result : bool, optional
-        If true, the result is exported to an excel file.
-
-    Returns
-    -------
-    list
-        A List of solver object for each operation point.
-
-    """
-
-    # Check if geometry is provided
-    if config["geometry"] is None:
-        raise ValueError("Geometry is not provided")
-
-    # Check the type of operation_points argument
-    if isinstance(operation_points, dict):
-        # Convert ranges to a list of operation points
-        operation_points = generate_operation_points(operation_points)
-    elif not isinstance(operation_points, list):
-        msg = "operation_points must be either list of dicts or a dict with ranges."
-        raise TypeError(msg)
-
-    # Validate all operation points
-    for operation_point in operation_points:
-        validate_operation_point(operation_point)
-
-    # Initialize lists to hold dataframes for each operation point
-    operation_point_data = []
-    overall_data = []
-    plane_data = []
-    cascade_data = []
-    stage_data = []
-    solver_data = []
-    solution_data = []
-    geometry_data = []
-    solver_container = []
-
-    # Loop through all operation points
-    print_operation_points(operation_points)
-    for i, operation_point in enumerate(operation_points):
-        print()
-        print(f" Computing operation point {i+1} of {len(operation_points)}")
-        print_boundary_conditions(operation_point)
-
-        try:
-            # Define initial guess
-            if i == 0:
-                # Use default initial guess for the first operation point
-                if initial_guess == None:
-                    print("Using default initial guess")
-                else:
-                    print("Using user defined initial guess")
-            else:
-                closest_x, closest_index = find_closest_operation_point(
-                    operation_point,
-                    operation_points[:i],  # Use up to the previous point
-                    solution_data[:i],  # Use solutions up to the previous point
-                )
-                print(f" Using solution from point {closest_index+1} as initial guess")
-                initial_guess = closest_x
-
-            # Compute performance
-            solver, results = compute_single_operation_point(
-                operation_point, initial_guess, config
-            )
-
-            # Retrieve solver data
-            solver_status = {
-                "completed": True,
-                "success": solver.success,
-                "message": solver.message,
-                "grad_count": solver.convergence_history["grad_count"][-1],
-                "func_count": solver.convergence_history["func_count"][-1],
-                "func_count_total": solver.convergence_history["func_count_total"][-1],
-                "norm_residual": solver.convergence_history["norm_residual"][-1],
-                "norm_step": solver.convergence_history["norm_step"][-1],
-            }
-
-            # Collect results
-            operation_point_data.append(pd.DataFrame([operation_point]))
-            overall_data.append(results["overall"])
-            plane_data.append(utils.flatten_dataframe(results["plane"]))
-            cascade_data.append(utils.flatten_dataframe(results["cascade"]))
-            stage_data.append(utils.flatten_dataframe(results["stage"]))
-            geometry_data.append(utils.flatten_dataframe(results["geometry"]))
-            solver_data.append(pd.DataFrame([solver_status]))
-            solution_data.append(solver.problem.vars_real)
-            solver_container.append(solver)
-
-        except Exception as e:
-            if stop_on_failure:
-                raise Exception(e)
-            else:
-                print(f" Computation of point {i+1}/{len(operation_points)} failed")
-                print(f" Error: {e}")
-
-            # Retrieve solver data
-            solver = None
-            solver_status = {"completed": False}
-
-            # Collect data
-            operation_point_data.append(pd.DataFrame([operation_point]))
-            overall_data.append(pd.DataFrame([{}]))
-            plane_data.append(pd.DataFrame([{}]))
-            cascade_data.append(pd.DataFrame([{}]))
-            stage_data.append(pd.DataFrame([{}]))
-            geometry_data.append(pd.DataFrame([{}]))
-            solver_data.append(pd.DataFrame([solver_status]))
-            solution_data.append([])
-            solver_container.append(solver)
-
-    # Dictionary to hold concatenated dataframes
-    dfs = {
-        "operation point": pd.concat(operation_point_data, ignore_index=True),
-        "overall": pd.concat(overall_data, ignore_index=True),
-        "plane": pd.concat(plane_data, ignore_index=True),
-        "cascade": pd.concat(cascade_data, ignore_index=True),
-        "stage": pd.concat(stage_data, ignore_index=True),
-        "geometry": pd.concat(geometry_data, ignore_index=True),
-        "solver": pd.concat(solver_data, ignore_index=True),
-    }
-
-    # Add 'operation_point' column to each dataframe
-    for sheet_name, df in dfs.items():
-        df.insert(0, "operation_point", range(1, 1 + len(df)))
-
-    # Write dataframes to excel
-    if export_results:
-        # Create a directory to save simulation results
-        if not os.path.exists(out_dir):
-            os.makedirs(out_dir)
-
-        # Define filename with unique date-time identifier
-        if out_filename == None:
-            current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
-            out_filename = f"performance_analysis_{current_time}"
-
-        # Export simulation configuration as YAML file
-        config_data = {k: v for k, v in config.items() if v}  # Filter empty entries
-        config_data = utils.convert_numpy_to_python(config_data, precision=12)
-        config_file = os.path.join(out_dir, f"{out_filename}.yaml")
-        with open(config_file, "w") as file:
-            yaml.dump(config_data, file, default_flow_style=False, sort_keys=False)
-
-        # Export performance results in excel file
-        filepath = os.path.join(out_dir, f"{out_filename}.xlsx")
-        with pd.ExcelWriter(filepath, engine="openpyxl") as writer:
-            for sheet_name, df in dfs.items():
-                df.to_excel(writer, sheet_name=sheet_name, index=False)
-
-        print(f" Performance data successfully written to {filepath}")
-
-    # Print final report
-    print_simulation_summary(solver_container)
-
-    return solver_container
-
-
-def compute_single_operation_point(
-    operating_point,
-    initial_guess,
-    config,
-):
-    """
-    Compute an operation point for a given set of boundary conditions using multiple solver methods and initial guesses.
-
-    The initial guess make take three forms:
-
-        - None, which generate a default initial guess
-        - A dictionary which generates an initial guess through `compute_heuristic_initial_guess`. Required elements are:
-
-            - `enthalpy_loss_fractions`, which is a list containing the assumed fractions of enthalpy loss that occurs for each cascade.
-            - `eta_ts`, which is the assumed total-to-static efficiency.
-            - `eta_tt`, which is the assumed total-to-total efficiency.
-            - `Ma_crit`, which is the assumed critical mash number.
-
-        - A dictionary containing the full set of variables needed to evaluate turbine performance. This option require that the user has complete knowledge of what are the required variables, and the setup of the inital guess dictionary for the given configuration.
-
-    Parameters
-    ----------
-    boundary_conditions : dict
-        A dictionary containing boundary conditions for the operation point.
-    initial_guess : dict, optional
-        A dictionary with the required elements to generate an initial guess (see description above).
-    config : dict
-        A dictionary containing necessary configuration options for computing performance at the operational point.
-
-    Returns
-    -------
-    psv.NonlinearSystemSolver
-        The solution object containing the results of the operation point calculation.
-
-    """
-
-    # Initialize problem object
-    problem = CascadesNonlinearSystemProblem(config)
-    # TODO: A limitation of defining a new problem for each operation point is that the geometry generated and checked once for every point
-    # TODO: Performing the computations is not a big problem, but displaying the geometry report for every point can be very long.
-    # TODO: Perhaps we could add options of verbosity and perhaps only display the full geometry report when it fails
-    problem.update_boundary_conditions(operating_point)
-    solver_options = copy.deepcopy(config["performance_analysis"]["solver_options"])
-
-    initial_guesses = [initial_guess] + get_heuristic_guess_input(
-        problem.geometry["number_of_cascades"]
-    )
-    methods_to_try = [solver_options["method"]] + [
-        method for method in SOLVER_MAP.keys() if method != solver_options["method"]
-    ]
-
-    for initial_guess in initial_guesses:
-        for method in methods_to_try:
-            success = False
-            x0 = problem.get_initial_guess(
-                initial_guess
-            )  # TODO: Roberto_17.05.2023: It seems we are not using this value. Why take it as output then?
-            print(f" Trying to solve the problem using {SOLVER_MAP[method]} method")
-            solver_options["method"] = method
-
-            solver = psv.NonlinearSystemSolver(problem, **solver_options)
-            # TODO: Roberto: add the option to use optimizers as solver depending on the method specified?
-            # TODO: Roberto: at some point in the past we tried to solve the system of equations with SLSQP, right?
-
-            try: 
-                solver.solve(problem.x0)                
-                
-            except Exception as e:
-                print(f" Error during solving: {e}")
-                solver.success = False
-
-            if solver.success:
-                break
-        if solver.success:
-            break
-
-    if not solver.success:
-        print(" WARNING: All attempts failed to converge")
-        # TODO: Add messages to Log file
-
-    return solver, problem.results
-
-
-def find_closest_operation_point(current_op_point, operation_points, solution_data):
-    """
-    Find the solution vector and index of the closest operation point in the historical data.
-
-    Parameters
-    ----------
-    current_op_point : dict
-        The current operation point we want to compare.
-    operation_points : list of dict
-        A list of historical operation points to search through.
-    solution_data : list
-        A list of solution vectors corresponding to each operation point.
-
-    Returns
-    -------
-    tuple
-        A tuple containing the closest solution vector and the one-based index of the closest operation point.
-
-    """
-    min_distance = float("inf")
-    closest_point_x = None
-    closest_index = None
-
-    for i, op_point in enumerate(operation_points):
-        distance = get_operation_point_distance(current_op_point, op_point)
-        if distance < min_distance:
-            min_distance = distance
-            closest_point_x = solution_data[i]
-            closest_index = i
-
-    return closest_point_x, closest_index
-
-
-def get_operation_point_distance(point_1, point_2, delta=1e-8):
-    """
-    Calculate the normalized distance between two operation points, with special consideration
-    for angle measurements and prevention of division by zero for very small values.
-
-    Parameters
-    ----------
-    point_1 : dict
-        First operation point with numeric values.
-    point_2 : dict
-        Second operation point with numeric values.
-    delta : float, optional
-        A small constant to prevent division by zero. Default is 1e-8.
-
-    Returns
-    -------
-    float
-        The calculated normalized distance.
-    """
-    deviation_array = []
-    for key in point_1:
-        if isinstance(point_1[key], (int, float)) and key in point_2:
-            value_1 = point_1[key]
-            value_2 = point_2[key]
-
-            if key == "alpha_in":
-                # Handle angle measurements with absolute scale normalization
-                deviation = np.abs(value_1 - value_2) / 90
-            else:
-                # Compute the relative difference with protection against division by zero
-                max_val = max(abs(value_1), abs(value_2), delta)
-                deviation = abs(value_1 - value_2) / max_val
-
-            deviation_array.append(deviation)
-
-    # Calculate the two-norm of the deviations
-    return np.linalg.norm(deviation_array)
-
-
-def generate_operation_points(performance_map):
-    """
-    Generates a list of dictionaries representing all possible combinations of
-    operation points from a given performance map. The performance map is a
-    dictionary where keys represent parameter names and values are the ranges
-    of values for those parameters. The function ensures that the combinations
-    are generated such that the parameters related to pressure ('p0_in' and
-    'p_out') are the last ones to vary, effectively making them the first
-    parameters to sweep through in the operation points.
-
-    Parameters
-    ----------
-    - performance_map (dict): A dictionary with parameter names as keys and
-      lists of parameter values as values.
-
-    Returns
-    -------
-    - operation_points (list of dict): A list of dictionaries, each representing
-      a unique combination of parameters from the performance_map.
-    """
-
-    # Make sure all values in the performance_map are iterables
-    performance_map = {k: utils.ensure_iterable(v) for k, v in performance_map.items()}
-
-    # Reorder performance map keys so first sweep is always through pressure
-    priority_keys = ["p0_in", "p_out"]
-    other_keys = [k for k in performance_map.keys() if k not in priority_keys]
-    keys_order = other_keys + priority_keys
-    performance_map = {
-        k: performance_map[k] for k in keys_order if k in performance_map
-    }
-
-    # Create all combinations of operation points
-    keys, values = zip(*performance_map.items())
-    operation_points = [
-        dict(zip(keys, combination)) for combination in itertools.product(*values)
-    ]
-
-    return operation_points
-
-
-def validate_operation_point(op_point):
-    """
-    Validates that an operation point has exactly the required fields:
-    'fluid_name', 'p0_in', 'T0_in', 'p_out', 'alpha_in', 'omega'.
-
-    Parameters
-    ----------
-    op_point: dict
-        A dictionary representing an operation point.
-
-    Returns
-    -------
-    ValueError: If the dictionary does not contain the required fields or contains extra fields.
-    """
-    REQUIRED_FIELDS = {"fluid_name", "p0_in", "T0_in", "p_out", "alpha_in", "omega"}
-    fields = set(op_point.keys())
-    if fields != REQUIRED_FIELDS:
-        missing = REQUIRED_FIELDS - fields
-        extra = fields - REQUIRED_FIELDS
-        raise ValueError(
-            f"Operation point validation error: "
-            f"Missing fields: {missing}, Extra fields: {extra}"
-        )
-
-
-# ------------------------------------------------------------------------------------------ #
-# ------------------------------------------------------------------------------------------ #
-# ------------------------------------------------------------------------------------------ #
-
-
-class CascadesNonlinearSystemProblem(psv.NonlinearSystemProblem):
-    """
-    A class representing a nonlinear system problem for cascade analysis.
-
-    This class is designed for solving nonlinear systems of equations related to cascade analysis.
-    Derived classes must implement the `residual` method to evaluate the system of equations for a given set of decision variables.
-
-    Additionally, specific problem classes can define the `get_jacobian` method to compute Jacobians.
-    If this method is not present in the derived class, the solver will revert to using forward finite differences for Jacobian calculations.
-
-    Attributes
-    ----------
-    fluid : FluidCoolProp_2Phase
-        An instance of the FluidCoolProp_2Phase class representing the fluid properties.
-    results : dict
-        A dictionary to store results.
-    boundary_conditions : dict
-        A dictionary containing boundary condition data.
-    geometry : dict
-        A dictionary containing geometry-related data.
-    model_options : dict
-        A dictionary containing options related to the analysis model.
-    reference_values : dict
-        A dictionary containing reference values for calculations.
-    vars_scaled
-        A dicionary of scaled variables used to evaluate turbine performance.
-    vars_real
-        A dicionary of real variables used to evaluate turbine performance.
-
-    Methods
-    -------
-    get_values(x)
-        Evaluate the system of equations for a given set of decision variables.
-    update_boundary_conditions(operation_point)
-        Update the boundary conditions of the problem with the provided operation point.
-    scale_values(variables, to_normalized = False)
-        Convert values between normalized and real values.
-    get_initial_guess(initial_guess = None)
-        Determine the initial guess for the performance analysis based on the given parameters or default values.
-    compute_heuristic_initial_guess(enthalpy_loss_fractions, eta_tt, eta_ts, Ma_crit)
-        Compute the heuristic initial guess for the performance analysis based on the given parameters.
-    print_simulation_summary(solvers)
-        Print a formatted footer summarizing the performance of all operation points.
-    print_boundary_conditions(BC)
-        Print the boundary conditions.
-    print_operation_points(operation_points)
-        Prints a summary table of operation points scheduled for simulation.
-
-    Examples
-    --------
-    Here's an example of how to derive from `CascadesNonlinearSystemProblem`::
-
-        class MyCascadeProblem(CascadesNonlinearSystemProblem):
-            def get_values(self, x):
-                # Implement evaluation logic here
-                pass
-    """
-
-    def __init__(self, config):
-        """
-        Initialize a CascadesNonlinearSystemProblem.
-
-        Parameters
-        ----------
-        config : dict
-            A dictionary containing case-specific data.
-        """
-
-        # Process turbine geometry
-        geom.validate_turbine_geometry(config["geometry"])
-        self.geometry = geom.calculate_full_geometry(config["geometry"])
-        # self.geom_info = geom.check_turbine_geometry(self.geometry, display=True)
-
-        # Initialize other attributes
-        self.model_options = config["simulation_options"]
-        self.keys = []
-
-    def residual(self, x):
-        """
-        Evaluate the system of equations for a given set of decision variables.
-
-        Parameters
-        ----------
-        x : array-like
-            Vector of decision variables.
-
-        Returns
-        -------
-        numpy nd.array
-            An array containing residual values.
-        """
-
-        # Create dictionary of scaled variables
-        self.vars_scaled = dict(zip(self.keys, x))
-
-        # Create dictionary of real variables
-        self.vars_real = self.scale_values(self.vars_scaled, to_normalized=False)
-
-        # Evaluate cascade series
-        self.results = flow.evaluate_axial_turbine(
-            self.vars_scaled,
-            self.boundary_conditions,
-            self.geometry,
-            self.fluid,
-            self.model_options,
-            self.reference_values,
-        )
-
-        return np.array(list(self.results["residuals"].values()))
-
-    def update_boundary_conditions(self, operation_point):
-        """
-        Update the boundary conditions of the problem with the provided operation point.
-
-        This method updates the boundary conditions attributes used to evaluate the turbine performance.
-        It also initializes a Fluid object using the 'fluid_name' specified in the operation point.
-        The method computes additional properties and reference values like stagnation properties at
-        the inlet, exit static properties, spouting velocity, and reference mass flow rate.
-        These are stored in the object's internal state for further use in calculations.
-
-        Parameters
-        ----------
-        operation_point : dict
-            A dictionary containing the boundary conditions defining the operation point. It must include the following keys:
-
-            - `fluid_name` (str) : The name of the fluid to be used in the Fluid object.
-            - `T0_in` (float): The inlet temperature (in Kelvin).
-            - `p0_in` (float): The inlet pressure (in Pascals).
-            - `p_out` (float): The outlet pressure (in Pascals).
-            - `omega` (float): The rotational speed (in rad/s).
-            - `alpha_in` (float): The inlet flow angle (in degrees).
-
-        Returns
-        -------
-        None
-            This method does not return a value but updates the internal state of the object.
-
-        """
-
-        # Define current operating point
-        self.boundary_conditions = operation_point
-
-        # Initialize fluid object
-        self.fluid = props.Fluid(operation_point["fluid_name"], exceptions=True)
-
-        # Rename variables
-        p0_in = operation_point["p0_in"]
-        T0_in = operation_point["T0_in"]
-        p_out = operation_point["p_out"]
-
-        # Compute stagnation properties at inlet
-        state_in_stag = self.fluid.get_props(cp.PT_INPUTS, p0_in, T0_in)
-        h0_in = state_in_stag["h"]
-        s_in = state_in_stag["s"]
-
-        # Store the inlet stagnation (h,s) for the first stage
-        # TODO: Improve logic of implementation?
-        self.boundary_conditions["h0_in"] = h0_in
-        self.boundary_conditions["s_in"] = s_in
-
-        # Calculate exit static properties for a isentropic expansion
-        state_out_s = self.fluid.get_props(cp.PSmass_INPUTS, p_out, s_in)
-        h_isentropic = state_out_s["h"]
-        d_isentropic = state_out_s["d"]
-
-        # Calculate exit static properties for a isenthalpic expansion
-        state_out_h = self.fluid.get_props(cp.HmassP_INPUTS, h0_in, p_out)
-        s_isenthalpic = state_out_h["s"]
-
-        # Calculate spouting velocity
-        v0 = np.sqrt(2 * (h0_in - h_isentropic))
-
-        # Define a reference mass flow rate
-        A_out = self.geometry["A_out"][-1]
-        mass_flow_ref = A_out * v0 * d_isentropic
-
-        # Define reference_values
-        self.reference_values = {
-            "s_range": s_isenthalpic - s_in,
-            "s_min": s_in,
-            "v0": v0,
-            "h_out_s": h_isentropic,
-            "d_out_s": d_isentropic,
-            "mass_flow_ref": mass_flow_ref,
-            "angle_range": 180,
-            "angle_min": -90,
-        }
-
-        return
-
-    def scale_values(self, variables, to_normalized=True):
-        """
-        Convert values between normalized and real values.
-
-        Parameters
-        ----------
-        variables: dict
-            A dictionary containing values to be scaled.
-        to_real: bool
-            If True, scale to real values; if False, scale to normalized values.
-
-        Returns
-        -------
-        An array of values converted between scales.
-        """
-
-        # Load parameters
-        v0 = self.reference_values["v0"]
-        s_range = self.reference_values["s_range"]
-        s_min = self.reference_values["s_min"]
-        angle_range = self.reference_values["angle_range"]
-        angle_min = self.reference_values["angle_min"]
-
-        # Define dictionary of scaled values
-        scaled_variables = {}
-
-        for key, val in variables.items():
-            if key.startswith("v") or key.startswith("w"):
-                scaled_variables[key] = val / v0 if to_normalized else val * v0
-            elif key.startswith("s"):
-                scaled_variables[key] = (
-                    (val - s_min) / s_range if to_normalized else val * s_range + s_min
-                )
-            elif key.startswith("b"):
-                scaled_variables[key] = (
-                    (val - angle_min) / angle_range
-                    if to_normalized
-                    else val * angle_range + angle_min
-                )
-
-        return scaled_variables
-
-    def get_initial_guess(self, initial_guess=None):
-        """
-        Determine the initial guess for the performance analysis based on the given parameters or default values.
-
-        The given `initial_guess` make take three forms:
-
-            - None, which generate a default initial guess
-            - A dictionary which generates an initial guess through `compute_heuristic_initial_guess`. Required elements are:
-
-                - `enthalpy_loss_fractions`, which is a list containing the assumed fractions of enthalpy loss that occurs for each cascade.
-                - `eta_ts`, which is the assumed total-to-static efficiency.
-                - `eta_tt`, which is the assumed total-to-total efficiency.
-                - `Ma_crit`, which is the assumed critical mash number.
-
-            - A dictionary containing the full set of variables needed to evaluate turbine performance. This option require that the user has complete knowledge of what are the required variables, and the setup of the inital guess dictionary for the given configuration.
-
-        The initial guess is scaled in this function.
-
-        Parameters
-        ----------
-        initial_guess : dict, optional
-            A dictionary containing the initial guess parameters.
-
-        Returns
-        -------
-        dict
-            Initial guess for the performance analysis.
-
-        Raises
-        ------
-        ValueError
-            If the provided initial_guess is invalid or incompatible with the model options.
-
-        """
-
-        number_of_cascades = self.geometry["number_of_cascades"]
-
-        # Define initial guess
-        if isinstance(initial_guess, dict):
-            valid_keys_1 = ["enthalpy_loss_fractions", "eta_ts", "eta_tt", "Ma_crit"]
-            valid_keys_2 = [
-                "w_out",
-                "s_out",
-                "beta_out",
-                "v*_in",
-                "w*_throat",
-                "s*_throat",
-            ]
-            valid_keys_2 = ["v_in"] + [
-                f"{key}_{i+1}"
-                for i in range(number_of_cascades)
-                for key in valid_keys_2
-            ]
-            valid_keys_3 = [
-                key for key in valid_keys_2 if not key.startswith("v*_in")
-            ] + [f"beta*_throat_{i+1}" for i in range(number_of_cascades)]
-            valid_keys_4 = [
-                key
-                for key in valid_keys_2
-                if not (
-                    key.startswith("v*_in")
-                    or key.startswith("s*_throat")
-                    or key.startswith("beta*_throat")
-                )
-            ]
-
-            check = []
-            check.append(set(valid_keys_1) == set(list(initial_guess.keys())))
-            check.append(set(valid_keys_2) == set(list(initial_guess.keys())))
-            check.append(set(valid_keys_3) == set(list(initial_guess.keys())))
-            check.append(set(valid_keys_4) == set(list(initial_guess.keys())))
-
-            if check[0]:
-                enthalpy_loss_fractions = initial_guess["enthalpy_loss_fractions"]
-                eta_tt = initial_guess["eta_tt"]
-                eta_ts = initial_guess["eta_ts"]
-                Ma_crit = initial_guess["Ma_crit"]
-
-                # Check that eta_tt, eta_ts and Ma_crit is in reasonable range
-                for label, variable in zip(
-                    ["eta_tt", "eta_ts", "Ma_crit"], [eta_tt, eta_ts, Ma_crit]
-                ):
-                    if not 0 <= variable <= 1:
-                        raise ValueError(f"{label} should be between {0} and {1}.")
-
-                # Check if enthalpy_loss_fractions is a list or a NumPy array
-                if not isinstance(enthalpy_loss_fractions, (list, np.ndarray)):
-                    raise ValueError(
-                        "enthalpy_loss_fractions must be a list or NumPy array"
-                    )
-
-                # Check that enthalpy_loss_fractions is of the same length as the number_of_cascades
-                if len(enthalpy_loss_fractions) != number_of_cascades:
-                    raise ValueError(
-                        f"enthalpy_loss_fractions must be of length {number_of_cascades}"
-                    )
-
-                # Check that the sum of enthalpy loss fractions is 1
-                sum_fractions = np.sum(enthalpy_loss_fractions)
-                epsilon = 1e-8  # Small epsilon value for floating-point comparison
-                if not np.isclose(sum_fractions, 1, atol=epsilon):
-                    raise ValueError(
-                        f"Sum of enthalpy_loss_fractions must be 1 (now: {sum_fractions})."
-                    )
-
-                print(" Generating heuristic initial guess with given parameters")
-                initial_guess = self.compute_heuristic_initial_guess(
-                    enthalpy_loss_fractions, eta_tt, eta_ts, Ma_crit
-                )
-
-            elif check[1]:
-                # Check that set of input correspond with model option
-                if (
-                    not self.model_options["choking_model"]
-                    == "evaluate_cascade_critical"
-                ):
-                    raise ValueError(
-                        "Set of input corresponds with different choking_model (evaluate_cascade_critical)"
-                    )
-
-                # Check that all values are a number
-                if not all(
-                    isinstance(val, (int, float)) for val in initial_guess.values()
-                ):
-                    raise ValueError("All dictionary values must be a float or int")
-
-            elif check[2]:
-                # Check that set of input correspond with model option
-                if not self.model_options["choking_model"] == "evaluate_cascade_throat":
-                    raise ValueError(
-                        "Set of input corresponds with different choking_model (evaluate_cascade_throat)"
-                    )
-
-                # Check that all values are a number
-                if not all(
-                    isinstance(val, (int, float)) for val in initial_guess.values()
-                ):
-                    raise ValueError("All dictionary values must be a float or int")
-
-            elif check[3]:
-                # Check that set of input correspond with model option
-                if (
-                    not self.model_options["choking_model"]
-                    == "evaluate_cascade_isentropic_throat"
-                ):
-                    raise ValueError(
-                        "Set of input corresponds with different choking_model (evaluate_cascade_isentropic_throat)"
-                    )
-
-                # Check that all values are a number
-                if not all(
-                    isinstance(val, (int, float)) for val in initial_guess.values()
-                ):
-                    raise ValueError("All dictionary values must be a float or int")
-
-            else:
-                raise ValueError(
-                    f"Invalid keys provided for initial_guess. "
-                    f"Valid keys include either:"
-                    f"{valid_keys_1} \n"
-                    f"{valid_keys_2} \n"
-                    f"{valid_keys_3} \n"
-                )
-
-        elif initial_guess == None:
-            enthalpy_loss_fractions = np.full(
-                number_of_cascades, 1 / number_of_cascades
-            )
-            eta_tt = 0.9
-            eta_ts = 0.8
-            Ma_crit = 0.95
-
-            # Compute initial guess using several approximations
-            initial_guess = self.compute_heuristic_initial_guess(
-                enthalpy_loss_fractions, eta_tt, eta_ts, Ma_crit
-            )
-
-        else:
-            raise ValueError("Initial guess must be either None or a dictionary.")
-
-        if self.model_options["choking_model"] == "evaluate_cascade_throat":
-            initial_guess = {
-                key: val
-                for key, val in initial_guess.items()
-                if not key.startswith("v*_in")
-            }
-        elif self.model_options["choking_model"] == "evaluate_cascade_critical":
-            initial_guess = {
-                key: val
-                for key, val in initial_guess.items()
-                if not key.startswith("beta*_throat")
-            }
-        elif (
-            self.model_options["choking_model"] == "evaluate_cascade_isentropic_throat"
-        ):
-            initial_guess = {
-                key: val
-                for key, val in initial_guess.items()
-                if not (
-                    key.startswith("v*_in")
-                    or key.startswith("s*_throat")
-                    or key.startswith("beta*_throat")
-                )
-            }
-
-        # Always normalize initial guess
-        initial_guess_scaled = self.scale_values(initial_guess)
-
-        # Store labels
-        self.keys = initial_guess_scaled.keys()
-        self.x0 = np.array(list(initial_guess_scaled.values()))
-
-        return initial_guess_scaled
-
-    def compute_heuristic_initial_guess(
-        self, enthalpy_loss_fractions, eta_tt, eta_ts, Ma_crit
-    ):
-        """
-        Compute the heuristic initial guess for the performance analysis based on the given parameters.
-
-        This function calculates the heuristic initial guess based on the provided enthalpy loss fractions for each cascade,
-        total-to-static and total-to-total efficiencies, and critical Mach number.
-
-        Parameters
-        ----------
-        enthalpy_loss_fractions : array-like
-            Enthalpy loss fractions for each cascade.
-        eta_tt : float
-            Total-to-total efficiency.
-        eta_ts : float
-            Total-to-static efficiency.
-        Ma_crit : float
-            Critical Mach number.
-
-        Returns
-        -------
-        dict
-            Heuristic initial guess for the performance analysis.
-
-        """
-
-        # Load object attributes
-        geometry = self.geometry
-        boundary_conditions = self.boundary_conditions
-        fluid = self.fluid
-
-        # Rename variables
-        number_of_cascades = geometry["number_of_cascades"]
-        p0_in = boundary_conditions["p0_in"]
-        T0_in = boundary_conditions["T0_in"]
-        alpha_in = boundary_conditions["alpha_in"]
-        angular_speed = boundary_conditions["omega"]
-        p_out = boundary_conditions["p_out"]
-        h_out_s = self.reference_values["h_out_s"]
-
-        # Calculate inlet stagnation state
-        stagnation_properties_in = fluid.get_props(cp.PT_INPUTS, p0_in, T0_in)
-        h0_in = stagnation_properties_in["h"]
-        s_in = stagnation_properties_in["s"]
-        rho0_in = stagnation_properties_in["d"]
-
-        # Calculate exit enthalpy
-        h0_out = h0_in - eta_ts * (h0_in - h_out_s)
-        v_out = np.sqrt(2 * (h0_in - h_out_s - (h0_in - h0_out) / eta_tt))
-        h_out = h0_out - 0.5 * v_out**2
-
-        # Calculate exit static state for expansion with guessed efficiency
-        static_properties_exit = fluid.get_props(cp.HmassP_INPUTS, h_out, p_out)
-        s_out = static_properties_exit["s"]
-
-        # Define entropy distribution
-        entropy_distribution = np.linspace(s_in, s_out, number_of_cascades + 1)[1:]
-
-        # Define enthalpy distribution
-        total_enthalpy_loss = h0_in - h_out
-        enthalpy_loss_per_cascade = [
-            fraction * total_enthalpy_loss for fraction in enthalpy_loss_fractions
-        ]
-        enthalpy_distribution = [
-            h0_in - sum(enthalpy_loss_per_cascade[: i + 1])
-            for i in range(number_of_cascades)
-        ]
-
-        # Assums h0_in approx h_in for first inlet
-        h_in = h0_in
-
-        # Define initial guess dictionary
-        initial_guess = {}
-
-        for i in range(number_of_cascades):
-            geometry_cascade = {
-                key: values[i]
-                for key, values in geometry.items()
-                if key not in ["number_of_cascades", "number_of_stages"]
-            }
-
-            # Load enthalpy from initial guess
-            h_out = enthalpy_distribution[i]
-
-            # Load entropy from assumed entropy distribution
-            s_out = entropy_distribution[i]
-
-            # Rename necessary geometry
-            theta_in = geometry_cascade["leading_edge_angle"]
-            theta_out = geometry_cascade["gauging_angle"]
-            A_out = geometry_cascade["A_out"]
-            A_throat = geometry_cascade["A_throat"]
-            A_in = geometry_cascade["A_in"]
-            radius_mean_in = geometry_cascade["radius_mean_in"]
-            radius_mean_throat = geometry_cascade["radius_mean_throat"]
-            radius_mean_out = geometry_cascade["radius_mean_out"]
-
-            # Calculate rothalpy at inlet of cascade
-            blade_speed_in = angular_speed * (i % 2) * radius_mean_in
-            if i == 0:
-                h0_rel_in = h_in
-                m_temp = rho0_in * A_in * math.cosd(alpha_in)
-            else:
-                v_in = np.sqrt(2 * (h0_in - h_in))
-                velocity_triangle_in = flow.evaluate_velocity_triangle_in(
-                    blade_speed_in, v_in, alpha_in
-                )
-                w_in = velocity_triangle_in["w"]
-                h0_rel_in = h_in + 0.5 * w_in**2
-
-            rothalpy = h0_rel_in - 0.5 * blade_speed_in**2
-
-            # Calculate static state at cascade inlet
-            static_state_in = fluid.get_props(cp.HmassSmass_INPUTS, h_in, s_in)
-            rho_in = static_state_in["d"]
-
-            # Calculate exit velocity from rothalpy and enthalpy distirbution
-            blade_speed_out = angular_speed * (i % 2) * radius_mean_out
-            h0_rel_out = rothalpy + 0.5 * blade_speed_out**2
-            w_out = np.sqrt(2 * (h0_rel_out - h_out))
-            velocity_triangle_out = flow.evaluate_velocity_triangle_out(
-                blade_speed_out, w_out, theta_out
-            )
-            v_t_out = velocity_triangle_out["v_t"]
-            v_m_out = velocity_triangle_out["v_m"]
-            v_out = velocity_triangle_out["v"]
-            h0_out = h_out + 0.5 * v_out**2
-
-            # Calculate static state at cascade exit
-            static_state_out = fluid.get_props(cp.HmassSmass_INPUTS, h_out, s_out)
-            a_out = static_state_out["a"]
-            rho_out = static_state_out["d"]
-
-            # Calculate mass flow rate
-            mass_flow = rho_out * v_m_out * A_out
-
-            # Calculate throat velocity depending on subsonic or supersonic conditions
-            if w_out < a_out * Ma_crit:
-                w_throat = w_out
-                s_throat = s_out
-            else:
-                w_throat = a_out * Ma_crit
-                blade_speed_throat = angular_speed * (i % 2) * radius_mean_throat
-                h0_rel_throat = rothalpy + 0.5 * blade_speed_throat**2
-                h_throat = h0_rel_throat - 0.5 * w_throat**2
-                rho_throat = rho_out
-                static_state_throat = fluid.get_props(
-                    cp.DmassHmass_INPUTS, rho_throat, h_throat
-                )
-                s_throat = static_state_throat["s"]
-
-            # Calculate critical state
-            w_throat_crit = a_out * Ma_crit
-            h_throat_crit = (
-                h0_rel_in - 0.5 * w_throat_crit**2
-            )  # FIXME: h0_rel_in works less good?
-            static_state_throat_crit = fluid.get_props(
-                cp.HmassSmass_INPUTS, h_throat_crit, s_throat
-            )
-            rho_throat_crit = static_state_throat_crit["d"]
-            m_crit = w_throat_crit * math.cosd(theta_out) * rho_throat_crit * A_throat
-            w_m_in_crit = m_crit / rho_in / A_in
-            w_in_crit = w_m_in_crit / math.cosd(
-                theta_in
-            )  # XXX Works better with metal angle than inlet flow angle?
-            velocity_triangle_crit_in = flow.evaluate_velocity_triangle_out(
-                blade_speed_in, w_in_crit, theta_in
-            )
-            v_in_crit = velocity_triangle_crit_in["v"]
-
-            rho_out_crit = rho_throat_crit
-            w_out_crit = m_crit / (rho_out_crit * A_out * math.cosd(theta_out))
-
-            # Store initial guess
-            index = f"_{i+1}"
-            initial_guess.update(
-                {
-                    "w_out" + index: w_out,
-                    "s_out" + index: s_out,
-                    "beta_out"
-                    + index: np.sign(theta_out) * math.arccosd(A_throat / A_out),
-                    "v*_in" + index: v_in_crit,
-                    "beta*_throat"
-                    + index: np.sign(theta_out) * math.arccosd(A_throat / A_out),
-                    "w*_throat" + index: w_throat_crit,
-                    "s*_throat" + index: s_throat,
-                }
-            )
-
-            # Update variables for next cascade
-            if i != (number_of_cascades - 1):
-                A_next = geometry["A_in"][i + 1]
-                radius_mean_next = geometry["radius_mean_in"][i + 1]
-                v_m_in = v_m_out * A_out / A_next
-                v_t_in = v_t_out * radius_mean_out / radius_mean_next
-                v_in = np.sqrt(v_m_in**2 + v_t_in**2)
-                alpha_in = math.arctand(v_t_in / v_m_in)
-                h0_in = h0_out
-                h_in = h0_in - 0.5 * v_in**2
-                s_in = s_out
-
-        # Calculate inlet velocity from
-        initial_guess["v_in"] = mass_flow / m_temp
-
-        return initial_guess
-
-
-def print_simulation_summary(solvers):
-    """
-    Print a formatted footer summarizing the performance of all operation points.
-
-    This function processes a list of solver objects to provide a summary of the performance
-    analysis calculations. It calculates and displays the number of successful points and a summary of
-    simulation tme statistics. Additionally, it lists the indices of failed operation points, if any.
-
-    The function is robust against solvers that failed and lack certain attributes like 'elapsed_time'.
-    In such cases, these solvers are included in the count of failed operation points, but not in the
-    calculation time statistics.
-
-    Parameters
-    ----------
-    solvers : list
-        A list of solver objects. Each solver object should contain attributes related to the
-        calculation of an operation point, such as 'elapsed_time' and the 'solution' status.
-
-    """
-
-    # Initialize times list and track failed points
-    times = []
-    failed_points = []
-
-    for i, solver in enumerate(solvers):
-        # Check if the solver is not None and has the required attribute
-        if solver and hasattr(solver, "elapsed_time"):
-            times.append(solver.elapsed_time)
-            if not solver.success:
-                failed_points.append(i)
-        else:
-            # Handle failed solver or missing attributes
-            failed_points.append(i)
-
-    # Convert times to a numpy array for calculations
-    times = np.asarray(times)
-    total_points = len(solvers)
-
-    # Define footer content
-    width = 80
-    separator = "-" * width
-    lines_to_output = [
-        "",
-        separator,
-        "Final summary of performance analysis calculations".center(width),
-        separator,
-        f" Simulation successful for {total_points - len(failed_points)} out of {total_points} points",
-    ]
-
-    # Add failed points message only if there are failed points
-    if failed_points:
-        lines_to_output.append(
-            f"Failed operation points: {', '.join(map(str, failed_points))}"
-        )
-
-    # Add time statistics only if there are valid times
-    if times.size > 0:
-        lines_to_output.extend(
-            [
-                f" Average calculation time per operation point: {np.mean(times):.3f} seconds",
-                f" Minimum calculation time of all operation points: {np.min(times):.3f} seconds",
-                f" Maximum calculation time of all operation points: {np.max(times):.3f} seconds",
-                f" Total calculation time for all operation points: {np.sum(times):.3f} seconds",
-            ]
-        )
-    else:
-        lines_to_output.append(" No valid calculation times available.")
-
-    lines_to_output.append(separator)
-    lines_to_output.append("")
-
-    # Display to stdout
-    for line in lines_to_output:
-        print(line)
-
-
-def print_boundary_conditions(BC):
-    """
-    Print the boundary conditions.
-
-    This function prints the boundary conditions in a formatted manner. It takes a dictionary `BC`
-    containing the following keys:
-
-        - `fluid_name` (str): Name of the fluid.
-        - `alpha_in` (float): Flow angle at inlet in degrees.
-        - `T0_in` (float): Total temperature at inlet in Kelvin.
-        - `p0_in` (float): Total pressure at inlet in Pascal.
-        - `p_out` (float): Static pressure at outlet in Pascal.
-        - `omega` (float): Angular speed in radians per second.
-
-    Parameters
-    ----------
-    BC : dict
-        A dictionary containing the boundary conditions.
-
-    """
-
-    column_width = 25  # Adjust this to your desired width
-    print("-" * 80)
-    print(" Operating point: ")
-    print("-" * 80)
-    print(f" {'Fluid: ':<{column_width}} {BC['fluid_name']:<}")
-    print(f" {'Flow angle in: ':<{column_width}} {BC['alpha_in']:<.2f} deg")
-    print(f" {'Total temperature in: ':<{column_width}} {BC['T0_in']-273.15:<.2f} degC")
-    print(f" {'Total pressure in: ':<{column_width}} {BC['p0_in']/1e5:<.3f} bar")
-    print(f" {'Static pressure out: ':<{column_width}} {BC['p_out']/1e5:<.3f} bar")
-    print(f" {'Angular speed: ':<{column_width}} {BC['omega']*60/2/np.pi:<.1f} RPM")
-    print("-" * 80)
-    print()
-
-
-def print_operation_points(operation_points):
-    """
-    Prints a summary table of operation points scheduled for simulation.
-
-    This function takes a list of operation point dictionaries, formats them
-    according to predefined specifications, applies unit conversions where
-    necessary, and prints them in a neatly aligned table with headers and units.
-
-    Parameters
-    ----------
-    - operation_points (list of dict): A list where each dictionary contains
-      key-value pairs representing operation parameters and their corresponding
-      values.
-
-    Notes
-    -----
-    - This function assumes that all necessary keys exist within each operation
-      point dictionary.
-    - The function directly prints the output; it does not return any value.
-    - Unit conversions are hardcoded and specific to known parameters.
-    - If the units of the parameters change or if different parameters are added,
-      the unit conversion logic and `field_specs` need to be updated accordingly.
-    """
-    length = 80
-    index_width = 8
-    output_lines = [
-        "-" * length,
-        " Summary of operation points scheduled for simulation",
-        "-" * length,
-    ]
-
-    # Configuration for each field with specified width and decimal places
-    field_specs = {
-        "fluid_name": {"name": "Fluid", "unit": "", "width": 8},
-        "alpha_in": {"name": "angle_in", "unit": "[deg]", "width": 10, "decimals": 1},
-        "T0_in": {"name": "T0_in", "unit": "[degC]", "width": 12, "decimals": 2},
-        "p0_in": {"name": "p0_in", "unit": "[kPa]", "width": 12, "decimals": 2},
-        "p_out": {"name": "p_out", "unit": "[kPa]", "width": 12, "decimals": 2},
-        "omega": {"name": "omega", "unit": "[RPM]", "width": 12, "decimals": 0},
-    }
-
-    # Create formatted header and unit strings using f-strings and field widths
-    header_str = f"{'Index':>{index_width}}"  # Start with "Index" header
-    unit_str = f"{'':>{index_width}}"  # Start with empty string for unit alignment
-
-    for spec in field_specs.values():
-        header_str += f" {spec['name']:>{spec['width']}}"
-        unit_str += f" {spec['unit']:>{spec['width']}}"
-
-    # Append formatted strings to the output lines
-    output_lines.append(header_str)
-    output_lines.append(unit_str)
-
-    # Unit conversion functions
-    def convert_units(key, value):
-        if key == "T0_in":  # Convert Kelvin to Celsius
-            return value - 273.15
-        elif key == "omega":  # Convert rad/s to RPM
-            return (value * 60) / (2 * np.pi)
-        elif key == "alpha_in":  # Convert radians to degrees
-            return np.degrees(value)
-        elif key in ["p0_in", "p_out"]:  # Pa to kPa
-            return value / 1e3
-        return value
-
-    # Process and format each operation point
-    for index, op_point in enumerate(operation_points, start=1):
-        row = [f"{index:>{index_width}}"]
-        for key, spec in field_specs.items():
-            value = convert_units(key, op_point[key])
-            if isinstance(value, float):
-                # Format floats with the specified width and number of decimal places
-                row.append(f"{value:>{spec['width']}.{spec['decimals']}f}")
-            else:
-                # Format strings to the specified width without decimals
-                row.append(f"{value:>{spec['width']}}")
-        output_lines.append(" ".join(row))  # Ensure spaces between columns
-
-    output_lines.append("-" * length)  # Add a closing separator line
-
-    # Join the lines and print the output
-    formatted_output = "\n".join(output_lines)
-
-    for line in output_lines:
-        print(line)
-    return formatted_output
+import os
+import yaml
+import copy
+import time
+import datetime
+import itertools
+import numpy as np
+import pandas as pd
+import CoolProp as cp
+import matplotlib.pyplot as plt
+
+from .. import math
+from .. import pysolver_view as psv
+from .. import utilities as utils
+from .. import properties as props
+from . import geometry_model as geom
+from . import flow_model as flow
+
+SOLVER_MAP = {"lm": "Lavenberg-Marquardt", "hybr": "Powell's hybrid"}
+"""
+Available solvers for performance analysis.
+"""
+
+
+def get_heuristic_guess_input(n):
+    r"""
+
+    Generate a list of `n` number of different sets of dictionaries used to generate initial guesses.
+
+    Total-to-total efficiency, total-to-static efficiency, enthalpy loss fractions for each cascade and critical mach can be used to generate
+    an initial guess for performance analysis. This function generate a list of such different sets used to generate a full initial guess through the function `compute_heuristic_initial_guess`.
+
+    The total-to-static efficiency, varies between  0.6, 0.7, or 0.8, while the total-to-total efficiency can be 0.7, 0.8 or 0.9. The enthalpy loss fractions can either split equally between the cascade,
+    or with equal increments for neighbouring values (see `utils.fill_array_with_increment` for more information). The critical mach is assumed to be 0.95.
+
+    Parameters
+     ----------
+     n : int
+         Number of cascades.
+
+     Returns
+     -------
+     list
+         List of sets of variables used to generate an initial guess.
+
+    """
+
+    eta_ts_vec = [0.8, 0.7, 0.6]
+
+    array = utils.fill_array_with_increment(n)
+    enthalpy_distributions = []
+    enthalpy_distributions.append(np.ones(n) * 1 / n)
+    enthalpy_distributions.append(array)
+    enthalpy_distributions.append(np.flip(array))
+
+    initial_guesses = []
+    for eta_ts in eta_ts_vec:
+        for enthalpy_distribution in enthalpy_distributions:
+            initial_guesses.append(
+                {
+                    "enthalpy_loss_fractions": enthalpy_distribution,
+                    "eta_ts": eta_ts,
+                    "eta_tt": eta_ts + 0.1,
+                    "Ma_crit": 0.95,
+                }
+            )
+
+    return initial_guesses
+
+
+def compute_performance(
+    operation_points,
+    config,
+    initial_guess=None,
+    out_filename=None,
+    out_dir="output",
+    stop_on_failure=False,
+    export_results=True,
+):
+    r"""
+    Compute and export the performance of each specified operation point to an Excel file.
+
+    This function handles two types of input for operation points:
+
+        1. An explicit list of dictionaries, each detailing a specific operation point.
+        2. A dictionary where each key has a range of values, representing the cross-product of all possible operation points. It generates the Cartesian product of these ranges internally.
+
+    For each operation point, it computes performance based on the provided case data and compiles
+    the results into an Excel workbook with multiple sheets for various data sections.
+
+    The function validates the input operation points, and if they are given as ranges, it generates
+    all possible combinations. Performance is computed for each operation point, and the results are
+    then stored in a structured Excel file with separate sheets for each aspect of the data (e.g.,
+    overall, plane, cascade, stage, solver, and solution data).
+
+    The initial guess variable is used for the first operation point. If given, it must be a dictionary with the following keys:
+
+        - `enthalpy_loss_fractions`, which is a list containing the assumed fractions of enthalpy loss that occurs for each cascade.
+        - `eta_ts`, which is the assumed total-to-static efficiency.
+        - `eta_tt`, which is the assumed total-to-total efficiency.
+        - `Ma_crit`, which is the assumed critical mash number.
+
+    It can also be a dictionary containing the full set of initial guess that is provided directly to the solver. This
+    require care as the user must have a complete knowledge of the different variables, and setup, of the initial guess that must be given that
+    corresponds with the rest of the configuration file. If the initial guess is not given, it is set to a default value.
+    For subsequent operation points, the function employs a strategy to use the closest previously computed operation point's solution
+    as the initial guess. This approach is based on the heuristic that similar operation points have similar
+    performance characteristics, which can improve convergence speed and robustness of the solution process.
+    If the solution fails to converge, a set of initial guesses is provided to try other guesses (see `get_heuristic_guess_input`).
+
+    The function returns a list of solver object for each operation point. This contain information on both solver related performance (see psv.NonlinearSystemSolver)
+    and the object of the performance analysis problem (see CascadesNonlinearSystemProblem).
+
+    Parameters
+    ----------
+    operation_points : list of dict or dict
+        A list of operation points where each is a dictionary of parameters, or a dictionary of parameter
+        ranges from which operation points will be generated.
+    config : dict
+        A dictionary containing necessary configuration options for computing performance at each operation point.
+    initial_guess : optional
+        A dictionary with the required elements to generate an initial guess (see description above).
+    out_file : str, optional
+        The name for the output Excel file. If not provided, a default name with a timestamp is generated.
+    out_dir : str, optional
+        The directory where the Excel file will be saved. Defaults to 'output'.
+    stop_on_failure: bool, optional
+        If true, the analysis stops if the solution fails to converge for an operating point.
+    export_result : bool, optional
+        If true, the result is exported to an excel file.
+
+    Returns
+    -------
+    list
+        A List of solver object for each operation point.
+
+    """
+
+    # Check if geometry is provided
+    if config["geometry"] is None:
+        raise ValueError("Geometry is not provided")
+
+    # Check the type of operation_points argument
+    if isinstance(operation_points, dict):
+        # Convert ranges to a list of operation points
+        operation_points = generate_operation_points(operation_points)
+    elif not isinstance(operation_points, list):
+        msg = "operation_points must be either list of dicts or a dict with ranges."
+        raise TypeError(msg)
+
+    # Validate all operation points
+    for operation_point in operation_points:
+        validate_operation_point(operation_point)
+
+    # Initialize lists to hold dataframes for each operation point
+    operation_point_data = []
+    overall_data = []
+    plane_data = []
+    cascade_data = []
+    stage_data = []
+    solver_data = []
+    solution_data = []
+    geometry_data = []
+    solver_container = []
+
+    # Loop through all operation points
+    print_operation_points(operation_points)
+    for i, operation_point in enumerate(operation_points):
+        print()
+        print(f" Computing operation point {i+1} of {len(operation_points)}")
+        print_boundary_conditions(operation_point)
+
+        try:
+            # Define initial guess
+            if i == 0:
+                # Use default initial guess for the first operation point
+                if initial_guess == None:
+                    print("Using default initial guess")
+                else:
+                    print("Using user defined initial guess")
+            else:
+                closest_x, closest_index = find_closest_operation_point(
+                    operation_point,
+                    operation_points[:i],  # Use up to the previous point
+                    solution_data[:i],  # Use solutions up to the previous point
+                )
+                print(f" Using solution from point {closest_index+1} as initial guess")
+                initial_guess = closest_x
+
+            # Compute performance
+            solver, results = compute_single_operation_point(
+                operation_point, initial_guess, config
+            )
+
+            # Retrieve solver data
+            solver_status = {
+                "completed": True,
+                "success": solver.success,
+                "message": solver.message,
+                "grad_count": solver.convergence_history["grad_count"][-1],
+                "func_count": solver.convergence_history["func_count"][-1],
+                "func_count_total": solver.convergence_history["func_count_total"][-1],
+                "norm_residual": solver.convergence_history["norm_residual"][-1],
+                "norm_step": solver.convergence_history["norm_step"][-1],
+            }
+
+            # Collect results
+            operation_point_data.append(pd.DataFrame([operation_point]))
+            overall_data.append(results["overall"])
+            plane_data.append(utils.flatten_dataframe(results["plane"]))
+            cascade_data.append(utils.flatten_dataframe(results["cascade"]))
+            stage_data.append(utils.flatten_dataframe(results["stage"]))
+            geometry_data.append(utils.flatten_dataframe(results["geometry"]))
+            solver_data.append(pd.DataFrame([solver_status]))
+            solution_data.append(solver.problem.vars_real)
+            solver_container.append(solver)
+
+        except Exception as e:
+            if stop_on_failure:
+                raise Exception(e)
+            else:
+                print(f" Computation of point {i+1}/{len(operation_points)} failed")
+                print(f" Error: {e}")
+
+            # Retrieve solver data
+            solver = None
+            solver_status = {"completed": False}
+
+            # Collect data
+            operation_point_data.append(pd.DataFrame([operation_point]))
+            overall_data.append(pd.DataFrame([{}]))
+            plane_data.append(pd.DataFrame([{}]))
+            cascade_data.append(pd.DataFrame([{}]))
+            stage_data.append(pd.DataFrame([{}]))
+            geometry_data.append(pd.DataFrame([{}]))
+            solver_data.append(pd.DataFrame([solver_status]))
+            solution_data.append([])
+            solver_container.append(solver)
+
+    # Dictionary to hold concatenated dataframes
+    dfs = {
+        "operation point": pd.concat(operation_point_data, ignore_index=True),
+        "overall": pd.concat(overall_data, ignore_index=True),
+        "plane": pd.concat(plane_data, ignore_index=True),
+        "cascade": pd.concat(cascade_data, ignore_index=True),
+        "stage": pd.concat(stage_data, ignore_index=True),
+        "geometry": pd.concat(geometry_data, ignore_index=True),
+        "solver": pd.concat(solver_data, ignore_index=True),
+    }
+
+    # Add 'operation_point' column to each dataframe
+    for sheet_name, df in dfs.items():
+        df.insert(0, "operation_point", range(1, 1 + len(df)))
+
+    # Write dataframes to excel
+    if export_results:
+        # Create a directory to save simulation results
+        if not os.path.exists(out_dir):
+            os.makedirs(out_dir)
+
+        # Define filename with unique date-time identifier
+        if out_filename == None:
+            current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
+            out_filename = f"performance_analysis_{current_time}"
+
+        # Export simulation configuration as YAML file
+        config_data = {k: v for k, v in config.items() if v}  # Filter empty entries
+        config_data = utils.convert_numpy_to_python(config_data, precision=12)
+        config_file = os.path.join(out_dir, f"{out_filename}.yaml")
+        with open(config_file, "w") as file:
+            yaml.dump(config_data, file, default_flow_style=False, sort_keys=False)
+
+        # Export performance results in excel file
+        filepath = os.path.join(out_dir, f"{out_filename}.xlsx")
+        with pd.ExcelWriter(filepath, engine="openpyxl") as writer:
+            for sheet_name, df in dfs.items():
+                df.to_excel(writer, sheet_name=sheet_name, index=False)
+
+        print(f" Performance data successfully written to {filepath}")
+
+    # Print final report
+    print_simulation_summary(solver_container)
+
+    return solver_container
+
+
+def compute_single_operation_point(
+    operating_point,
+    initial_guess,
+    config,
+):
+    """
+    Compute an operation point for a given set of boundary conditions using multiple solver methods and initial guesses.
+
+    The initial guess make take three forms:
+
+        - None, which generate a default initial guess
+        - A dictionary which generates an initial guess through `compute_heuristic_initial_guess`. Required elements are:
+
+            - `enthalpy_loss_fractions`, which is a list containing the assumed fractions of enthalpy loss that occurs for each cascade.
+            - `eta_ts`, which is the assumed total-to-static efficiency.
+            - `eta_tt`, which is the assumed total-to-total efficiency.
+            - `Ma_crit`, which is the assumed critical mash number.
+
+        - A dictionary containing the full set of variables needed to evaluate turbine performance. This option require that the user has complete knowledge of what are the required variables, and the setup of the inital guess dictionary for the given configuration.
+
+    Parameters
+    ----------
+    boundary_conditions : dict
+        A dictionary containing boundary conditions for the operation point.
+    initial_guess : dict, optional
+        A dictionary with the required elements to generate an initial guess (see description above).
+    config : dict
+        A dictionary containing necessary configuration options for computing performance at the operational point.
+
+    Returns
+    -------
+    psv.NonlinearSystemSolver
+        The solution object containing the results of the operation point calculation.
+
+    """
+
+    # Initialize problem object
+    problem = CascadesNonlinearSystemProblem(config)
+    # TODO: A limitation of defining a new problem for each operation point is that the geometry generated and checked once for every point
+    # TODO: Performing the computations is not a big problem, but displaying the geometry report for every point can be very long.
+    # TODO: Perhaps we could add options of verbosity and perhaps only display the full geometry report when it fails
+    problem.update_boundary_conditions(operating_point)
+    solver_options = copy.deepcopy(config["performance_analysis"]["solver_options"])
+
+    initial_guesses = [initial_guess] + get_heuristic_guess_input(
+        problem.geometry["number_of_cascades"]
+    )
+    methods_to_try = [solver_options["method"]] + [
+        method for method in SOLVER_MAP.keys() if method != solver_options["method"]
+    ]
+
+    for initial_guess in initial_guesses:
+        for method in methods_to_try:
+            success = False
+            x0 = problem.get_initial_guess(
+                initial_guess
+            )  # TODO: Roberto_17.05.2023: It seems we are not using this value. Why take it as output then?
+            print(f" Trying to solve the problem using {SOLVER_MAP[method]} method")
+            solver_options["method"] = method
+
+            solver = psv.NonlinearSystemSolver(problem, **solver_options)
+            # TODO: Roberto: add the option to use optimizers as solver depending on the method specified?
+            # TODO: Roberto: at some point in the past we tried to solve the system of equations with SLSQP, right?
+
+            try: 
+                solver.solve(problem.x0)                
+                
+            except Exception as e:
+                print(f" Error during solving: {e}")
+                solver.success = False
+
+            if solver.success:
+                break
+        if solver.success:
+            break
+
+    if not solver.success:
+        print(" WARNING: All attempts failed to converge")
+        # TODO: Add messages to Log file
+
+    return solver, problem.results
+
+
+def find_closest_operation_point(current_op_point, operation_points, solution_data):
+    """
+    Find the solution vector and index of the closest operation point in the historical data.
+
+    Parameters
+    ----------
+    current_op_point : dict
+        The current operation point we want to compare.
+    operation_points : list of dict
+        A list of historical operation points to search through.
+    solution_data : list
+        A list of solution vectors corresponding to each operation point.
+
+    Returns
+    -------
+    tuple
+        A tuple containing the closest solution vector and the one-based index of the closest operation point.
+
+    """
+    min_distance = float("inf")
+    closest_point_x = None
+    closest_index = None
+
+    for i, op_point in enumerate(operation_points):
+        distance = get_operation_point_distance(current_op_point, op_point)
+        if distance < min_distance:
+            min_distance = distance
+            closest_point_x = solution_data[i]
+            closest_index = i
+
+    return closest_point_x, closest_index
+
+
+def get_operation_point_distance(point_1, point_2, delta=1e-8):
+    """
+    Calculate the normalized distance between two operation points, with special consideration
+    for angle measurements and prevention of division by zero for very small values.
+
+    Parameters
+    ----------
+    point_1 : dict
+        First operation point with numeric values.
+    point_2 : dict
+        Second operation point with numeric values.
+    delta : float, optional
+        A small constant to prevent division by zero. Default is 1e-8.
+
+    Returns
+    -------
+    float
+        The calculated normalized distance.
+    """
+    deviation_array = []
+    for key in point_1:
+        if isinstance(point_1[key], (int, float)) and key in point_2:
+            value_1 = point_1[key]
+            value_2 = point_2[key]
+
+            if key == "alpha_in":
+                # Handle angle measurements with absolute scale normalization
+                deviation = np.abs(value_1 - value_2) / 90
+            else:
+                # Compute the relative difference with protection against division by zero
+                max_val = max(abs(value_1), abs(value_2), delta)
+                deviation = abs(value_1 - value_2) / max_val
+
+            deviation_array.append(deviation)
+
+    # Calculate the two-norm of the deviations
+    return np.linalg.norm(deviation_array)
+
+
+def generate_operation_points(performance_map):
+    """
+    Generates a list of dictionaries representing all possible combinations of
+    operation points from a given performance map. The performance map is a
+    dictionary where keys represent parameter names and values are the ranges
+    of values for those parameters. The function ensures that the combinations
+    are generated such that the parameters related to pressure ('p0_in' and
+    'p_out') are the last ones to vary, effectively making them the first
+    parameters to sweep through in the operation points.
+
+    Parameters
+    ----------
+    - performance_map (dict): A dictionary with parameter names as keys and
+      lists of parameter values as values.
+
+    Returns
+    -------
+    - operation_points (list of dict): A list of dictionaries, each representing
+      a unique combination of parameters from the performance_map.
+    """
+
+    # Make sure all values in the performance_map are iterables
+    performance_map = {k: utils.ensure_iterable(v) for k, v in performance_map.items()}
+
+    # Reorder performance map keys so first sweep is always through pressure
+    priority_keys = ["p0_in", "p_out"]
+    other_keys = [k for k in performance_map.keys() if k not in priority_keys]
+    keys_order = other_keys + priority_keys
+    performance_map = {
+        k: performance_map[k] for k in keys_order if k in performance_map
+    }
+
+    # Create all combinations of operation points
+    keys, values = zip(*performance_map.items())
+    operation_points = [
+        dict(zip(keys, combination)) for combination in itertools.product(*values)
+    ]
+
+    return operation_points
+
+
+def validate_operation_point(op_point):
+    """
+    Validates that an operation point has exactly the required fields:
+    'fluid_name', 'p0_in', 'T0_in', 'p_out', 'alpha_in', 'omega'.
+
+    Parameters
+    ----------
+    op_point: dict
+        A dictionary representing an operation point.
+
+    Returns
+    -------
+    ValueError: If the dictionary does not contain the required fields or contains extra fields.
+    """
+    REQUIRED_FIELDS = {"fluid_name", "p0_in", "T0_in", "p_out", "alpha_in", "omega"}
+    fields = set(op_point.keys())
+    if fields != REQUIRED_FIELDS:
+        missing = REQUIRED_FIELDS - fields
+        extra = fields - REQUIRED_FIELDS
+        raise ValueError(
+            f"Operation point validation error: "
+            f"Missing fields: {missing}, Extra fields: {extra}"
+        )
+
+
+# ------------------------------------------------------------------------------------------ #
+# ------------------------------------------------------------------------------------------ #
+# ------------------------------------------------------------------------------------------ #
+
+
+class CascadesNonlinearSystemProblem(psv.NonlinearSystemProblem):
+    """
+    A class representing a nonlinear system problem for cascade analysis.
+
+    This class is designed for solving nonlinear systems of equations related to cascade analysis.
+    Derived classes must implement the `residual` method to evaluate the system of equations for a given set of decision variables.
+
+    Additionally, specific problem classes can define the `get_jacobian` method to compute Jacobians.
+    If this method is not present in the derived class, the solver will revert to using forward finite differences for Jacobian calculations.
+
+    Attributes
+    ----------
+    fluid : FluidCoolProp_2Phase
+        An instance of the FluidCoolProp_2Phase class representing the fluid properties.
+    results : dict
+        A dictionary to store results.
+    boundary_conditions : dict
+        A dictionary containing boundary condition data.
+    geometry : dict
+        A dictionary containing geometry-related data.
+    model_options : dict
+        A dictionary containing options related to the analysis model.
+    reference_values : dict
+        A dictionary containing reference values for calculations.
+    vars_scaled
+        A dicionary of scaled variables used to evaluate turbine performance.
+    vars_real
+        A dicionary of real variables used to evaluate turbine performance.
+
+    Methods
+    -------
+    get_values(x)
+        Evaluate the system of equations for a given set of decision variables.
+    update_boundary_conditions(operation_point)
+        Update the boundary conditions of the problem with the provided operation point.
+    scale_values(variables, to_normalized = False)
+        Convert values between normalized and real values.
+    get_initial_guess(initial_guess = None)
+        Determine the initial guess for the performance analysis based on the given parameters or default values.
+    compute_heuristic_initial_guess(enthalpy_loss_fractions, eta_tt, eta_ts, Ma_crit)
+        Compute the heuristic initial guess for the performance analysis based on the given parameters.
+    print_simulation_summary(solvers)
+        Print a formatted footer summarizing the performance of all operation points.
+    print_boundary_conditions(BC)
+        Print the boundary conditions.
+    print_operation_points(operation_points)
+        Prints a summary table of operation points scheduled for simulation.
+
+    Examples
+    --------
+    Here's an example of how to derive from `CascadesNonlinearSystemProblem`::
+
+        class MyCascadeProblem(CascadesNonlinearSystemProblem):
+            def get_values(self, x):
+                # Implement evaluation logic here
+                pass
+    """
+
+    def __init__(self, config):
+        """
+        Initialize a CascadesNonlinearSystemProblem.
+
+        Parameters
+        ----------
+        config : dict
+            A dictionary containing case-specific data.
+        """
+
+        # Process turbine geometry
+        geom.validate_turbine_geometry(config["geometry"])
+        self.geometry = geom.calculate_full_geometry(config["geometry"])
+        # self.geom_info = geom.check_turbine_geometry(self.geometry, display=True)
+
+        # Initialize other attributes
+        self.model_options = config["simulation_options"]
+        self.keys = []
+
+    def residual(self, x):
+        """
+        Evaluate the system of equations for a given set of decision variables.
+
+        Parameters
+        ----------
+        x : array-like
+            Vector of decision variables.
+
+        Returns
+        -------
+        numpy nd.array
+            An array containing residual values.
+        """
+
+        # Create dictionary of scaled variables
+        self.vars_scaled = dict(zip(self.keys, x))
+
+        # Create dictionary of real variables
+        self.vars_real = self.scale_values(self.vars_scaled, to_normalized=False)
+
+        # Evaluate cascade series
+        self.results = flow.evaluate_axial_turbine(
+            self.vars_scaled,
+            self.boundary_conditions,
+            self.geometry,
+            self.fluid,
+            self.model_options,
+            self.reference_values,
+        )
+
+        return np.array(list(self.results["residuals"].values()))
+
+    def update_boundary_conditions(self, operation_point):
+        """
+        Update the boundary conditions of the problem with the provided operation point.
+
+        This method updates the boundary conditions attributes used to evaluate the turbine performance.
+        It also initializes a Fluid object using the 'fluid_name' specified in the operation point.
+        The method computes additional properties and reference values like stagnation properties at
+        the inlet, exit static properties, spouting velocity, and reference mass flow rate.
+        These are stored in the object's internal state for further use in calculations.
+
+        Parameters
+        ----------
+        operation_point : dict
+            A dictionary containing the boundary conditions defining the operation point. It must include the following keys:
+
+            - `fluid_name` (str) : The name of the fluid to be used in the Fluid object.
+            - `T0_in` (float): The inlet temperature (in Kelvin).
+            - `p0_in` (float): The inlet pressure (in Pascals).
+            - `p_out` (float): The outlet pressure (in Pascals).
+            - `omega` (float): The rotational speed (in rad/s).
+            - `alpha_in` (float): The inlet flow angle (in degrees).
+
+        Returns
+        -------
+        None
+            This method does not return a value but updates the internal state of the object.
+
+        """
+
+        # Define current operating point
+        self.boundary_conditions = operation_point
+
+        # Initialize fluid object
+        self.fluid = props.Fluid(operation_point["fluid_name"], exceptions=True)
+
+        # Rename variables
+        p0_in = operation_point["p0_in"]
+        T0_in = operation_point["T0_in"]
+        p_out = operation_point["p_out"]
+
+        # Compute stagnation properties at inlet
+        state_in_stag = self.fluid.get_props(cp.PT_INPUTS, p0_in, T0_in)
+        h0_in = state_in_stag["h"]
+        s_in = state_in_stag["s"]
+
+        # Store the inlet stagnation (h,s) for the first stage
+        # TODO: Improve logic of implementation?
+        self.boundary_conditions["h0_in"] = h0_in
+        self.boundary_conditions["s_in"] = s_in
+
+        # Calculate exit static properties for a isentropic expansion
+        state_out_s = self.fluid.get_props(cp.PSmass_INPUTS, p_out, s_in)
+        h_isentropic = state_out_s["h"]
+        d_isentropic = state_out_s["d"]
+
+        # Calculate exit static properties for a isenthalpic expansion
+        state_out_h = self.fluid.get_props(cp.HmassP_INPUTS, h0_in, p_out)
+        s_isenthalpic = state_out_h["s"]
+
+        # Calculate spouting velocity
+        v0 = np.sqrt(2 * (h0_in - h_isentropic))
+
+        # Define a reference mass flow rate
+        A_out = self.geometry["A_out"][-1]
+        mass_flow_ref = A_out * v0 * d_isentropic
+
+        # Define reference_values
+        self.reference_values = {
+            "s_range": s_isenthalpic - s_in,
+            "s_min": s_in,
+            "v0": v0,
+            "h_out_s": h_isentropic,
+            "d_out_s": d_isentropic,
+            "mass_flow_ref": mass_flow_ref,
+            "angle_range": 180,
+            "angle_min": -90,
+        }
+
+        return
+
+    def scale_values(self, variables, to_normalized=True):
+        """
+        Convert values between normalized and real values.
+
+        Parameters
+        ----------
+        variables: dict
+            A dictionary containing values to be scaled.
+        to_real: bool
+            If True, scale to real values; if False, scale to normalized values.
+
+        Returns
+        -------
+        An array of values converted between scales.
+        """
+
+        # Load parameters
+        v0 = self.reference_values["v0"]
+        s_range = self.reference_values["s_range"]
+        s_min = self.reference_values["s_min"]
+        angle_range = self.reference_values["angle_range"]
+        angle_min = self.reference_values["angle_min"]
+
+        # Define dictionary of scaled values
+        scaled_variables = {}
+
+        for key, val in variables.items():
+            if key.startswith("v") or key.startswith("w"):
+                scaled_variables[key] = val / v0 if to_normalized else val * v0
+            elif key.startswith("s"):
+                scaled_variables[key] = (
+                    (val - s_min) / s_range if to_normalized else val * s_range + s_min
+                )
+            elif key.startswith("b"):
+                scaled_variables[key] = (
+                    (val - angle_min) / angle_range
+                    if to_normalized
+                    else val * angle_range + angle_min
+                )
+
+        return scaled_variables
+
+    def get_initial_guess(self, initial_guess=None):
+        """
+        Determine the initial guess for the performance analysis based on the given parameters or default values.
+
+        The given `initial_guess` make take three forms:
+
+            - None, which generate a default initial guess
+            - A dictionary which generates an initial guess through `compute_heuristic_initial_guess`. Required elements are:
+
+                - `enthalpy_loss_fractions`, which is a list containing the assumed fractions of enthalpy loss that occurs for each cascade.
+                - `eta_ts`, which is the assumed total-to-static efficiency.
+                - `eta_tt`, which is the assumed total-to-total efficiency.
+                - `Ma_crit`, which is the assumed critical mash number.
+
+            - A dictionary containing the full set of variables needed to evaluate turbine performance. This option require that the user has complete knowledge of what are the required variables, and the setup of the inital guess dictionary for the given configuration.
+
+        The initial guess is scaled in this function.
+
+        Parameters
+        ----------
+        initial_guess : dict, optional
+            A dictionary containing the initial guess parameters.
+
+        Returns
+        -------
+        dict
+            Initial guess for the performance analysis.
+
+        Raises
+        ------
+        ValueError
+            If the provided initial_guess is invalid or incompatible with the model options.
+
+        """
+
+        number_of_cascades = self.geometry["number_of_cascades"]
+
+        # Define initial guess
+        if isinstance(initial_guess, dict):
+            valid_keys_1 = ["enthalpy_loss_fractions", "eta_ts", "eta_tt", "Ma_crit"]
+            valid_keys_2 = [
+                "w_out",
+                "s_out",
+                "beta_out",
+                "v*_in",
+                "w*_throat",
+                "s*_throat",
+            ]
+            valid_keys_2 = ["v_in"] + [
+                f"{key}_{i+1}"
+                for i in range(number_of_cascades)
+                for key in valid_keys_2
+            ]
+            valid_keys_3 = [
+                key for key in valid_keys_2 if not key.startswith("v*_in")
+            ] + [f"beta*_throat_{i+1}" for i in range(number_of_cascades)]
+            valid_keys_4 = [
+                key
+                for key in valid_keys_2
+                if not (
+                    key.startswith("v*_in")
+                    or key.startswith("s*_throat")
+                    or key.startswith("beta*_throat")
+                )
+            ]
+
+            check = []
+            check.append(set(valid_keys_1) == set(list(initial_guess.keys())))
+            check.append(set(valid_keys_2) == set(list(initial_guess.keys())))
+            check.append(set(valid_keys_3) == set(list(initial_guess.keys())))
+            check.append(set(valid_keys_4) == set(list(initial_guess.keys())))
+
+            if check[0]:
+                enthalpy_loss_fractions = initial_guess["enthalpy_loss_fractions"]
+                eta_tt = initial_guess["eta_tt"]
+                eta_ts = initial_guess["eta_ts"]
+                Ma_crit = initial_guess["Ma_crit"]
+
+                # Check that eta_tt, eta_ts and Ma_crit is in reasonable range
+                for label, variable in zip(
+                    ["eta_tt", "eta_ts", "Ma_crit"], [eta_tt, eta_ts, Ma_crit]
+                ):
+                    if not 0 <= variable <= 1:
+                        raise ValueError(f"{label} should be between {0} and {1}.")
+
+                # Check if enthalpy_loss_fractions is a list or a NumPy array
+                if not isinstance(enthalpy_loss_fractions, (list, np.ndarray)):
+                    raise ValueError(
+                        "enthalpy_loss_fractions must be a list or NumPy array"
+                    )
+
+                # Check that enthalpy_loss_fractions is of the same length as the number_of_cascades
+                if len(enthalpy_loss_fractions) != number_of_cascades:
+                    raise ValueError(
+                        f"enthalpy_loss_fractions must be of length {number_of_cascades}"
+                    )
+
+                # Check that the sum of enthalpy loss fractions is 1
+                sum_fractions = np.sum(enthalpy_loss_fractions)
+                epsilon = 1e-8  # Small epsilon value for floating-point comparison
+                if not np.isclose(sum_fractions, 1, atol=epsilon):
+                    raise ValueError(
+                        f"Sum of enthalpy_loss_fractions must be 1 (now: {sum_fractions})."
+                    )
+
+                print(" Generating heuristic initial guess with given parameters")
+                initial_guess = self.compute_heuristic_initial_guess(
+                    enthalpy_loss_fractions, eta_tt, eta_ts, Ma_crit
+                )
+
+            elif check[1]:
+                # Check that set of input correspond with model option
+                if (
+                    not self.model_options["choking_model"]
+                    == "evaluate_cascade_critical"
+                ):
+                    raise ValueError(
+                        "Set of input corresponds with different choking_model (evaluate_cascade_critical)"
+                    )
+
+                # Check that all values are a number
+                if not all(
+                    isinstance(val, (int, float)) for val in initial_guess.values()
+                ):
+                    raise ValueError("All dictionary values must be a float or int")
+
+            elif check[2]:
+                # Check that set of input correspond with model option
+                if not self.model_options["choking_model"] == "evaluate_cascade_throat":
+                    raise ValueError(
+                        "Set of input corresponds with different choking_model (evaluate_cascade_throat)"
+                    )
+
+                # Check that all values are a number
+                if not all(
+                    isinstance(val, (int, float)) for val in initial_guess.values()
+                ):
+                    raise ValueError("All dictionary values must be a float or int")
+
+            elif check[3]:
+                # Check that set of input correspond with model option
+                if (
+                    not self.model_options["choking_model"]
+                    == "evaluate_cascade_isentropic_throat"
+                ):
+                    raise ValueError(
+                        "Set of input corresponds with different choking_model (evaluate_cascade_isentropic_throat)"
+                    )
+
+                # Check that all values are a number
+                if not all(
+                    isinstance(val, (int, float)) for val in initial_guess.values()
+                ):
+                    raise ValueError("All dictionary values must be a float or int")
+
+            else:
+                raise ValueError(
+                    f"Invalid keys provided for initial_guess. "
+                    f"Valid keys include either:"
+                    f"{valid_keys_1} \n"
+                    f"{valid_keys_2} \n"
+                    f"{valid_keys_3} \n"
+                )
+
+        elif initial_guess == None:
+            enthalpy_loss_fractions = np.full(
+                number_of_cascades, 1 / number_of_cascades
+            )
+            eta_tt = 0.9
+            eta_ts = 0.8
+            Ma_crit = 0.95
+
+            # Compute initial guess using several approximations
+            initial_guess = self.compute_heuristic_initial_guess(
+                enthalpy_loss_fractions, eta_tt, eta_ts, Ma_crit
+            )
+
+        else:
+            raise ValueError("Initial guess must be either None or a dictionary.")
+
+        if self.model_options["choking_model"] == "evaluate_cascade_throat":
+            initial_guess = {
+                key: val
+                for key, val in initial_guess.items()
+                if not key.startswith("v*_in")
+            }
+        elif self.model_options["choking_model"] == "evaluate_cascade_critical":
+            initial_guess = {
+                key: val
+                for key, val in initial_guess.items()
+                if not key.startswith("beta*_throat")
+            }
+        elif (
+            self.model_options["choking_model"] == "evaluate_cascade_isentropic_throat"
+        ):
+            initial_guess = {
+                key: val
+                for key, val in initial_guess.items()
+                if not (
+                    key.startswith("v*_in")
+                    or key.startswith("s*_throat")
+                    or key.startswith("beta*_throat")
+                )
+            }
+
+        # Always normalize initial guess
+        initial_guess_scaled = self.scale_values(initial_guess)
+
+        # Store labels
+        self.keys = initial_guess_scaled.keys()
+        self.x0 = np.array(list(initial_guess_scaled.values()))
+
+        return initial_guess_scaled
+
+    def compute_heuristic_initial_guess(
+        self, enthalpy_loss_fractions, eta_tt, eta_ts, Ma_crit
+    ):
+        """
+        Compute the heuristic initial guess for the performance analysis based on the given parameters.
+
+        This function calculates the heuristic initial guess based on the provided enthalpy loss fractions for each cascade,
+        total-to-static and total-to-total efficiencies, and critical Mach number.
+
+        Parameters
+        ----------
+        enthalpy_loss_fractions : array-like
+            Enthalpy loss fractions for each cascade.
+        eta_tt : float
+            Total-to-total efficiency.
+        eta_ts : float
+            Total-to-static efficiency.
+        Ma_crit : float
+            Critical Mach number.
+
+        Returns
+        -------
+        dict
+            Heuristic initial guess for the performance analysis.
+
+        """
+
+        # Load object attributes
+        geometry = self.geometry
+        boundary_conditions = self.boundary_conditions
+        fluid = self.fluid
+
+        # Rename variables
+        number_of_cascades = geometry["number_of_cascades"]
+        p0_in = boundary_conditions["p0_in"]
+        T0_in = boundary_conditions["T0_in"]
+        alpha_in = boundary_conditions["alpha_in"]
+        angular_speed = boundary_conditions["omega"]
+        p_out = boundary_conditions["p_out"]
+        h_out_s = self.reference_values["h_out_s"]
+
+        # Calculate inlet stagnation state
+        stagnation_properties_in = fluid.get_props(cp.PT_INPUTS, p0_in, T0_in)
+        h0_in = stagnation_properties_in["h"]
+        s_in = stagnation_properties_in["s"]
+        rho0_in = stagnation_properties_in["d"]
+
+        # Calculate exit enthalpy
+        h0_out = h0_in - eta_ts * (h0_in - h_out_s)
+        v_out = np.sqrt(2 * (h0_in - h_out_s - (h0_in - h0_out) / eta_tt))
+        h_out = h0_out - 0.5 * v_out**2
+
+        # Calculate exit static state for expansion with guessed efficiency
+        static_properties_exit = fluid.get_props(cp.HmassP_INPUTS, h_out, p_out)
+        s_out = static_properties_exit["s"]
+
+        # Define entropy distribution
+        entropy_distribution = np.linspace(s_in, s_out, number_of_cascades + 1)[1:]
+
+        # Define enthalpy distribution
+        total_enthalpy_loss = h0_in - h_out
+        enthalpy_loss_per_cascade = [
+            fraction * total_enthalpy_loss for fraction in enthalpy_loss_fractions
+        ]
+        enthalpy_distribution = [
+            h0_in - sum(enthalpy_loss_per_cascade[: i + 1])
+            for i in range(number_of_cascades)
+        ]
+
+        # Assums h0_in approx h_in for first inlet
+        h_in = h0_in
+
+        # Define initial guess dictionary
+        initial_guess = {}
+
+        for i in range(number_of_cascades):
+            geometry_cascade = {
+                key: values[i]
+                for key, values in geometry.items()
+                if key not in ["number_of_cascades", "number_of_stages"]
+            }
+
+            # Load enthalpy from initial guess
+            h_out = enthalpy_distribution[i]
+
+            # Load entropy from assumed entropy distribution
+            s_out = entropy_distribution[i]
+
+            # Rename necessary geometry
+            theta_in = geometry_cascade["leading_edge_angle"]
+            theta_out = geometry_cascade["gauging_angle"]
+            A_out = geometry_cascade["A_out"]
+            A_throat = geometry_cascade["A_throat"]
+            A_in = geometry_cascade["A_in"]
+            radius_mean_in = geometry_cascade["radius_mean_in"]
+            radius_mean_throat = geometry_cascade["radius_mean_throat"]
+            radius_mean_out = geometry_cascade["radius_mean_out"]
+
+            # Calculate rothalpy at inlet of cascade
+            blade_speed_in = angular_speed * (i % 2) * radius_mean_in
+            if i == 0:
+                h0_rel_in = h_in
+                m_temp = rho0_in * A_in * math.cosd(alpha_in)
+            else:
+                v_in = np.sqrt(2 * (h0_in - h_in))
+                velocity_triangle_in = flow.evaluate_velocity_triangle_in(
+                    blade_speed_in, v_in, alpha_in
+                )
+                w_in = velocity_triangle_in["w"]
+                h0_rel_in = h_in + 0.5 * w_in**2
+
+            rothalpy = h0_rel_in - 0.5 * blade_speed_in**2
+
+            # Calculate static state at cascade inlet
+            static_state_in = fluid.get_props(cp.HmassSmass_INPUTS, h_in, s_in)
+            rho_in = static_state_in["d"]
+
+            # Calculate exit velocity from rothalpy and enthalpy distirbution
+            blade_speed_out = angular_speed * (i % 2) * radius_mean_out
+            h0_rel_out = rothalpy + 0.5 * blade_speed_out**2
+            w_out = np.sqrt(2 * (h0_rel_out - h_out))
+            velocity_triangle_out = flow.evaluate_velocity_triangle_out(
+                blade_speed_out, w_out, theta_out
+            )
+            v_t_out = velocity_triangle_out["v_t"]
+            v_m_out = velocity_triangle_out["v_m"]
+            v_out = velocity_triangle_out["v"]
+            h0_out = h_out + 0.5 * v_out**2
+
+            # Calculate static state at cascade exit
+            static_state_out = fluid.get_props(cp.HmassSmass_INPUTS, h_out, s_out)
+            a_out = static_state_out["a"]
+            rho_out = static_state_out["d"]
+
+            # Calculate mass flow rate
+            mass_flow = rho_out * v_m_out * A_out
+
+            # Calculate throat velocity depending on subsonic or supersonic conditions
+            if w_out < a_out * Ma_crit:
+                w_throat = w_out
+                s_throat = s_out
+            else:
+                w_throat = a_out * Ma_crit
+                blade_speed_throat = angular_speed * (i % 2) * radius_mean_throat
+                h0_rel_throat = rothalpy + 0.5 * blade_speed_throat**2
+                h_throat = h0_rel_throat - 0.5 * w_throat**2
+                rho_throat = rho_out
+                static_state_throat = fluid.get_props(
+                    cp.DmassHmass_INPUTS, rho_throat, h_throat
+                )
+                s_throat = static_state_throat["s"]
+
+            # Calculate critical state
+            w_throat_crit = a_out * Ma_crit
+            h_throat_crit = (
+                h0_rel_in - 0.5 * w_throat_crit**2
+            )  # FIXME: h0_rel_in works less good?
+            static_state_throat_crit = fluid.get_props(
+                cp.HmassSmass_INPUTS, h_throat_crit, s_throat
+            )
+            rho_throat_crit = static_state_throat_crit["d"]
+            m_crit = w_throat_crit * math.cosd(theta_out) * rho_throat_crit * A_throat
+            w_m_in_crit = m_crit / rho_in / A_in
+            w_in_crit = w_m_in_crit / math.cosd(
+                theta_in
+            )  # XXX Works better with metal angle than inlet flow angle?
+            velocity_triangle_crit_in = flow.evaluate_velocity_triangle_out(
+                blade_speed_in, w_in_crit, theta_in
+            )
+            v_in_crit = velocity_triangle_crit_in["v"]
+
+            rho_out_crit = rho_throat_crit
+            w_out_crit = m_crit / (rho_out_crit * A_out * math.cosd(theta_out))
+
+            # Store initial guess
+            index = f"_{i+1}"
+            initial_guess.update(
+                {
+                    "w_out" + index: w_out,
+                    "s_out" + index: s_out,
+                    "beta_out"
+                    + index: np.sign(theta_out) * math.arccosd(A_throat / A_out),
+                    "v*_in" + index: v_in_crit,
+                    "beta*_throat"
+                    + index: np.sign(theta_out) * math.arccosd(A_throat / A_out),
+                    "w*_throat" + index: w_throat_crit,
+                    "s*_throat" + index: s_throat,
+                }
+            )
+
+            # Update variables for next cascade
+            if i != (number_of_cascades - 1):
+                A_next = geometry["A_in"][i + 1]
+                radius_mean_next = geometry["radius_mean_in"][i + 1]
+                v_m_in = v_m_out * A_out / A_next
+                v_t_in = v_t_out * radius_mean_out / radius_mean_next
+                v_in = np.sqrt(v_m_in**2 + v_t_in**2)
+                alpha_in = math.arctand(v_t_in / v_m_in)
+                h0_in = h0_out
+                h_in = h0_in - 0.5 * v_in**2
+                s_in = s_out
+
+        # Calculate inlet velocity from
+        initial_guess["v_in"] = mass_flow / m_temp
+
+        return initial_guess
+
+
+def print_simulation_summary(solvers):
+    """
+    Print a formatted footer summarizing the performance of all operation points.
+
+    This function processes a list of solver objects to provide a summary of the performance
+    analysis calculations. It calculates and displays the number of successful points and a summary of
+    simulation tme statistics. Additionally, it lists the indices of failed operation points, if any.
+
+    The function is robust against solvers that failed and lack certain attributes like 'elapsed_time'.
+    In such cases, these solvers are included in the count of failed operation points, but not in the
+    calculation time statistics.
+
+    Parameters
+    ----------
+    solvers : list
+        A list of solver objects. Each solver object should contain attributes related to the
+        calculation of an operation point, such as 'elapsed_time' and the 'solution' status.
+
+    """
+
+    # Initialize times list and track failed points
+    times = []
+    failed_points = []
+
+    for i, solver in enumerate(solvers):
+        # Check if the solver is not None and has the required attribute
+        if solver and hasattr(solver, "elapsed_time"):
+            times.append(solver.elapsed_time)
+            if not solver.success:
+                failed_points.append(i)
+        else:
+            # Handle failed solver or missing attributes
+            failed_points.append(i)
+
+    # Convert times to a numpy array for calculations
+    times = np.asarray(times)
+    total_points = len(solvers)
+
+    # Define footer content
+    width = 80
+    separator = "-" * width
+    lines_to_output = [
+        "",
+        separator,
+        "Final summary of performance analysis calculations".center(width),
+        separator,
+        f" Simulation successful for {total_points - len(failed_points)} out of {total_points} points",
+    ]
+
+    # Add failed points message only if there are failed points
+    if failed_points:
+        lines_to_output.append(
+            f"Failed operation points: {', '.join(map(str, failed_points))}"
+        )
+
+    # Add time statistics only if there are valid times
+    if times.size > 0:
+        lines_to_output.extend(
+            [
+                f" Average calculation time per operation point: {np.mean(times):.3f} seconds",
+                f" Minimum calculation time of all operation points: {np.min(times):.3f} seconds",
+                f" Maximum calculation time of all operation points: {np.max(times):.3f} seconds",
+                f" Total calculation time for all operation points: {np.sum(times):.3f} seconds",
+            ]
+        )
+    else:
+        lines_to_output.append(" No valid calculation times available.")
+
+    lines_to_output.append(separator)
+    lines_to_output.append("")
+
+    # Display to stdout
+    for line in lines_to_output:
+        print(line)
+
+
+def print_boundary_conditions(BC):
+    """
+    Print the boundary conditions.
+
+    This function prints the boundary conditions in a formatted manner. It takes a dictionary `BC`
+    containing the following keys:
+
+        - `fluid_name` (str): Name of the fluid.
+        - `alpha_in` (float): Flow angle at inlet in degrees.
+        - `T0_in` (float): Total temperature at inlet in Kelvin.
+        - `p0_in` (float): Total pressure at inlet in Pascal.
+        - `p_out` (float): Static pressure at outlet in Pascal.
+        - `omega` (float): Angular speed in radians per second.
+
+    Parameters
+    ----------
+    BC : dict
+        A dictionary containing the boundary conditions.
+
+    """
+
+    column_width = 25  # Adjust this to your desired width
+    print("-" * 80)
+    print(" Operating point: ")
+    print("-" * 80)
+    print(f" {'Fluid: ':<{column_width}} {BC['fluid_name']:<}")
+    print(f" {'Flow angle in: ':<{column_width}} {BC['alpha_in']:<.2f} deg")
+    print(f" {'Total temperature in: ':<{column_width}} {BC['T0_in']-273.15:<.2f} degC")
+    print(f" {'Total pressure in: ':<{column_width}} {BC['p0_in']/1e5:<.3f} bar")
+    print(f" {'Static pressure out: ':<{column_width}} {BC['p_out']/1e5:<.3f} bar")
+    print(f" {'Angular speed: ':<{column_width}} {BC['omega']*60/2/np.pi:<.1f} RPM")
+    print("-" * 80)
+    print()
+
+
+def print_operation_points(operation_points):
+    """
+    Prints a summary table of operation points scheduled for simulation.
+
+    This function takes a list of operation point dictionaries, formats them
+    according to predefined specifications, applies unit conversions where
+    necessary, and prints them in a neatly aligned table with headers and units.
+
+    Parameters
+    ----------
+    - operation_points (list of dict): A list where each dictionary contains
+      key-value pairs representing operation parameters and their corresponding
+      values.
+
+    Notes
+    -----
+    - This function assumes that all necessary keys exist within each operation
+      point dictionary.
+    - The function directly prints the output; it does not return any value.
+    - Unit conversions are hardcoded and specific to known parameters.
+    - If the units of the parameters change or if different parameters are added,
+      the unit conversion logic and `field_specs` need to be updated accordingly.
+    """
+    length = 80
+    index_width = 8
+    output_lines = [
+        "-" * length,
+        " Summary of operation points scheduled for simulation",
+        "-" * length,
+    ]
+
+    # Configuration for each field with specified width and decimal places
+    field_specs = {
+        "fluid_name": {"name": "Fluid", "unit": "", "width": 8},
+        "alpha_in": {"name": "angle_in", "unit": "[deg]", "width": 10, "decimals": 1},
+        "T0_in": {"name": "T0_in", "unit": "[degC]", "width": 12, "decimals": 2},
+        "p0_in": {"name": "p0_in", "unit": "[kPa]", "width": 12, "decimals": 2},
+        "p_out": {"name": "p_out", "unit": "[kPa]", "width": 12, "decimals": 2},
+        "omega": {"name": "omega", "unit": "[RPM]", "width": 12, "decimals": 0},
+    }
+
+    # Create formatted header and unit strings using f-strings and field widths
+    header_str = f"{'Index':>{index_width}}"  # Start with "Index" header
+    unit_str = f"{'':>{index_width}}"  # Start with empty string for unit alignment
+
+    for spec in field_specs.values():
+        header_str += f" {spec['name']:>{spec['width']}}"
+        unit_str += f" {spec['unit']:>{spec['width']}}"
+
+    # Append formatted strings to the output lines
+    output_lines.append(header_str)
+    output_lines.append(unit_str)
+
+    # Unit conversion functions
+    def convert_units(key, value):
+        if key == "T0_in":  # Convert Kelvin to Celsius
+            return value - 273.15
+        elif key == "omega":  # Convert rad/s to RPM
+            return (value * 60) / (2 * np.pi)
+        elif key == "alpha_in":  # Convert radians to degrees
+            return np.degrees(value)
+        elif key in ["p0_in", "p_out"]:  # Pa to kPa
+            return value / 1e3
+        return value
+
+    # Process and format each operation point
+    for index, op_point in enumerate(operation_points, start=1):
+        row = [f"{index:>{index_width}}"]
+        for key, spec in field_specs.items():
+            value = convert_units(key, op_point[key])
+            if isinstance(value, float):
+                # Format floats with the specified width and number of decimal places
+                row.append(f"{value:>{spec['width']}.{spec['decimals']}f}")
+            else:
+                # Format strings to the specified width without decimals
+                row.append(f"{value:>{spec['width']}}")
+        output_lines.append(" ".join(row))  # Ensure spaces between columns
+
+    output_lines.append("-" * length)  # Add a closing separator line
+
+    # Join the lines and print the output
+    formatted_output = "\n".join(output_lines)
+
+    for line in output_lines:
+        print(line)
+    return formatted_output
```

### Comparing `turboflow-0.1.2/turboflow/math.py` & `turboflow-0.1.3/turboflow/math.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,440 +1,440 @@
-import numpy as np
-
-
-def smooth_max(x, method="boltzmann", alpha=10, axis=None, keepdims=False):
-    r"""
-    Compute a smooth approximation to the maximum of an array using the specified method.
-
-    The p-norm approximation to the maximum is given by :cite:p:`weisstein_vector_2023`:
-
-    .. math::
-
-       f_{\alpha}(x) = \left( \sum_i x_i^\alpha \right)^{\frac{1}{\alpha}}
-
-    The Boltzmann approximation is given by :cite:p:`blanchard_accurately_2021`:
-
-    .. math::
-
-       f_{\alpha}(x) = \frac{1}{\alpha} \log \left( \sum_i e^{\alpha \,(x_i - \hat{x})} \right) + \hat{x}
-
-
-    This LogSumExp approximation is given by :cite:p:`blanchard_accurately_2021`:
-
-    .. math::
-
-       f_{\alpha}(x) = \frac{1}{\alpha} \log \left( \sum_i e^{\alpha \,(x_i - \hat{x})} \right) + \hat{x}
-
-    where the shift :math:`\hat{x}` is defined as:
-
-    .. math::
-
-       \hat{x} = \text{sign}(\alpha) \cdot \max(\text{sign}(\alpha) \cdot x)
-
-    Shifting `x` prevents numerical overflow due to the finite precision of floating-point operations
-
-    Parameters
-    ----------
-    x : array_like
-        Input data.
-    method : str, optional
-        Method to be used for the approximation. Supported methods are:
-
-        - ``boltzmann`` (default)
-        - ``logsumexp``
-        - ``p-norm``
-
-    alpha : float, optional
-        Sharpness parameter. Large values produce a tight approximation to the
-        maximum function
-    axis : None or int or tuple of ints, optional
-        Axis or axes along which to operate. By default, flattened input is
-        used. If this is a tuple of ints, the maximum is selected over
-        multiple axes, instead of a single axis or all the axes as before.
-    keepdims : bool, optional
-        If this is set to True, the axes which are reduced are left
-        in the result as dimensions with size one. With this option,
-        the result will broadcast correctly against the input array.
-
-    Returns
-    -------
-    max_approx : ndarray or scalar
-        Approximate maximum of `x` using the specified method.
-
-    Raises
-    ------
-    ValueError
-        If an unsupported method is specified.
-    """
-
-    if method == "logsumexp":
-        return _smooth_max_logsumexp(x, np.abs(alpha), axis, keepdims)
-    elif method == "boltzmann":
-        return _smooth_max_boltzmann(x, np.abs(alpha), axis, keepdims)
-    elif method == "p-norm":
-        return _smooth_max_pnorm(x, np.abs(alpha), axis, keepdims)
-    else:
-        raise ValueError(
-            f"Unsupported method '{method}'. Supported methods are:\n"
-            "- 'logsumexp'\n"
-            "- 'boltzmann'\n"
-            "- 'p-norm'"
-        )
-
-
-def smooth_min(x, method="boltzmann", alpha=10, axis=None, keepdims=False):
-    r"""
-    Compute a smooth approximation to the minimum of an array using the specified method.
-
-    The smooth minimum approximation is equivalent to the smooth maximum with a
-    negative value of the sharpness parameter :math:`\alpha`. See documentation
-    of the ```smooth_max()`` function for more information about the approximation
-    methods available.
-
-    Parameters
-    ----------
-    x : array_like
-        Input data.
-    method : str, optional
-        Method to be used for the approximation. Supported methods are:
-
-        - ``boltzmann`` (default)
-        - ``logsumexp``
-        - ``p-norm``
-
-    alpha : float, optional
-        Sharpness parameter.Large values produce a tight approximation to the
-        minimum function
-    axis : None or int or tuple of ints, optional
-        Axis or axes along which to operate. By default, flattened input is
-        used. If this is a tuple of ints, the minimum is selected over
-        multiple axes, instead of a single axis or all the axes as before.
-    keepdims : bool, optional
-        If this is set to True, the axes which are reduced are left
-        in the result as dimensions with size one. With this option,
-        the result will broadcast correctly against the input array.
-
-    Returns
-    -------
-    min_approx : ndarray or scalar
-        Approximate minimum of `x` using the specified method.
-
-    Raises
-    ------
-    ValueError
-        If an unsupported method is specified.
-    """
-
-    if method == "logsumexp":
-        return _smooth_max_logsumexp(x, -np.abs(alpha), axis, keepdims)
-    elif method == "boltzmann":
-        return _smooth_max_boltzmann(x, -np.abs(alpha), axis, keepdims)
-    elif method == "p-norm":
-        return _smooth_max_pnorm(x, -np.abs(alpha), axis, keepdims)
-    else:
-        raise ValueError(
-            f"Unsupported method '{method}'. Supported methods are:\n"
-            "- 'logsumexp'\n"
-            "- 'boltzmann'\n"
-            "- 'p-norm'"
-        )
-
-
-def _smooth_max_logsumexp(x, alpha, axis=None, keepdims=False):
-    """Smooth approximation to the maximum of an array using the log-sum-exp method"""
-
-    # Determine the shift for numerical stability
-    shift_value = np.sign(alpha) * np.max(np.sign(alpha) * x, axis=axis, keepdims=True)
-
-    # Compute log-sum-exp with the shift and scale by alpha
-    log_sum = np.log(
-        np.sum(np.exp(alpha * (x - shift_value)), axis=axis, keepdims=True)
-    )
-
-    # Normalize the result by alpha and correct for the shift
-    approx_max = (log_sum + alpha * shift_value) / alpha
-
-    # Handle keepdims
-    if not keepdims:
-        if isinstance(axis, tuple):
-            for ax in sorted(axis, reverse=True):
-                approx_max = np.squeeze(approx_max, axis=ax)
-        elif isinstance(axis, int):
-            approx_max = np.squeeze(approx_max, axis=axis)
-
-    return approx_max
-
-
-def _smooth_max_boltzmann(x, alpha, axis=None, keepdims=False):
-    """Smooth approximation to the maximum of an array using the Boltzmann weighted average"""
-
-    # Compute the shift for numerical stability
-    shift = np.sign(alpha) * np.max(np.sign(alpha) * x, axis=axis, keepdims=True)
-
-    # Compute the weighted sum (numerator) of the elements of x
-    weighted_sum = np.sum(x * np.exp(alpha * (x - shift)), axis=axis, keepdims=True)
-
-    # Compute the sum of the smooth_max weights (denominator)
-    weight_sum = np.sum(np.exp(alpha * (x - shift)), axis=axis, keepdims=True)
-
-    # Compute the Boltzmann-weighted average avoiding division by zero
-    max_approx = weighted_sum / (weight_sum + np.finfo(float).eps)
-
-    if not keepdims:
-        # Remove the dimensions of size one if keepdims is False
-        max_approx = np.squeeze(max_approx, axis=axis)
-
-    return max_approx
-
-
-def _smooth_max_pnorm(x, alpha, axis=None, keepdims=False):
-    """Smooth approximation to the maximum of an array using the p-norm method"""
-
-    # Compute the p-norm approximation
-    pnorm_approx = np.sum(np.power(x, alpha), axis=axis, keepdims=True) ** (1 / alpha)
-
-    if not keepdims:
-        # Remove the dimensions of size one if keepdims is False
-        pnorm_approx = np.squeeze(pnorm_approx, axis=axis)
-
-    return pnorm_approx
-
-
-def smooth_abs(x, method="quadratic", epsilon=1e-5):
-    r"""
-    Compute a smooth approximation of the absolute value function according to the specified method.
-
-    1. The quadratic approximation is given by :cite:p:`ramirez_x_2013`:
-
-    .. math::
-       f_{\epsilon}(x) = \sqrt{x^2 + \epsilon}
-
-    2. The hyperbolic tangent approximation is given by :cite:p:`bagul_smooth_2017`:
-
-    .. math::
-       f_{\epsilon}(x) = \epsilon \tanh{x / \epsilon}
-
-    3. The log-cosh approximation is given by :cite:p:`saleh_statistical_2022`:
-
-    .. math::
-       f_{\epsilon}(x) = \epsilon \log\left(\cosh\left(\frac{x}{\epsilon}\right)\right)
-
-    The quadratic method is the most computationally efficient, but also requires a smaller value of :math:`\epsilon` to yield a good approximation.
-    The transcendental methods give a better approximation to the absolute value function, at the expense of a higher computational time.
-
-    Parameters
-    ----------
-    x : float or np.ndarray
-        Input value(s) for which the approximation is computed.
-    method : str, optional
-        The method of approximation:
-
-        - ``quadratic`` (default)
-        - ``hyperbolic``
-        - ``logcosh``
-
-    epsilon : float, optional
-        A small positive constant affecting the approximation. Default is 1e-5.
-
-    Returns
-    -------
-    float or np.ndarray
-        Smooth approximation value(s) of the absolute function.
-
-    Raises
-    ------
-    ValueError
-        If an unsupported approximation method is provided.
-    """
-
-    if method == "quadratic":
-        return _smooth_abs_quadratic(x, epsilon)
-    elif method == "hyperbolic":
-        return _smooth_abs_tanh(x, epsilon)
-    elif method == "logarithmic":
-        return _smooth_abs_logcosh(x, epsilon)
-
-    else:
-        raise ValueError(
-            f"Unsupported method '{method}'. Supported methods are:\n"
-            "- 'quadratic'\n"
-            "- 'hyperbolic'\n"
-            "- 'logcosh'"
-        )
-
-
-def _smooth_abs_quadratic(x, epsilon):
-    """Quadratic approximation of the absolute value function."""
-    return np.sqrt(x**2 + epsilon)
-
-
-def _smooth_abs_tanh(x, epsilon):
-    """Hyperbolic tangent approximation of the absolute value function."""
-    return x * np.tanh(x / epsilon)
-
-
-def _smooth_abs_logcosh(x, epsilon):
-    """Log-cosh approximation of the absolute value function."""
-    return epsilon * np.log(np.cosh(x / epsilon))
-
-
-def sind(x):
-    """Compute the sine of an angle given in degrees."""
-    return np.sin(x * np.pi / 180)
-
-
-def cosd(x):
-    """Compute the cosine of an angle given in degrees."""
-
-    return np.cos(x * np.pi / 180)
-
-
-def tand(x):
-    """Compute the tangent of an angle given in degrees."""
-    return np.tan(x * np.pi / 180)
-
-
-def arcsind(x):
-    """Compute the arcsine of a value and return the result in degrees."""
-    return np.arcsin(x) * 180 / np.pi
-
-
-def arccosd(x):
-    """Compute the arccosine of a value and return the result in degrees."""
-    return np.arccos(x) * 180 / np.pi
-
-
-def arctand(x):
-    """Compute the arctangent of a value and return the result in degrees."""
-    return np.arctan(x) * 180 / np.pi
-
-
-def is_odd(number):
-    """Check if a number is odd. Returns True if the provided number is odd, and False otherwise."""
-    return number % 2 != 0
-
-
-def is_even(number):
-    """Check if a number is even. Returns True if the provided number is even, and False otherwise."""
-    return number % 2 == 0
-
-
-def all_numeric(array):
-    """Check if all items in Numpy array are numeric (floats or ints)"""
-    return np.issubdtype(array.dtype, np.number)
-
-
-def all_non_negative(array):
-    "Check if all items in Numpy array are non-negative"
-    return np.all(array >= 0)
-
-
-def sigmoid_hyperbolic(x, x0=0.5, alpha=1):
-    r"""
-    Compute the sigmoid hyperbolic function.
-
-    This function calculates a sigmoid function based on the hyperbolic tangent.
-    The formula is given by:
-
-    .. math::
-        \sigma(x) = \frac{1 + \tanh\left(\frac{x - x_0}{\alpha}\right)}{2}
-
-    Parameters
-    ----------
-    x : array_like
-        Input data.
-    x0 : float, optional
-        Center of the sigmoid function. Default is 0.5.
-    alpha : float, optional
-        Scale parameter. Default is 0.1.
-
-    Returns
-    -------
-    array_like
-        Output of the sigmoid hyperbolic function.
-
-    """
-    x = np.array(x)  # Ensure x is a NumPy array for vectorized operations
-    sigma = (1 + np.tanh((x - x0) / alpha)) / 2
-    return sigma
-
-
-def sigmoid_rational(x, n, m):
-    r"""
-    Compute the sigmoid rational function.
-
-    This function calculates a sigmoid function using an algebraic approach based on a rational function.
-    The formula is given by:
-
-    .. math::
-        \sigma(x) = \frac{x^n}{x^n + (1-x)^m}
-
-    Parameters
-    ----------
-    x : array_like
-        Input data.
-    n : int
-        Power of the numerator.
-    m : int
-        Power of the denominator.
-
-    Returns
-    -------
-    array_like
-        Output of the sigmoid algebraic function.
-
-    """
-    x = np.array(x)  # Ensure x is a NumPy array for vectorized operations
-    x = np.where(x < 0, 0, x)  # Set x to 0 where x < 0
-    x = np.where(x > 1, 1, x)  # Set x to 1 where x > 1
-    sigma = x**n / (x**n + (1 - x) ** m)
-    return sigma
-
-
-def sigmoid_smoothstep(x):
-    """
-    Compute the smooth step function.
-
-    This function calculates a smooth step function with zero first-order
-    derivatives at the endpoints. More information available at:
-    https://resources.wolframcloud.com/FunctionRepository/resources/SmoothStep/
-
-    Parameters
-    ----------
-    x : array_like
-        Input data.
-
-    Returns
-    -------
-    array_like
-        Output of the sigmoid smoothstep function.
-
-    """
-    x = np.array(x)  # Ensure x is a NumPy array for vectorized operations
-    x = np.where(x < 0, 0, x)  # Set x to 0 where x < 0
-    x = np.where(x > 1, 1, x)  # Set x to 1 where x > 1
-    return x**2 * (3 - 2 * x)
-
-
-def sigmoid_smootherstep(x):
-    """
-    Compute the smoother step function.
-
-    This function calculates a smoother step function with zero second-order
-    derivatives at the endpoints. It is a modification of the smoothstep
-    function to provide smoother transitions.
-
-    Parameters
-    ----------
-    x : array_like
-        Input data.
-
-    Returns
-    -------
-    array_like
-        Output of the sigmoid smootherstep function.
-
-    """
-    x = np.array(x)  # Ensure x is a NumPy array for vectorized operations
-    x = np.where(x < 0, 0, x)  # Set x to 0 where x < 0
-    x = np.where(x > 1, 1, x)  # Set x to 1 where x > 1
-    return 6 * x**5 - 15 * x**4 + 10 * x**3
+import numpy as np
+
+
+def smooth_max(x, method="boltzmann", alpha=10, axis=None, keepdims=False):
+    r"""
+    Compute a smooth approximation to the maximum of an array using the specified method.
+
+    The p-norm approximation to the maximum is given by :cite:p:`weisstein_vector_2023`:
+
+    .. math::
+
+       f_{\alpha}(x) = \left( \sum_i x_i^\alpha \right)^{\frac{1}{\alpha}}
+
+    The Boltzmann approximation is given by :cite:p:`blanchard_accurately_2021`:
+
+    .. math::
+
+       f_{\alpha}(x) = \frac{1}{\alpha} \log \left( \sum_i e^{\alpha \,(x_i - \hat{x})} \right) + \hat{x}
+
+
+    This LogSumExp approximation is given by :cite:p:`blanchard_accurately_2021`:
+
+    .. math::
+
+       f_{\alpha}(x) = \frac{1}{\alpha} \log \left( \sum_i e^{\alpha \,(x_i - \hat{x})} \right) + \hat{x}
+
+    where the shift :math:`\hat{x}` is defined as:
+
+    .. math::
+
+       \hat{x} = \text{sign}(\alpha) \cdot \max(\text{sign}(\alpha) \cdot x)
+
+    Shifting `x` prevents numerical overflow due to the finite precision of floating-point operations
+
+    Parameters
+    ----------
+    x : array_like
+        Input data.
+    method : str, optional
+        Method to be used for the approximation. Supported methods are:
+
+        - ``boltzmann`` (default)
+        - ``logsumexp``
+        - ``p-norm``
+
+    alpha : float, optional
+        Sharpness parameter. Large values produce a tight approximation to the
+        maximum function
+    axis : None or int or tuple of ints, optional
+        Axis or axes along which to operate. By default, flattened input is
+        used. If this is a tuple of ints, the maximum is selected over
+        multiple axes, instead of a single axis or all the axes as before.
+    keepdims : bool, optional
+        If this is set to True, the axes which are reduced are left
+        in the result as dimensions with size one. With this option,
+        the result will broadcast correctly against the input array.
+
+    Returns
+    -------
+    max_approx : ndarray or scalar
+        Approximate maximum of `x` using the specified method.
+
+    Raises
+    ------
+    ValueError
+        If an unsupported method is specified.
+    """
+
+    if method == "logsumexp":
+        return _smooth_max_logsumexp(x, np.abs(alpha), axis, keepdims)
+    elif method == "boltzmann":
+        return _smooth_max_boltzmann(x, np.abs(alpha), axis, keepdims)
+    elif method == "p-norm":
+        return _smooth_max_pnorm(x, np.abs(alpha), axis, keepdims)
+    else:
+        raise ValueError(
+            f"Unsupported method '{method}'. Supported methods are:\n"
+            "- 'logsumexp'\n"
+            "- 'boltzmann'\n"
+            "- 'p-norm'"
+        )
+
+
+def smooth_min(x, method="boltzmann", alpha=10, axis=None, keepdims=False):
+    r"""
+    Compute a smooth approximation to the minimum of an array using the specified method.
+
+    The smooth minimum approximation is equivalent to the smooth maximum with a
+    negative value of the sharpness parameter :math:`\alpha`. See documentation
+    of the ```smooth_max()`` function for more information about the approximation
+    methods available.
+
+    Parameters
+    ----------
+    x : array_like
+        Input data.
+    method : str, optional
+        Method to be used for the approximation. Supported methods are:
+
+        - ``boltzmann`` (default)
+        - ``logsumexp``
+        - ``p-norm``
+
+    alpha : float, optional
+        Sharpness parameter.Large values produce a tight approximation to the
+        minimum function
+    axis : None or int or tuple of ints, optional
+        Axis or axes along which to operate. By default, flattened input is
+        used. If this is a tuple of ints, the minimum is selected over
+        multiple axes, instead of a single axis or all the axes as before.
+    keepdims : bool, optional
+        If this is set to True, the axes which are reduced are left
+        in the result as dimensions with size one. With this option,
+        the result will broadcast correctly against the input array.
+
+    Returns
+    -------
+    min_approx : ndarray or scalar
+        Approximate minimum of `x` using the specified method.
+
+    Raises
+    ------
+    ValueError
+        If an unsupported method is specified.
+    """
+
+    if method == "logsumexp":
+        return _smooth_max_logsumexp(x, -np.abs(alpha), axis, keepdims)
+    elif method == "boltzmann":
+        return _smooth_max_boltzmann(x, -np.abs(alpha), axis, keepdims)
+    elif method == "p-norm":
+        return _smooth_max_pnorm(x, -np.abs(alpha), axis, keepdims)
+    else:
+        raise ValueError(
+            f"Unsupported method '{method}'. Supported methods are:\n"
+            "- 'logsumexp'\n"
+            "- 'boltzmann'\n"
+            "- 'p-norm'"
+        )
+
+
+def _smooth_max_logsumexp(x, alpha, axis=None, keepdims=False):
+    """Smooth approximation to the maximum of an array using the log-sum-exp method"""
+
+    # Determine the shift for numerical stability
+    shift_value = np.sign(alpha) * np.max(np.sign(alpha) * x, axis=axis, keepdims=True)
+
+    # Compute log-sum-exp with the shift and scale by alpha
+    log_sum = np.log(
+        np.sum(np.exp(alpha * (x - shift_value)), axis=axis, keepdims=True)
+    )
+
+    # Normalize the result by alpha and correct for the shift
+    approx_max = (log_sum + alpha * shift_value) / alpha
+
+    # Handle keepdims
+    if not keepdims:
+        if isinstance(axis, tuple):
+            for ax in sorted(axis, reverse=True):
+                approx_max = np.squeeze(approx_max, axis=ax)
+        elif isinstance(axis, int):
+            approx_max = np.squeeze(approx_max, axis=axis)
+
+    return approx_max
+
+
+def _smooth_max_boltzmann(x, alpha, axis=None, keepdims=False):
+    """Smooth approximation to the maximum of an array using the Boltzmann weighted average"""
+
+    # Compute the shift for numerical stability
+    shift = np.sign(alpha) * np.max(np.sign(alpha) * x, axis=axis, keepdims=True)
+
+    # Compute the weighted sum (numerator) of the elements of x
+    weighted_sum = np.sum(x * np.exp(alpha * (x - shift)), axis=axis, keepdims=True)
+
+    # Compute the sum of the smooth_max weights (denominator)
+    weight_sum = np.sum(np.exp(alpha * (x - shift)), axis=axis, keepdims=True)
+
+    # Compute the Boltzmann-weighted average avoiding division by zero
+    max_approx = weighted_sum / (weight_sum + np.finfo(float).eps)
+
+    if not keepdims:
+        # Remove the dimensions of size one if keepdims is False
+        max_approx = np.squeeze(max_approx, axis=axis)
+
+    return max_approx
+
+
+def _smooth_max_pnorm(x, alpha, axis=None, keepdims=False):
+    """Smooth approximation to the maximum of an array using the p-norm method"""
+
+    # Compute the p-norm approximation
+    pnorm_approx = np.sum(np.power(x, alpha), axis=axis, keepdims=True) ** (1 / alpha)
+
+    if not keepdims:
+        # Remove the dimensions of size one if keepdims is False
+        pnorm_approx = np.squeeze(pnorm_approx, axis=axis)
+
+    return pnorm_approx
+
+
+def smooth_abs(x, method="quadratic", epsilon=1e-5):
+    r"""
+    Compute a smooth approximation of the absolute value function according to the specified method.
+
+    1. The quadratic approximation is given by :cite:p:`ramirez_x_2013`:
+
+    .. math::
+       f_{\epsilon}(x) = \sqrt{x^2 + \epsilon}
+
+    2. The hyperbolic tangent approximation is given by :cite:p:`bagul_smooth_2017`:
+
+    .. math::
+       f_{\epsilon}(x) = \epsilon \tanh{x / \epsilon}
+
+    3. The log-cosh approximation is given by :cite:p:`saleh_statistical_2022`:
+
+    .. math::
+       f_{\epsilon}(x) = \epsilon \log\left(\cosh\left(\frac{x}{\epsilon}\right)\right)
+
+    The quadratic method is the most computationally efficient, but also requires a smaller value of :math:`\epsilon` to yield a good approximation.
+    The transcendental methods give a better approximation to the absolute value function, at the expense of a higher computational time.
+
+    Parameters
+    ----------
+    x : float or np.ndarray
+        Input value(s) for which the approximation is computed.
+    method : str, optional
+        The method of approximation:
+
+        - ``quadratic`` (default)
+        - ``hyperbolic``
+        - ``logcosh``
+
+    epsilon : float, optional
+        A small positive constant affecting the approximation. Default is 1e-5.
+
+    Returns
+    -------
+    float or np.ndarray
+        Smooth approximation value(s) of the absolute function.
+
+    Raises
+    ------
+    ValueError
+        If an unsupported approximation method is provided.
+    """
+
+    if method == "quadratic":
+        return _smooth_abs_quadratic(x, epsilon)
+    elif method == "hyperbolic":
+        return _smooth_abs_tanh(x, epsilon)
+    elif method == "logarithmic":
+        return _smooth_abs_logcosh(x, epsilon)
+
+    else:
+        raise ValueError(
+            f"Unsupported method '{method}'. Supported methods are:\n"
+            "- 'quadratic'\n"
+            "- 'hyperbolic'\n"
+            "- 'logcosh'"
+        )
+
+
+def _smooth_abs_quadratic(x, epsilon):
+    """Quadratic approximation of the absolute value function."""
+    return np.sqrt(x**2 + epsilon)
+
+
+def _smooth_abs_tanh(x, epsilon):
+    """Hyperbolic tangent approximation of the absolute value function."""
+    return x * np.tanh(x / epsilon)
+
+
+def _smooth_abs_logcosh(x, epsilon):
+    """Log-cosh approximation of the absolute value function."""
+    return epsilon * np.log(np.cosh(x / epsilon))
+
+
+def sind(x):
+    """Compute the sine of an angle given in degrees."""
+    return np.sin(x * np.pi / 180)
+
+
+def cosd(x):
+    """Compute the cosine of an angle given in degrees."""
+
+    return np.cos(x * np.pi / 180)
+
+
+def tand(x):
+    """Compute the tangent of an angle given in degrees."""
+    return np.tan(x * np.pi / 180)
+
+
+def arcsind(x):
+    """Compute the arcsine of a value and return the result in degrees."""
+    return np.arcsin(x) * 180 / np.pi
+
+
+def arccosd(x):
+    """Compute the arccosine of a value and return the result in degrees."""
+    return np.arccos(x) * 180 / np.pi
+
+
+def arctand(x):
+    """Compute the arctangent of a value and return the result in degrees."""
+    return np.arctan(x) * 180 / np.pi
+
+
+def is_odd(number):
+    """Check if a number is odd. Returns True if the provided number is odd, and False otherwise."""
+    return number % 2 != 0
+
+
+def is_even(number):
+    """Check if a number is even. Returns True if the provided number is even, and False otherwise."""
+    return number % 2 == 0
+
+
+def all_numeric(array):
+    """Check if all items in Numpy array are numeric (floats or ints)"""
+    return np.issubdtype(array.dtype, np.number)
+
+
+def all_non_negative(array):
+    "Check if all items in Numpy array are non-negative"
+    return np.all(array >= 0)
+
+
+def sigmoid_hyperbolic(x, x0=0.5, alpha=1):
+    r"""
+    Compute the sigmoid hyperbolic function.
+
+    This function calculates a sigmoid function based on the hyperbolic tangent.
+    The formula is given by:
+
+    .. math::
+        \sigma(x) = \frac{1 + \tanh\left(\frac{x - x_0}{\alpha}\right)}{2}
+
+    Parameters
+    ----------
+    x : array_like
+        Input data.
+    x0 : float, optional
+        Center of the sigmoid function. Default is 0.5.
+    alpha : float, optional
+        Scale parameter. Default is 0.1.
+
+    Returns
+    -------
+    array_like
+        Output of the sigmoid hyperbolic function.
+
+    """
+    x = np.array(x)  # Ensure x is a NumPy array for vectorized operations
+    sigma = (1 + np.tanh((x - x0) / alpha)) / 2
+    return sigma
+
+
+def sigmoid_rational(x, n, m):
+    r"""
+    Compute the sigmoid rational function.
+
+    This function calculates a sigmoid function using an algebraic approach based on a rational function.
+    The formula is given by:
+
+    .. math::
+        \sigma(x) = \frac{x^n}{x^n + (1-x)^m}
+
+    Parameters
+    ----------
+    x : array_like
+        Input data.
+    n : int
+        Power of the numerator.
+    m : int
+        Power of the denominator.
+
+    Returns
+    -------
+    array_like
+        Output of the sigmoid algebraic function.
+
+    """
+    x = np.array(x)  # Ensure x is a NumPy array for vectorized operations
+    x = np.where(x < 0, 0, x)  # Set x to 0 where x < 0
+    x = np.where(x > 1, 1, x)  # Set x to 1 where x > 1
+    sigma = x**n / (x**n + (1 - x) ** m)
+    return sigma
+
+
+def sigmoid_smoothstep(x):
+    """
+    Compute the smooth step function.
+
+    This function calculates a smooth step function with zero first-order
+    derivatives at the endpoints. More information available at:
+    https://resources.wolframcloud.com/FunctionRepository/resources/SmoothStep/
+
+    Parameters
+    ----------
+    x : array_like
+        Input data.
+
+    Returns
+    -------
+    array_like
+        Output of the sigmoid smoothstep function.
+
+    """
+    x = np.array(x)  # Ensure x is a NumPy array for vectorized operations
+    x = np.where(x < 0, 0, x)  # Set x to 0 where x < 0
+    x = np.where(x > 1, 1, x)  # Set x to 1 where x > 1
+    return x**2 * (3 - 2 * x)
+
+
+def sigmoid_smootherstep(x):
+    """
+    Compute the smoother step function.
+
+    This function calculates a smoother step function with zero second-order
+    derivatives at the endpoints. It is a modification of the smoothstep
+    function to provide smoother transitions.
+
+    Parameters
+    ----------
+    x : array_like
+        Input data.
+
+    Returns
+    -------
+    array_like
+        Output of the sigmoid smootherstep function.
+
+    """
+    x = np.array(x)  # Ensure x is a NumPy array for vectorized operations
+    x = np.where(x < 0, 0, x)  # Set x to 0 where x < 0
+    x = np.where(x > 1, 1, x)  # Set x to 1 where x > 1
+    return 6 * x**5 - 15 * x**4 + 10 * x**3
```

### Comparing `turboflow-0.1.2/turboflow/plot_functions.py` & `turboflow-0.1.3/turboflow/plot_functions.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,707 +1,707 @@
-import os
-import numpy as np
-import pandas as pd
-import matplotlib.pyplot as plt
-
-
-def load_data(filename):
-    """
-    Load performance data from an Excel file.
-
-    This function imports an Excel file containing performance parameters. It reads the data into a dictionary
-    named 'performance_data' where each key corresponds to a sheet name in the Excel file, and each value is
-    a pandas DataFrame containing the data from that sheet. The data is rounded to avoid precision loss when
-    loading from Excel.
-
-    The excel file must contain the following sheets:
-
-        - `operation point`
-        - `plane`
-        - `cascade`
-        - `overall`
-
-    Parameters
-    ----------
-    filename : str
-        The name of the Excel file containing performance parameters.
-
-    Returns
-    -------
-    dict
-        A dictionary containing performance data, where keys are sheet names and values are DataFrames.
-
-    """
-
-    # Read excel file
-    performance_data = pd.read_excel(
-        filename,
-        sheet_name=[
-            "operation point",
-            "plane",
-            "cascade",
-            # "stage",
-            "overall",
-        ],
-    )
-
-    # Round off to ignore precision loss by loading data from excel
-    for key, df in performance_data.items():
-        performance_data[key] = df.round(10)
-
-    return performance_data
-
-
-def plot_lines(
-    performance_data,
-    x_key,
-    y_keys,
-    subsets=None,
-    fig=None,
-    ax=None,
-    xlabel="",
-    ylabel="",
-    title="",
-    labels=None,
-    filename=None,
-    outdir="figures",
-    stack=False,
-    color_map="viridis",
-    colors=None,
-    close_fig=False,
-    save_figs=False,
-    linestyles=None,
-    save_formats=[".png"],
-    legend_loc="best",
-):
-    """
-    Plot lines from performance data.
-
-    This function plots lines from performance data. It supports plotting multiple lines on the same axes
-    with customizable labels, colors, and linestyles. The resulting plot can be saved to a file if desired.
-
-    Parameters
-    ----------
-    performance_data : DataFrame
-        The performance data to plot.
-    x_key : str
-        Name of the column in performance_data to be used as x-axis values.
-    y_keys : list of str
-        Names of the columns in performance_data to be plotted.
-    subsets : list, optional
-        Name and value of subsets to plot from performance_data. First instance should be a string representing the column name, while the
-        remaining elemnts represet the values defining the subset. Default is None.
-    fig : matplotlib.figure.Figure, optional
-        An existing figure object. If None, a new figure is created.
-    ax : matplotlib.axes.Axes, optional
-        An existing axes object. If None, a new axes is created.
-    xlabel : str, optional
-        The label for the x-axis.
-    ylabel : str, optional
-        The label for the y-axis.
-    title : str, optional
-        The title of the plot.
-    labels : list of str, optional
-        Labels for the plotted lines. Default is None.
-    filename : str, optional
-        The filename for saving the figure. Default is None.
-    outdir : str, optional
-        The directory where figures should be saved. Default is 'figures'.
-    stack : bool, optional
-        Whether to stack the plotted lines. Default is False.
-    color_map : str, optional
-        The colormap used for the lines. Default is 'viridis'.
-    colors : list of str or colors, optional
-        Colors for the plotted lines. Default is None.
-    close_fig : bool, optional
-        Whether to close the figure after plotting. Default is False.
-    save_figs : bool, optional
-        Whether to save the figure. Default is False.
-    linestyles : list of str, optional
-        Linestyles for the plotted lines. Default is None.
-    save_formats : list of str, optional
-        File formats for saving the figure. Default is ['.png'].
-    legend_loc : str, optional
-        Location for the legend. Default is 'best'.
-
-    Returns
-    -------
-    matplotlib.figure.Figure
-        The figure object.
-    matplotlib.axes.Axes
-        The axes object.
-
-    """
-
-    # Create figure if not provided
-    if fig is None or ax is None:
-        fig, ax = plt.subplots(figsize=(6.4, 4.8))
-
-    # Specify title and axes labels
-    ax.set_title(title)
-    ax.set_xlabel(xlabel)
-    ax.set_ylabel(ylabel)
-
-    # Get data
-    if subsets == None:
-        x = get_lines(performance_data, x_key)
-        y = get_lines(performance_data, y_keys)
-    else:
-        x = get_lines(performance_data, x_key, subsets=subsets)
-        y = get_lines(performance_data, y_keys, subsets=subsets)
-
-    # Get colors
-    if colors == None:
-        colors = plt.get_cmap(color_map)(np.linspace(0.2, 1.0, len(y)))
-
-    # Get labels
-    if labels == None:
-        if subsets is not None:
-            labels = [f"{subsets[0]} = {subsets[i+1]}" for i in range(len(subsets) - 1)]
-        elif len(y_keys) > 1:
-            labels = [f"{y_keys[i]}" for i in range(len(y))]
-        else:
-            labels = y_keys
-
-    # Get linestyles
-    if linestyles == None:
-        linestyles = ["-"] * len(y)
-
-    # Plot figure
-    if stack == True:
-        ax.stackplot(x, y, labels=y_keys, colors=colors)
-
-        # Add edges by overlaying lines
-        y_arrays = [series.values for series in y]
-        cumulative_y = np.cumsum(y_arrays, axis=0)
-        for i, series in enumerate(y_arrays):
-            if i == 0:
-                ax.plot(x, series, color="black", linewidth=0.5)
-            ax.plot(x, cumulative_y[i], color="black", linewidth=0.5)
-    elif subsets is not None:
-        for i in range(len(y)):
-            ax.plot(
-                x[i], y[i], label=labels[i], color=colors[i], linestyle=linestyles[i]
-            )
-    else:
-        for i in range(len(y)):
-            ax.plot(x, y[i], label=labels[i], color=colors[i], linestyle=linestyles[i])
-
-    # Set margins to zero
-    ax.margins(x=0.01, y=0.01)
-
-    # Add legend
-    if len(y) > 1:
-        ax.legend(loc=legend_loc)
-    fig.tight_layout(pad=1, w_pad=None, h_pad=None)
-
-    # Create output directory if it does not exist
-    if not os.path.exists(outdir):
-        os.makedirs(outdir)
-
-    # Save figure
-    if save_figs:
-        base_filename = os.path.join(outdir, filename)
-        savefig_in_formats(fig, base_filename, formats=save_formats)
-
-    if close_fig:
-        plt.close(fig)
-
-    return fig, ax
-
-
-def get_lines(performance_data, column_name, subsets=None):
-    """
-    Retrieve lines of data from the specified column in `performance_data`.
-
-    This function returns a list of array from the specified column(s) of the performance data.
-    If no subset is specified, it returns lines covering all rows in `performance_data`. If a subset
-    is specified, it returns lines covering only the rows that match the specified subset.
-
-    Parameters
-    ----------
-    performance_data : DataFrame
-        The DataFrame containing the performance data.
-    column_name : str or list of str
-        The name(s) of the column(s) from which to retrieve data.
-    subsets : list, optional
-        Name and value of subsets to get data from performance_data. First instance should be a string representing the column name, while the
-        remaining elemnts represet the values defining the subset. Default is None.
-
-    Returns
-    -------
-    list
-        A list of arrays from the specified column(s) of performance_data.
-
-    """
-
-    # Get lines covering all rows in performance_data
-    if subsets == None:
-        # Get single line
-        if isinstance(column_name, str):
-            return get_column(performance_data, column_name)
-        # Get several columns
-        else:
-            lines = []
-            for column in column_name:
-                lines.append(get_column(performance_data, column))
-            return lines
-
-    subsets[1:] = [round(value, 10) for value in subsets[1:]]
-    # Get lines covering given subset
-    lines = []
-    for val in subsets[1:]:
-        if isinstance(column_name, str):
-            indices = get_subset(performance_data, subsets[0], val)
-            lines.append(get_column(performance_data, column_name)[indices])
-        else:
-            for column in column_name:
-                indices = get_subset(performance_data, subsets[0], val)
-                lines.append(get_column(performance_data, column)[indices])
-
-    return lines
-
-
-def find_column(performance_data, column_name):
-    """
-    Find the sheet containing the specified column in `performance_data`.
-
-    This function searches through all sheets in `performance_data` to find the one containing the specified column.
-    If the column is found, it returns the name of the sheet. If the column is not found, it raises an exception.
-
-    Parameters
-    ----------
-    performance_data : dict
-        A dictionary containing sheets of performance data, where keys are sheet names and values are DataFrames.
-    column_name : str
-        The name of the column to find.
-
-    Returns
-    -------
-    str
-        The name of the sheet containing the specified column.
-
-    Raises
-    ------
-    Exception
-        If the specified column is not found in any sheet of performance_data.
-
-    """
-
-    for key in performance_data.keys():
-        if any(element == column_name for element in performance_data[key].columns):
-            return key
-
-    raise Exception(f"Could not find column {column_name} in performance_data")
-
-
-def get_column(performance_data, column_name):
-    """
-    Retrieve a column of data from `performance_data`.
-
-    This function retrieves the specified column of data from `performance_data` by finding the sheet containing
-    the column using the `find_column` function. It then returns the column as a pandas Series.
-
-    Parameters
-    ----------
-    performance_data : dict
-        A dictionary containing sheets of performance data, where keys are sheet names and values are DataFrames.
-    column_name : str
-        The name of the column to retrieve.
-
-    Returns
-    -------
-    pandas.Series
-        The column of data corresponding to column_name.
-
-    """
-    sheet = find_column(performance_data, column_name)
-
-    return performance_data[sheet][column_name]
-
-
-def get_subset(performance_data, column_name, row_value):
-    """
-    Retrieve the index of rows in `performance_data` where `column_name` equals `row_value`.
-
-    This function retrieves the index of rows in `performance_data` where the specified column (`column_name`)
-    equals the specified value (`row_value`). It first finds the sheet containing the column using the
-    `find_column` function, then returns the index of rows where the column has the specified value.
-
-    Parameters
-    ----------
-    performance_data : dict
-        A dictionary containing sheets of performance data, where keys are sheet names and values are DataFrames.
-    column_name : str
-        The name of the column to search for `row_value`.
-    row_value : object
-        The value to match in `column_name`.
-
-    Returns
-    -------
-    pandas.Index
-        The index of rows where `column_name` equals `row_value`.
-
-    """
-    sheet = find_column(performance_data, column_name)
-
-    return performance_data[sheet][
-        performance_data[sheet][column_name] == row_value
-    ].index
-
-
-def savefig_in_formats(
-    fig, path_without_extension, formats=[".png", ".svg", ".pdf", ".eps"]
-):
-    """
-    Save a given matplotlib figure in multiple file formats.
-
-    Parameters
-    ----------
-    fig : matplotlib.figure.Figure
-        The figure object to be saved.
-    path_without_extension : str
-        The full path to save the figure excluding the file extension.
-    formats : list of str, optional
-        A list of string file extensions to specify which formats the figure should be saved in.
-        Default is ['.png', '.svg', '.pdf', ".eps"].
-
-    Examples
-    --------
-    >>> import matplotlib.pyplot as plt
-    >>> fig, ax = plt.subplots()
-    >>> ax.plot([0, 1], [0, 1])
-    >>> save_fig_in_formats(fig, "/path/to/figure/filename")
-
-    This will save the figure as "filename.png", "filename.svg", and "filename.pdf" in the "/path/to/figure/" directory.
-    """
-    for ext in formats:
-        fig.savefig(f"{path_without_extension}{ext}", bbox_inches="tight")
-
-
-def plot_axial_radial_plane(geometry):
-    r"""
-    Plot the turbine geometry in an axial-radial plane.
-
-    This function plots the turbine geometry in an axial-radial plane. It takes the turbine geometry data
-    as input, including the radii at the inner and outer hub and tip, the number of cascades, and the axial
-    chord lengths. It then constructs and displays the plot, which represents the turbine blades in the
-    axial-radial plane.
-
-    Parameters
-    ----------
-    geometry : dict
-        A dictionary containing turbine geometry data including:
-
-        - `radius_hub_in` (array-like) : Inner hub radii at each cascade.
-        - `radius_hub_out` (array-like) : Outer hub radii at each cascade.
-        - `radius_tip_in` (array-like) : Inner tip radii at each cascade.
-        - `radius_tip_out` (array-like) : Outer tip radii at each cascade.
-        - `number_of_cascades` (int) : Number of cascades in the turbine.
-        - `axial_chord` (array-like) : Axial chord lengths at each cascade.
-
-    Returns
-    -------
-    matplotlib.figure.Figure
-        The generated figure.
-    matplotlib.axes.Axes
-        The generated axes.
-
-    """
-
-    # Load data
-    radius_hub_in = geometry["radius_hub_in"]
-    radius_hub_out = geometry["radius_hub_out"]
-    radius_tip_in = geometry["radius_tip_in"]
-    radius_tip_out = geometry["radius_tip_out"]
-    number_of_cascades = geometry["number_of_cascades"]
-    axial_chord = geometry["axial_chord"]
-    tip_clearance = geometry["tip_clearance"]
-
-    # Define unspecified geometrical parameters
-    ct = 0.05  # Thickness of casing relative to maximum tip radius
-    cl = 1.2  # Length of casing to turbine
-    cs = 0.1  # Axial spacing between cascades relative to max axial chord
-
-    # Get x-points
-    dx = cs * max(axial_chord)
-    x = np.array([])
-    for i in range(len(axial_chord)):
-        x = np.append(x, np.sum(axial_chord[0 : i + 1]) + i * dx)
-        x = np.append(x, np.sum(axial_chord[0 : i + 1]) + (i + 1) * dx)
-    x[1:] = x[0:-1]
-    x[0] = 0
-
-    # Get tip and hub points
-    y_hub = np.array(
-        [val for pair in zip(radius_hub_in, radius_hub_out) for val in pair]
-    )
-    y_tip = np.array(
-        [val for pair in zip(radius_tip_in, radius_tip_out) for val in pair]
-    )
-
-    # Initialize figure and axis object
-    fig, ax = plt.subplots()
-
-    # Plot upper casing
-    tip_clearance_extended = np.array(
-        [val for pair in zip(tip_clearance, tip_clearance) for val in pair]
-    )  # Extend tip clearance vector
-    casing_thickness = max(y_tip) * ct
-    x_casing = x
-    x_casing = np.insert(x_casing, 0, x[0] - (cl - 1) * x[-1] / 2)
-    x_casing = np.append(x_casing, x[-1] + (cl - 1) * x[-1] / 2)
-
-    y_casing_lower = y_tip + tip_clearance_extended
-    y_casing_lower = np.insert(y_casing_lower, 0, y_casing_lower[0])
-    y_casing_lower = np.append(y_casing_lower, y_casing_lower[-1])
-    y_casing_upper = np.ones(len(x_casing)) * (max(y_tip) + casing_thickness)
-
-    ax.plot(x_casing, y_casing_lower, "k")
-    ax.plot(x_casing, y_casing_upper, "k")
-    ax.plot([x_casing[0], x_casing[0]], [y_casing_lower[0], y_casing_upper[0]], "k")
-    ax.plot([x_casing[-1], x_casing[-1]], [y_casing_lower[-1], y_casing_upper[-1]], "k")
-    ax.fill_between(x_casing, y_casing_lower, y_casing_upper, color="0.5")
-
-    # Plot lower casing
-    tip_clearance_extended = np.array(
-        [
-            val
-            for pair in zip(np.flip(tip_clearance), np.flip(tip_clearance))
-            for val in pair
-        ]
-    )  # Extend tip clearance vector
-    y_casing_upper = y_hub - tip_clearance_extended
-    y_casing_upper = np.insert(y_casing_upper, 0, y_casing_upper[0])
-    y_casing_upper = np.append(y_casing_upper, y_casing_upper[-1])
-    y_casing_lower = y_casing_upper - casing_thickness
-
-    ax.plot(x_casing, y_casing_lower, "k")
-    ax.plot(x_casing, y_casing_upper, "k")
-    ax.plot([x_casing[0], x_casing[0]], [y_casing_lower[0], y_casing_upper[0]], "k")
-    ax.plot([x_casing[-1], x_casing[-1]], [y_casing_lower[-1], y_casing_upper[-1]], "k")
-    ax.fill_between(x_casing, y_casing_lower, y_casing_upper, color="0.5")
-
-    # Plot cascades
-    for i in range(number_of_cascades):
-        ax.plot(x[i * 2 : (i + 1) * 2], y_tip[i * 2 : (i + 1) * 2], "k")  # Plot tip
-        ax.plot(x[i * 2 : (i + 1) * 2], y_hub[i * 2 : (i + 1) * 2], "k")  # Plot hub
-        ax.plot(
-            [x[i * 2 : (i + 1) * 2][0], x[i * 2 : (i + 1) * 2][0]],
-            [y_hub[i * 2 : (i + 1) * 2][0], y_tip[i * 2 : (i + 1) * 2][0]],
-            "k",
-        )  # Plot inlet vertical line
-        ax.plot(
-            [x[i * 2 : (i + 1) * 2][1], x[i * 2 : (i + 1) * 2][1]],
-            [y_hub[i * 2 : (i + 1) * 2][1], y_tip[i * 2 : (i + 1) * 2][1]],
-            "k",
-        )  # Plot exit vertical line
-        ax.fill_between(
-            x[i * 2 : (i + 1) * 2],
-            y_hub[i * 2 : (i + 1) * 2],
-            y_tip[i * 2 : (i + 1) * 2],
-            color="1",
-        )  # Fill cascade with color
-
-    # Plot axis of rotation
-    plt.annotate(
-        "",
-        xy=(x[-1], 0),
-        xytext=(0, 0),
-        arrowprops=dict(facecolor="black", arrowstyle="-|>"),
-    )
-
-    ax.grid(False)
-    ax.tick_params(which="both", direction="out", right=False)
-    ax.tick_params(which="minor", left=False)
-    # ax.set_yticks([])
-    ax.set_xticks([])
-    ax.spines["top"].set_visible(False)
-    ax.spines["right"].set_visible(False)
-    ax.spines["bottom"].set_visible(False)
-    # ax.spines['left'].set_visible(False)
-    ax.set_xlabel("Axis of rotation")
-    ax.set_ylabel("Radius")
-    ax.set_aspect("equal")
-    lims = ax.get_ylim()
-    ax.set_ylim([0, lims[-1]])
-    plt.show()
-
-    return fig, ax
-
-
-def plot_velocity_triangles(plane):
-    """
-    Plot velocity triangles for each stage of a turbine.
-
-    Parameters
-    ----------
-    plane : dict
-        A dictionary containing velocity components for each plane of the turbine.
-        The dictionary should have the following keys:
-        - 'v_m': Meridional velocities.
-        - 'v_t': Tangential velocities.
-        - 'w_m': Meridional velocities of relative motion.
-        - 'w_t': Tangential velocities of relative motion.
-
-    Returns
-    -------
-    matplotlib.figure.Figure
-        The matplotlib figure object containing the velocity triangle plot.
-
-    matplotlib.axes._subplots.AxesSubplot
-        The matplotlib axis object containing the velocity triangle plot.
-
-    """
-
-    # Load variables
-    v_ms = plane["v_m"]
-    v_ts = plane["v_t"]
-    w_ms = plane["w_m"]
-    w_ts = plane["w_t"]
-
-    # Get maximum an minimum tangential velocities
-    max_w_t = max(np.array([val for pair in zip(v_ts, w_ts) for val in pair]))
-    min_w_t = min(np.array([val for pair in zip(v_ts, w_ts) for val in pair]))
-
-    # Nameing of planes (for marking)
-    plane_name = ["inlet", "exit"]
-
-    # Tuning options
-    dx = 0.1  # Spacing between triangles
-    hs = 10  # Horizontal spacing between line and symbol (absolute)
-    start = 0  # Starting point in y direction
-    fontsize = 12  # Fontsize
-
-    # Get rotor indexes
-    i_rotor = np.array(
-        [val for pair in zip(plane.index[2::4], plane.index[3::4]) for val in pair]
-    )
-
-    # initialize figure and axis
-    fig, ax = plt.subplots()
-
-    # Plot velocity triangles
-    for i in range(len(plane)):
-
-        v_m = v_ms[i]
-        v_t = v_ts[i]
-
-        w_m = w_ms[i]
-        w_t = w_ts[i]
-
-        # Define cascade name
-        cascade = "Stator"  # Changed if rotor (see below)
-
-        # Plot absolute velocity
-        ax.plot(
-            [0, 0], [start, start - v_m], color="k", linestyle=":"
-        )  # Plot meridional velocity
-        ax.plot(
-            [0, v_t], [start - v_m, start - v_m], color="k", linestyle=":"
-        )  # Plot tangntial velocity
-        ax.plot([0, v_t], [start, start - v_m], "k")  # Plot velocity
-        x_mid = v_t / 2
-        y_mid = (start + start - v_m) / 2
-        plt.annotate(
-            "",
-            xy=(x_mid, y_mid),
-            xytext=(0, start),
-            arrowprops=dict(facecolor="black", arrowstyle="-|>"),
-        )  # Arrow for absolute velocity
-        sign = -1 if x_mid < 0 else 1
-        ax.text(
-            x_mid + hs * sign,
-            y_mid,
-            f"$v$",
-            fontsize=fontsize,
-            ha="center",
-            va="bottom",
-            color="k",
-        )  # Symbol for absolute velocity
-
-        # Plot relative velocity
-        if i in i_rotor:
-
-            cascade = "Rotor"
-            ax.plot(
-                [0, w_t], [start - w_m, start - w_m], color="k", linestyle=":"
-            )  # Plot tangntial velocity
-            ax.plot([0, w_t], [start, start - w_m], color="k")  # Plot velocity
-            ax.plot([w_t, v_t], [start - w_m, start - w_m], "k")  # Plot blade speed
-            x_mid = w_t / 2
-            y_mid = (start + start - w_m) / 2
-            plt.annotate(
-                "",
-                xy=(x_mid, y_mid),
-                xytext=(0, start),
-                arrowprops=dict(facecolor="black", arrowstyle="-|>"),
-            )  # Arrow for relative velocity
-            sign = -1 if x_mid < 0 else 1
-            ax.text(
-                x_mid + hs * np.sign(x_mid),
-                y_mid,
-                f"$w$",
-                fontsize=fontsize,
-                ha="center",
-                va="bottom",
-                color="k",
-            )  # Symbol for relative velocity
-            x_mid = (w_t + v_t) / 2
-            y_mid = start - w_m
-            plt.annotate(
-                "",
-                xy=(x_mid, y_mid),
-                xytext=(w_t, start - w_m),
-                arrowprops=dict(facecolor="k", arrowstyle="-|>"),
-            )  # Arrow for blade speed
-            ax.text(
-                x_mid,
-                y_mid,
-                f"$u$",
-                fontsize=fontsize,
-                ha="center",
-                va="bottom",
-                color="k",
-            )  # Symbol for blade speed
-
-        # Add explanatory text
-        if w_t < 0:
-            ax.text(
-                max_w_t / 2,
-                start - v_m / 2,
-                f"{cascade} {plane_name[i%2]}",
-                fontsize=fontsize,
-                ha="center",
-                va="center",
-                color="k",
-            )
-        else:
-            ax.text(
-                min_w_t / 2,
-                start - v_m / 2,
-                f"{cascade} {plane_name[i%2]}",
-                fontsize=fontsize,
-                ha="center",
-                va="center",
-                color="k",
-            )
-
-        # Plot line diciding the different triangles
-        ax.plot(
-            [min_w_t, max_w_t], [start - w_m - dx * v_m, start - w_m - dx * v_m], "k:"
-        )
-
-        # Update starting point for the next velocity triangle
-        start = start - w_m - 2 * dx * v_m
-
-    # Set plot options
-    ax.grid(False)
-    ax.set_yticks([])
-    ax.set_xticks([])
-    ax.spines["top"].set_visible(False)
-    ax.spines["right"].set_visible(False)
-    ax.spines["bottom"].set_visible(False)
-    ax.spines["left"].set_visible(False)
-    plt.show()
-
-    return fig, ax
+import os
+import numpy as np
+import pandas as pd
+import matplotlib.pyplot as plt
+
+
+def load_data(filename):
+    """
+    Load performance data from an Excel file.
+
+    This function imports an Excel file containing performance parameters. It reads the data into a dictionary
+    named 'performance_data' where each key corresponds to a sheet name in the Excel file, and each value is
+    a pandas DataFrame containing the data from that sheet. The data is rounded to avoid precision loss when
+    loading from Excel.
+
+    The excel file must contain the following sheets:
+
+        - `operation point`
+        - `plane`
+        - `cascade`
+        - `overall`
+
+    Parameters
+    ----------
+    filename : str
+        The name of the Excel file containing performance parameters.
+
+    Returns
+    -------
+    dict
+        A dictionary containing performance data, where keys are sheet names and values are DataFrames.
+
+    """
+
+    # Read excel file
+    performance_data = pd.read_excel(
+        filename,
+        sheet_name=[
+            "operation point",
+            "plane",
+            "cascade",
+            # "stage",
+            "overall",
+        ],
+    )
+
+    # Round off to ignore precision loss by loading data from excel
+    for key, df in performance_data.items():
+        performance_data[key] = df.round(10)
+
+    return performance_data
+
+
+def plot_lines(
+    performance_data,
+    x_key,
+    y_keys,
+    subsets=None,
+    fig=None,
+    ax=None,
+    xlabel="",
+    ylabel="",
+    title="",
+    labels=None,
+    filename=None,
+    outdir="figures",
+    stack=False,
+    color_map="viridis",
+    colors=None,
+    close_fig=False,
+    save_figs=False,
+    linestyles=None,
+    save_formats=[".png"],
+    legend_loc="best",
+):
+    """
+    Plot lines from performance data.
+
+    This function plots lines from performance data. It supports plotting multiple lines on the same axes
+    with customizable labels, colors, and linestyles. The resulting plot can be saved to a file if desired.
+
+    Parameters
+    ----------
+    performance_data : DataFrame
+        The performance data to plot.
+    x_key : str
+        Name of the column in performance_data to be used as x-axis values.
+    y_keys : list of str
+        Names of the columns in performance_data to be plotted.
+    subsets : list, optional
+        Name and value of subsets to plot from performance_data. First instance should be a string representing the column name, while the
+        remaining elemnts represet the values defining the subset. Default is None.
+    fig : matplotlib.figure.Figure, optional
+        An existing figure object. If None, a new figure is created.
+    ax : matplotlib.axes.Axes, optional
+        An existing axes object. If None, a new axes is created.
+    xlabel : str, optional
+        The label for the x-axis.
+    ylabel : str, optional
+        The label for the y-axis.
+    title : str, optional
+        The title of the plot.
+    labels : list of str, optional
+        Labels for the plotted lines. Default is None.
+    filename : str, optional
+        The filename for saving the figure. Default is None.
+    outdir : str, optional
+        The directory where figures should be saved. Default is 'figures'.
+    stack : bool, optional
+        Whether to stack the plotted lines. Default is False.
+    color_map : str, optional
+        The colormap used for the lines. Default is 'viridis'.
+    colors : list of str or colors, optional
+        Colors for the plotted lines. Default is None.
+    close_fig : bool, optional
+        Whether to close the figure after plotting. Default is False.
+    save_figs : bool, optional
+        Whether to save the figure. Default is False.
+    linestyles : list of str, optional
+        Linestyles for the plotted lines. Default is None.
+    save_formats : list of str, optional
+        File formats for saving the figure. Default is ['.png'].
+    legend_loc : str, optional
+        Location for the legend. Default is 'best'.
+
+    Returns
+    -------
+    matplotlib.figure.Figure
+        The figure object.
+    matplotlib.axes.Axes
+        The axes object.
+
+    """
+
+    # Create figure if not provided
+    if fig is None or ax is None:
+        fig, ax = plt.subplots(figsize=(6.4, 4.8))
+
+    # Specify title and axes labels
+    ax.set_title(title)
+    ax.set_xlabel(xlabel)
+    ax.set_ylabel(ylabel)
+
+    # Get data
+    if subsets == None:
+        x = get_lines(performance_data, x_key)
+        y = get_lines(performance_data, y_keys)
+    else:
+        x = get_lines(performance_data, x_key, subsets=subsets)
+        y = get_lines(performance_data, y_keys, subsets=subsets)
+
+    # Get colors
+    if colors == None:
+        colors = plt.get_cmap(color_map)(np.linspace(0.2, 1.0, len(y)))
+
+    # Get labels
+    if labels == None:
+        if subsets is not None:
+            labels = [f"{subsets[0]} = {subsets[i+1]}" for i in range(len(subsets) - 1)]
+        elif len(y_keys) > 1:
+            labels = [f"{y_keys[i]}" for i in range(len(y))]
+        else:
+            labels = y_keys
+
+    # Get linestyles
+    if linestyles == None:
+        linestyles = ["-"] * len(y)
+
+    # Plot figure
+    if stack == True:
+        ax.stackplot(x, y, labels=y_keys, colors=colors)
+
+        # Add edges by overlaying lines
+        y_arrays = [series.values for series in y]
+        cumulative_y = np.cumsum(y_arrays, axis=0)
+        for i, series in enumerate(y_arrays):
+            if i == 0:
+                ax.plot(x, series, color="black", linewidth=0.5)
+            ax.plot(x, cumulative_y[i], color="black", linewidth=0.5)
+    elif subsets is not None:
+        for i in range(len(y)):
+            ax.plot(
+                x[i], y[i], label=labels[i], color=colors[i], linestyle=linestyles[i]
+            )
+    else:
+        for i in range(len(y)):
+            ax.plot(x, y[i], label=labels[i], color=colors[i], linestyle=linestyles[i])
+
+    # Set margins to zero
+    ax.margins(x=0.01, y=0.01)
+
+    # Add legend
+    if len(y) > 1:
+        ax.legend(loc=legend_loc)
+    fig.tight_layout(pad=1, w_pad=None, h_pad=None)
+
+    # Create output directory if it does not exist
+    if not os.path.exists(outdir):
+        os.makedirs(outdir)
+
+    # Save figure
+    if save_figs:
+        base_filename = os.path.join(outdir, filename)
+        savefig_in_formats(fig, base_filename, formats=save_formats)
+
+    if close_fig:
+        plt.close(fig)
+
+    return fig, ax
+
+
+def get_lines(performance_data, column_name, subsets=None):
+    """
+    Retrieve lines of data from the specified column in `performance_data`.
+
+    This function returns a list of array from the specified column(s) of the performance data.
+    If no subset is specified, it returns lines covering all rows in `performance_data`. If a subset
+    is specified, it returns lines covering only the rows that match the specified subset.
+
+    Parameters
+    ----------
+    performance_data : DataFrame
+        The DataFrame containing the performance data.
+    column_name : str or list of str
+        The name(s) of the column(s) from which to retrieve data.
+    subsets : list, optional
+        Name and value of subsets to get data from performance_data. First instance should be a string representing the column name, while the
+        remaining elemnts represet the values defining the subset. Default is None.
+
+    Returns
+    -------
+    list
+        A list of arrays from the specified column(s) of performance_data.
+
+    """
+
+    # Get lines covering all rows in performance_data
+    if subsets == None:
+        # Get single line
+        if isinstance(column_name, str):
+            return get_column(performance_data, column_name)
+        # Get several columns
+        else:
+            lines = []
+            for column in column_name:
+                lines.append(get_column(performance_data, column))
+            return lines
+
+    subsets[1:] = [round(value, 10) for value in subsets[1:]]
+    # Get lines covering given subset
+    lines = []
+    for val in subsets[1:]:
+        if isinstance(column_name, str):
+            indices = get_subset(performance_data, subsets[0], val)
+            lines.append(get_column(performance_data, column_name)[indices])
+        else:
+            for column in column_name:
+                indices = get_subset(performance_data, subsets[0], val)
+                lines.append(get_column(performance_data, column)[indices])
+
+    return lines
+
+
+def find_column(performance_data, column_name):
+    """
+    Find the sheet containing the specified column in `performance_data`.
+
+    This function searches through all sheets in `performance_data` to find the one containing the specified column.
+    If the column is found, it returns the name of the sheet. If the column is not found, it raises an exception.
+
+    Parameters
+    ----------
+    performance_data : dict
+        A dictionary containing sheets of performance data, where keys are sheet names and values are DataFrames.
+    column_name : str
+        The name of the column to find.
+
+    Returns
+    -------
+    str
+        The name of the sheet containing the specified column.
+
+    Raises
+    ------
+    Exception
+        If the specified column is not found in any sheet of performance_data.
+
+    """
+
+    for key in performance_data.keys():
+        if any(element == column_name for element in performance_data[key].columns):
+            return key
+
+    raise Exception(f"Could not find column {column_name} in performance_data")
+
+
+def get_column(performance_data, column_name):
+    """
+    Retrieve a column of data from `performance_data`.
+
+    This function retrieves the specified column of data from `performance_data` by finding the sheet containing
+    the column using the `find_column` function. It then returns the column as a pandas Series.
+
+    Parameters
+    ----------
+    performance_data : dict
+        A dictionary containing sheets of performance data, where keys are sheet names and values are DataFrames.
+    column_name : str
+        The name of the column to retrieve.
+
+    Returns
+    -------
+    pandas.Series
+        The column of data corresponding to column_name.
+
+    """
+    sheet = find_column(performance_data, column_name)
+
+    return performance_data[sheet][column_name]
+
+
+def get_subset(performance_data, column_name, row_value):
+    """
+    Retrieve the index of rows in `performance_data` where `column_name` equals `row_value`.
+
+    This function retrieves the index of rows in `performance_data` where the specified column (`column_name`)
+    equals the specified value (`row_value`). It first finds the sheet containing the column using the
+    `find_column` function, then returns the index of rows where the column has the specified value.
+
+    Parameters
+    ----------
+    performance_data : dict
+        A dictionary containing sheets of performance data, where keys are sheet names and values are DataFrames.
+    column_name : str
+        The name of the column to search for `row_value`.
+    row_value : object
+        The value to match in `column_name`.
+
+    Returns
+    -------
+    pandas.Index
+        The index of rows where `column_name` equals `row_value`.
+
+    """
+    sheet = find_column(performance_data, column_name)
+
+    return performance_data[sheet][
+        performance_data[sheet][column_name] == row_value
+    ].index
+
+
+def savefig_in_formats(
+    fig, path_without_extension, formats=[".png", ".svg", ".pdf", ".eps"]
+):
+    """
+    Save a given matplotlib figure in multiple file formats.
+
+    Parameters
+    ----------
+    fig : matplotlib.figure.Figure
+        The figure object to be saved.
+    path_without_extension : str
+        The full path to save the figure excluding the file extension.
+    formats : list of str, optional
+        A list of string file extensions to specify which formats the figure should be saved in.
+        Default is ['.png', '.svg', '.pdf', ".eps"].
+
+    Examples
+    --------
+    >>> import matplotlib.pyplot as plt
+    >>> fig, ax = plt.subplots()
+    >>> ax.plot([0, 1], [0, 1])
+    >>> save_fig_in_formats(fig, "/path/to/figure/filename")
+
+    This will save the figure as "filename.png", "filename.svg", and "filename.pdf" in the "/path/to/figure/" directory.
+    """
+    for ext in formats:
+        fig.savefig(f"{path_without_extension}{ext}", bbox_inches="tight")
+
+
+def plot_axial_radial_plane(geometry):
+    r"""
+    Plot the turbine geometry in an axial-radial plane.
+
+    This function plots the turbine geometry in an axial-radial plane. It takes the turbine geometry data
+    as input, including the radii at the inner and outer hub and tip, the number of cascades, and the axial
+    chord lengths. It then constructs and displays the plot, which represents the turbine blades in the
+    axial-radial plane.
+
+    Parameters
+    ----------
+    geometry : dict
+        A dictionary containing turbine geometry data including:
+
+        - `radius_hub_in` (array-like) : Inner hub radii at each cascade.
+        - `radius_hub_out` (array-like) : Outer hub radii at each cascade.
+        - `radius_tip_in` (array-like) : Inner tip radii at each cascade.
+        - `radius_tip_out` (array-like) : Outer tip radii at each cascade.
+        - `number_of_cascades` (int) : Number of cascades in the turbine.
+        - `axial_chord` (array-like) : Axial chord lengths at each cascade.
+
+    Returns
+    -------
+    matplotlib.figure.Figure
+        The generated figure.
+    matplotlib.axes.Axes
+        The generated axes.
+
+    """
+
+    # Load data
+    radius_hub_in = geometry["radius_hub_in"]
+    radius_hub_out = geometry["radius_hub_out"]
+    radius_tip_in = geometry["radius_tip_in"]
+    radius_tip_out = geometry["radius_tip_out"]
+    number_of_cascades = geometry["number_of_cascades"]
+    axial_chord = geometry["axial_chord"]
+    tip_clearance = geometry["tip_clearance"]
+
+    # Define unspecified geometrical parameters
+    ct = 0.05  # Thickness of casing relative to maximum tip radius
+    cl = 1.2  # Length of casing to turbine
+    cs = 0.1  # Axial spacing between cascades relative to max axial chord
+
+    # Get x-points
+    dx = cs * max(axial_chord)
+    x = np.array([])
+    for i in range(len(axial_chord)):
+        x = np.append(x, np.sum(axial_chord[0 : i + 1]) + i * dx)
+        x = np.append(x, np.sum(axial_chord[0 : i + 1]) + (i + 1) * dx)
+    x[1:] = x[0:-1]
+    x[0] = 0
+
+    # Get tip and hub points
+    y_hub = np.array(
+        [val for pair in zip(radius_hub_in, radius_hub_out) for val in pair]
+    )
+    y_tip = np.array(
+        [val for pair in zip(radius_tip_in, radius_tip_out) for val in pair]
+    )
+
+    # Initialize figure and axis object
+    fig, ax = plt.subplots()
+
+    # Plot upper casing
+    tip_clearance_extended = np.array(
+        [val for pair in zip(tip_clearance, tip_clearance) for val in pair]
+    )  # Extend tip clearance vector
+    casing_thickness = max(y_tip) * ct
+    x_casing = x
+    x_casing = np.insert(x_casing, 0, x[0] - (cl - 1) * x[-1] / 2)
+    x_casing = np.append(x_casing, x[-1] + (cl - 1) * x[-1] / 2)
+
+    y_casing_lower = y_tip + tip_clearance_extended
+    y_casing_lower = np.insert(y_casing_lower, 0, y_casing_lower[0])
+    y_casing_lower = np.append(y_casing_lower, y_casing_lower[-1])
+    y_casing_upper = np.ones(len(x_casing)) * (max(y_tip) + casing_thickness)
+
+    ax.plot(x_casing, y_casing_lower, "k")
+    ax.plot(x_casing, y_casing_upper, "k")
+    ax.plot([x_casing[0], x_casing[0]], [y_casing_lower[0], y_casing_upper[0]], "k")
+    ax.plot([x_casing[-1], x_casing[-1]], [y_casing_lower[-1], y_casing_upper[-1]], "k")
+    ax.fill_between(x_casing, y_casing_lower, y_casing_upper, color="0.5")
+
+    # Plot lower casing
+    tip_clearance_extended = np.array(
+        [
+            val
+            for pair in zip(np.flip(tip_clearance), np.flip(tip_clearance))
+            for val in pair
+        ]
+    )  # Extend tip clearance vector
+    y_casing_upper = y_hub - tip_clearance_extended
+    y_casing_upper = np.insert(y_casing_upper, 0, y_casing_upper[0])
+    y_casing_upper = np.append(y_casing_upper, y_casing_upper[-1])
+    y_casing_lower = y_casing_upper - casing_thickness
+
+    ax.plot(x_casing, y_casing_lower, "k")
+    ax.plot(x_casing, y_casing_upper, "k")
+    ax.plot([x_casing[0], x_casing[0]], [y_casing_lower[0], y_casing_upper[0]], "k")
+    ax.plot([x_casing[-1], x_casing[-1]], [y_casing_lower[-1], y_casing_upper[-1]], "k")
+    ax.fill_between(x_casing, y_casing_lower, y_casing_upper, color="0.5")
+
+    # Plot cascades
+    for i in range(number_of_cascades):
+        ax.plot(x[i * 2 : (i + 1) * 2], y_tip[i * 2 : (i + 1) * 2], "k")  # Plot tip
+        ax.plot(x[i * 2 : (i + 1) * 2], y_hub[i * 2 : (i + 1) * 2], "k")  # Plot hub
+        ax.plot(
+            [x[i * 2 : (i + 1) * 2][0], x[i * 2 : (i + 1) * 2][0]],
+            [y_hub[i * 2 : (i + 1) * 2][0], y_tip[i * 2 : (i + 1) * 2][0]],
+            "k",
+        )  # Plot inlet vertical line
+        ax.plot(
+            [x[i * 2 : (i + 1) * 2][1], x[i * 2 : (i + 1) * 2][1]],
+            [y_hub[i * 2 : (i + 1) * 2][1], y_tip[i * 2 : (i + 1) * 2][1]],
+            "k",
+        )  # Plot exit vertical line
+        ax.fill_between(
+            x[i * 2 : (i + 1) * 2],
+            y_hub[i * 2 : (i + 1) * 2],
+            y_tip[i * 2 : (i + 1) * 2],
+            color="1",
+        )  # Fill cascade with color
+
+    # Plot axis of rotation
+    plt.annotate(
+        "",
+        xy=(x[-1], 0),
+        xytext=(0, 0),
+        arrowprops=dict(facecolor="black", arrowstyle="-|>"),
+    )
+
+    ax.grid(False)
+    ax.tick_params(which="both", direction="out", right=False)
+    ax.tick_params(which="minor", left=False)
+    # ax.set_yticks([])
+    ax.set_xticks([])
+    ax.spines["top"].set_visible(False)
+    ax.spines["right"].set_visible(False)
+    ax.spines["bottom"].set_visible(False)
+    # ax.spines['left'].set_visible(False)
+    ax.set_xlabel("Axis of rotation")
+    ax.set_ylabel("Radius")
+    ax.set_aspect("equal")
+    lims = ax.get_ylim()
+    ax.set_ylim([0, lims[-1]])
+    plt.show()
+
+    return fig, ax
+
+
+def plot_velocity_triangles(plane):
+    """
+    Plot velocity triangles for each stage of a turbine.
+
+    Parameters
+    ----------
+    plane : dict
+        A dictionary containing velocity components for each plane of the turbine.
+        The dictionary should have the following keys:
+        - 'v_m': Meridional velocities.
+        - 'v_t': Tangential velocities.
+        - 'w_m': Meridional velocities of relative motion.
+        - 'w_t': Tangential velocities of relative motion.
+
+    Returns
+    -------
+    matplotlib.figure.Figure
+        The matplotlib figure object containing the velocity triangle plot.
+
+    matplotlib.axes._subplots.AxesSubplot
+        The matplotlib axis object containing the velocity triangle plot.
+
+    """
+
+    # Load variables
+    v_ms = plane["v_m"]
+    v_ts = plane["v_t"]
+    w_ms = plane["w_m"]
+    w_ts = plane["w_t"]
+
+    # Get maximum an minimum tangential velocities
+    max_w_t = max(np.array([val for pair in zip(v_ts, w_ts) for val in pair]))
+    min_w_t = min(np.array([val for pair in zip(v_ts, w_ts) for val in pair]))
+
+    # Nameing of planes (for marking)
+    plane_name = ["inlet", "exit"]
+
+    # Tuning options
+    dx = 0.1  # Spacing between triangles
+    hs = 10  # Horizontal spacing between line and symbol (absolute)
+    start = 0  # Starting point in y direction
+    fontsize = 12  # Fontsize
+
+    # Get rotor indexes
+    i_rotor = np.array(
+        [val for pair in zip(plane.index[2::4], plane.index[3::4]) for val in pair]
+    )
+
+    # initialize figure and axis
+    fig, ax = plt.subplots()
+
+    # Plot velocity triangles
+    for i in range(len(plane)):
+
+        v_m = v_ms[i]
+        v_t = v_ts[i]
+
+        w_m = w_ms[i]
+        w_t = w_ts[i]
+
+        # Define cascade name
+        cascade = "Stator"  # Changed if rotor (see below)
+
+        # Plot absolute velocity
+        ax.plot(
+            [0, 0], [start, start - v_m], color="k", linestyle=":"
+        )  # Plot meridional velocity
+        ax.plot(
+            [0, v_t], [start - v_m, start - v_m], color="k", linestyle=":"
+        )  # Plot tangntial velocity
+        ax.plot([0, v_t], [start, start - v_m], "k")  # Plot velocity
+        x_mid = v_t / 2
+        y_mid = (start + start - v_m) / 2
+        plt.annotate(
+            "",
+            xy=(x_mid, y_mid),
+            xytext=(0, start),
+            arrowprops=dict(facecolor="black", arrowstyle="-|>"),
+        )  # Arrow for absolute velocity
+        sign = -1 if x_mid < 0 else 1
+        ax.text(
+            x_mid + hs * sign,
+            y_mid,
+            f"$v$",
+            fontsize=fontsize,
+            ha="center",
+            va="bottom",
+            color="k",
+        )  # Symbol for absolute velocity
+
+        # Plot relative velocity
+        if i in i_rotor:
+
+            cascade = "Rotor"
+            ax.plot(
+                [0, w_t], [start - w_m, start - w_m], color="k", linestyle=":"
+            )  # Plot tangntial velocity
+            ax.plot([0, w_t], [start, start - w_m], color="k")  # Plot velocity
+            ax.plot([w_t, v_t], [start - w_m, start - w_m], "k")  # Plot blade speed
+            x_mid = w_t / 2
+            y_mid = (start + start - w_m) / 2
+            plt.annotate(
+                "",
+                xy=(x_mid, y_mid),
+                xytext=(0, start),
+                arrowprops=dict(facecolor="black", arrowstyle="-|>"),
+            )  # Arrow for relative velocity
+            sign = -1 if x_mid < 0 else 1
+            ax.text(
+                x_mid + hs * np.sign(x_mid),
+                y_mid,
+                f"$w$",
+                fontsize=fontsize,
+                ha="center",
+                va="bottom",
+                color="k",
+            )  # Symbol for relative velocity
+            x_mid = (w_t + v_t) / 2
+            y_mid = start - w_m
+            plt.annotate(
+                "",
+                xy=(x_mid, y_mid),
+                xytext=(w_t, start - w_m),
+                arrowprops=dict(facecolor="k", arrowstyle="-|>"),
+            )  # Arrow for blade speed
+            ax.text(
+                x_mid,
+                y_mid,
+                f"$u$",
+                fontsize=fontsize,
+                ha="center",
+                va="bottom",
+                color="k",
+            )  # Symbol for blade speed
+
+        # Add explanatory text
+        if w_t < 0:
+            ax.text(
+                max_w_t / 2,
+                start - v_m / 2,
+                f"{cascade} {plane_name[i%2]}",
+                fontsize=fontsize,
+                ha="center",
+                va="center",
+                color="k",
+            )
+        else:
+            ax.text(
+                min_w_t / 2,
+                start - v_m / 2,
+                f"{cascade} {plane_name[i%2]}",
+                fontsize=fontsize,
+                ha="center",
+                va="center",
+                color="k",
+            )
+
+        # Plot line diciding the different triangles
+        ax.plot(
+            [min_w_t, max_w_t], [start - w_m - dx * v_m, start - w_m - dx * v_m], "k:"
+        )
+
+        # Update starting point for the next velocity triangle
+        start = start - w_m - 2 * dx * v_m
+
+    # Set plot options
+    ax.grid(False)
+    ax.set_yticks([])
+    ax.set_xticks([])
+    ax.spines["top"].set_visible(False)
+    ax.spines["right"].set_visible(False)
+    ax.spines["bottom"].set_visible(False)
+    ax.spines["left"].set_visible(False)
+    plt.show()
+
+    return fig, ax
```

### Comparing `turboflow-0.1.2/turboflow/properties/fluid_properties.py` & `turboflow-0.1.3/turboflow/properties/fluid_properties.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,1405 +1,1405 @@
-import numpy as np
-import matplotlib.pyplot as plt
-import CoolProp.CoolProp as CP
-import copy
-
-from ..pysolver_view import (
-    NonlinearSystemSolver,
-    NonlinearSystemProblem,
-    OptimizationProblem,
-    OptimizationSolver,
-)
-
-# Define property aliases
-PROPERTY_ALIAS = {
-    "P": "p",
-    "rho": "rhomass",
-    "d": "rhomass",
-    "u": "umass",
-    "h": "hmass",
-    "s": "smass",
-    "cv": "cvmass",
-    "cp": "cpmass",
-    "a": "speed_sound",
-    "Z": "compressibility_factor",
-    "mu": "viscosity",
-    "k": "conductivity",
-}
-
-# Dynamically add INPUTS fields to the module
-# for attr in dir(CP):
-#     if attr.endswith('_INPUTS'):
-#         globals()[attr] = getattr(CP, attr)
-
-# Statically add phase indices to the module (IDE autocomplete)
-iphase_critical_point = CP.iphase_critical_point
-iphase_gas = CP.iphase_gas
-iphase_liquid = CP.iphase_liquid
-iphase_not_imposed = CP.iphase_not_imposed
-iphase_supercritical = CP.iphase_supercritical
-iphase_supercritical_gas = CP.iphase_supercritical_gas
-iphase_supercritical_liquid = CP.iphase_supercritical_liquid
-iphase_twophase = CP.iphase_twophase
-iphase_unknown = CP.iphase_unknown
-
-# Statically add INPUT fields to the module (IDE autocomplete)
-QT_INPUTS = CP.QT_INPUTS
-PQ_INPUTS = CP.PQ_INPUTS
-QSmolar_INPUTS = CP.QSmolar_INPUTS
-QSmass_INPUTS = CP.QSmass_INPUTS
-HmolarQ_INPUTS = CP.HmolarQ_INPUTS
-HmassQ_INPUTS = CP.HmassQ_INPUTS
-DmolarQ_INPUTS = CP.DmolarQ_INPUTS
-DmassQ_INPUTS = CP.DmassQ_INPUTS
-PT_INPUTS = CP.PT_INPUTS
-DmassT_INPUTS = CP.DmassT_INPUTS
-DmolarT_INPUTS = CP.DmolarT_INPUTS
-HmolarT_INPUTS = CP.HmolarT_INPUTS
-HmassT_INPUTS = CP.HmassT_INPUTS
-SmolarT_INPUTS = CP.SmolarT_INPUTS
-SmassT_INPUTS = CP.SmassT_INPUTS
-TUmolar_INPUTS = CP.TUmolar_INPUTS
-TUmass_INPUTS = CP.TUmass_INPUTS
-DmassP_INPUTS = CP.DmassP_INPUTS
-DmolarP_INPUTS = CP.DmolarP_INPUTS
-HmassP_INPUTS = CP.HmassP_INPUTS
-HmolarP_INPUTS = CP.HmolarP_INPUTS
-PSmass_INPUTS = CP.PSmass_INPUTS
-PSmolar_INPUTS = CP.PSmolar_INPUTS
-PUmass_INPUTS = CP.PUmass_INPUTS
-PUmolar_INPUTS = CP.PUmolar_INPUTS
-HmassSmass_INPUTS = CP.HmassSmass_INPUTS
-HmolarSmolar_INPUTS = CP.HmolarSmolar_INPUTS
-SmassUmass_INPUTS = CP.SmassUmass_INPUTS
-SmolarUmolar_INPUTS = CP.SmolarUmolar_INPUTS
-DmassHmass_INPUTS = CP.DmassHmass_INPUTS
-DmolarHmolar_INPUTS = CP.DmolarHmolar_INPUTS
-DmassSmass_INPUTS = CP.DmassSmass_INPUTS
-DmolarSmolar_INPUTS = CP.DmolarSmolar_INPUTS
-DmassUmass_INPUTS = CP.DmassUmass_INPUTS
-DmolarUmolar_INPUTS = CP.DmolarUmolar_INPUTS
-
-# Define dictionary with dynamically generated fields
-PHASE_INDEX = {attr: getattr(CP, attr) for attr in dir(CP) if attr.startswith("iphase")}
-INPUT_PAIRS = {attr: getattr(CP, attr) for attr in dir(CP) if attr.endswith("_INPUTS")}
-PHASE_INDEX = sorted(PHASE_INDEX.items(), key=lambda x: x[1])
-INPUT_PAIRS = sorted(INPUT_PAIRS.items(), key=lambda x: x[1])
-
-
-def _generate_coolprop_input_table():
-    """Create table of input pairs as string to be copy-pasted in Sphinx documentation"""
-
-    inputs_table = ".. list-table:: CoolProp Input Mappings\n"
-    inputs_table += "   :widths: 50 30\n"
-    inputs_table += "   :header-rows: 1\n\n"
-    inputs_table += "   * - Input pair name\n"
-    inputs_table += "     - Input pair mapping\n"
-
-    for name, value in INPUT_PAIRS:
-        inputs_table += f"   * - {name}\n"
-        inputs_table += f"     - {value}\n"
-
-    return inputs_table
-
-
-def states_to_dict(states):
-    """
-    Convert a list of state objects into a dictionary.
-    Each key is a field name of the state objects, and each value is a NumPy array of all the values for that field.
-    """
-    state_dict = {}
-    for field in states[0].keys():
-        state_dict[field] = np.array([getattr(state, field) for state in states])
-    return state_dict
-
-
-def states_to_dict_2d(states_grid):
-    """
-    Convert a 2D list (grid) of state objects into a dictionary.
-    Each key is a field name of the state objects, and each value is a 2D NumPy array of all the values for that field.
-
-    Parameters
-    ----------
-    states_grid : list of list of objects
-        A 2D grid where each element is a state object with the same keys.
-
-    Returns
-    -------
-    dict
-        A dictionary where keys are field names and values are 2D arrays of field values.
-    """
-    state_dict_2d = {}
-    for i, row in enumerate(states_grid):
-        for j, state in enumerate(row):
-            for field in state.keys():
-                if field not in state_dict_2d:
-                    state_dict_2d[field] = np.empty(
-                        (len(states_grid), len(row)), dtype=object
-                    )
-                state_dict_2d[field][i, j] = getattr(state, field)
-
-    return state_dict_2d
-
-
-class FluidState:
-    """
-    A class representing the thermodynamic state of a fluid.
-
-    This class is used to store and access the properties of a fluid state.
-    Properties can be accessed directly as attributes (e.g., `fluid_state.p` for pressure)
-    or through dictionary-like access (e.g., `fluid_state['T']` for temperature).
-
-    Methods
-    -------
-    to_dict():
-        Convert the FluidState properties to a dictionary.
-    keys():
-        Return the keys of the FluidState properties.
-    items():
-        Return the items (key-value pairs) of the FluidState properties.
-
-    """
-
-    def __init__(self, fluid):
-        # Use an internal dictionary to store properties
-        self._properties = fluid.properties
-        self._properties["fluid_name"] = fluid.name
-        self._properties["converged"] = fluid.converged_flag
-        self._properties["identifier"] = fluid.identifier
-
-    def __getattr__(self, name):
-        # This method is called when an attribute is accessed
-        try:
-            return self._properties[name]
-        except KeyError:
-            raise AttributeError(f"Attribute '{name}' not found in FluidState.")
-
-    def __setattr__(self, name, value):
-        # This method is called when an attribute is set
-        if name == "_properties":
-            # Initialize the _properties dictionary
-            super().__setattr__(name, value)
-        else:
-            self._properties[name] = value
-
-    def __getitem__(self, key):
-        # This method is called when using dictionary-like access
-        try:
-            return self._properties[key]
-        except KeyError:
-            raise KeyError(
-                f"Key '{key}' not found in FluidState. Available keys: {', '.join(self._properties.keys())}"
-            )
-
-    def __setitem__(self, key, value):
-        # This method is called when using dictionary-like assignment
-        self._properties[key] = value
-
-    def __str__(self):
-        properties_str = "\n   ".join(
-            [f"{key}: {value}" for key, value in self._properties.items()]
-        )
-        return f"FluidState:\n   {properties_str}"
-
-    def to_dict(self):
-        return self._properties.copy()
-
-    def keys(self):
-        return self._properties.keys()
-
-    def items(self):
-        return self._properties.items()
-
-    def values(self):
-        return self._properties.values()
-
-
-class Fluid:
-    """
-    Represents a fluid with various thermodynamic properties computed via CoolProp.
-
-    This class provides a convenient interface to CoolProp for various fluid property calculations.
-
-    Properties can be accessed directly as attributes (e.g., `fluid.properties["p"]` for pressure)
-    or through dictionary-like access (e.g., `fluid.T` for temperature).
-
-    Critical and triple point properties are computed upon initialization and stored internally for convenience.
-
-    Attributes
-    ----------
-    name : str
-        Name of the fluid.
-    backend : str
-        Backend used for CoolProp, default is 'HEOS'.
-    exceptions : bool
-        Determines if exceptions should be raised during state calculations. Default is True.
-    converged_flag : bool
-        Flag indicating whether properties calculations converged.
-    properties : dict
-        Dictionary of various fluid properties. Accessible directly as attributes (e.g., `fluid.p` for pressure).
-    critical_point : FluidState
-        Properties at the fluid's critical point.
-    triple_point_liquid : FluidState
-        Properties at the fluid's triple point in the liquid state.
-    triple_point_vapor : FluidState
-        Properties at the fluid's triple point in the vapor state.
-
-    Methods
-    -------
-    set_state(input_type, prop_1, prop_2):
-        Set the thermodynamic state of the fluid using specified property inputs.
-
-    Examples
-    --------
-    Accessing properties:
-
-        - fluid.T - Retrieves temperature directly as an attribute.
-        - fluid.properties['p'] - Retrieves pressure through dictionary-like access.
-
-    Accessing critical point properties:
-
-        - fluid.critical_point.p - Retrieves critical pressure.
-        - fluid.critical_point['T'] - Retrieves critical temperature.
-
-    Accessing triple point properties:
-
-        - fluid.triple_point_liquid.h - Retrieves liquid enthalpy at the triple point.
-        - fluid.triple_point_vapor.s - Retrieves vapor entropy at the triple point.
-    """
-
-    def __init__(
-        self,
-        name,
-        backend="HEOS",
-        exceptions=True,
-        identifier=None,
-    ):
-        self.name = name
-        self.backend = backend
-        self._AS = CP.AbstractState(backend, name)
-        self.exceptions = exceptions
-        self.converged_flag = False
-        self.identifier = identifier
-        self.properties = {}
-
-        # Initialize variables
-        self.sat_liq = None
-        self.sat_vap = None
-        self.spinodal_liq = None
-        self.spinodal_vap = None
-        self.pseudo_critical_line = None
-        self.q_mesh = None
-        self.graphic_elements = {}
-
-        # Get critical and triple point properties
-        if self._AS.fluid_param_string("pure") == "true":
-            self.critical_point = self._compute_critical_point()
-            self.triple_point_liquid = self._compute_triple_point_liquid()
-            self.triple_point_vapor = self._compute_triple_point_vapor()
-
-        # Pressure and temperature limits
-        self.p_min = 1
-        self.p_max = self._AS.pmax()
-        self.T_min = self._AS.Tmin()
-        self.T_max = self._AS.Tmax()
-
-    def __getattr__(self, name):
-        if name in self.properties:
-            return self.properties[name]
-        raise AttributeError(f"'Fluid' object has no attribute '{name}'")
-
-    def _compute_critical_point(self):
-        """Calculate the properties at the critical point"""
-        rho_crit, T_crit = self._AS.rhomass_critical(), self._AS.T_critical()
-        self.set_state(DmassT_INPUTS, rho_crit, T_crit, generalize_quality=False)
-        return FluidState(self)
-
-    def _compute_triple_point_liquid(self):
-        """Calculate the properties at the triple point (liquid state)"""
-        self.set_state(QT_INPUTS, 0.00, self._AS.Ttriple(), generalize_quality=False)
-        return FluidState(self)
-
-    def _compute_triple_point_vapor(self):
-        """Calculate the properties at the triple point (vapor state)"""
-        self.set_state(QT_INPUTS, 1.00, self._AS.Ttriple(), generalize_quality=False)
-        return FluidState(self)
-
-    def get_props(self, input_type, prop_1, prop_2, generalize_quality=True):
-        return self.set_state(
-            input_type, prop_1, prop_2, generalize_quality=generalize_quality
-        )
-
-    def set_state(self, input_type, prop_1, prop_2, generalize_quality=True):
-        """
-        Set the thermodynamic state of the fluid based on input properties.
-
-        This method updates the thermodynamic state of the fluid in the CoolProp ``abstractstate`` object
-        using the given input properties. It then calculates either single-phase or two-phase
-        properties based on the current phase of the fluid.
-
-        If the calculation of properties fails, `converged_flag` is set to False, indicating an issue with
-        the property calculation. Otherwise, it's set to True.
-
-        Aliases of the properties are also added to the ``Fluid.properties`` dictionary for convenience.
-
-        Parameters
-        ----------
-        input_type : str or int
-            The variable pair used to define the thermodynamic state. This should be one of the
-            predefined input pairs in CoolProp, such as ``PT_INPUTS`` for pressure and temperature.
-            For all available input pairs, refer to :ref:`this list <module-input-pairs-table>`.
-        prop_1 : float
-            The first property value corresponding to the input type (e.g., pressure in Pa if the input
-            type is CP.PT_INPUTS).
-        prop_2 : float
-            The second property value corresponding to the input type (e.g., temperature in K if the input
-            type is CP.PT_INPUTS).
-
-        Returns
-        -------
-        dict
-            A dictionary of computed properties for the current state of the fluid. This includes both the
-            raw properties from CoolProp and any additional alias properties.
-
-        Raises
-        ------
-        Exception
-            If `throw_exceptions` attribute is set to True and an error occurs during property calculation,
-            the original exception is re-raised.
-
-
-        """
-        try:
-            # Update Coolprop thermodynamic state
-            self._AS.update(input_type, prop_1, prop_2)
-
-            if self._AS.fluid_param_string("pure") == "false":
-                generalize_quality = False
-
-            # Retrieve single-phase properties
-            if self._AS.phase() != CP.iphase_twophase:
-                self.properties = self.compute_properties_1phase(
-                    generalize_quality=generalize_quality
-                )
-            else:
-                self.properties = self.compute_properties_2phase()
-
-            # Add properties as aliases
-            for key, value in PROPERTY_ALIAS.items():
-                self.properties[key] = self.properties[value]
-
-            # No errors computing the properies
-            self.converged_flag = True
-
-        # Something went wrong while computing the properties
-        except Exception as e:
-            self.converged_flag = False
-            if self.exceptions:
-                raise e
-
-        # TODO Return a new state that is not mutated?
-        return FluidState(self)
-
-    def compute_properties_1phase(self, generalize_quality=True):
-        """Get single-phase properties from CoolProp low level interface"""
-        props = {}
-        props["T"] = self._AS.T()
-        props["p"] = self._AS.p()
-        props["rhomass"] = self._AS.rhomass()
-        props["umass"] = self._AS.umass()
-        props["hmass"] = self._AS.hmass()
-        props["smass"] = self._AS.smass()
-        props["gibbsmass"] = self._AS.gibbsmass()
-        props["cvmass"] = self._AS.cvmass()
-        props["cpmass"] = self._AS.cpmass()
-        props["gamma"] = props["cpmass"] / props["cvmass"]
-        props["compressibility_factor"] = self._AS.compressibility_factor()
-        props["speed_sound"] = self._AS.speed_sound()
-        props["isentropic_bulk_modulus"] = props["rhomass"] * props["speed_sound"] ** 2
-        props["isentropic_compressibility"] = 1 / props["isentropic_bulk_modulus"]
-        props["isothermal_bulk_modulus"] = 1 / self._AS.isothermal_compressibility()
-        props["isothermal_compressibility"] = self._AS.isothermal_compressibility()
-        isobaric_expansion_coefficient = self._AS.isobaric_expansion_coefficient()
-        props["isobaric_expansion_coefficient"] = isobaric_expansion_coefficient
-        props["viscosity"] = self._AS.viscosity()
-        props["conductivity"] = self._AS.conductivity()
-
-        if generalize_quality:
-            # Instantiate new fluid object to compute saturation properties without changing the state of the class
-            temp = CP.AbstractState(self.backend, self.name)
-            # Extend quality calculation beyond the two-phase region
-            if props["p"] < self.critical_point.p:
-                # Set the saturation state of the fluid at the given pressure
-                temp.update(PQ_INPUTS, props["p"], 0.00)
-                h_liq = temp.hmass()
-                temp.update(PQ_INPUTS, props["p"], 1.00)
-                h_vap = temp.hmass()
-                # print(props)
-                quality = (props["hmass"] - h_liq) / (h_vap - h_liq)
-            else:
-                # For states at or above the critical pressure, the concept of saturation states is not applicable
-                # Instead, use a 'pseudo-critical' state for comparison, where the density is set to the critical density
-                # but the pressure is the same as the state of interest
-                # Use a band of a certain width to prevent a discontinuity
-                temp.update(DmassP_INPUTS, self.critical_point.rho, props["p"])
-                # print(props)
-                quality = (props["hmass"] - 0.95 * temp.hmass()) / (
-                    1.05 * temp.hmass() - 0.95 * temp.hmass()
-                )
-
-        else:
-            quality = np.nan
-
-        props["Q"] = quality
-        props["quality_mass"] = np.nan
-        props["quality_volume"] = np.nan
-
-        return props
-
-    def compute_properties_2phase(self):
-        """Get two-phase properties from mixing rules and single-phase CoolProp properties"""
-
-        # Basic properties of the two-phase mixture
-        T_mix = self._AS.T()
-        p_mix = self._AS.p()
-        rho_mix = self._AS.rhomass()
-        u_mix = self._AS.umass()
-        h_mix = self._AS.hmass()
-        s_mix = self._AS.smass()
-        gibbs_mix = self._AS.gibbsmass()
-
-        # Instantiate new fluid object to compute saturation properties without changing the state of the class
-        temp = CP.AbstractState(self.backend, self.name)
-
-        # Saturated liquid properties
-        temp.update(CP.QT_INPUTS, 0.00, T_mix)
-        rho_L = temp.rhomass()
-        cp_L = temp.cpmass()
-        cv_L = temp.cvmass()
-        k_L = temp.conductivity()
-        mu_L = temp.viscosity()
-        speed_sound_L = temp.speed_sound()
-        dsdp_L = temp.first_saturation_deriv(CP.iSmass, CP.iP)
-
-        # Saturated vapor properties
-        temp.update(CP.QT_INPUTS, 1.00, T_mix)
-        rho_V = temp.rhomass()
-        cp_V = temp.cpmass()
-        cv_V = temp.cvmass()
-        k_V = temp.conductivity()
-        mu_V = temp.viscosity()
-        speed_sound_V = temp.speed_sound()
-        dsdp_V = temp.first_saturation_deriv(CP.iSmass, CP.iP)
-
-        # Volume fractions of vapor and liquid
-        vol_frac_V = (rho_mix - rho_L) / (rho_V - rho_L)
-        vol_frac_L = 1.00 - vol_frac_V
-
-        # Mass fractions of vapor and liquid
-        mass_frac_V = (1 / rho_mix - 1 / rho_L) / (1 / rho_V - 1 / rho_L)
-        mass_frac_L = 1.00 - mass_frac_V
-
-        # Heat capacities of the two-phase mixture
-        cp_mix = mass_frac_L * cp_L + mass_frac_V * cp_V
-        cv_mix = mass_frac_L * cv_L + mass_frac_V * cv_V
-
-        # Transport properties of the two-phase mixture
-        k_mix = vol_frac_L * k_L + vol_frac_V * k_V
-        mu_mix = vol_frac_L * mu_L + vol_frac_V * mu_V
-
-        # Compressibility factor of the two-phase mixture
-        M = self._AS.molar_mass()
-        R = self._AS.gas_constant()
-        Z_mix = p_mix / (rho_mix * (R / M) * T_mix)
-
-        # Speed of sound of the two-phase mixture
-        mechanical_equilibrium = vol_frac_L / (
-            rho_L * speed_sound_L**2
-        ) + vol_frac_V / (rho_V * speed_sound_V**2)
-        thermal_equilibrium = T_mix * (
-            vol_frac_L * rho_L / cp_L * dsdp_L**2
-            + vol_frac_V * rho_V / cp_V * dsdp_V**2
-        )
-        compressibility_HEM = mechanical_equilibrium + thermal_equilibrium
-        if mass_frac_V < 1e-6:  # Avoid discontinuity when Q_v=0
-            a_HEM = speed_sound_L
-        elif mass_frac_V > 1.0 - 1e-6:  # Avoid discontinuity when Q_v=1
-            a_HEM = speed_sound_V
-        else:
-            a_HEM = (1 / rho_mix / compressibility_HEM) ** 0.5
-
-        # Store properties in dictionary
-        properties = {}
-        properties["T"] = T_mix
-        properties["p"] = p_mix
-        properties["rhomass"] = rho_mix
-        properties["umass"] = u_mix
-        properties["hmass"] = h_mix
-        properties["smass"] = s_mix
-        properties["gibbsmass"] = gibbs_mix
-        properties["cvmass"] = cv_mix
-        properties["cpmass"] = cp_mix
-        properties["gamma"] = properties["cpmass"] / properties["cvmass"]
-        properties["compressibility_factor"] = Z_mix
-        properties["speed_sound"] = a_HEM
-        properties["isentropic_bulk_modulus"] = rho_mix * a_HEM**2
-        properties["isentropic_compressibility"] = (rho_mix * a_HEM**2) ** -1
-        properties["isothermal_bulk_modulus"] = np.nan
-        properties["isothermal_compressibility"] = np.nan
-        properties["isobaric_expansion_coefficient"] = np.nan
-        properties["viscosity"] = mu_mix
-        properties["conductivity"] = k_mix
-        properties["Q"] = mass_frac_V
-        properties["quality_mass"] = mass_frac_V
-        properties["quality_volume"] = vol_frac_V
-
-        return properties
-
-    def set_state_metastable(
-        self, prop_1, prop_1_value, prop_2, prop_2_value, rho_guess, T_guess
-    ):
-        # problem = PropertyRoot()
-
-        return
-
-    def set_state_metastable_rhoT(self, rho, T):
-        # TODO: Add check to see if we are inside thespinodal and return two phase properties if yes
-        # TODO: implement root finding functionality to accept p-h, T-s, p-s arguments [good initial guess required]
-        # TODO: can it be generalized so that it uses equilibrium as initial guess with any inputs? (even if they are T-d)
-        try:
-            # Update Coolprop thermodynamic state
-            self.properties = self.compute_properties_metastable_rhoT(rho, T, self._AS)
-
-            # Add properties as aliases
-            for key, value in self.aliases.items():
-                self.properties[key] = self.properties[value]
-
-            # No errors computing the properies
-            self.converged_flag = True
-
-        # Something went wrong while computing the properties
-        except Exception as e:
-            self.converged_flag = False
-            if self.exceptions:
-                raise e
-
-        return self.properties
-
-    @staticmethod
-    def compute_properties_metastable_rhoT(rho, T, AS):
-        """
-        Compute the thermodynamic properties of a fluid using the Helmholtz
-        energy equation of state. All properties thermodynamic properties can
-        be derived as combinations of the Helmholtz energy and its
-        derivatives with respect to density and pressure.
-
-        This function can be used to estimate metastable properties using the
-        equation of state beyond the saturation lines.
-        """
-
-        # Update thermodynamic state
-        AS.update(CP.DmassT_INPUTS, rho, T)
-
-        # Get fluid constant properties
-        R = AS.gas_constant()
-        M = AS.molar_mass()
-        T_crit = AS.T_critical()
-        rho_crit = AS.rhomass_critical()
-
-        # Compute reduced variables
-        tau = T_crit / T
-        delta = rho / rho_crit
-
-        # Compute from the Helmholtz energy derivatives
-        alpha = AS.alpha0() + AS.alphar()
-        dalpha_dTau = AS.dalpha0_dTau() + AS.dalphar_dTau()
-        dalpha_dDelta = AS.dalpha0_dDelta() + AS.dalphar_dDelta()
-        d2alpha_dTau2 = AS.d2alpha0_dTau2() + AS.d2alphar_dTau2()
-        d2alpha_dDelta2 = AS.d2alpha0_dDelta2() + AS.d2alphar_dDelta2()
-        d2alpha_dDelta_dTau = AS.d2alpha0_dDelta_dTau() + AS.d2alphar_dDelta_dTau()
-
-        # Compute thermodynamic properties from Helmholtz energy EOS
-        properties = {}
-        properties["T"] = T
-        properties["p"] = (R / M) * T * rho * delta * dalpha_dDelta
-        properties["rhomass"] = rho
-        properties["umass"] = (R / M) * T * (tau * dalpha_dTau)
-        properties["hmass"] = (R / M) * T * (tau * dalpha_dTau + delta * dalpha_dDelta)
-        properties["smass"] = (R / M) * (tau * dalpha_dTau - alpha)
-        properties["gibbsmass"] = (R / M) * T * (alpha + delta * dalpha_dDelta)
-        properties["cvmass"] = (R / M) * (-(tau**2) * d2alpha_dTau2)
-        properties["cpmass"] = (R / M) * (
-            -(tau**2) * d2alpha_dTau2
-            + (delta * dalpha_dDelta - delta * tau * d2alpha_dDelta_dTau) ** 2
-            / (2 * delta * dalpha_dDelta + delta**2 * d2alpha_dDelta2)
-        )
-        properties["gamma"] = properties["cpmass"] / properties["cvmass"]
-        properties["compressibility_factor"] = delta * dalpha_dDelta
-        properties["speed_sound"] = (
-            (R / M * T)
-            * (
-                (2 * delta * dalpha_dDelta + delta**2 * d2alpha_dDelta2)
-                - (delta * dalpha_dDelta - delta * tau * d2alpha_dDelta_dTau) ** 2
-                / (tau**2 * d2alpha_dTau2)
-            )
-        ) ** 0.5
-        properties["isentropic_bulk_modulus"] = (rho * R / M * T) * (
-            (2 * delta * dalpha_dDelta + delta**2 * d2alpha_dDelta2)
-            - (delta * dalpha_dDelta - delta * tau * d2alpha_dDelta_dTau) ** 2
-            / (tau**2 * d2alpha_dTau2)
-        )
-        properties["isentropic_compressibility"] = (
-            1 / properties["isentropic_bulk_modulus"]
-        )
-        properties["isothermal_bulk_modulus"] = (
-            R / M * T * rho * (2 * delta * dalpha_dDelta + delta**2 * d2alpha_dDelta2)
-        )
-        properties["isothermal_compressibility"] = 1 / (
-            R / M * T * rho * (2 * delta * dalpha_dDelta + delta**2 * d2alpha_dDelta2)
-        )
-        properties["isobaric_expansion_coefficient"] = (
-            (1 / T)
-            * (delta * dalpha_dDelta - delta * tau * d2alpha_dDelta_dTau)
-            / (2 * delta * dalpha_dDelta + delta**2 * d2alpha_dDelta2)
-        )
-        properties["viscosity"] = AS.viscosity()
-        properties["conductivity"] = AS.conductivity()
-        properties["Q"] = np.nan
-        properties["quality_mass"] = np.nan
-        properties["quality_volume"] = np.nan
-
-        return properties
-
-    def get_property(self, propname):
-        """Get the value of a single property"""
-        if propname in self.properties:
-            return self.properties[propname]
-        else:
-            valid_options = "\n\t".join(self.properties.keys())
-            raise ValueError(
-                f"The requested property '{propname}' is not available. The valid options are:\n\t{valid_options}"
-            )
-
-    def compute_properties_meanline(self, input_type, prop_1, prop_2):
-        """Extract fluid properties for meanline model"""
-
-        # Compute properties in the normal way
-        self.set_state(input_type, prop_1, prop_2)
-
-        # Store a subset of the properties in a dictionary
-        fluid_properties = {}
-        property_subset = [
-            "p",
-            "T",
-            "h",
-            "s",
-            "d",
-            "Z",
-            "a",
-            "mu",
-            "k",
-            "cp",
-            "cv",
-            "gamma",
-        ]
-        for item in property_subset:
-            fluid_properties[item] = self.properties[item]
-
-        return fluid_properties
-
-    def compute_sonic_state(self, input_type, prop_1, prop_2):
-        props = {}
-
-        return FluidState(props)
-
-    # ------------------------------------------------------------------------------------ #
-    # ------------------------------------------------------------------------------------ #
-    # ------------------------------------------------------------------------------------ #
-
-    def _get_label(self, label, show_in_legend):
-        """Returns the appropriate label value based on whether it should be shown in the legend."""
-        return label if show_in_legend else "_no_legend_"
-
-    def _plot_or_update_line(self, axes, x_data, y_data, line_name, **plot_params):
-        # Ensure there is a dictionary for this axes
-        if axes not in self.graphic_elements:
-            self.graphic_elements[axes] = {}
-
-        # Check if the line exists for this axes
-        if line_name in self.graphic_elements[axes]:
-            line = self.graphic_elements[axes][line_name]
-            line.set_data(np.atleast_1d(x_data), np.atleast_1d(y_data))
-            # Update line properties
-            for param, value in plot_params.items():
-                setattr(line, param, value)
-            line.set_visible(True)
-        else:
-            # Create a new line with the provided plot parameters
-            (line,) = axes.plot(x_data, y_data, **plot_params)
-            self.graphic_elements[axes][line_name] = line
-        return line
-
-    def _plot_or_update_contours(
-        self, axes, x_data, y_data, z_data, contour_levels, line_name, **contour_params
-    ):
-        # Ensure there is a dictionary for this axes
-        if axes not in self.graphic_elements:
-            self.graphic_elements[axes] = {}
-
-        # Check if the contour exists for this axes
-        if line_name in self.graphic_elements[axes]:
-            for coll in self.graphic_elements[axes][line_name].collections:
-                coll.remove()  # Remove the old contour collections
-
-        # Create a new contour
-        contour = axes.contour(x_data, y_data, z_data, contour_levels, **contour_params)
-        self.graphic_elements[axes][line_name] = contour
-        return contour
-
-    def _set_visibility(self, axes, line_name, visible):
-        if axes in self.graphic_elements and line_name in self.graphic_elements[axes]:
-            self.graphic_elements[axes][line_name].set_visible(visible)
-
-    def plot_phase_diagram(
-        self,
-        x_variable="s",
-        y_variable="T",
-        axes=None,
-        num_points=200,
-        plot_saturation_line=True,
-        plot_critical_point=True,
-        plot_triple_point_liquid=False,
-        plot_triple_point_vapor=False,
-        plot_spinodal_line=False,
-        spinodal_line_method="standard",
-        spinodal_line_color=0.5 * np.array([1, 1, 1]),
-        spinodal_line_width=0.75,
-        plot_quality_isolines=False,
-        plot_pseudocritical_line=False,
-        quality_levels=np.linspace(0.1, 1.0, 10),
-        quality_labels=False,
-        show_in_legend=False,
-        **kwargs,
-    ):
-        if axes is None:
-            axes = plt.gca()
-
-        # Saturation line
-        if plot_saturation_line:
-            if self.sat_liq is None or self.sat_vap is None:
-                self.sat_liq, self.sat_vap = compute_saturation_line(self, num_points)
-            x = self.sat_liq[x_variable] + self.sat_vap[x_variable]
-            y = self.sat_liq[y_variable] + self.sat_vap[y_variable]
-            label = self._get_label("Saturation line", show_in_legend)
-            params = {"label": label, "color": "black"}
-            self._graphic_saturation_line = self._plot_or_update_line(
-                axes, x, y, "saturation_line", **params
-            )
-        else:
-            self._set_visibility(axes, "saturation_line", False)
-
-        # Plot pseudocritical line
-        if plot_pseudocritical_line:
-            if self.pseudo_critical_line is None:
-                self.pseudo_critical_line = compute_pseudocritical_line(self)
-            x = self.pseudo_critical_line[x_variable]
-            y = self.pseudo_critical_line[y_variable]
-            label = self._get_label("Pseudocritical line", show_in_legend)
-            params = {
-                "label": label,
-                "color": "black",
-                "linestyle": "--",
-                "linewidth": 0.75,
-            }
-            self._graphic_pseudocritical_line = self._plot_or_update_line(
-                axes, x, y, "pseudocritical_line", **params
-            )
-        else:
-            self._set_visibility(axes, "pseudocritical_line", False)
-
-        # Plot quality isolines
-        if plot_quality_isolines:
-            if self.q_mesh is None:
-                self.q_mesh = compute_quality_grid(self, num_points, quality_levels)
-            x = self.q_mesh[x_variable]
-            y = self.q_mesh[y_variable]
-            _, m = np.shape(x)
-            z = np.tile(quality_levels, (m, 1)).T
-            params = {"colors": "black", "linestyles": ":", "linewidths": 0.75}
-            self._graphics_q_lines = self._plot_or_update_contours(
-                axes, x, y, z, quality_levels, "quality_isolines", **params
-            )
-
-            if quality_labels:
-                axes.clabel(self._graphics_q_lines, fontsize=9, rightside_up=True)
-
-        else:
-            # Remove existing contour lines if they exist
-            if "quality_isolines" in self.graphic_elements.get(axes, {}):
-                for coll in self.graphic_elements[axes]["quality_isolines"].collections:
-                    coll.remove()
-                del self.graphic_elements[axes]["quality_isolines"]
-
-        # Plot critical point
-        params = {
-            "color": "black",
-            "marker": "o",
-            "markersize": 4.5,
-            "markerfacecolor": "w",
-        }
-        if plot_critical_point:
-            x = self.critical_point[x_variable]
-            y = self.critical_point[y_variable]
-            label = self._get_label("Critical point", show_in_legend)
-            self._graphic_critical_point = self._plot_or_update_line(
-                axes, x, y, "critical_point", label=label, **params
-            )
-        else:
-            self._set_visibility(axes, "critical_point", False)
-
-        # Plot liquid triple point
-        if plot_triple_point_liquid:
-            x = self.triple_point_liquid[x_variable]
-            y = self.triple_point_liquid[y_variable]
-            label = self._get_label("Triple point liquid", show_in_legend)
-            self._graphic_triple_point_liquid = self._plot_or_update_line(
-                axes, x, y, "triple_point_liquid", label=label, **params
-            )
-        else:
-            self._set_visibility(axes, "triple_point_liquid", False)
-
-        # Plot vapor triple point
-        if plot_triple_point_vapor:
-            x = self.triple_point_vapor[x_variable]
-            y = self.triple_point_vapor[y_variable]
-            label = self._get_label("Triple point vapor", show_in_legend)
-            self._graphic_triple_point_vapor = self._plot_or_update_line(
-                axes, x, y, "triple_point_vapor", label=label, **params
-            )
-        else:
-            self._set_visibility(axes, "triple_point_vapor", False)
-
-        return axes
-
-
-def compute_saturation_line(fluid, N_points=100):
-    # Initialize objects to store properties
-    prop_names = fluid.properties.keys()
-    liquid_line = {name: [] for name in prop_names}
-    vapor_line = {name: [] for name in prop_names}
-
-    # Define temperature array with refinement close to the critical point
-    ratio = 1 - fluid.triple_point_liquid.T / fluid.critical_point.T
-    t1 = np.logspace(
-        np.log10(1 - 0.9999), np.log10(ratio / 10), int(np.ceil(N_points / 2))
-    )
-    t2 = np.logspace(np.log10(ratio / 10), np.log10(ratio), int(np.floor(N_points / 2)))
-    T_sat = (1 - np.concatenate([t1, t2])) * fluid.critical_point.T
-
-    # Loop over temperatures and property names in an efficient way
-    for T in T_sat:
-        # Compute liquid saturation line
-        for name in prop_names:
-            fluid.set_state(CP.QT_INPUTS, 0.00, T)
-            liquid_line[name].append(fluid.properties[name])
-
-        # Compute vapor saturation line
-        for name in prop_names:
-            fluid.set_state(CP.QT_INPUTS, 1.00, T)
-            vapor_line[name].append(fluid.properties[name])
-
-    # Add critical point as part of the spinodal line
-    for name in prop_names:
-        liquid_line[name] = [fluid.critical_point[name]] + liquid_line[name]
-        vapor_line[name] = [fluid.critical_point[name]] + vapor_line[name]
-
-    # Re-format for easy concatenation
-    for name in prop_names:
-        liquid_line[name] = list(reversed(liquid_line[name]))
-
-    return liquid_line, vapor_line
-
-
-def compute_spinodal_line(fluid, N_points=100, method="standard"):
-    raise NotImplementedError(
-        "The 'compute_spinodal_line' function has not been implemented yet."
-    )
-
-
-def compute_pseudocritical_line(fluid, N_points=100):
-    # Initialize objects to store properties
-    prop_names = fluid.properties.keys()
-    pseudocritical_line = {name: [] for name in prop_names}
-
-    # Define temperature array with refinement close to the critical point
-    tau = np.logspace(np.log10(1e-3), np.log10(1), N_points)
-    T_range = (1 + tau) * fluid.critical_point.T
-
-    # Loop over temperatures and compute pseudocritical properties
-    for T in T_range:
-        for name in prop_names:
-            fluid.set_state(DmassT_INPUTS, fluid.critical_point.d, T)
-            pseudocritical_line[name].append(fluid.properties[name])
-
-    return pseudocritical_line
-
-
-def compute_quality_grid(fluid, num_points, quality_levels):
-    # Define temperature levels
-    t1 = np.logspace(np.log10(1 - 0.9999), np.log10(0.1), int(num_points / 2))
-    t2 = np.logspace(
-        np.log10(0.1),
-        np.log10(1 - (fluid.triple_point_liquid.T) / fluid.critical_point.T),
-        int(num_points / 2),
-    )
-    temperature_levels = (1 - np.hstack((t1, t2))) * fluid.critical_point.T
-
-    # Calculate property grid
-    quality_grid = []
-    for q in quality_levels:
-        row = []
-        for T in temperature_levels:
-            row.append(fluid.set_state(CP.QT_INPUTS, q, T))
-        quality_grid.append(row)
-
-    return states_to_dict_2d(quality_grid)
-
-
-def compute_properties_meshgrid(fluid, input_pair, range_1, range_2):
-    """
-    Compute fluid properties over a specified range and store them in a dictionary.
-
-    This function creates a meshgrid of property values based on the specified ranges and input pair,
-    computes the properties of the fluid at each point on the grid, and stores the results in a
-    dictionary where each key corresponds to a fluid property.
-
-    Parameters
-    ----------
-    fluid : Fluid object
-        An instance of the Fluid class.
-    input_pair : tuple
-        The input pair specifying the property type (e.g., PT_INPUTS for pressure-temperature).
-    range1 : tuple
-        The range linspace(min, max, n) for the first property of the input pair.
-    range2 : tuple
-        The range linspace(min, max, n) for the second property of the input pair.
-
-    Returns
-    -------
-    properties_dict : dict
-        A dictionary where keys are property names and values are 2D numpy arrays of computed properties.
-    grid1, grid2 : numpy.ndarray
-        The meshgrid arrays for the first and second properties.
-    """
-
-    # Create the meshgrid
-    grid1, grid2 = np.meshgrid(range_1, range_2)
-
-    # Initialize dictionary to store properties and pre-allocate storage
-    properties_dict = {key: np.zeros_like(grid1) for key in fluid.properties}
-
-    # Compute properties at each point
-    for i in range(len(range_2)):
-        for j in range(len(range_1)):
-            # Set state of the fluid
-            fluid.set_state(input_pair, grid1[i, j], grid2[i, j])
-
-            # Store the properties
-            for key in fluid.properties:
-                properties_dict[key][i, j] = fluid.properties[key]
-
-    return properties_dict
-
-
-# Implement class to calculate intersection with saturation line?
-
-
-class PropertyRoot(NonlinearSystemProblem):
-    """
-    Find the root for thermodynamic state by iterating on the density-temperature
-    native inputs to the helmholtz energy equations of state.
-
-    Attributes
-    ----------
-    prop_1 : str
-        The first property to be compared.
-    prop_1_value : float
-        The value of the first property.
-    prop_2 : str
-        The second property to be compared.
-    prop_2_value : float
-        The value of the second property.
-    fluid : object
-        An instance of the fluid class which has the helmholtz energy equations of state.
-
-    Methods
-    -------
-    get_values(x)
-        Calculates the residuals based on the given input values of density and temperature.
-    """
-
-    def __init__(self, prop_1, prop_1_value, prop_2, prop_2_value, fluid):
-        self.prop_1 = prop_1
-        self.prop_2 = prop_2
-        self.prop_1_value = prop_1_value
-        self.prop_2_value = prop_2_value
-        self.fluid = fluid
-
-    def get_values(self, x):
-        """
-        Compute the residuals for the given density and temperature.
-
-        Parameters
-        ----------
-        x : list
-            List containing the values for density and temperature.
-
-        Returns
-        -------
-        np.ndarray
-            Array containing residuals (difference) for the two properties.
-        """
-        # Ensure x can be indexed and contains exactly two elements
-        if not hasattr(x, "__getitem__") or len(x) != 2:
-            raise ValueError(
-                "Input x must be a list, tuple or numpy array containing exactly two elements: density and temperature."
-            )
-
-        rho, T = x
-        props = self.fluid.set_state_metastable_rhoT(rho, T)
-
-        residual = np.asarray(
-            [
-                self.prop_1_value - props[self.prop_1],
-                self.prop_2_value - props[self.prop_2],
-            ]
-        )
-
-        return residual
-
-
-class SonicStateProblem(NonlinearSystemProblem):
-    """ """
-
-    def __init__(self, fluid, property_pair, prop_1, prop_2):
-        # Calculate the thermodynamic state
-        self.fluid = fluid
-        self.state = fluid.set_state(property_pair, prop_1, prop_2)
-
-        # Initial guess based in input sstate
-        self.initial_guess = [self.state.d, self.state.T]
-
-        # # Initial guess based on perfect gass relations
-        # gamma = self.state.gamma
-        # d_star = (2/(gamma + 1)) ** (1/(gamma-1)) * self.state.rho
-        # T_star =  (2/(gamma + 1)) * self.state.T
-        # self.initial_guess = [d_star, T_star]
-
-    def get_values(self, x):
-        # Ensure x can be indexed and contains exactly two elements
-        if not hasattr(x, "__getitem__") or len(x) != 2:
-            raise ValueError(
-                "Input x must be a list, tuple or numpy array containing exactly two elements: density and temperature."
-            )
-
-        # Calculate state for the current density-temperature pair
-        crit_state = self.fluid.set_state(DmassT_INPUTS, x[0], x[1])
-
-        # Calculate the sonic state residual
-        residual = np.asarray(
-            [
-                1.0 - (crit_state.h + 0.5 * crit_state.a**2) / self.state.h,
-                1.0 - crit_state.s / self.state.s,
-            ]
-        )
-
-        return residual
-
-
-class SonicStateProblem2(OptimizationProblem):
-    """ """
-
-    def __init__(self, fluid, property_pair, prop_1, prop_2):
-        # Calculate the thermodynamic state
-        self.fluid = fluid
-        self.state = fluid.set_state(property_pair, prop_1, prop_2)
-
-        # Initial guess based in input sstate
-        self.initial_guess = [self.state.d, self.state.T * 0.9]
-
-        # # Initial guess based on perfect gass relations
-        # gamma = self.state.gamma
-        # d_star = (2/(gamma + 1)) ** (1/(gamma-1)) * self.state.rho
-        # T_star =  (2/(gamma + 1)) * self.state.T
-        # self.initial_guess = [d_star, T_star]
-
-    def get_values(self, x):
-        """
-        Compute the residuals for the given density and temperature.
-
-        Parameters
-        ----------
-        x : list
-            List containing the values for density and temperature.
-
-        Returns
-        -------
-        np.ndarray
-            Array containing residuals (difference) for the two properties.
-        """
-
-        # Ensure x can be indexed and contains exactly two elements
-        if not hasattr(x, "__getitem__") or len(x) != 2:
-            raise ValueError(
-                "Input x must be a list, tuple or numpy array containing exactly two elements: density and temperature."
-            )
-
-        # Calculate state for the current density-temperature pair
-        crit_state = self.fluid.set_state(DmassT_INPUTS, x[0], x[1])
-
-        # Calculate the sonic state residual
-        residual = [
-            1.0 - crit_state.s / self.state.s,
-        ]
-
-        # Objective function
-        self.f = crit_state.d**2 * (self.state.h - crit_state.h)
-        self.f = -self.f / (self.state.d * self.state.a) ** 2
-
-        # Equality constraints
-        self.c_eq = residual
-
-        # No inequality constraints given for this problem
-        self.c_ineq = []
-
-        # Combine objective function and constraints
-        objective_and_constraints = self.merge_objective_and_constraints(
-            self.f, self.c_eq, self.c_ineq
-        )
-
-        return objective_and_constraints
-
-    def get_bounds(self):
-        bound_density = (
-            self.fluid.triple_point_vapor.d * 1.5,
-            self.fluid.critical_point.d * 3,
-        )
-        bound_temperature = (
-            self.fluid.triple_point_vapor.T * 1,
-            self.fluid.critical_point.T * 3,
-        )
-        # return [bound_density, bound_temperature]
-        return None
-
-    def get_n_eq(self):
-        return self.get_number_of_constraints(self.c_eq)
-
-    def get_n_ineq(self):
-        return self.get_number_of_constraints(self.c_ineq)
-
-
-def calculate_superheating(state, fluid):
-    """
-    Calculates the degree of superheating for a given state and adds this information to the state.
-
-    Parameters
-    ----------
-    state : dict
-        A dictionary representing the thermodynamic state, containing at least pressure (p), temperature (T),
-        and enthalpy (h) of the fluid.
-    fluid : object
-        An object representing the fluid with its properties, including methods to set state and critical point data.
-
-    Returns
-    -------
-    dict
-        The input state dictionary with an added field 'superheating' representing the degree of superheating.
-    """
-
-    # Check if the pressure is below the critical pressure of the fluid
-    if state["p"] < fluid.critical_point.p:
-        # Set the saturation state of the fluid at the given pressure
-        sat_state = fluid.set_state(PQ_INPUTS, state["p"], 1.00)
-
-        # Check if the fluid is in the two-phase region
-        if fluid._AS.phase() == CP.iphase_twophase:
-            # In the two-phase region, define superheating as the normalized difference in enthalpy
-            # The normalization is done using the specific heat capacity at saturation (cp)
-            # This provides a continuous measure of superheating, even in the two-phase region
-            state["superheating"] = (state["h"] - sat_state.h) / sat_state.cp
-        else:
-            # Outside the two-phase region, superheating is the difference in temperature
-            # from the saturation temperature at the same pressure
-            state["superheating"] = state["T"] - sat_state.T
-    else:
-        # For states at or above the critical pressure, the concept of saturation temperature is not applicable
-        # Instead, use a 'pseudo-critical' state for comparison, where the density is set to the critical density
-        # but the pressure is the same as the state of interest
-        pseudo_crit = fluid.set_state(
-            DmassP_INPUTS, fluid.critical_point.rho, state["p"]
-        )
-
-        # Define superheating as the difference in enthalpy from this 'pseudo-critical' state
-        # This approach extends the definition of superheating to conditions above the critical pressure
-        state["superheating"] = state.T - pseudo_crit.T
-
-    return state
-
-
-def calculate_subcooling(state, fluid):
-    """
-    Calculates the degree of subcooling for a given state and adds this information to the state.
-
-    Parameters
-    ----------
-    state : dict
-        A dictionary representing the thermodynamic state, containing at least pressure (p), temperature (T),
-        and enthalpy (h) of the fluid.
-    fluid : object
-        An object representing the fluid with its properties, including methods to set state and critical point data.
-
-    Returns
-    -------
-    dict
-        The input state dictionary with an added field 'subcooling' representing the degree of subcooling.
-    """
-
-    # Check if the pressure is below the critical pressure of the fluid
-    if state["p"] < fluid.critical_point.p:
-        # Set the saturation state of the fluid at the given pressure
-        sat_state = fluid.set_state(PQ_INPUTS, state["p"], 0.00)
-
-        # Check if the fluid is in the two-phase region
-        if fluid._AS.phase() == CP.iphase_twophase:
-            # In the two-phase region, define subcooling as the normalized difference in enthalpy
-            # The normalization is done using the specific heat capacity at saturation (cp)
-            # This provides a continuous measure of subcooling, even in the two-phase region
-            state["subcooling"] = (sat_state.h - state["h"]) / sat_state.cp
-        else:
-            # Outside the two-phase region, subcooling is the difference in temperature
-            # from the saturation temperature at the same pressure
-            state["subcooling"] = sat_state.T - state["T"]
-    else:
-        # For states at or above the critical pressure, the concept of saturation temperature is not applicable
-        # Instead, use a 'pseudo-critical' state for comparison, where the density is set to the critical density
-        # but the pressure is the same as the state of interest
-        pseudo_crit = fluid.set_state(
-            DmassP_INPUTS, fluid.critical_point.rho, state["p"]
-        )
-
-        # Define subcooling as the difference in enthalpy from this 'pseudo-critical' state
-        # This approach extends the definition of subcooling to conditions above the critical pressure
-        state["subcooling"] = pseudo_crit.T - state.T
-
-    return state
-
-
-# def compute_properties_metastable_rhoT(rho, T, fluid):
-#     """
-#     Compute the thermodynamic properties of a fluid using the Helmholtz
-#     energy equation of state. All properties thermodynamic properties can
-#     be derived as combinations of the Helmholtz energy and its
-#     derivatives with respect to density and pressure.
-
-#     This function can be used to estimate metastable properties using the
-#     equation of state beyond the saturation lines.
-#     """
-
-#     # Update thermodynamic state
-#     fluid.update(CP.DmassT_INPUTS, rho, T)
-
-#     # Get fluid constant properties
-#     R = fluid.gas_constant()
-#     M = fluid.molar_mass()
-#     T_crit = fluid.T_critical()
-#     rho_crit = fluid.rhomass_critical()
-
-#     # Compute reduced variables
-#     tau = T_crit / T
-#     delta = rho / rho_crit
-
-#     # Compute from the Helmholtz energy derivatives
-#     alpha = fluid.alpha0() + fluid.alphar()
-#     dalpha_dTau = fluid.dalpha0_dTau() + fluid.dalphar_dTau()
-#     dalpha_dDelta = fluid.dalpha0_dDelta() + fluid.dalphar_dDelta()
-#     d2alpha_dTau2 = fluid.d2alpha0_dTau2() + fluid.d2alphar_dTau2()
-#     d2alpha_dDelta2 = fluid.d2alpha0_dDelta2() + fluid.d2alphar_dDelta2()
-#     d2alpha_dDelta_dTau = fluid.d2alpha0_dDelta_dTau() + fluid.d2alphar_dDelta_dTau()
-
-#     # Compute thermodynamic properties from Helmholtz energy EOS
-#     properties = {}
-#     properties['T'] = T
-#     properties['p'] = (R/M)*T*rho*delta*dalpha_dDelta
-#     properties['rhomass'] = rho
-#     properties['umass'] = (R/M)*T*(tau*dalpha_dTau)
-#     properties['hmass'] = (R/M)*T*(tau*dalpha_dTau+delta*dalpha_dDelta)
-#     properties['smass'] = (R/M)*(tau*dalpha_dTau - alpha)
-#     properties['gibbsmass'] = (R/M)*T*(alpha + delta*dalpha_dDelta)
-#     properties['cvmass'] = (R/M)*(-tau**2*d2alpha_dTau2)
-#     properties['cpmass'] = (R/M)*(-tau**2*d2alpha_dTau2 + (delta*dalpha_dDelta - delta*tau*d2alpha_dDelta_dTau)**2/(2*delta*dalpha_dDelta + delta**2*d2alpha_dDelta2))
-#     properties['gamma'] = properties['cpmass']/properties['cvmass']
-#     properties['compressibility_factor'] = delta*dalpha_dDelta
-#     properties['speed_sound'] = ((R/M)*T*(2*delta*dalpha_dDelta + delta**2*d2alpha_dDelta2 - (delta*dalpha_dDelta - delta*tau*d2alpha_dDelta_dTau)**2/(tau**2*d2alpha_dTau2)))**0.5
-#     properties['isentropic_bulk_modulus'] = rho*(R/M)*T*(2*delta*dalpha_dDelta + delta**2*d2alpha_dDelta2 - (delta*dalpha_dDelta - delta*tau*d2alpha_dDelta_dTau)**2/(tau**2*d2alpha_dTau2))
-#     properties['isentropic_compressibility'] = 1 / properties["isentropic_bulk_modulus"]
-#     properties['isothermal_bulk_modulus'] = (R/M)*T*rho*(2*delta*dalpha_dDelta + delta**2*d2alpha_dDelta2)
-#     properties['isothermal_compressibility'] = 1/((R/M)*T*rho*(2*delta*dalpha_dDelta + delta**2*d2alpha_dDelta2))
-#     properties['isobaric_expansion_coefficient'] = 1/T*(delta*dalpha_dDelta - delta*tau*d2alpha_dDelta_dTau)/(2*delta*dalpha_dDelta + delta**2*d2alpha_dDelta2)
-#     properties['viscosity'] = fluid.viscosity()
-#     properties['conductivity'] = fluid.conductivity()
-#     properties['Q'] = np.nan
-#     properties['quality_mass'] = np.nan
-#     properties['quality_volume'] = np.nan
-
-#     return properties
-
-
-if __name__ == "__main__":
-    fluid = Fluid("Water", backend="HEOS")
-
-    # # Properties of liquid water
-    # props_stable = fluid.set_state(CP.PT_INPUTS, 101325, 300)
-    # print()
-    # print("Properties of liquid water")
-    # print(f"{'Property':35} {'value':6}")
-    # for key, value in props_stable.items():
-    #     print(f"{key:35} {value:.6e}")
-
-    # # Properties of water/steam mixture
-    # props = fluid.set_state(CP.QT_INPUTS, 0.5, 300)
-    # print()
-    # print("Properties of water/steam mixture")
-    # print(f"{'Property':35} {'value':6}")
-    # for key, value in props.items():
-    #     print(f"{key:35} {value:.6e}")
-
-    # # Get subset of properties for meanline code
-    # props = fluid.compute_properties_meanline(CP.QT_INPUTS, 0.5, 300)
-    # print()
-    # print("Properties for the meanline code")
-    # print(f"{'Property':15} {'value':6}")
-    # for key, value in props.items():
-    #     print(f"{key:15} {value:.6e}")
-
-    # #
-    # props = compute_properties_metastable_rhoT(10, 500, fluid.abstractstate)
-    # print("Metastable properties of water")
-    # print(f"{'Property':35} {'value':6}")
-    # for key, value in props.items():
-    #     print(f"{key:35} {value:.6e}")
-
-    # Check that the metastable property calculations match in the single-phase region
-    p, T = 101325, 300
-    props_stable = fluid.set_state(CP.PT_INPUTS, p, T)
-    print()
-    print(f"Properties of water at p={p} Pa and T={T} K")
-    print(f"{'Property':35} {'Equilibrium':>15} {'Metastable':>15} {'Deviation':>15}")
-    props_metastable = fluid.set_state_metastable_rhoT(
-        props_stable["rho"], props_stable["T"]
-    )
-    for key in props_stable.keys():
-        value_stable = props_stable[key]
-        value_metastable = props_metastable[key]
-        print(
-            f"{key:35} {value_stable:+15.6e} {value_metastable:+15.6e} {(value_stable - value_metastable)/value_stable:+15.6e}"
-        )
+import numpy as np
+import matplotlib.pyplot as plt
+import CoolProp.CoolProp as CP
+import copy
+
+from ..pysolver_view import (
+    NonlinearSystemSolver,
+    NonlinearSystemProblem,
+    OptimizationProblem,
+    OptimizationSolver,
+)
+
+# Define property aliases
+PROPERTY_ALIAS = {
+    "P": "p",
+    "rho": "rhomass",
+    "d": "rhomass",
+    "u": "umass",
+    "h": "hmass",
+    "s": "smass",
+    "cv": "cvmass",
+    "cp": "cpmass",
+    "a": "speed_sound",
+    "Z": "compressibility_factor",
+    "mu": "viscosity",
+    "k": "conductivity",
+}
+
+# Dynamically add INPUTS fields to the module
+# for attr in dir(CP):
+#     if attr.endswith('_INPUTS'):
+#         globals()[attr] = getattr(CP, attr)
+
+# Statically add phase indices to the module (IDE autocomplete)
+iphase_critical_point = CP.iphase_critical_point
+iphase_gas = CP.iphase_gas
+iphase_liquid = CP.iphase_liquid
+iphase_not_imposed = CP.iphase_not_imposed
+iphase_supercritical = CP.iphase_supercritical
+iphase_supercritical_gas = CP.iphase_supercritical_gas
+iphase_supercritical_liquid = CP.iphase_supercritical_liquid
+iphase_twophase = CP.iphase_twophase
+iphase_unknown = CP.iphase_unknown
+
+# Statically add INPUT fields to the module (IDE autocomplete)
+QT_INPUTS = CP.QT_INPUTS
+PQ_INPUTS = CP.PQ_INPUTS
+QSmolar_INPUTS = CP.QSmolar_INPUTS
+QSmass_INPUTS = CP.QSmass_INPUTS
+HmolarQ_INPUTS = CP.HmolarQ_INPUTS
+HmassQ_INPUTS = CP.HmassQ_INPUTS
+DmolarQ_INPUTS = CP.DmolarQ_INPUTS
+DmassQ_INPUTS = CP.DmassQ_INPUTS
+PT_INPUTS = CP.PT_INPUTS
+DmassT_INPUTS = CP.DmassT_INPUTS
+DmolarT_INPUTS = CP.DmolarT_INPUTS
+HmolarT_INPUTS = CP.HmolarT_INPUTS
+HmassT_INPUTS = CP.HmassT_INPUTS
+SmolarT_INPUTS = CP.SmolarT_INPUTS
+SmassT_INPUTS = CP.SmassT_INPUTS
+TUmolar_INPUTS = CP.TUmolar_INPUTS
+TUmass_INPUTS = CP.TUmass_INPUTS
+DmassP_INPUTS = CP.DmassP_INPUTS
+DmolarP_INPUTS = CP.DmolarP_INPUTS
+HmassP_INPUTS = CP.HmassP_INPUTS
+HmolarP_INPUTS = CP.HmolarP_INPUTS
+PSmass_INPUTS = CP.PSmass_INPUTS
+PSmolar_INPUTS = CP.PSmolar_INPUTS
+PUmass_INPUTS = CP.PUmass_INPUTS
+PUmolar_INPUTS = CP.PUmolar_INPUTS
+HmassSmass_INPUTS = CP.HmassSmass_INPUTS
+HmolarSmolar_INPUTS = CP.HmolarSmolar_INPUTS
+SmassUmass_INPUTS = CP.SmassUmass_INPUTS
+SmolarUmolar_INPUTS = CP.SmolarUmolar_INPUTS
+DmassHmass_INPUTS = CP.DmassHmass_INPUTS
+DmolarHmolar_INPUTS = CP.DmolarHmolar_INPUTS
+DmassSmass_INPUTS = CP.DmassSmass_INPUTS
+DmolarSmolar_INPUTS = CP.DmolarSmolar_INPUTS
+DmassUmass_INPUTS = CP.DmassUmass_INPUTS
+DmolarUmolar_INPUTS = CP.DmolarUmolar_INPUTS
+
+# Define dictionary with dynamically generated fields
+PHASE_INDEX = {attr: getattr(CP, attr) for attr in dir(CP) if attr.startswith("iphase")}
+INPUT_PAIRS = {attr: getattr(CP, attr) for attr in dir(CP) if attr.endswith("_INPUTS")}
+PHASE_INDEX = sorted(PHASE_INDEX.items(), key=lambda x: x[1])
+INPUT_PAIRS = sorted(INPUT_PAIRS.items(), key=lambda x: x[1])
+
+
+def _generate_coolprop_input_table():
+    """Create table of input pairs as string to be copy-pasted in Sphinx documentation"""
+
+    inputs_table = ".. list-table:: CoolProp Input Mappings\n"
+    inputs_table += "   :widths: 50 30\n"
+    inputs_table += "   :header-rows: 1\n\n"
+    inputs_table += "   * - Input pair name\n"
+    inputs_table += "     - Input pair mapping\n"
+
+    for name, value in INPUT_PAIRS:
+        inputs_table += f"   * - {name}\n"
+        inputs_table += f"     - {value}\n"
+
+    return inputs_table
+
+
+def states_to_dict(states):
+    """
+    Convert a list of state objects into a dictionary.
+    Each key is a field name of the state objects, and each value is a NumPy array of all the values for that field.
+    """
+    state_dict = {}
+    for field in states[0].keys():
+        state_dict[field] = np.array([getattr(state, field) for state in states])
+    return state_dict
+
+
+def states_to_dict_2d(states_grid):
+    """
+    Convert a 2D list (grid) of state objects into a dictionary.
+    Each key is a field name of the state objects, and each value is a 2D NumPy array of all the values for that field.
+
+    Parameters
+    ----------
+    states_grid : list of list of objects
+        A 2D grid where each element is a state object with the same keys.
+
+    Returns
+    -------
+    dict
+        A dictionary where keys are field names and values are 2D arrays of field values.
+    """
+    state_dict_2d = {}
+    for i, row in enumerate(states_grid):
+        for j, state in enumerate(row):
+            for field in state.keys():
+                if field not in state_dict_2d:
+                    state_dict_2d[field] = np.empty(
+                        (len(states_grid), len(row)), dtype=object
+                    )
+                state_dict_2d[field][i, j] = getattr(state, field)
+
+    return state_dict_2d
+
+
+class FluidState:
+    """
+    A class representing the thermodynamic state of a fluid.
+
+    This class is used to store and access the properties of a fluid state.
+    Properties can be accessed directly as attributes (e.g., `fluid_state.p` for pressure)
+    or through dictionary-like access (e.g., `fluid_state['T']` for temperature).
+
+    Methods
+    -------
+    to_dict():
+        Convert the FluidState properties to a dictionary.
+    keys():
+        Return the keys of the FluidState properties.
+    items():
+        Return the items (key-value pairs) of the FluidState properties.
+
+    """
+
+    def __init__(self, fluid):
+        # Use an internal dictionary to store properties
+        self._properties = fluid.properties
+        self._properties["fluid_name"] = fluid.name
+        self._properties["converged"] = fluid.converged_flag
+        self._properties["identifier"] = fluid.identifier
+
+    def __getattr__(self, name):
+        # This method is called when an attribute is accessed
+        try:
+            return self._properties[name]
+        except KeyError:
+            raise AttributeError(f"Attribute '{name}' not found in FluidState.")
+
+    def __setattr__(self, name, value):
+        # This method is called when an attribute is set
+        if name == "_properties":
+            # Initialize the _properties dictionary
+            super().__setattr__(name, value)
+        else:
+            self._properties[name] = value
+
+    def __getitem__(self, key):
+        # This method is called when using dictionary-like access
+        try:
+            return self._properties[key]
+        except KeyError:
+            raise KeyError(
+                f"Key '{key}' not found in FluidState. Available keys: {', '.join(self._properties.keys())}"
+            )
+
+    def __setitem__(self, key, value):
+        # This method is called when using dictionary-like assignment
+        self._properties[key] = value
+
+    def __str__(self):
+        properties_str = "\n   ".join(
+            [f"{key}: {value}" for key, value in self._properties.items()]
+        )
+        return f"FluidState:\n   {properties_str}"
+
+    def to_dict(self):
+        return self._properties.copy()
+
+    def keys(self):
+        return self._properties.keys()
+
+    def items(self):
+        return self._properties.items()
+
+    def values(self):
+        return self._properties.values()
+
+
+class Fluid:
+    """
+    Represents a fluid with various thermodynamic properties computed via CoolProp.
+
+    This class provides a convenient interface to CoolProp for various fluid property calculations.
+
+    Properties can be accessed directly as attributes (e.g., `fluid.properties["p"]` for pressure)
+    or through dictionary-like access (e.g., `fluid.T` for temperature).
+
+    Critical and triple point properties are computed upon initialization and stored internally for convenience.
+
+    Attributes
+    ----------
+    name : str
+        Name of the fluid.
+    backend : str
+        Backend used for CoolProp, default is 'HEOS'.
+    exceptions : bool
+        Determines if exceptions should be raised during state calculations. Default is True.
+    converged_flag : bool
+        Flag indicating whether properties calculations converged.
+    properties : dict
+        Dictionary of various fluid properties. Accessible directly as attributes (e.g., `fluid.p` for pressure).
+    critical_point : FluidState
+        Properties at the fluid's critical point.
+    triple_point_liquid : FluidState
+        Properties at the fluid's triple point in the liquid state.
+    triple_point_vapor : FluidState
+        Properties at the fluid's triple point in the vapor state.
+
+    Methods
+    -------
+    set_state(input_type, prop_1, prop_2):
+        Set the thermodynamic state of the fluid using specified property inputs.
+
+    Examples
+    --------
+    Accessing properties:
+
+        - fluid.T - Retrieves temperature directly as an attribute.
+        - fluid.properties['p'] - Retrieves pressure through dictionary-like access.
+
+    Accessing critical point properties:
+
+        - fluid.critical_point.p - Retrieves critical pressure.
+        - fluid.critical_point['T'] - Retrieves critical temperature.
+
+    Accessing triple point properties:
+
+        - fluid.triple_point_liquid.h - Retrieves liquid enthalpy at the triple point.
+        - fluid.triple_point_vapor.s - Retrieves vapor entropy at the triple point.
+    """
+
+    def __init__(
+        self,
+        name,
+        backend="HEOS",
+        exceptions=True,
+        identifier=None,
+    ):
+        self.name = name
+        self.backend = backend
+        self._AS = CP.AbstractState(backend, name)
+        self.exceptions = exceptions
+        self.converged_flag = False
+        self.identifier = identifier
+        self.properties = {}
+
+        # Initialize variables
+        self.sat_liq = None
+        self.sat_vap = None
+        self.spinodal_liq = None
+        self.spinodal_vap = None
+        self.pseudo_critical_line = None
+        self.q_mesh = None
+        self.graphic_elements = {}
+
+        # Get critical and triple point properties
+        if self._AS.fluid_param_string("pure") == "true":
+            self.critical_point = self._compute_critical_point()
+            self.triple_point_liquid = self._compute_triple_point_liquid()
+            self.triple_point_vapor = self._compute_triple_point_vapor()
+
+        # Pressure and temperature limits
+        self.p_min = 1
+        self.p_max = self._AS.pmax()
+        self.T_min = self._AS.Tmin()
+        self.T_max = self._AS.Tmax()
+
+    def __getattr__(self, name):
+        if name in self.properties:
+            return self.properties[name]
+        raise AttributeError(f"'Fluid' object has no attribute '{name}'")
+
+    def _compute_critical_point(self):
+        """Calculate the properties at the critical point"""
+        rho_crit, T_crit = self._AS.rhomass_critical(), self._AS.T_critical()
+        self.set_state(DmassT_INPUTS, rho_crit, T_crit, generalize_quality=False)
+        return FluidState(self)
+
+    def _compute_triple_point_liquid(self):
+        """Calculate the properties at the triple point (liquid state)"""
+        self.set_state(QT_INPUTS, 0.00, self._AS.Ttriple(), generalize_quality=False)
+        return FluidState(self)
+
+    def _compute_triple_point_vapor(self):
+        """Calculate the properties at the triple point (vapor state)"""
+        self.set_state(QT_INPUTS, 1.00, self._AS.Ttriple(), generalize_quality=False)
+        return FluidState(self)
+
+    def get_props(self, input_type, prop_1, prop_2, generalize_quality=True):
+        return self.set_state(
+            input_type, prop_1, prop_2, generalize_quality=generalize_quality
+        )
+
+    def set_state(self, input_type, prop_1, prop_2, generalize_quality=True):
+        """
+        Set the thermodynamic state of the fluid based on input properties.
+
+        This method updates the thermodynamic state of the fluid in the CoolProp ``abstractstate`` object
+        using the given input properties. It then calculates either single-phase or two-phase
+        properties based on the current phase of the fluid.
+
+        If the calculation of properties fails, `converged_flag` is set to False, indicating an issue with
+        the property calculation. Otherwise, it's set to True.
+
+        Aliases of the properties are also added to the ``Fluid.properties`` dictionary for convenience.
+
+        Parameters
+        ----------
+        input_type : str or int
+            The variable pair used to define the thermodynamic state. This should be one of the
+            predefined input pairs in CoolProp, such as ``PT_INPUTS`` for pressure and temperature.
+            For all available input pairs, refer to :ref:`this list <module-input-pairs-table>`.
+        prop_1 : float
+            The first property value corresponding to the input type (e.g., pressure in Pa if the input
+            type is CP.PT_INPUTS).
+        prop_2 : float
+            The second property value corresponding to the input type (e.g., temperature in K if the input
+            type is CP.PT_INPUTS).
+
+        Returns
+        -------
+        dict
+            A dictionary of computed properties for the current state of the fluid. This includes both the
+            raw properties from CoolProp and any additional alias properties.
+
+        Raises
+        ------
+        Exception
+            If `throw_exceptions` attribute is set to True and an error occurs during property calculation,
+            the original exception is re-raised.
+
+
+        """
+        try:
+            # Update Coolprop thermodynamic state
+            self._AS.update(input_type, prop_1, prop_2)
+
+            if self._AS.fluid_param_string("pure") == "false":
+                generalize_quality = False
+
+            # Retrieve single-phase properties
+            if self._AS.phase() != CP.iphase_twophase:
+                self.properties = self.compute_properties_1phase(
+                    generalize_quality=generalize_quality
+                )
+            else:
+                self.properties = self.compute_properties_2phase()
+
+            # Add properties as aliases
+            for key, value in PROPERTY_ALIAS.items():
+                self.properties[key] = self.properties[value]
+
+            # No errors computing the properies
+            self.converged_flag = True
+
+        # Something went wrong while computing the properties
+        except Exception as e:
+            self.converged_flag = False
+            if self.exceptions:
+                raise e
+
+        # TODO Return a new state that is not mutated?
+        return FluidState(self)
+
+    def compute_properties_1phase(self, generalize_quality=True):
+        """Get single-phase properties from CoolProp low level interface"""
+        props = {}
+        props["T"] = self._AS.T()
+        props["p"] = self._AS.p()
+        props["rhomass"] = self._AS.rhomass()
+        props["umass"] = self._AS.umass()
+        props["hmass"] = self._AS.hmass()
+        props["smass"] = self._AS.smass()
+        props["gibbsmass"] = self._AS.gibbsmass()
+        props["cvmass"] = self._AS.cvmass()
+        props["cpmass"] = self._AS.cpmass()
+        props["gamma"] = props["cpmass"] / props["cvmass"]
+        props["compressibility_factor"] = self._AS.compressibility_factor()
+        props["speed_sound"] = self._AS.speed_sound()
+        props["isentropic_bulk_modulus"] = props["rhomass"] * props["speed_sound"] ** 2
+        props["isentropic_compressibility"] = 1 / props["isentropic_bulk_modulus"]
+        props["isothermal_bulk_modulus"] = 1 / self._AS.isothermal_compressibility()
+        props["isothermal_compressibility"] = self._AS.isothermal_compressibility()
+        isobaric_expansion_coefficient = self._AS.isobaric_expansion_coefficient()
+        props["isobaric_expansion_coefficient"] = isobaric_expansion_coefficient
+        props["viscosity"] = self._AS.viscosity()
+        props["conductivity"] = self._AS.conductivity()
+
+        if generalize_quality:
+            # Instantiate new fluid object to compute saturation properties without changing the state of the class
+            temp = CP.AbstractState(self.backend, self.name)
+            # Extend quality calculation beyond the two-phase region
+            if props["p"] < self.critical_point.p:
+                # Set the saturation state of the fluid at the given pressure
+                temp.update(PQ_INPUTS, props["p"], 0.00)
+                h_liq = temp.hmass()
+                temp.update(PQ_INPUTS, props["p"], 1.00)
+                h_vap = temp.hmass()
+                # print(props)
+                quality = (props["hmass"] - h_liq) / (h_vap - h_liq)
+            else:
+                # For states at or above the critical pressure, the concept of saturation states is not applicable
+                # Instead, use a 'pseudo-critical' state for comparison, where the density is set to the critical density
+                # but the pressure is the same as the state of interest
+                # Use a band of a certain width to prevent a discontinuity
+                temp.update(DmassP_INPUTS, self.critical_point.rho, props["p"])
+                # print(props)
+                quality = (props["hmass"] - 0.95 * temp.hmass()) / (
+                    1.05 * temp.hmass() - 0.95 * temp.hmass()
+                )
+
+        else:
+            quality = np.nan
+
+        props["Q"] = quality
+        props["quality_mass"] = np.nan
+        props["quality_volume"] = np.nan
+
+        return props
+
+    def compute_properties_2phase(self):
+        """Get two-phase properties from mixing rules and single-phase CoolProp properties"""
+
+        # Basic properties of the two-phase mixture
+        T_mix = self._AS.T()
+        p_mix = self._AS.p()
+        rho_mix = self._AS.rhomass()
+        u_mix = self._AS.umass()
+        h_mix = self._AS.hmass()
+        s_mix = self._AS.smass()
+        gibbs_mix = self._AS.gibbsmass()
+
+        # Instantiate new fluid object to compute saturation properties without changing the state of the class
+        temp = CP.AbstractState(self.backend, self.name)
+
+        # Saturated liquid properties
+        temp.update(CP.QT_INPUTS, 0.00, T_mix)
+        rho_L = temp.rhomass()
+        cp_L = temp.cpmass()
+        cv_L = temp.cvmass()
+        k_L = temp.conductivity()
+        mu_L = temp.viscosity()
+        speed_sound_L = temp.speed_sound()
+        dsdp_L = temp.first_saturation_deriv(CP.iSmass, CP.iP)
+
+        # Saturated vapor properties
+        temp.update(CP.QT_INPUTS, 1.00, T_mix)
+        rho_V = temp.rhomass()
+        cp_V = temp.cpmass()
+        cv_V = temp.cvmass()
+        k_V = temp.conductivity()
+        mu_V = temp.viscosity()
+        speed_sound_V = temp.speed_sound()
+        dsdp_V = temp.first_saturation_deriv(CP.iSmass, CP.iP)
+
+        # Volume fractions of vapor and liquid
+        vol_frac_V = (rho_mix - rho_L) / (rho_V - rho_L)
+        vol_frac_L = 1.00 - vol_frac_V
+
+        # Mass fractions of vapor and liquid
+        mass_frac_V = (1 / rho_mix - 1 / rho_L) / (1 / rho_V - 1 / rho_L)
+        mass_frac_L = 1.00 - mass_frac_V
+
+        # Heat capacities of the two-phase mixture
+        cp_mix = mass_frac_L * cp_L + mass_frac_V * cp_V
+        cv_mix = mass_frac_L * cv_L + mass_frac_V * cv_V
+
+        # Transport properties of the two-phase mixture
+        k_mix = vol_frac_L * k_L + vol_frac_V * k_V
+        mu_mix = vol_frac_L * mu_L + vol_frac_V * mu_V
+
+        # Compressibility factor of the two-phase mixture
+        M = self._AS.molar_mass()
+        R = self._AS.gas_constant()
+        Z_mix = p_mix / (rho_mix * (R / M) * T_mix)
+
+        # Speed of sound of the two-phase mixture
+        mechanical_equilibrium = vol_frac_L / (
+            rho_L * speed_sound_L**2
+        ) + vol_frac_V / (rho_V * speed_sound_V**2)
+        thermal_equilibrium = T_mix * (
+            vol_frac_L * rho_L / cp_L * dsdp_L**2
+            + vol_frac_V * rho_V / cp_V * dsdp_V**2
+        )
+        compressibility_HEM = mechanical_equilibrium + thermal_equilibrium
+        if mass_frac_V < 1e-6:  # Avoid discontinuity when Q_v=0
+            a_HEM = speed_sound_L
+        elif mass_frac_V > 1.0 - 1e-6:  # Avoid discontinuity when Q_v=1
+            a_HEM = speed_sound_V
+        else:
+            a_HEM = (1 / rho_mix / compressibility_HEM) ** 0.5
+
+        # Store properties in dictionary
+        properties = {}
+        properties["T"] = T_mix
+        properties["p"] = p_mix
+        properties["rhomass"] = rho_mix
+        properties["umass"] = u_mix
+        properties["hmass"] = h_mix
+        properties["smass"] = s_mix
+        properties["gibbsmass"] = gibbs_mix
+        properties["cvmass"] = cv_mix
+        properties["cpmass"] = cp_mix
+        properties["gamma"] = properties["cpmass"] / properties["cvmass"]
+        properties["compressibility_factor"] = Z_mix
+        properties["speed_sound"] = a_HEM
+        properties["isentropic_bulk_modulus"] = rho_mix * a_HEM**2
+        properties["isentropic_compressibility"] = (rho_mix * a_HEM**2) ** -1
+        properties["isothermal_bulk_modulus"] = np.nan
+        properties["isothermal_compressibility"] = np.nan
+        properties["isobaric_expansion_coefficient"] = np.nan
+        properties["viscosity"] = mu_mix
+        properties["conductivity"] = k_mix
+        properties["Q"] = mass_frac_V
+        properties["quality_mass"] = mass_frac_V
+        properties["quality_volume"] = vol_frac_V
+
+        return properties
+
+    def set_state_metastable(
+        self, prop_1, prop_1_value, prop_2, prop_2_value, rho_guess, T_guess
+    ):
+        # problem = PropertyRoot()
+
+        return
+
+    def set_state_metastable_rhoT(self, rho, T):
+        # TODO: Add check to see if we are inside thespinodal and return two phase properties if yes
+        # TODO: implement root finding functionality to accept p-h, T-s, p-s arguments [good initial guess required]
+        # TODO: can it be generalized so that it uses equilibrium as initial guess with any inputs? (even if they are T-d)
+        try:
+            # Update Coolprop thermodynamic state
+            self.properties = self.compute_properties_metastable_rhoT(rho, T, self._AS)
+
+            # Add properties as aliases
+            for key, value in self.aliases.items():
+                self.properties[key] = self.properties[value]
+
+            # No errors computing the properies
+            self.converged_flag = True
+
+        # Something went wrong while computing the properties
+        except Exception as e:
+            self.converged_flag = False
+            if self.exceptions:
+                raise e
+
+        return self.properties
+
+    @staticmethod
+    def compute_properties_metastable_rhoT(rho, T, AS):
+        """
+        Compute the thermodynamic properties of a fluid using the Helmholtz
+        energy equation of state. All properties thermodynamic properties can
+        be derived as combinations of the Helmholtz energy and its
+        derivatives with respect to density and pressure.
+
+        This function can be used to estimate metastable properties using the
+        equation of state beyond the saturation lines.
+        """
+
+        # Update thermodynamic state
+        AS.update(CP.DmassT_INPUTS, rho, T)
+
+        # Get fluid constant properties
+        R = AS.gas_constant()
+        M = AS.molar_mass()
+        T_crit = AS.T_critical()
+        rho_crit = AS.rhomass_critical()
+
+        # Compute reduced variables
+        tau = T_crit / T
+        delta = rho / rho_crit
+
+        # Compute from the Helmholtz energy derivatives
+        alpha = AS.alpha0() + AS.alphar()
+        dalpha_dTau = AS.dalpha0_dTau() + AS.dalphar_dTau()
+        dalpha_dDelta = AS.dalpha0_dDelta() + AS.dalphar_dDelta()
+        d2alpha_dTau2 = AS.d2alpha0_dTau2() + AS.d2alphar_dTau2()
+        d2alpha_dDelta2 = AS.d2alpha0_dDelta2() + AS.d2alphar_dDelta2()
+        d2alpha_dDelta_dTau = AS.d2alpha0_dDelta_dTau() + AS.d2alphar_dDelta_dTau()
+
+        # Compute thermodynamic properties from Helmholtz energy EOS
+        properties = {}
+        properties["T"] = T
+        properties["p"] = (R / M) * T * rho * delta * dalpha_dDelta
+        properties["rhomass"] = rho
+        properties["umass"] = (R / M) * T * (tau * dalpha_dTau)
+        properties["hmass"] = (R / M) * T * (tau * dalpha_dTau + delta * dalpha_dDelta)
+        properties["smass"] = (R / M) * (tau * dalpha_dTau - alpha)
+        properties["gibbsmass"] = (R / M) * T * (alpha + delta * dalpha_dDelta)
+        properties["cvmass"] = (R / M) * (-(tau**2) * d2alpha_dTau2)
+        properties["cpmass"] = (R / M) * (
+            -(tau**2) * d2alpha_dTau2
+            + (delta * dalpha_dDelta - delta * tau * d2alpha_dDelta_dTau) ** 2
+            / (2 * delta * dalpha_dDelta + delta**2 * d2alpha_dDelta2)
+        )
+        properties["gamma"] = properties["cpmass"] / properties["cvmass"]
+        properties["compressibility_factor"] = delta * dalpha_dDelta
+        properties["speed_sound"] = (
+            (R / M * T)
+            * (
+                (2 * delta * dalpha_dDelta + delta**2 * d2alpha_dDelta2)
+                - (delta * dalpha_dDelta - delta * tau * d2alpha_dDelta_dTau) ** 2
+                / (tau**2 * d2alpha_dTau2)
+            )
+        ) ** 0.5
+        properties["isentropic_bulk_modulus"] = (rho * R / M * T) * (
+            (2 * delta * dalpha_dDelta + delta**2 * d2alpha_dDelta2)
+            - (delta * dalpha_dDelta - delta * tau * d2alpha_dDelta_dTau) ** 2
+            / (tau**2 * d2alpha_dTau2)
+        )
+        properties["isentropic_compressibility"] = (
+            1 / properties["isentropic_bulk_modulus"]
+        )
+        properties["isothermal_bulk_modulus"] = (
+            R / M * T * rho * (2 * delta * dalpha_dDelta + delta**2 * d2alpha_dDelta2)
+        )
+        properties["isothermal_compressibility"] = 1 / (
+            R / M * T * rho * (2 * delta * dalpha_dDelta + delta**2 * d2alpha_dDelta2)
+        )
+        properties["isobaric_expansion_coefficient"] = (
+            (1 / T)
+            * (delta * dalpha_dDelta - delta * tau * d2alpha_dDelta_dTau)
+            / (2 * delta * dalpha_dDelta + delta**2 * d2alpha_dDelta2)
+        )
+        properties["viscosity"] = AS.viscosity()
+        properties["conductivity"] = AS.conductivity()
+        properties["Q"] = np.nan
+        properties["quality_mass"] = np.nan
+        properties["quality_volume"] = np.nan
+
+        return properties
+
+    def get_property(self, propname):
+        """Get the value of a single property"""
+        if propname in self.properties:
+            return self.properties[propname]
+        else:
+            valid_options = "\n\t".join(self.properties.keys())
+            raise ValueError(
+                f"The requested property '{propname}' is not available. The valid options are:\n\t{valid_options}"
+            )
+
+    def compute_properties_meanline(self, input_type, prop_1, prop_2):
+        """Extract fluid properties for meanline model"""
+
+        # Compute properties in the normal way
+        self.set_state(input_type, prop_1, prop_2)
+
+        # Store a subset of the properties in a dictionary
+        fluid_properties = {}
+        property_subset = [
+            "p",
+            "T",
+            "h",
+            "s",
+            "d",
+            "Z",
+            "a",
+            "mu",
+            "k",
+            "cp",
+            "cv",
+            "gamma",
+        ]
+        for item in property_subset:
+            fluid_properties[item] = self.properties[item]
+
+        return fluid_properties
+
+    def compute_sonic_state(self, input_type, prop_1, prop_2):
+        props = {}
+
+        return FluidState(props)
+
+    # ------------------------------------------------------------------------------------ #
+    # ------------------------------------------------------------------------------------ #
+    # ------------------------------------------------------------------------------------ #
+
+    def _get_label(self, label, show_in_legend):
+        """Returns the appropriate label value based on whether it should be shown in the legend."""
+        return label if show_in_legend else "_no_legend_"
+
+    def _plot_or_update_line(self, axes, x_data, y_data, line_name, **plot_params):
+        # Ensure there is a dictionary for this axes
+        if axes not in self.graphic_elements:
+            self.graphic_elements[axes] = {}
+
+        # Check if the line exists for this axes
+        if line_name in self.graphic_elements[axes]:
+            line = self.graphic_elements[axes][line_name]
+            line.set_data(np.atleast_1d(x_data), np.atleast_1d(y_data))
+            # Update line properties
+            for param, value in plot_params.items():
+                setattr(line, param, value)
+            line.set_visible(True)
+        else:
+            # Create a new line with the provided plot parameters
+            (line,) = axes.plot(x_data, y_data, **plot_params)
+            self.graphic_elements[axes][line_name] = line
+        return line
+
+    def _plot_or_update_contours(
+        self, axes, x_data, y_data, z_data, contour_levels, line_name, **contour_params
+    ):
+        # Ensure there is a dictionary for this axes
+        if axes not in self.graphic_elements:
+            self.graphic_elements[axes] = {}
+
+        # Check if the contour exists for this axes
+        if line_name in self.graphic_elements[axes]:
+            for coll in self.graphic_elements[axes][line_name].collections:
+                coll.remove()  # Remove the old contour collections
+
+        # Create a new contour
+        contour = axes.contour(x_data, y_data, z_data, contour_levels, **contour_params)
+        self.graphic_elements[axes][line_name] = contour
+        return contour
+
+    def _set_visibility(self, axes, line_name, visible):
+        if axes in self.graphic_elements and line_name in self.graphic_elements[axes]:
+            self.graphic_elements[axes][line_name].set_visible(visible)
+
+    def plot_phase_diagram(
+        self,
+        x_variable="s",
+        y_variable="T",
+        axes=None,
+        num_points=200,
+        plot_saturation_line=True,
+        plot_critical_point=True,
+        plot_triple_point_liquid=False,
+        plot_triple_point_vapor=False,
+        plot_spinodal_line=False,
+        spinodal_line_method="standard",
+        spinodal_line_color=0.5 * np.array([1, 1, 1]),
+        spinodal_line_width=0.75,
+        plot_quality_isolines=False,
+        plot_pseudocritical_line=False,
+        quality_levels=np.linspace(0.1, 1.0, 10),
+        quality_labels=False,
+        show_in_legend=False,
+        **kwargs,
+    ):
+        if axes is None:
+            axes = plt.gca()
+
+        # Saturation line
+        if plot_saturation_line:
+            if self.sat_liq is None or self.sat_vap is None:
+                self.sat_liq, self.sat_vap = compute_saturation_line(self, num_points)
+            x = self.sat_liq[x_variable] + self.sat_vap[x_variable]
+            y = self.sat_liq[y_variable] + self.sat_vap[y_variable]
+            label = self._get_label("Saturation line", show_in_legend)
+            params = {"label": label, "color": "black"}
+            self._graphic_saturation_line = self._plot_or_update_line(
+                axes, x, y, "saturation_line", **params
+            )
+        else:
+            self._set_visibility(axes, "saturation_line", False)
+
+        # Plot pseudocritical line
+        if plot_pseudocritical_line:
+            if self.pseudo_critical_line is None:
+                self.pseudo_critical_line = compute_pseudocritical_line(self)
+            x = self.pseudo_critical_line[x_variable]
+            y = self.pseudo_critical_line[y_variable]
+            label = self._get_label("Pseudocritical line", show_in_legend)
+            params = {
+                "label": label,
+                "color": "black",
+                "linestyle": "--",
+                "linewidth": 0.75,
+            }
+            self._graphic_pseudocritical_line = self._plot_or_update_line(
+                axes, x, y, "pseudocritical_line", **params
+            )
+        else:
+            self._set_visibility(axes, "pseudocritical_line", False)
+
+        # Plot quality isolines
+        if plot_quality_isolines:
+            if self.q_mesh is None:
+                self.q_mesh = compute_quality_grid(self, num_points, quality_levels)
+            x = self.q_mesh[x_variable]
+            y = self.q_mesh[y_variable]
+            _, m = np.shape(x)
+            z = np.tile(quality_levels, (m, 1)).T
+            params = {"colors": "black", "linestyles": ":", "linewidths": 0.75}
+            self._graphics_q_lines = self._plot_or_update_contours(
+                axes, x, y, z, quality_levels, "quality_isolines", **params
+            )
+
+            if quality_labels:
+                axes.clabel(self._graphics_q_lines, fontsize=9, rightside_up=True)
+
+        else:
+            # Remove existing contour lines if they exist
+            if "quality_isolines" in self.graphic_elements.get(axes, {}):
+                for coll in self.graphic_elements[axes]["quality_isolines"].collections:
+                    coll.remove()
+                del self.graphic_elements[axes]["quality_isolines"]
+
+        # Plot critical point
+        params = {
+            "color": "black",
+            "marker": "o",
+            "markersize": 4.5,
+            "markerfacecolor": "w",
+        }
+        if plot_critical_point:
+            x = self.critical_point[x_variable]
+            y = self.critical_point[y_variable]
+            label = self._get_label("Critical point", show_in_legend)
+            self._graphic_critical_point = self._plot_or_update_line(
+                axes, x, y, "critical_point", label=label, **params
+            )
+        else:
+            self._set_visibility(axes, "critical_point", False)
+
+        # Plot liquid triple point
+        if plot_triple_point_liquid:
+            x = self.triple_point_liquid[x_variable]
+            y = self.triple_point_liquid[y_variable]
+            label = self._get_label("Triple point liquid", show_in_legend)
+            self._graphic_triple_point_liquid = self._plot_or_update_line(
+                axes, x, y, "triple_point_liquid", label=label, **params
+            )
+        else:
+            self._set_visibility(axes, "triple_point_liquid", False)
+
+        # Plot vapor triple point
+        if plot_triple_point_vapor:
+            x = self.triple_point_vapor[x_variable]
+            y = self.triple_point_vapor[y_variable]
+            label = self._get_label("Triple point vapor", show_in_legend)
+            self._graphic_triple_point_vapor = self._plot_or_update_line(
+                axes, x, y, "triple_point_vapor", label=label, **params
+            )
+        else:
+            self._set_visibility(axes, "triple_point_vapor", False)
+
+        return axes
+
+
+def compute_saturation_line(fluid, N_points=100):
+    # Initialize objects to store properties
+    prop_names = fluid.properties.keys()
+    liquid_line = {name: [] for name in prop_names}
+    vapor_line = {name: [] for name in prop_names}
+
+    # Define temperature array with refinement close to the critical point
+    ratio = 1 - fluid.triple_point_liquid.T / fluid.critical_point.T
+    t1 = np.logspace(
+        np.log10(1 - 0.9999), np.log10(ratio / 10), int(np.ceil(N_points / 2))
+    )
+    t2 = np.logspace(np.log10(ratio / 10), np.log10(ratio), int(np.floor(N_points / 2)))
+    T_sat = (1 - np.concatenate([t1, t2])) * fluid.critical_point.T
+
+    # Loop over temperatures and property names in an efficient way
+    for T in T_sat:
+        # Compute liquid saturation line
+        for name in prop_names:
+            fluid.set_state(CP.QT_INPUTS, 0.00, T)
+            liquid_line[name].append(fluid.properties[name])
+
+        # Compute vapor saturation line
+        for name in prop_names:
+            fluid.set_state(CP.QT_INPUTS, 1.00, T)
+            vapor_line[name].append(fluid.properties[name])
+
+    # Add critical point as part of the spinodal line
+    for name in prop_names:
+        liquid_line[name] = [fluid.critical_point[name]] + liquid_line[name]
+        vapor_line[name] = [fluid.critical_point[name]] + vapor_line[name]
+
+    # Re-format for easy concatenation
+    for name in prop_names:
+        liquid_line[name] = list(reversed(liquid_line[name]))
+
+    return liquid_line, vapor_line
+
+
+def compute_spinodal_line(fluid, N_points=100, method="standard"):
+    raise NotImplementedError(
+        "The 'compute_spinodal_line' function has not been implemented yet."
+    )
+
+
+def compute_pseudocritical_line(fluid, N_points=100):
+    # Initialize objects to store properties
+    prop_names = fluid.properties.keys()
+    pseudocritical_line = {name: [] for name in prop_names}
+
+    # Define temperature array with refinement close to the critical point
+    tau = np.logspace(np.log10(1e-3), np.log10(1), N_points)
+    T_range = (1 + tau) * fluid.critical_point.T
+
+    # Loop over temperatures and compute pseudocritical properties
+    for T in T_range:
+        for name in prop_names:
+            fluid.set_state(DmassT_INPUTS, fluid.critical_point.d, T)
+            pseudocritical_line[name].append(fluid.properties[name])
+
+    return pseudocritical_line
+
+
+def compute_quality_grid(fluid, num_points, quality_levels):
+    # Define temperature levels
+    t1 = np.logspace(np.log10(1 - 0.9999), np.log10(0.1), int(num_points / 2))
+    t2 = np.logspace(
+        np.log10(0.1),
+        np.log10(1 - (fluid.triple_point_liquid.T) / fluid.critical_point.T),
+        int(num_points / 2),
+    )
+    temperature_levels = (1 - np.hstack((t1, t2))) * fluid.critical_point.T
+
+    # Calculate property grid
+    quality_grid = []
+    for q in quality_levels:
+        row = []
+        for T in temperature_levels:
+            row.append(fluid.set_state(CP.QT_INPUTS, q, T))
+        quality_grid.append(row)
+
+    return states_to_dict_2d(quality_grid)
+
+
+def compute_properties_meshgrid(fluid, input_pair, range_1, range_2):
+    """
+    Compute fluid properties over a specified range and store them in a dictionary.
+
+    This function creates a meshgrid of property values based on the specified ranges and input pair,
+    computes the properties of the fluid at each point on the grid, and stores the results in a
+    dictionary where each key corresponds to a fluid property.
+
+    Parameters
+    ----------
+    fluid : Fluid object
+        An instance of the Fluid class.
+    input_pair : tuple
+        The input pair specifying the property type (e.g., PT_INPUTS for pressure-temperature).
+    range1 : tuple
+        The range linspace(min, max, n) for the first property of the input pair.
+    range2 : tuple
+        The range linspace(min, max, n) for the second property of the input pair.
+
+    Returns
+    -------
+    properties_dict : dict
+        A dictionary where keys are property names and values are 2D numpy arrays of computed properties.
+    grid1, grid2 : numpy.ndarray
+        The meshgrid arrays for the first and second properties.
+    """
+
+    # Create the meshgrid
+    grid1, grid2 = np.meshgrid(range_1, range_2)
+
+    # Initialize dictionary to store properties and pre-allocate storage
+    properties_dict = {key: np.zeros_like(grid1) for key in fluid.properties}
+
+    # Compute properties at each point
+    for i in range(len(range_2)):
+        for j in range(len(range_1)):
+            # Set state of the fluid
+            fluid.set_state(input_pair, grid1[i, j], grid2[i, j])
+
+            # Store the properties
+            for key in fluid.properties:
+                properties_dict[key][i, j] = fluid.properties[key]
+
+    return properties_dict
+
+
+# Implement class to calculate intersection with saturation line?
+
+
+class PropertyRoot(NonlinearSystemProblem):
+    """
+    Find the root for thermodynamic state by iterating on the density-temperature
+    native inputs to the helmholtz energy equations of state.
+
+    Attributes
+    ----------
+    prop_1 : str
+        The first property to be compared.
+    prop_1_value : float
+        The value of the first property.
+    prop_2 : str
+        The second property to be compared.
+    prop_2_value : float
+        The value of the second property.
+    fluid : object
+        An instance of the fluid class which has the helmholtz energy equations of state.
+
+    Methods
+    -------
+    get_values(x)
+        Calculates the residuals based on the given input values of density and temperature.
+    """
+
+    def __init__(self, prop_1, prop_1_value, prop_2, prop_2_value, fluid):
+        self.prop_1 = prop_1
+        self.prop_2 = prop_2
+        self.prop_1_value = prop_1_value
+        self.prop_2_value = prop_2_value
+        self.fluid = fluid
+
+    def get_values(self, x):
+        """
+        Compute the residuals for the given density and temperature.
+
+        Parameters
+        ----------
+        x : list
+            List containing the values for density and temperature.
+
+        Returns
+        -------
+        np.ndarray
+            Array containing residuals (difference) for the two properties.
+        """
+        # Ensure x can be indexed and contains exactly two elements
+        if not hasattr(x, "__getitem__") or len(x) != 2:
+            raise ValueError(
+                "Input x must be a list, tuple or numpy array containing exactly two elements: density and temperature."
+            )
+
+        rho, T = x
+        props = self.fluid.set_state_metastable_rhoT(rho, T)
+
+        residual = np.asarray(
+            [
+                self.prop_1_value - props[self.prop_1],
+                self.prop_2_value - props[self.prop_2],
+            ]
+        )
+
+        return residual
+
+
+class SonicStateProblem(NonlinearSystemProblem):
+    """ """
+
+    def __init__(self, fluid, property_pair, prop_1, prop_2):
+        # Calculate the thermodynamic state
+        self.fluid = fluid
+        self.state = fluid.set_state(property_pair, prop_1, prop_2)
+
+        # Initial guess based in input sstate
+        self.initial_guess = [self.state.d, self.state.T]
+
+        # # Initial guess based on perfect gass relations
+        # gamma = self.state.gamma
+        # d_star = (2/(gamma + 1)) ** (1/(gamma-1)) * self.state.rho
+        # T_star =  (2/(gamma + 1)) * self.state.T
+        # self.initial_guess = [d_star, T_star]
+
+    def get_values(self, x):
+        # Ensure x can be indexed and contains exactly two elements
+        if not hasattr(x, "__getitem__") or len(x) != 2:
+            raise ValueError(
+                "Input x must be a list, tuple or numpy array containing exactly two elements: density and temperature."
+            )
+
+        # Calculate state for the current density-temperature pair
+        crit_state = self.fluid.set_state(DmassT_INPUTS, x[0], x[1])
+
+        # Calculate the sonic state residual
+        residual = np.asarray(
+            [
+                1.0 - (crit_state.h + 0.5 * crit_state.a**2) / self.state.h,
+                1.0 - crit_state.s / self.state.s,
+            ]
+        )
+
+        return residual
+
+
+class SonicStateProblem2(OptimizationProblem):
+    """ """
+
+    def __init__(self, fluid, property_pair, prop_1, prop_2):
+        # Calculate the thermodynamic state
+        self.fluid = fluid
+        self.state = fluid.set_state(property_pair, prop_1, prop_2)
+
+        # Initial guess based in input sstate
+        self.initial_guess = [self.state.d, self.state.T * 0.9]
+
+        # # Initial guess based on perfect gass relations
+        # gamma = self.state.gamma
+        # d_star = (2/(gamma + 1)) ** (1/(gamma-1)) * self.state.rho
+        # T_star =  (2/(gamma + 1)) * self.state.T
+        # self.initial_guess = [d_star, T_star]
+
+    def get_values(self, x):
+        """
+        Compute the residuals for the given density and temperature.
+
+        Parameters
+        ----------
+        x : list
+            List containing the values for density and temperature.
+
+        Returns
+        -------
+        np.ndarray
+            Array containing residuals (difference) for the two properties.
+        """
+
+        # Ensure x can be indexed and contains exactly two elements
+        if not hasattr(x, "__getitem__") or len(x) != 2:
+            raise ValueError(
+                "Input x must be a list, tuple or numpy array containing exactly two elements: density and temperature."
+            )
+
+        # Calculate state for the current density-temperature pair
+        crit_state = self.fluid.set_state(DmassT_INPUTS, x[0], x[1])
+
+        # Calculate the sonic state residual
+        residual = [
+            1.0 - crit_state.s / self.state.s,
+        ]
+
+        # Objective function
+        self.f = crit_state.d**2 * (self.state.h - crit_state.h)
+        self.f = -self.f / (self.state.d * self.state.a) ** 2
+
+        # Equality constraints
+        self.c_eq = residual
+
+        # No inequality constraints given for this problem
+        self.c_ineq = []
+
+        # Combine objective function and constraints
+        objective_and_constraints = self.merge_objective_and_constraints(
+            self.f, self.c_eq, self.c_ineq
+        )
+
+        return objective_and_constraints
+
+    def get_bounds(self):
+        bound_density = (
+            self.fluid.triple_point_vapor.d * 1.5,
+            self.fluid.critical_point.d * 3,
+        )
+        bound_temperature = (
+            self.fluid.triple_point_vapor.T * 1,
+            self.fluid.critical_point.T * 3,
+        )
+        # return [bound_density, bound_temperature]
+        return None
+
+    def get_n_eq(self):
+        return self.get_number_of_constraints(self.c_eq)
+
+    def get_n_ineq(self):
+        return self.get_number_of_constraints(self.c_ineq)
+
+
+def calculate_superheating(state, fluid):
+    """
+    Calculates the degree of superheating for a given state and adds this information to the state.
+
+    Parameters
+    ----------
+    state : dict
+        A dictionary representing the thermodynamic state, containing at least pressure (p), temperature (T),
+        and enthalpy (h) of the fluid.
+    fluid : object
+        An object representing the fluid with its properties, including methods to set state and critical point data.
+
+    Returns
+    -------
+    dict
+        The input state dictionary with an added field 'superheating' representing the degree of superheating.
+    """
+
+    # Check if the pressure is below the critical pressure of the fluid
+    if state["p"] < fluid.critical_point.p:
+        # Set the saturation state of the fluid at the given pressure
+        sat_state = fluid.set_state(PQ_INPUTS, state["p"], 1.00)
+
+        # Check if the fluid is in the two-phase region
+        if fluid._AS.phase() == CP.iphase_twophase:
+            # In the two-phase region, define superheating as the normalized difference in enthalpy
+            # The normalization is done using the specific heat capacity at saturation (cp)
+            # This provides a continuous measure of superheating, even in the two-phase region
+            state["superheating"] = (state["h"] - sat_state.h) / sat_state.cp
+        else:
+            # Outside the two-phase region, superheating is the difference in temperature
+            # from the saturation temperature at the same pressure
+            state["superheating"] = state["T"] - sat_state.T
+    else:
+        # For states at or above the critical pressure, the concept of saturation temperature is not applicable
+        # Instead, use a 'pseudo-critical' state for comparison, where the density is set to the critical density
+        # but the pressure is the same as the state of interest
+        pseudo_crit = fluid.set_state(
+            DmassP_INPUTS, fluid.critical_point.rho, state["p"]
+        )
+
+        # Define superheating as the difference in enthalpy from this 'pseudo-critical' state
+        # This approach extends the definition of superheating to conditions above the critical pressure
+        state["superheating"] = state.T - pseudo_crit.T
+
+    return state
+
+
+def calculate_subcooling(state, fluid):
+    """
+    Calculates the degree of subcooling for a given state and adds this information to the state.
+
+    Parameters
+    ----------
+    state : dict
+        A dictionary representing the thermodynamic state, containing at least pressure (p), temperature (T),
+        and enthalpy (h) of the fluid.
+    fluid : object
+        An object representing the fluid with its properties, including methods to set state and critical point data.
+
+    Returns
+    -------
+    dict
+        The input state dictionary with an added field 'subcooling' representing the degree of subcooling.
+    """
+
+    # Check if the pressure is below the critical pressure of the fluid
+    if state["p"] < fluid.critical_point.p:
+        # Set the saturation state of the fluid at the given pressure
+        sat_state = fluid.set_state(PQ_INPUTS, state["p"], 0.00)
+
+        # Check if the fluid is in the two-phase region
+        if fluid._AS.phase() == CP.iphase_twophase:
+            # In the two-phase region, define subcooling as the normalized difference in enthalpy
+            # The normalization is done using the specific heat capacity at saturation (cp)
+            # This provides a continuous measure of subcooling, even in the two-phase region
+            state["subcooling"] = (sat_state.h - state["h"]) / sat_state.cp
+        else:
+            # Outside the two-phase region, subcooling is the difference in temperature
+            # from the saturation temperature at the same pressure
+            state["subcooling"] = sat_state.T - state["T"]
+    else:
+        # For states at or above the critical pressure, the concept of saturation temperature is not applicable
+        # Instead, use a 'pseudo-critical' state for comparison, where the density is set to the critical density
+        # but the pressure is the same as the state of interest
+        pseudo_crit = fluid.set_state(
+            DmassP_INPUTS, fluid.critical_point.rho, state["p"]
+        )
+
+        # Define subcooling as the difference in enthalpy from this 'pseudo-critical' state
+        # This approach extends the definition of subcooling to conditions above the critical pressure
+        state["subcooling"] = pseudo_crit.T - state.T
+
+    return state
+
+
+# def compute_properties_metastable_rhoT(rho, T, fluid):
+#     """
+#     Compute the thermodynamic properties of a fluid using the Helmholtz
+#     energy equation of state. All properties thermodynamic properties can
+#     be derived as combinations of the Helmholtz energy and its
+#     derivatives with respect to density and pressure.
+
+#     This function can be used to estimate metastable properties using the
+#     equation of state beyond the saturation lines.
+#     """
+
+#     # Update thermodynamic state
+#     fluid.update(CP.DmassT_INPUTS, rho, T)
+
+#     # Get fluid constant properties
+#     R = fluid.gas_constant()
+#     M = fluid.molar_mass()
+#     T_crit = fluid.T_critical()
+#     rho_crit = fluid.rhomass_critical()
+
+#     # Compute reduced variables
+#     tau = T_crit / T
+#     delta = rho / rho_crit
+
+#     # Compute from the Helmholtz energy derivatives
+#     alpha = fluid.alpha0() + fluid.alphar()
+#     dalpha_dTau = fluid.dalpha0_dTau() + fluid.dalphar_dTau()
+#     dalpha_dDelta = fluid.dalpha0_dDelta() + fluid.dalphar_dDelta()
+#     d2alpha_dTau2 = fluid.d2alpha0_dTau2() + fluid.d2alphar_dTau2()
+#     d2alpha_dDelta2 = fluid.d2alpha0_dDelta2() + fluid.d2alphar_dDelta2()
+#     d2alpha_dDelta_dTau = fluid.d2alpha0_dDelta_dTau() + fluid.d2alphar_dDelta_dTau()
+
+#     # Compute thermodynamic properties from Helmholtz energy EOS
+#     properties = {}
+#     properties['T'] = T
+#     properties['p'] = (R/M)*T*rho*delta*dalpha_dDelta
+#     properties['rhomass'] = rho
+#     properties['umass'] = (R/M)*T*(tau*dalpha_dTau)
+#     properties['hmass'] = (R/M)*T*(tau*dalpha_dTau+delta*dalpha_dDelta)
+#     properties['smass'] = (R/M)*(tau*dalpha_dTau - alpha)
+#     properties['gibbsmass'] = (R/M)*T*(alpha + delta*dalpha_dDelta)
+#     properties['cvmass'] = (R/M)*(-tau**2*d2alpha_dTau2)
+#     properties['cpmass'] = (R/M)*(-tau**2*d2alpha_dTau2 + (delta*dalpha_dDelta - delta*tau*d2alpha_dDelta_dTau)**2/(2*delta*dalpha_dDelta + delta**2*d2alpha_dDelta2))
+#     properties['gamma'] = properties['cpmass']/properties['cvmass']
+#     properties['compressibility_factor'] = delta*dalpha_dDelta
+#     properties['speed_sound'] = ((R/M)*T*(2*delta*dalpha_dDelta + delta**2*d2alpha_dDelta2 - (delta*dalpha_dDelta - delta*tau*d2alpha_dDelta_dTau)**2/(tau**2*d2alpha_dTau2)))**0.5
+#     properties['isentropic_bulk_modulus'] = rho*(R/M)*T*(2*delta*dalpha_dDelta + delta**2*d2alpha_dDelta2 - (delta*dalpha_dDelta - delta*tau*d2alpha_dDelta_dTau)**2/(tau**2*d2alpha_dTau2))
+#     properties['isentropic_compressibility'] = 1 / properties["isentropic_bulk_modulus"]
+#     properties['isothermal_bulk_modulus'] = (R/M)*T*rho*(2*delta*dalpha_dDelta + delta**2*d2alpha_dDelta2)
+#     properties['isothermal_compressibility'] = 1/((R/M)*T*rho*(2*delta*dalpha_dDelta + delta**2*d2alpha_dDelta2))
+#     properties['isobaric_expansion_coefficient'] = 1/T*(delta*dalpha_dDelta - delta*tau*d2alpha_dDelta_dTau)/(2*delta*dalpha_dDelta + delta**2*d2alpha_dDelta2)
+#     properties['viscosity'] = fluid.viscosity()
+#     properties['conductivity'] = fluid.conductivity()
+#     properties['Q'] = np.nan
+#     properties['quality_mass'] = np.nan
+#     properties['quality_volume'] = np.nan
+
+#     return properties
+
+
+if __name__ == "__main__":
+    fluid = Fluid("Water", backend="HEOS")
+
+    # # Properties of liquid water
+    # props_stable = fluid.set_state(CP.PT_INPUTS, 101325, 300)
+    # print()
+    # print("Properties of liquid water")
+    # print(f"{'Property':35} {'value':6}")
+    # for key, value in props_stable.items():
+    #     print(f"{key:35} {value:.6e}")
+
+    # # Properties of water/steam mixture
+    # props = fluid.set_state(CP.QT_INPUTS, 0.5, 300)
+    # print()
+    # print("Properties of water/steam mixture")
+    # print(f"{'Property':35} {'value':6}")
+    # for key, value in props.items():
+    #     print(f"{key:35} {value:.6e}")
+
+    # # Get subset of properties for meanline code
+    # props = fluid.compute_properties_meanline(CP.QT_INPUTS, 0.5, 300)
+    # print()
+    # print("Properties for the meanline code")
+    # print(f"{'Property':15} {'value':6}")
+    # for key, value in props.items():
+    #     print(f"{key:15} {value:.6e}")
+
+    # #
+    # props = compute_properties_metastable_rhoT(10, 500, fluid.abstractstate)
+    # print("Metastable properties of water")
+    # print(f"{'Property':35} {'value':6}")
+    # for key, value in props.items():
+    #     print(f"{key:35} {value:.6e}")
+
+    # Check that the metastable property calculations match in the single-phase region
+    p, T = 101325, 300
+    props_stable = fluid.set_state(CP.PT_INPUTS, p, T)
+    print()
+    print(f"Properties of water at p={p} Pa and T={T} K")
+    print(f"{'Property':35} {'Equilibrium':>15} {'Metastable':>15} {'Deviation':>15}")
+    props_metastable = fluid.set_state_metastable_rhoT(
+        props_stable["rho"], props_stable["T"]
+    )
+    for key in props_stable.keys():
+        value_stable = props_stable[key]
+        value_metastable = props_metastable[key]
+        print(
+            f"{key:35} {value_stable:+15.6e} {value_metastable:+15.6e} {(value_stable - value_metastable)/value_stable:+15.6e}"
+        )
```

### Comparing `turboflow-0.1.2/turboflow/pysolver_view/__init__.py` & `turboflow-0.1.3/turboflow/pysolver_view/__init__.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-# Highlight exception messages
-# https://stackoverflow.com/questions/25109105/how-to-colorize-the-output-of-python-errors-in-the-gnome-terminal/52797444#52797444
-try:
-    import IPython.core.ultratb
-except ImportError:
-    # No IPython. Use default exception printing.
-    pass
-else:
-    import sys
-
-    sys.excepthook = IPython.core.ultratb.ColorTB()
-
-
-from .pysolver_utilities import *
-from .numerical_differentiation import *
-from .optimization import *
-from .optimization_problems import *
-from .optimization_wrappers import *
-from .nonlinear_system import *
-from .nonlinear_system_problems import *
+# Highlight exception messages
+# https://stackoverflow.com/questions/25109105/how-to-colorize-the-output-of-python-errors-in-the-gnome-terminal/52797444#52797444
+try:
+    import IPython.core.ultratb
+except ImportError:
+    # No IPython. Use default exception printing.
+    pass
+else:
+    import sys
+
+    sys.excepthook = IPython.core.ultratb.ColorTB()
+
+
+from .pysolver_utilities import *
+from .numerical_differentiation import *
+from .optimization import *
+from .optimization_problems import *
+from .optimization_wrappers import *
+from .nonlinear_system import *
+from .nonlinear_system_problems import *
```

### Comparing `turboflow-0.1.2/turboflow/pysolver_view/numerical_differentiation.py` & `turboflow-0.1.3/turboflow/pysolver_view/numerical_differentiation.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,236 +1,236 @@
-import numpy as np
-from scipy.optimize._numdiff import approx_derivative
-
-DERIVATIVE_METHODS = [
-    "2-point",
-    "3-point",
-    "Complex step method",
-    "forward_finite_differences",
-    "central_finite_differences",
-    "complex_step",
-]
-
-
-def approx_gradient(
-    function_handle, x, f0=None, abs_step=None, method="central_finite_differences"
-):
-    """
-    Approximate the derivatives of a given function at a point using various differentiation methods.
-
-    Parameters
-    ----------
-    function_handle : callable
-        The function for which derivatives are to be approximated.
-    x : array_like
-        The point at which derivatives are to be approximated.
-    abs_step : float or np.ndarray with the same size as x, optional
-        Step size for finite difference methods. By default uses a suitable value for each method.
-    method : str, optional
-        The method to use for differentiation. Available options are:
-        - 'forward_finite_differences' or '2-point': Forward finite differences.
-        - 'central_finite_differences' or '3-point': Central finite differences.
-        - 'complex_step' or 'cs': Complex step method.
-
-        Defaults to 'central_finite_differences'.
-
-    Returns
-    -------
-    numpy.ndarray
-        The approximated derivatives of the function at the given point `x`.
-
-    Raises
-    ------
-    ValueError
-        If an invalid method is provided.
-    """
-    if method == "forward_finite_differences" or method == "2-point":
-        return forward_finite_differences(function_handle, x, abs_step, f0)
-    elif method == "central_finite_differences" or method == "3-point":
-        return central_finite_differences(function_handle, x, abs_step)
-    elif method == "complex_step" or method == "cs":
-        return complex_step_derivative(function_handle, x, abs_step)
-    else:
-        raise ValueError(
-            f"Invalid method '{method}' provided. Available methods: 'forward_finite_differences', "
-            "'central_finite_differences', 'complex_step'."
-        )
-
-
-def forward_finite_differences(function_handle, x, abs_step=None, f0=None):
-    """
-    Gradient approximation by forward finite differences.
-
-    Parameters
-    ----------
-    function_handle : callable
-        The function for which the derivative is calculated.
-    x : np.ndarray
-        The point at which the derivative is calculated.
-    abs_step : float or np.ndarray with the same size as x, optional
-        The step size for finite differences. Default is square root of machine epsilon.
-
-    Returns
-    -------
-    np.ndarray
-        The gradient of the function at point x.
-    """
-
-    # Default step size
-    if abs_step is None:
-        # abs_step = np.finfo(float).eps ** (1 / 2)
-        abs_step = np.finfo(float).eps ** (1 / 2)
-
-    # Ensure abs_step is an array of the same shape as x
-    abs_step = np.broadcast_to(abs_step, x.shape)
-
-    # Avoid one function evaluation if f0 is provided
-    if f0 is None:
-        f0 = np.atleast_1d(function_handle(x))
-    else:
-        f0 = np.atleast_1d(f0)
-
-    # Compute gradient
-    m = np.size(f0)
-    n = np.size(x)
-    df = np.zeros((m, n))
-    for i in range(n):
-        x_step = np.copy(x)
-        x_step[i] += abs_step[i]
-        df[:, i] = (np.atleast_1d(function_handle(x_step)) - f0) / abs_step[i]
-
-    return df.squeeze()
-
-
-def central_finite_differences(function_handle, x, abs_step=None):
-    """
-    Gradient approximation by central finite differences.
-
-    Parameters
-    ----------
-    function_handle : callable
-        The function for which the derivative is calculated.
-    x : np.ndarray
-        The point at which the derivative is calculated.
-    abs_step : float or np.ndarray with the same size as x, optional
-        The step size for finite differences. Default is cobic root of machine epsilon.
-
-    Returns
-    -------
-    np.ndarray
-        The gradient of the function at point x.
-    """
-
-    # Default step size
-    if abs_step is None:
-        abs_step = np.finfo(float).eps ** (1 / 3)
-
-    # Ensure abs_step is an array of the same shape as x
-    abs_step = np.broadcast_to(abs_step, x.shape)
-
-    # Compute gradient
-    f0 = np.atleast_1d(function_handle(x))
-    m = np.size(f0)
-    n = np.size(x)
-    df = np.zeros((m, n))
-    for i in range(n):
-        x_step_forward = np.copy(x)
-        x_step_backward = np.copy(x)
-        x_step_forward[i] += abs_step[i]
-        x_step_backward[i] -= abs_step[i]
-        df[:, i] = (
-            np.atleast_1d(function_handle(x_step_forward))
-            - np.atleast_1d(function_handle(x_step_backward))
-        ) / (2 * abs_step[i])
-
-    return df.squeeze()
-
-
-def complex_step_derivative(function_handle, x, abs_step=None):
-    """
-    Gradient approximation using the complex step method.
-
-    Parameters
-    ----------
-    function_handle : callable
-        The function for which the derivative is calculated.
-    x : np.ndarray
-        The point at which the derivative is calculated.
-    abs_step : float or np.ndarray with the same size as x, optional
-        The step size for the complex step. Default is machine epsilon.
-
-    Returns
-    -------
-    np.ndarray
-        The gradient of the function at point x.
-    """
-
-    # Default step size
-    if abs_step is None:
-        abs_step = np.finfo(float).eps
-
-    # Ensure abs_step is an array of the same shape as x
-    abs_step = np.broadcast_to(abs_step, x.shape)
-
-    # Compute gradient
-    f0 = np.atleast_1d(function_handle(x))
-    m = np.size(f0)
-    n = np.size(x)
-    df = np.zeros((m, n), dtype=np.complex128)
-    for i in range(n):
-        x_step = x.astype(np.complex128)
-        x_step[i] += abs_step[i] * 1j
-        df[:, i] = np.imag(np.atleast_1d(function_handle(x_step))) / abs_step[i]
-
-    return df.squeeze().real
-
-
-def approx_jacobian_hessians(f, x, abs_step=1e-5, lower_triangular=True):
-    """
-    Calculate the Hessian matrices for each component of a vector-valued function using finite differences.
-
-    Parameters
-    ----------
-    f : callable
-        The function for which to find the Hessian matrices. It must take a
-        single argument which is a numpy array and can return a scalar or a numpy array.
-    x : numpy array
-        The point at which the Hessian matrices are calculated.
-    abs_step : float, optional
-        The step size for the finite differences, default is 1e-5.
-    lower_triangular : bool, optional
-        If True, the Hessians are returned in a lower triangular form suitable for Pygmo, default is True.
-
-    Returns
-    -------
-    Hessians : numpy array
-        A tensor where each slice along the first dimension corresponds to the Hessian matrix
-        of each component of the function f at x. If f returns a scalar, the first dimension size is 1.
-        If lower_triangular is True, the Hessians are returned in a lower triangular form.
-    """
-    x = np.asarray(x)
-    f0 = np.atleast_1d(f(x))
-    m = len(f0)
-    n = len(x)
-    Hessians = np.zeros((m, n, n))
-
-    hh = np.eye(n) * abs_step
-
-    for i in range(n):
-        for j in range(i, n):
-            f_ij = np.atleast_1d(f(x + hh[i] + hh[j]))
-            f_i = np.atleast_1d(f(x + hh[i]))
-            f_j = np.atleast_1d(f(x + hh[j]))
-
-            Hessians[:, i, j] = (f_ij - f_i - f_j + f0) / abs_step**2
-            if i != j:  # Symmetry
-                Hessians[:, j, i] = Hessians[:, i, j]
-
-    # Reformat to lower-triangular form suitable for Pygmo
-    if lower_triangular:
-        k = (n * (n + 1)) // 2
-        Hessians_L = np.zeros((m, k))
-        for i in range(m):
-            Hessians_L[i] = Hessians[i][np.tril_indices(n)]
-        Hessians = Hessians_L
-
-    return Hessians
+import numpy as np
+from scipy.optimize._numdiff import approx_derivative
+
+DERIVATIVE_METHODS = [
+    "2-point",
+    "3-point",
+    "Complex step method",
+    "forward_finite_differences",
+    "central_finite_differences",
+    "complex_step",
+]
+
+
+def approx_gradient(
+    function_handle, x, f0=None, abs_step=None, method="central_finite_differences"
+):
+    """
+    Approximate the derivatives of a given function at a point using various differentiation methods.
+
+    Parameters
+    ----------
+    function_handle : callable
+        The function for which derivatives are to be approximated.
+    x : array_like
+        The point at which derivatives are to be approximated.
+    abs_step : float or np.ndarray with the same size as x, optional
+        Step size for finite difference methods. By default uses a suitable value for each method.
+    method : str, optional
+        The method to use for differentiation. Available options are:
+        - 'forward_finite_differences' or '2-point': Forward finite differences.
+        - 'central_finite_differences' or '3-point': Central finite differences.
+        - 'complex_step' or 'cs': Complex step method.
+
+        Defaults to 'central_finite_differences'.
+
+    Returns
+    -------
+    numpy.ndarray
+        The approximated derivatives of the function at the given point `x`.
+
+    Raises
+    ------
+    ValueError
+        If an invalid method is provided.
+    """
+    if method == "forward_finite_differences" or method == "2-point":
+        return forward_finite_differences(function_handle, x, abs_step, f0)
+    elif method == "central_finite_differences" or method == "3-point":
+        return central_finite_differences(function_handle, x, abs_step)
+    elif method == "complex_step" or method == "cs":
+        return complex_step_derivative(function_handle, x, abs_step)
+    else:
+        raise ValueError(
+            f"Invalid method '{method}' provided. Available methods: 'forward_finite_differences', "
+            "'central_finite_differences', 'complex_step'."
+        )
+
+
+def forward_finite_differences(function_handle, x, abs_step=None, f0=None):
+    """
+    Gradient approximation by forward finite differences.
+
+    Parameters
+    ----------
+    function_handle : callable
+        The function for which the derivative is calculated.
+    x : np.ndarray
+        The point at which the derivative is calculated.
+    abs_step : float or np.ndarray with the same size as x, optional
+        The step size for finite differences. Default is square root of machine epsilon.
+
+    Returns
+    -------
+    np.ndarray
+        The gradient of the function at point x.
+    """
+
+    # Default step size
+    if abs_step is None:
+        # abs_step = np.finfo(float).eps ** (1 / 2)
+        abs_step = np.finfo(float).eps ** (1 / 2)
+
+    # Ensure abs_step is an array of the same shape as x
+    abs_step = np.broadcast_to(abs_step, x.shape)
+
+    # Avoid one function evaluation if f0 is provided
+    if f0 is None:
+        f0 = np.atleast_1d(function_handle(x))
+    else:
+        f0 = np.atleast_1d(f0)
+
+    # Compute gradient
+    m = np.size(f0)
+    n = np.size(x)
+    df = np.zeros((m, n))
+    for i in range(n):
+        x_step = np.copy(x)
+        x_step[i] += abs_step[i]
+        df[:, i] = (np.atleast_1d(function_handle(x_step)) - f0) / abs_step[i]
+
+    return df.squeeze()
+
+
+def central_finite_differences(function_handle, x, abs_step=None):
+    """
+    Gradient approximation by central finite differences.
+
+    Parameters
+    ----------
+    function_handle : callable
+        The function for which the derivative is calculated.
+    x : np.ndarray
+        The point at which the derivative is calculated.
+    abs_step : float or np.ndarray with the same size as x, optional
+        The step size for finite differences. Default is cobic root of machine epsilon.
+
+    Returns
+    -------
+    np.ndarray
+        The gradient of the function at point x.
+    """
+
+    # Default step size
+    if abs_step is None:
+        abs_step = np.finfo(float).eps ** (1 / 3)
+
+    # Ensure abs_step is an array of the same shape as x
+    abs_step = np.broadcast_to(abs_step, x.shape)
+
+    # Compute gradient
+    f0 = np.atleast_1d(function_handle(x))
+    m = np.size(f0)
+    n = np.size(x)
+    df = np.zeros((m, n))
+    for i in range(n):
+        x_step_forward = np.copy(x)
+        x_step_backward = np.copy(x)
+        x_step_forward[i] += abs_step[i]
+        x_step_backward[i] -= abs_step[i]
+        df[:, i] = (
+            np.atleast_1d(function_handle(x_step_forward))
+            - np.atleast_1d(function_handle(x_step_backward))
+        ) / (2 * abs_step[i])
+
+    return df.squeeze()
+
+
+def complex_step_derivative(function_handle, x, abs_step=None):
+    """
+    Gradient approximation using the complex step method.
+
+    Parameters
+    ----------
+    function_handle : callable
+        The function for which the derivative is calculated.
+    x : np.ndarray
+        The point at which the derivative is calculated.
+    abs_step : float or np.ndarray with the same size as x, optional
+        The step size for the complex step. Default is machine epsilon.
+
+    Returns
+    -------
+    np.ndarray
+        The gradient of the function at point x.
+    """
+
+    # Default step size
+    if abs_step is None:
+        abs_step = np.finfo(float).eps
+
+    # Ensure abs_step is an array of the same shape as x
+    abs_step = np.broadcast_to(abs_step, x.shape)
+
+    # Compute gradient
+    f0 = np.atleast_1d(function_handle(x))
+    m = np.size(f0)
+    n = np.size(x)
+    df = np.zeros((m, n), dtype=np.complex128)
+    for i in range(n):
+        x_step = x.astype(np.complex128)
+        x_step[i] += abs_step[i] * 1j
+        df[:, i] = np.imag(np.atleast_1d(function_handle(x_step))) / abs_step[i]
+
+    return df.squeeze().real
+
+
+def approx_jacobian_hessians(f, x, abs_step=1e-5, lower_triangular=True):
+    """
+    Calculate the Hessian matrices for each component of a vector-valued function using finite differences.
+
+    Parameters
+    ----------
+    f : callable
+        The function for which to find the Hessian matrices. It must take a
+        single argument which is a numpy array and can return a scalar or a numpy array.
+    x : numpy array
+        The point at which the Hessian matrices are calculated.
+    abs_step : float, optional
+        The step size for the finite differences, default is 1e-5.
+    lower_triangular : bool, optional
+        If True, the Hessians are returned in a lower triangular form suitable for Pygmo, default is True.
+
+    Returns
+    -------
+    Hessians : numpy array
+        A tensor where each slice along the first dimension corresponds to the Hessian matrix
+        of each component of the function f at x. If f returns a scalar, the first dimension size is 1.
+        If lower_triangular is True, the Hessians are returned in a lower triangular form.
+    """
+    x = np.asarray(x)
+    f0 = np.atleast_1d(f(x))
+    m = len(f0)
+    n = len(x)
+    Hessians = np.zeros((m, n, n))
+
+    hh = np.eye(n) * abs_step
+
+    for i in range(n):
+        for j in range(i, n):
+            f_ij = np.atleast_1d(f(x + hh[i] + hh[j]))
+            f_i = np.atleast_1d(f(x + hh[i]))
+            f_j = np.atleast_1d(f(x + hh[j]))
+
+            Hessians[:, i, j] = (f_ij - f_i - f_j + f0) / abs_step**2
+            if i != j:  # Symmetry
+                Hessians[:, j, i] = Hessians[:, i, j]
+
+    # Reformat to lower-triangular form suitable for Pygmo
+    if lower_triangular:
+        k = (n * (n + 1)) // 2
+        Hessians_L = np.zeros((m, k))
+        for i in range(m):
+            Hessians_L[i] = Hessians[i][np.tril_indices(n)]
+        Hessians = Hessians_L
+
+    return Hessians
```

### Comparing `turboflow-0.1.2/turboflow/pysolver_view/optimization.py` & `turboflow-0.1.3/turboflow/pysolver_view/optimization.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,976 +1,976 @@
-import os
-import time
-import copy
-import logging
-import warnings
-import numpy as np
-import matplotlib.pyplot as plt
-
-from datetime import datetime
-from abc import ABC, abstractmethod
-from matplotlib.ticker import MaxNLocator
-
-from . import numerical_differentiation
-from . import optimization_wrappers as _opt
-from .pysolver_utilities import savefig_in_formats
-
-
-# Define valid libraries and their corresponding methods
-OPTIMIZATION_LIBRARIES = {
-    "scipy": _opt.minimize_scipy,
-    "pygmo": _opt.minimize_pygmo,
-    "pygmo_nlopt": _opt.minimize_nlopt,
-}
-
-VALID_LIBRARIES_AND_METHODS = {
-    "scipy": _opt.SCIPY_SOLVERS,
-    "pygmo": _opt.PYGMO_SOLVERS,
-    "pygmo_nlopt": _opt.NLOPT_SOLVERS,
-}
-
-
-class OptimizationSolver:
-    r"""
-
-    Solver class for general nonlinear programming problems.
-
-    The solver is designed to handle constrained optimization problems of the form:
-
-    Minimize:
-
-    .. math::
-        f(\mathbf{x}) \; \mathrm{with} \; \mathbf{x} \in \mathbb{R}^n
-
-    Subject to:
-
-    .. math::
-        c_{\mathrm{eq}}(\mathbf{x}) = 0
-    .. math::
-        c_{\mathrm{in}}(\mathbf{x}) \leq 0
-    .. math::
-        \mathbf{x}_l \leq \mathbf{x} \leq \mathbf{x}_u
-
-    where:
-
-    - :math:`\mathbf{x}` is the vector of decision variables (i.e., degree of freedom).
-    - :math:`f(\mathbf{x})` is the objective function to be minimized. Maximization problems can be casted into minimization problems by changing the sign of the objective function.
-    - :math:`c_{\mathrm{eq}}(\mathbf{x})` are the equality constraints of the problem.
-    - :math:`c_{\mathrm{in}}(\mathbf{x})` are the inequality constraints of the problem. Constraints of type :math:`c_{\mathrm{in}}(\mathbf{x}) \leq 0` can be casted into :math:`c_{\mathrm{in}}(\mathbf{x}) \geq 0` type by changing the sign of the constraint functions.
-    - :math:`\mathbf{x}_l` and :math:`\mathbf{x}_u` are the lower and upper bounds on the decision variables.
-
-    The class interfaces with various optimization methods provided by libraries such as `scipy` and `pygmo` to solve the problem and provides a structured framework for initialization, solution monitoring, and post-processing.
-
-    This class employs a caching mechanism to avoid redundant evaluations. For a given set of independent variables, x, the optimizer requires the objective function, equality constraints, and inequality constraints to be provided separately. When working with complex models, these values are typically calculated all at once. If x hasn't changed from a previous evaluation, the caching system ensures that previously computed values are used, preventing unnecessary recalculations.
-
-    Parameters
-    ----------
-    problem : OptimizationProblem
-        An instance of the optimization problem to be solved.
-    library : str, optional
-        The library to use for solving the optimization problem (default is 'scipy').
-    method : str, optional
-        The optimization method to use from the specified library (default is 'slsqp').
-    tol : float, optional
-        Tolerance for termination. The selected minimization algorithm sets some relevant solver-specific tolerance(s) equal to tol. The termination tolerances can be fine-tuned through the `options` dictionary. (default is 1e-5).
-    max_iter : int, optional
-        Maximum number of iterations for the optimizer (default is 100).
-    options : dict, optional
-        A dictionary of solver-specific options that prevails over 'tol' and 'max_iter'
-    derivative_method : str, optional
-        Method to use for derivative calculation (default is '2-point').
-    derivative_abs_step : float, optional
-        Finite difference absolute step size to be used when the problem Jacobian is not provided. Defaults to 1e-6
-    display : bool, optional
-        If True, displays the convergence progress (default is True).
-    plot : bool, optional
-        If True, plots the convergence progress (default is False).
-    plot_scale_objective : str, optional
-        Specifies the scale of the objective function axis in the convergence plot (default is 'linear').
-    plot_scale_constraints : str, optional
-        Specifies the scale of the constraint violation axis in the convergence plot (default is 'linear').
-    logger : logging.Logger, optional
-        Logger object to which logging messages will be directed. Logging is disabled if `logger` is None.
-    update_on : str, optional
-        Specifies if the convergence report should be updated based on new function evaluations or gradient evaluations (default is 'gradient', alternative is 'function').
-    callback_functions : list of callable or callable, optional
-        Optional list of callback functions to pass to the solver.
-
-    Methods
-    -------
-    solve(x0):
-        Solve the optimization problem using the specified initial guess `x0`.
-    fitness(x):
-        Evaluates the optimization problem objective function and constraints at a given point `x`.
-    gradient(x):
-        Evaluates the Jacobians of the optimization problem at a given point `x`.
-    print_convergence_history():
-        Print the final result and convergence history of the optimization problem.
-    plot_convergence_history():
-        Plot the convergence history of the optimization problem.
-    """
-
-    def __init__(
-        self,
-        problem,
-        library="scipy",
-        method="slsqp",
-        tolerance=1e-6,
-        max_iterations=100,
-        options={},
-        derivative_method="2-point",
-        derivative_abs_step=None,
-        print_convergence=True,
-        plot_convergence=False,
-        plot_scale_objective="linear",
-        plot_scale_constraints="linear",
-        logger=None,
-        update_on="gradient",
-        callback_functions=None,
-    ):
-        # Initialize class variables
-        self.problem = problem
-        self.display = print_convergence
-        self.plot = plot_convergence
-        self.plot_scale_objective = plot_scale_objective
-        self.plot_scale_constraints = plot_scale_constraints
-        self.logger = logger
-        self.library = library
-        self.method = method
-        self.derivative_method = derivative_method
-        self.derivative_abs_step = derivative_abs_step
-        self.callback_functions = self._validate_callback(callback_functions)
-        self.callback_function_call_count = 0
-
-        # # Validate library and method
-        self._validate_library_and_method()
-
-        # Define options dictionary
-        self.options = copy.deepcopy(options) if options else {}
-        self.options["tol"] = tolerance
-        self.options["max_iter"] = max_iterations
-
-        # Check for logger validity
-        if self.logger is not None:
-            if not isinstance(self.logger, logging.Logger):
-                raise ValueError(
-                    "The provided logger is not a valid logging.Logger instance."
-                )
-
-        # Check for valid display_on value
-        self.update_on = update_on
-        if update_on not in ["function", "gradient"]:
-            raise ValueError(
-                "Invalid value for 'update_on'. It should be either 'function' or 'gradient'."
-            )
-
-        # Rename number of constraints
-        self.N_eq = self.problem.get_nec()
-        self.N_ineq = self.problem.get_nic()
-
-        # Initialize variables for convergence report
-        self.f_final = None
-        self.x_final = None
-        self.x_last = None
-        self.grad_count = 0
-        self.func_count = 0
-        self.func_count_tot = 0
-        self.success = None
-        self.message = None
-        self.solution_report = []
-        self.elapsed_time = None
-        self.include_solution_in_footer = False
-        self.convergence_history = {
-            "grad_count": [],
-            "func_count": [],
-            "func_count_total": [],
-            "objective_value": [],
-            "constraint_violation": [],
-            "norm_step": [],
-        }
-
-        # Initialize convergence plot
-        if self.plot:
-            self._plot_callback(initialize=True)
-
-        # Initialize dictionary for cached variables
-        self.cache = {
-            "x": None,
-            "f": None,
-            "c_eq": None,
-            "c_ineq": None,
-            "x_jac": None,
-            "f_jac": None,
-            "c_eq_jac": None,
-            "c_ineq_jac": None,
-            "fitness": None,
-            "gradient": None,
-        }
-
-    def _validate_library_and_method(self):
-        # Check if the library is valid
-        if self.library not in VALID_LIBRARIES_AND_METHODS:
-            error_message = (
-                f"Invalid optimization library '{self.library}'. \nAvailable libraries:\n   - "
-                + "\n   - ".join(VALID_LIBRARIES_AND_METHODS.keys())
-                + "."
-            )
-            raise ValueError(error_message)
-
-        # Check if the method is valid for the selected library
-        if self.method and self.method not in VALID_LIBRARIES_AND_METHODS[self.library]:
-            error_message = (
-                f"Invalid method '{self.method}' for library '{self.library}'. \nValid methods are:\n   - "
-                + "\n   - ".join(VALID_LIBRARIES_AND_METHODS[self.library])
-                + "."
-            )
-            raise ValueError(error_message)
-
-    def _validate_callback(self, callback):
-        """Validate the callback functions argument."""
-        if callback is None:
-            return []
-        if callable(callback):
-            return [callback]
-        elif isinstance(callback, list):
-            non_callable_items = [item for item in callback if not callable(item)]
-            if not non_callable_items:
-                return callback
-            else:
-                error_msg = f"All elements in the callback list must be callable functions. Non-callable items: {non_callable_items}"
-                raise TypeError(error_msg)
-        else:
-            error_msg = f"callback_func must be a function or a list of functions. Received type: {type(callback)} ({callback})"
-            raise TypeError(error_msg)
-
-    def solve(self, x0):
-        """
-        Solve the optimization problem using the specified library and solver.
-
-        This method initializes the optimization process, manages the flow of the optimization,
-        and handles the results, utilizing the solver from a specified library such as scipy or pygmo.
-
-        Parameters
-        ----------
-        x0 : array-like, optional
-            Initial guess for the solution of the optimization problem.
-
-        Returns
-        -------
-        x_final : array-like
-            An array with the optimal vector of design variables
-
-        """
-        # Get start datetime
-        self.start_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
-
-        # Start timing with high-resolution timer
-        start_time = time.perf_counter()
-
-        # Print report header
-        self._write_header()
-
-        # Define new problem with anonymous methods (avoid problems when Pygmo creates a deep copy)
-        problem = _PygmoProblem(self)
-
-        # Fetch the solver function
-        lib_wrapper = OPTIMIZATION_LIBRARIES[self.library]
-        solution = lib_wrapper(problem, x0, self.method, self.options)
-
-        # Retrieve last solution (also works for gradient-free solvers when updating on gradient)
-        x_last = self.x_last if self.x_last is not None else self.cache["x"]
-
-        # Save solution
-        self.x_final = copy.deepcopy(x_last)
-        self.f_final = copy.deepcopy(self.fitness(self.x_final)[0])
-        self.success, self.message = solution
-
-        # Calculate elapsed time
-        self.elapsed_time = time.perf_counter() - start_time
-
-        # Print report footer
-        self._print_convergence_progress(self.x_final)
-        self._write_footer()
-
-        return self.x_final
-
-    def fitness(self, x, called_from_grad=False):
-        """
-        Evaluates the optimization problem values at a given point x.
-
-        This method queries the `fitness` method of the OptimizationProblem class to
-        compute the objective function value and constraint values. It first checks the cache
-        to avoid redundant evaluations. If no matching cached result exists, it proceeds to
-        evaluate the objective function and constraints.
-
-        Parameters
-        ----------
-        x : array-like
-            Vector of independent variables (i.e., degrees of freedom).
-        called_from_grad : bool, optional
-            Flag used to indicate if the method is called during gradient evaluation.
-            This helps in preventing redundant increments in evaluation counts during
-            finite-differences gradient calculations. Default is False.
-
-        Returns
-        -------
-        fitness : numpy.ndarray
-            A 1D array containing the objective function, equality constraints, and inequality constraints at `x`.
-
-        """
-        # If x hasn't changed, use cached values
-        if np.array_equal(x, self.cache["x"]):
-            return self.cache["fitness"]
-
-        # Increase total counter (includes finite differences)
-        self.func_count_tot += 1
-
-        # Evaluate objective function and constraints at once
-        fitness = self.problem.fitness(x)
-
-        # Does not include finite differences
-        if not called_from_grad:
-            # Update cached variabled
-            self.cache.update(
-                {
-                    "x": x.copy(),  # Needed for finite differences
-                    "f": fitness[0],
-                    "c_eq": fitness[1 : 1 + self.N_eq],
-                    "c_ineq": fitness[1 + self.N_eq :],
-                    "fitness": fitness,
-                }
-            )
-
-            # Increase minor iteration counter (line search)
-            self.func_count += 1
-
-            # Update progress report
-            if self.update_on == "function":
-                self._print_convergence_progress(x)
-
-        return fitness
-
-    def gradient(self, x):
-        """
-        Evaluates the Jacobian matrix of the optimization problem at the given point x.
-
-        This method utilizes the `gradient` method of the OptimizationProblem class if implemented.
-        If the `gradient` method is not implemented, the Jacobian is approximated using forward finite differences.
-
-        To prevent redundant calculations, cached results are checked first. If a matching
-        cached result is found, it is returned; otherwise, a fresh calculation is performed.
-
-        Parameters
-        ----------
-        x : array-like
-            Vector of independent variables (i.e., degrees of freedom).
-
-        Returns
-        -------
-        numpy.ndarray
-            A 2D array representing the Jacobian matrix of the optimization problem at `x`.
-            The Jacobian matrix includes:
-            - Gradient of the objective function
-            - Jacobian of equality constraints
-            - Jacobian of inequality constraints
-        """
-
-        # If x hasn't changed, use cached values
-        if np.array_equal(x, self.cache["x_jac"]):
-            return self.cache["gradient"]
-
-        # Use problem gradient method if it exists
-        if hasattr(self.problem, "gradient"):
-            grad = self.problem.gradient(x)
-        else:
-            # Fall back to finite differences
-            fun = lambda x: self.fitness(x, called_from_grad=True)
-            grad = numerical_differentiation.approx_gradient(
-                fun,
-                x,
-                f0=fun(x),
-                method=self.derivative_method,
-                abs_step=self.derivative_abs_step,  ## TODO make sure it works when design variable takes value 0 * np.abs(x),
-            )
-
-        # Reshape gradient for unconstrained problems
-        grad = np.atleast_2d(grad)
-
-        # Update cache
-        self.cache.update(
-            {
-                "x_jac": x.copy(),
-                "f_jac": grad[0, :],
-                "c_eq_jac": grad[1 : 1 + self.N_eq, :],
-                "c_ineq_jac": grad[1 + self.N_eq :, :],
-                "gradient": grad,
-            }
-        )
-
-        # Update progress report
-        # TODO check that the initial X is exact in cycle optimization
-        self.grad_count += 1
-        if self.update_on == "gradient":
-            self._print_convergence_progress(x)
-
-        return grad
-
-    def _write_header(self):
-        """
-        Print a formatted header for the optimization report.
-
-        This internal method is used to display a consistent header format at the
-        beginning of the optimization process. The header includes columns for function
-        evaluations, gradient evaluations, objective function value, constraint violations,
-        and norm of the steps.
-        """
-
-        # Define header text
-        initial_message = (
-            f" Starting optimization process for {type(self.problem).__name__}\n"
-            f" Optimization algorithm employed: {self.method}"
-        )
-        self.header = f" {'Grad-eval':>13}{'Func-eval':>13}{'Func-value':>16}{'Infeasibility':>18}{'Norm of step':>18} "
-        separator = "-" * len(self.header)
-        lines_to_output = [
-            separator,
-            initial_message,
-            separator,
-            self.header,
-            separator,
-        ]
-
-        # Display to stdout
-        if self.display:
-            for line in lines_to_output:
-                print(line)
-
-        # Write to log
-        if self.logger:
-            for line in lines_to_output:
-                self.logger.info(line)
-
-        # Store text in memory
-        self.solution_report.extend(lines_to_output)
-
-    def _print_convergence_progress(self, x):
-        """
-        Print the current optimization status and update convergence history.
-
-        This method captures and prints the following metrics:
-        - Number of gradient evaluations
-        - Number of function evaluations
-        - Objective function value
-        - Maximum constraint violation
-        - Norm of the update step
-
-        The method also updates the stored convergence history for potential future analysis.
-
-        Parameters
-        ----------
-        x : array-like
-            The current solution (i.e., vector of independent variable values)
-
-        Notes
-        -----
-        The norm of the update step is calculated as the two-norm of the difference
-        between the current and the last independent variables. Constraints violation is
-        computed as the infinity norm of the active constraints.
-        """
-
-        # Ensure fitness is computed at least once before printing
-        if self.cache["fitness"] is None:
-            self.fitness(x)
-
-        # Compute the norm of the last step
-        norm_step = np.linalg.norm(x - self.x_last) if self.x_last is not None else 0
-        self.x_last = x.copy()
-
-        # Compute the maximun constraint violation
-        c_eq = self.cache["c_eq"]
-        c_ineq = self.cache["c_ineq"]
-        violation_all = np.concatenate((c_eq, np.minimum(c_ineq, 0)))
-        violation_max = np.max(np.abs(violation_all)) if len(violation_all) > 0 else 0.0
-
-        # Store convergence status
-        self.convergence_history["grad_count"].append(self.grad_count)
-        self.convergence_history["func_count"].append(self.func_count)
-        self.convergence_history["func_count_total"].append(self.func_count_tot)
-        self.convergence_history["objective_value"].append(self.cache["f"])
-        self.convergence_history["constraint_violation"].append(violation_max)
-        self.convergence_history["norm_step"].append(norm_step)
-
-        # Current convergence message
-        status = f" {self.grad_count:13d}{self.func_count:13d}{self.cache['f']:+16.3e}{violation_max:+18.3e}{norm_step:+18.3e} "
-
-        # Display to stdout
-        if self.display:
-            print(status)
-
-        # Write to log
-        if self.logger:
-            self.logger.info(status)
-
-        # Store text in memory
-        self.solution_report.append(status)
-
-        # Refresh the plot with current values
-        if self.plot:
-            self._plot_callback()
-
-        # Evaluate callback functions
-        if self.callback_functions:
-            self.callback_function_call_count += 1
-            for func in self.callback_functions:
-                func(x, self.callback_function_call_count)
-
-    def _write_footer(self):
-        """
-        Print a formatted footer for the optimization report.
-
-        This method displays the final optimization result, including the
-        exit message, success status, objective function value, and decision variables.
-
-        Notes
-        -----
-        The footer's structure is intended to match the header's style,
-        providing a consistent look to the optimization report.
-        """
-        # Define footer text
-        separator = "-" * len(self.header)
-        exit_message = f"Exit message: {self.message}"
-        success_status = f"Success: {self.success}"
-        time_message = f"Solution time: {self.elapsed_time:.3f} seconds"
-        solution_header = "Solution:"
-        solution_objective = f"   f  = {self.f_final:+6e}"
-        solution_vars = [f"   x{i} = {x:+6e}" for i, x in enumerate(self.x_final)]
-        lines_to_output = [separator, success_status, exit_message, time_message]
-        if self.include_solution_in_footer:
-            lines_to_output += [solution_header]
-            lines_to_output += solution_objective
-            lines_to_output += solution_vars
-        lines_to_output += [separator, ""]
-
-        # Display to stdout
-        if self.display:
-            for line in lines_to_output:
-                print(line)
-
-        # Write to log
-        if self.logger:
-            for line in lines_to_output:
-                self.logger.info(line)
-
-        # Store text in memory
-        self.solution_report.extend(lines_to_output)
-
-    def _plot_callback(self, initialize=False):
-        """
-        Callback function to dynamically update the convergence progress plot.
-
-        This method initializes a matplotlib plot on the first iteration and updates
-        the data for each subsequent iteration. The plot showcases the evolution of
-        the objective function and the constraint violation with respect to the
-        number of iterations.
-
-        The left y-axis depicts the objective function values, while the right y-axis
-        showcases the constraint violation values. The x-axis represents the number
-        of iterations. Both lines are updated and redrawn dynamically as iterations progress.
-
-        Note:
-            This is an internal method, meant to be called within the optimization process.
-        """
-
-        # Initialize figure before first iteration
-        if initialize:
-            self.fig, self.ax_1 = plt.subplots()
-            (self.obj_line_1,) = self.ax_1.plot(
-                [], [], color="#0072BD", marker="o", label="Objective function"
-            )
-            self.ax_1.set_xlabel("Number of iterations")
-            self.ax_1.set_ylabel("Objective function")
-            self.ax_1.set_yscale(self.plot_scale_objective)
-            self.ax_1.xaxis.set_major_locator(
-                MaxNLocator(integer=True)
-            )  # Integer ticks
-            if self.N_eq > 0 or self.N_ineq > 0:
-                self.ax_2 = self.ax_1.twinx()
-                self.ax_2.set_ylabel("Constraint violation")
-                self.ax_2.set_yscale(self.plot_scale_constraints)
-                (self.obj_line_2,) = self.ax_2.plot(
-                    [], [], color="#D95319", marker="o", label="Constraint violation"
-                )
-                lines = [self.obj_line_1, self.obj_line_2]
-                labels = [l.get_label() for l in lines]
-                self.ax_2.legend(lines, labels, loc="upper right")
-            else:
-                self.ax_1.legend(loc="upper right")
-
-            self.fig.tight_layout(pad=1)
-
-        # Update plot data with current values
-        iteration = (
-            self.convergence_history["func_count"]
-            if self.update_on == "function"
-            else self.convergence_history["grad_count"]
-        )
-        objective_function = self.convergence_history["objective_value"]
-        constraint_violation = self.convergence_history["constraint_violation"]
-        self.obj_line_1.set_xdata(iteration)
-        self.obj_line_1.set_ydata(objective_function)
-        if self.N_eq > 0 or self.N_ineq > 0:
-            self.obj_line_2.set_xdata(iteration)
-            self.obj_line_2.set_ydata(constraint_violation)
-
-        # Adjust the plot limits
-        self.ax_1.relim()
-        self.ax_1.autoscale_view()
-        if self.N_eq > 0 or self.N_ineq > 0:
-            self.ax_2.relim()
-            self.ax_2.autoscale_view()
-
-        # Redraw the plot
-        plt.draw()
-        plt.pause(0.01)  # small pause to allow for update
-
-    def print_convergence_history(
-        self, savefile=False, filename=None, output_dir="output"
-    ):
-        """
-        Print the convergence history of the problem.
-
-        The convergence history includes:
-            - Number of function evaluations
-            - Number of gradient evaluations
-            - Objective function value
-            - Maximum constraint violation
-            - Two-norm of the update step
-
-        The method provides a detailed report on:
-            - Exit message
-            - Success status
-            - Execution time
-
-        This method should be called only after the optimization problem has been solved, as it relies on data generated by the solving process.
-
-        Parameters
-        ----------
-        savefile : bool, optional
-            If True, the convergence history will be saved to a file, otherwise printed to standard output. Default is False.
-        filename : str, optional
-            The name of the file to save the convergence history. If not specified, the filename is automatically generated
-            using the problem name and the start datetime. The file extension is not required.
-        output_dir : str, optional
-            The directory where the plot file will be saved if savefile is True. Default is "output".
-
-        Raises
-        ------
-        ValueError
-            If this method is called before the problem has been solved.
-
-        """
-        if self.x_final is not None:
-            if savefile:
-                # Create output directory if it does not exist
-                if not os.path.exists(output_dir):
-                    os.makedirs(output_dir)
-
-                # Give a name to the file if it is not specified
-                if filename is None:
-                    filename = f"convergence_{type(self.problem).__name__}_{self.start_datetime}.txt"
-
-                # Write report to file
-                fullfile = os.path.join(output_dir, filename)
-                with open(fullfile, "w") as f:
-                    f.write("\n".join(self.solution_report))
-
-            else:
-                for line in self.solution_report:
-                    print(line)
-
-        else:
-            raise ValueError(
-                "This method can only be used after invoking the 'solve()' method."
-            )
-
-    def plot_convergence_history(
-        self, savefile=False, filename=None, output_dir="output"
-    ):
-        """
-        Plot the convergence history of the problem.
-
-        This method plots the optimization progress against the number of iterations:
-            - Objective function value (left y-axis)
-            - Maximum constraint violation (right y-axis)
-
-        The constraint violation is only displayed if the problem has nonlinear constraints
-
-        This method should be called only after the optimization problem has been solved, as it relies on data generated by the solving process.
-
-        Parameters
-        ----------
-        savefile : bool, optional
-            If True, the plot is saved to a file instead of being displayed. Default is False.
-        filename : str, optional
-            The name of the file to save the plot to. If not specified, the filename is automatically generated
-            using the problem name and the start datetime. The file extension is not required.
-        output_dir : str, optional
-            The directory where the plot file will be saved if savefile is True. Default is "output".
-
-        Returns
-        -------
-        matplotlib.figure.Figure
-            The Matplotlib figure object for the plot. This can be used for further customization or display.
-
-        Raises
-        ------
-        ValueError
-            If this method is called before the problem has been solved.
-        """
-        if self.x_final is not None:
-            self._plot_callback(initialize=True)
-        else:
-            raise ValueError(
-                "This method can only be used after invoking the 'solve()' method."
-            )
-
-        if savefile:
-            # Create output directory if it does not exist
-            if not os.path.exists(output_dir):
-                os.makedirs(output_dir)
-
-            # Give a name to the file if it is not specified
-            if filename is None:
-                filename = (
-                    f"convergence_{type(self.problem).__name__}_{self.start_datetime}"
-                )
-
-            # Save plots
-            fullfile = os.path.join(output_dir, filename)
-            savefig_in_formats(self.fig, fullfile, formats=[".png", ".svg"])
-
-        return self.fig
-
-
-class OptimizationProblem(ABC):
-    """
-    Abstract base class for optimization problems.
-
-    Derived optimization problem objects must implement the following methods:
-
-    - `fitness`: Evaluate the objective function and constraints for a given set of decision variables.
-    - `get_bounds`: Get the bounds for each decision variable.
-    - `get_neq`: Return the number of equality constraints associated with the problem.
-    - `get_nineq`: Return the number of inequality constraints associated with the problem.
-
-    Additionally, specific problem classes can define the `gradient` method to compute the Jacobians. If this method is not present in the derived class, the solver will revert to using forward finite differences for Jacobian calculations.
-
-    Methods
-    -------
-    fitness(x)
-        Evaluate the objective function and constraints for a given set of decision variables.
-    get_bounds()
-        Get the bounds for each decision variable.
-    get_neq()
-        Return the number of equality constraints associated with the problem.
-    get_nineq()
-        Return the number of inequality constraints associated with the problem.
-
-    """
-
-    @abstractmethod
-    def fitness(self, x):
-        """
-        Evaluate the objective function and constraints for given decision variables.
-
-        Parameters
-        ----------
-        x : array-like
-            Vector of independent variables (i.e., degrees of freedom).
-
-        Returns
-        -------
-        array_like
-            Vector containing the objective function, equality constraints, and inequality constraints.
-        """
-        pass
-
-    @abstractmethod
-    def get_bounds(self):
-        """
-        Get the bounds for each decision variable (Pygmo format)
-
-        Returns
-        -------
-        bounds : tuple of lists
-            A tuple of two items where the first item is the list of lower bounds and the second
-            item of the list of upper bounds for the vector of decision variables. For example,
-            ([-2 -1], [2, 1]) indicates that the first decision variable has bounds between
-            -2 and 2, and the second has bounds between -1 and 1.
-        """
-        pass
-
-    @abstractmethod
-    def get_nec(self):
-        """
-        Return the number of equality constraints associated with the problem.
-
-        Returns
-        -------
-        neq : int
-            Number of equality constraints.
-        """
-        pass
-
-    @abstractmethod
-    def get_nic(self):
-        """
-        Return the number of inequality constraints associated with the problem.
-
-        Returns
-        -------
-        nineq : int
-            Number of inequality constraints.
-        """
-        pass
-
-
-def count_constraints(var):
-    """
-    Retrieve the number of constraints based on the provided input.
-
-    This function returns the count of constraints based on the nature of the
-    input:
-
-    - `None` returns 0
-    - Scalar values return 1
-    - Array-like structures return their length
-
-    Parameters
-    ----------
-    var : None, scalar, or array-like (list, tuple, np.ndarray)
-        The input representing the constraint(s). This can be `None`, a scalar value,
-        or an array-like structure containing multiple constraints.
-
-    Returns
-    -------
-    int
-        The number of constraints:
-
-        - 0 for `None`
-        - 1 for scalar values
-        - Length of the array-like for array-like inputs
-
-    Examples
-    --------
-    >>> count_constraints(None)
-    0
-
-    >>> count_constraints(5.0)
-    1
-
-    >>> count_constraints([1.0, 2.0, 3.0])
-    3
-    """
-    # If constraint is None
-    if var is None:
-        return 0
-    # If constraint is a scalar (assuming it's numeric)
-    elif np.isscalar(var):
-        return 1
-    # If constraint is array-like
-    else:
-        return len(var)
-
-
-def combine_objective_and_constraints(f, c_eq=None, c_ineq=None):
-    """
-    Combine an objective function with its associated equality and inequality constraints.
-
-    This function takes in an objective function value, a set of equality constraints,
-    and a set of inequality constraints. It then returns a combined Numpy array of
-    these values. The constraints can be given as a list, tuple, numpy array, or as
-    individual values.
-
-    Parameters
-    ----------
-    f : float
-        The value of the objective function.
-    c_eq : float, list, tuple, np.ndarray, or None
-        The equality constraint(s). This can be a single value or a collection of values.
-        If `None`, no equality constraints will be added.
-    c_ineq : float, list, tuple, np.ndarray, or None
-        The inequality constraint(s). This can be a single value or a collection of values.
-        If `None`, no inequality constraints will be added.
-
-    Returns
-    -------
-    np.ndarray
-        A numpy array consisting of the objective function value followed by equality and
-        inequality constraints.
-
-    Examples
-    --------
-    >>> combine_objective_and_constraints(1.0, [0.5, 0.6], [0.7, 0.8])
-    array([1. , 0.5, 0.6, 0.7, 0.8])
-
-    >>> combine_objective_and_constraints(1.0, 0.5, 0.7)
-    array([1. , 0.5, 0.7])
-    """
-
-    # Validate objective function value
-    if isinstance(f, (list, tuple, np.ndarray)):
-        if len(f) != 1:
-            raise ValueError(
-                "Objective function value 'f' must be a scalar or single-element array."
-            )
-        f = f[0]  # Unwrap the single element to ensure it's treated as a scalar
-
-    # Add objective function
-    combined_list = [f]
-
-    # Add equality constraints
-    if c_eq is not None:
-        if isinstance(c_eq, (list, tuple, np.ndarray)):
-            combined_list.extend(c_eq)
-        else:
-            combined_list.append(c_eq)
-
-    # Add inequality constraints
-    if c_ineq is not None:
-        if isinstance(c_ineq, (list, tuple, np.ndarray)):
-            combined_list.extend(c_ineq)
-        else:
-            combined_list.append(c_ineq)
-
-    return np.array(combined_list)
-
-
-class _PygmoProblem:
-    """
-    A wrapper class for optimization problems to be compatible with Pygmo's need for deep-copiable problems.
-    This class uses anonymous functions (lambda) to prevent issues with deep copying complex objects,
-    (like Coolprop's AbstractState objects) which are not deep-copiable.
-    """
-
-    def __init__(self, wrapped_problem):
-        # Pygmo requires a flattened Jacobian for gradients, unlike SciPy's two-dimensional array.
-        self.fitness = lambda x: wrapped_problem.fitness(x)
-        self.gradient = lambda x: wrapped_problem.gradient(x).flatten()
-
-        # Directly link bounds and constraint counts from the original problem.
-        self.get_bounds = lambda: wrapped_problem.problem.get_bounds()
-        self.get_nec = lambda: wrapped_problem.problem.get_nec()
-        self.get_nic = lambda: wrapped_problem.problem.get_nic()
-
-        # If the original problem defines Hessians, provide them as well.
-        if hasattr(wrapped_problem.problem, "hessians"):
-            self.hessians = lambda x: wrapped_problem.problem.hessians(x)
-
-        # Define anonymous functions for objective and constraints with their Jacobians.
-        self.f = lambda x: wrapped_problem.fitness(x)[0]
-        self.c_eq = lambda x: wrapped_problem.fitness(x)[1 : 1 + self.get_nec()]
-        self.c_ineq = lambda x: wrapped_problem.fitness(x)[1 + self.get_nec() :]
-
-        self.f_jac = lambda x: wrapped_problem.gradient(x)[0, :]
-        self.c_eq_jac = lambda x: wrapped_problem.gradient(x)[1 : 1 + self.get_nec(), :]
-        self.c_ineq_jac = lambda x: wrapped_problem.gradient(x)[1 + self.get_nec() :, :]
+import os
+import time
+import copy
+import logging
+import warnings
+import numpy as np
+import matplotlib.pyplot as plt
+
+from datetime import datetime
+from abc import ABC, abstractmethod
+from matplotlib.ticker import MaxNLocator
+
+from . import numerical_differentiation
+from . import optimization_wrappers as _opt
+from .pysolver_utilities import savefig_in_formats
+
+
+# Define valid libraries and their corresponding methods
+OPTIMIZATION_LIBRARIES = {
+    "scipy": _opt.minimize_scipy,
+    "pygmo": _opt.minimize_pygmo,
+    "pygmo_nlopt": _opt.minimize_nlopt,
+}
+
+VALID_LIBRARIES_AND_METHODS = {
+    "scipy": _opt.SCIPY_SOLVERS,
+    "pygmo": _opt.PYGMO_SOLVERS,
+    "pygmo_nlopt": _opt.NLOPT_SOLVERS,
+}
+
+
+class OptimizationSolver:
+    r"""
+
+    Solver class for general nonlinear programming problems.
+
+    The solver is designed to handle constrained optimization problems of the form:
+
+    Minimize:
+
+    .. math::
+        f(\mathbf{x}) \; \mathrm{with} \; \mathbf{x} \in \mathbb{R}^n
+
+    Subject to:
+
+    .. math::
+        c_{\mathrm{eq}}(\mathbf{x}) = 0
+    .. math::
+        c_{\mathrm{in}}(\mathbf{x}) \leq 0
+    .. math::
+        \mathbf{x}_l \leq \mathbf{x} \leq \mathbf{x}_u
+
+    where:
+
+    - :math:`\mathbf{x}` is the vector of decision variables (i.e., degree of freedom).
+    - :math:`f(\mathbf{x})` is the objective function to be minimized. Maximization problems can be casted into minimization problems by changing the sign of the objective function.
+    - :math:`c_{\mathrm{eq}}(\mathbf{x})` are the equality constraints of the problem.
+    - :math:`c_{\mathrm{in}}(\mathbf{x})` are the inequality constraints of the problem. Constraints of type :math:`c_{\mathrm{in}}(\mathbf{x}) \leq 0` can be casted into :math:`c_{\mathrm{in}}(\mathbf{x}) \geq 0` type by changing the sign of the constraint functions.
+    - :math:`\mathbf{x}_l` and :math:`\mathbf{x}_u` are the lower and upper bounds on the decision variables.
+
+    The class interfaces with various optimization methods provided by libraries such as `scipy` and `pygmo` to solve the problem and provides a structured framework for initialization, solution monitoring, and post-processing.
+
+    This class employs a caching mechanism to avoid redundant evaluations. For a given set of independent variables, x, the optimizer requires the objective function, equality constraints, and inequality constraints to be provided separately. When working with complex models, these values are typically calculated all at once. If x hasn't changed from a previous evaluation, the caching system ensures that previously computed values are used, preventing unnecessary recalculations.
+
+    Parameters
+    ----------
+    problem : OptimizationProblem
+        An instance of the optimization problem to be solved.
+    library : str, optional
+        The library to use for solving the optimization problem (default is 'scipy').
+    method : str, optional
+        The optimization method to use from the specified library (default is 'slsqp').
+    tol : float, optional
+        Tolerance for termination. The selected minimization algorithm sets some relevant solver-specific tolerance(s) equal to tol. The termination tolerances can be fine-tuned through the `options` dictionary. (default is 1e-5).
+    max_iter : int, optional
+        Maximum number of iterations for the optimizer (default is 100).
+    options : dict, optional
+        A dictionary of solver-specific options that prevails over 'tol' and 'max_iter'
+    derivative_method : str, optional
+        Method to use for derivative calculation (default is '2-point').
+    derivative_abs_step : float, optional
+        Finite difference absolute step size to be used when the problem Jacobian is not provided. Defaults to 1e-6
+    display : bool, optional
+        If True, displays the convergence progress (default is True).
+    plot : bool, optional
+        If True, plots the convergence progress (default is False).
+    plot_scale_objective : str, optional
+        Specifies the scale of the objective function axis in the convergence plot (default is 'linear').
+    plot_scale_constraints : str, optional
+        Specifies the scale of the constraint violation axis in the convergence plot (default is 'linear').
+    logger : logging.Logger, optional
+        Logger object to which logging messages will be directed. Logging is disabled if `logger` is None.
+    update_on : str, optional
+        Specifies if the convergence report should be updated based on new function evaluations or gradient evaluations (default is 'gradient', alternative is 'function').
+    callback_functions : list of callable or callable, optional
+        Optional list of callback functions to pass to the solver.
+
+    Methods
+    -------
+    solve(x0):
+        Solve the optimization problem using the specified initial guess `x0`.
+    fitness(x):
+        Evaluates the optimization problem objective function and constraints at a given point `x`.
+    gradient(x):
+        Evaluates the Jacobians of the optimization problem at a given point `x`.
+    print_convergence_history():
+        Print the final result and convergence history of the optimization problem.
+    plot_convergence_history():
+        Plot the convergence history of the optimization problem.
+    """
+
+    def __init__(
+        self,
+        problem,
+        library="scipy",
+        method="slsqp",
+        tolerance=1e-6,
+        max_iterations=100,
+        options={},
+        derivative_method="2-point",
+        derivative_abs_step=None,
+        print_convergence=True,
+        plot_convergence=False,
+        plot_scale_objective="linear",
+        plot_scale_constraints="linear",
+        logger=None,
+        update_on="gradient",
+        callback_functions=None,
+    ):
+        # Initialize class variables
+        self.problem = problem
+        self.display = print_convergence
+        self.plot = plot_convergence
+        self.plot_scale_objective = plot_scale_objective
+        self.plot_scale_constraints = plot_scale_constraints
+        self.logger = logger
+        self.library = library
+        self.method = method
+        self.derivative_method = derivative_method
+        self.derivative_abs_step = derivative_abs_step
+        self.callback_functions = self._validate_callback(callback_functions)
+        self.callback_function_call_count = 0
+
+        # # Validate library and method
+        self._validate_library_and_method()
+
+        # Define options dictionary
+        self.options = copy.deepcopy(options) if options else {}
+        self.options["tol"] = tolerance
+        self.options["max_iter"] = max_iterations
+
+        # Check for logger validity
+        if self.logger is not None:
+            if not isinstance(self.logger, logging.Logger):
+                raise ValueError(
+                    "The provided logger is not a valid logging.Logger instance."
+                )
+
+        # Check for valid display_on value
+        self.update_on = update_on
+        if update_on not in ["function", "gradient"]:
+            raise ValueError(
+                "Invalid value for 'update_on'. It should be either 'function' or 'gradient'."
+            )
+
+        # Rename number of constraints
+        self.N_eq = self.problem.get_nec()
+        self.N_ineq = self.problem.get_nic()
+
+        # Initialize variables for convergence report
+        self.f_final = None
+        self.x_final = None
+        self.x_last = None
+        self.grad_count = 0
+        self.func_count = 0
+        self.func_count_tot = 0
+        self.success = None
+        self.message = None
+        self.solution_report = []
+        self.elapsed_time = None
+        self.include_solution_in_footer = False
+        self.convergence_history = {
+            "grad_count": [],
+            "func_count": [],
+            "func_count_total": [],
+            "objective_value": [],
+            "constraint_violation": [],
+            "norm_step": [],
+        }
+
+        # Initialize convergence plot
+        if self.plot:
+            self._plot_callback(initialize=True)
+
+        # Initialize dictionary for cached variables
+        self.cache = {
+            "x": None,
+            "f": None,
+            "c_eq": None,
+            "c_ineq": None,
+            "x_jac": None,
+            "f_jac": None,
+            "c_eq_jac": None,
+            "c_ineq_jac": None,
+            "fitness": None,
+            "gradient": None,
+        }
+
+    def _validate_library_and_method(self):
+        # Check if the library is valid
+        if self.library not in VALID_LIBRARIES_AND_METHODS:
+            error_message = (
+                f"Invalid optimization library '{self.library}'. \nAvailable libraries:\n   - "
+                + "\n   - ".join(VALID_LIBRARIES_AND_METHODS.keys())
+                + "."
+            )
+            raise ValueError(error_message)
+
+        # Check if the method is valid for the selected library
+        if self.method and self.method not in VALID_LIBRARIES_AND_METHODS[self.library]:
+            error_message = (
+                f"Invalid method '{self.method}' for library '{self.library}'. \nValid methods are:\n   - "
+                + "\n   - ".join(VALID_LIBRARIES_AND_METHODS[self.library])
+                + "."
+            )
+            raise ValueError(error_message)
+
+    def _validate_callback(self, callback):
+        """Validate the callback functions argument."""
+        if callback is None:
+            return []
+        if callable(callback):
+            return [callback]
+        elif isinstance(callback, list):
+            non_callable_items = [item for item in callback if not callable(item)]
+            if not non_callable_items:
+                return callback
+            else:
+                error_msg = f"All elements in the callback list must be callable functions. Non-callable items: {non_callable_items}"
+                raise TypeError(error_msg)
+        else:
+            error_msg = f"callback_func must be a function or a list of functions. Received type: {type(callback)} ({callback})"
+            raise TypeError(error_msg)
+
+    def solve(self, x0):
+        """
+        Solve the optimization problem using the specified library and solver.
+
+        This method initializes the optimization process, manages the flow of the optimization,
+        and handles the results, utilizing the solver from a specified library such as scipy or pygmo.
+
+        Parameters
+        ----------
+        x0 : array-like, optional
+            Initial guess for the solution of the optimization problem.
+
+        Returns
+        -------
+        x_final : array-like
+            An array with the optimal vector of design variables
+
+        """
+        # Get start datetime
+        self.start_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
+
+        # Start timing with high-resolution timer
+        start_time = time.perf_counter()
+
+        # Print report header
+        self._write_header()
+
+        # Define new problem with anonymous methods (avoid problems when Pygmo creates a deep copy)
+        problem = _PygmoProblem(self)
+
+        # Fetch the solver function
+        lib_wrapper = OPTIMIZATION_LIBRARIES[self.library]
+        solution = lib_wrapper(problem, x0, self.method, self.options)
+
+        # Retrieve last solution (also works for gradient-free solvers when updating on gradient)
+        x_last = self.x_last if self.x_last is not None else self.cache["x"]
+
+        # Save solution
+        self.x_final = copy.deepcopy(x_last)
+        self.f_final = copy.deepcopy(self.fitness(self.x_final)[0])
+        self.success, self.message = solution
+
+        # Calculate elapsed time
+        self.elapsed_time = time.perf_counter() - start_time
+
+        # Print report footer
+        self._print_convergence_progress(self.x_final)
+        self._write_footer()
+
+        return self.x_final
+
+    def fitness(self, x, called_from_grad=False):
+        """
+        Evaluates the optimization problem values at a given point x.
+
+        This method queries the `fitness` method of the OptimizationProblem class to
+        compute the objective function value and constraint values. It first checks the cache
+        to avoid redundant evaluations. If no matching cached result exists, it proceeds to
+        evaluate the objective function and constraints.
+
+        Parameters
+        ----------
+        x : array-like
+            Vector of independent variables (i.e., degrees of freedom).
+        called_from_grad : bool, optional
+            Flag used to indicate if the method is called during gradient evaluation.
+            This helps in preventing redundant increments in evaluation counts during
+            finite-differences gradient calculations. Default is False.
+
+        Returns
+        -------
+        fitness : numpy.ndarray
+            A 1D array containing the objective function, equality constraints, and inequality constraints at `x`.
+
+        """
+        # If x hasn't changed, use cached values
+        if np.array_equal(x, self.cache["x"]):
+            return self.cache["fitness"]
+
+        # Increase total counter (includes finite differences)
+        self.func_count_tot += 1
+
+        # Evaluate objective function and constraints at once
+        fitness = self.problem.fitness(x)
+
+        # Does not include finite differences
+        if not called_from_grad:
+            # Update cached variabled
+            self.cache.update(
+                {
+                    "x": x.copy(),  # Needed for finite differences
+                    "f": fitness[0],
+                    "c_eq": fitness[1 : 1 + self.N_eq],
+                    "c_ineq": fitness[1 + self.N_eq :],
+                    "fitness": fitness,
+                }
+            )
+
+            # Increase minor iteration counter (line search)
+            self.func_count += 1
+
+            # Update progress report
+            if self.update_on == "function":
+                self._print_convergence_progress(x)
+
+        return fitness
+
+    def gradient(self, x):
+        """
+        Evaluates the Jacobian matrix of the optimization problem at the given point x.
+
+        This method utilizes the `gradient` method of the OptimizationProblem class if implemented.
+        If the `gradient` method is not implemented, the Jacobian is approximated using forward finite differences.
+
+        To prevent redundant calculations, cached results are checked first. If a matching
+        cached result is found, it is returned; otherwise, a fresh calculation is performed.
+
+        Parameters
+        ----------
+        x : array-like
+            Vector of independent variables (i.e., degrees of freedom).
+
+        Returns
+        -------
+        numpy.ndarray
+            A 2D array representing the Jacobian matrix of the optimization problem at `x`.
+            The Jacobian matrix includes:
+            - Gradient of the objective function
+            - Jacobian of equality constraints
+            - Jacobian of inequality constraints
+        """
+
+        # If x hasn't changed, use cached values
+        if np.array_equal(x, self.cache["x_jac"]):
+            return self.cache["gradient"]
+
+        # Use problem gradient method if it exists
+        if hasattr(self.problem, "gradient"):
+            grad = self.problem.gradient(x)
+        else:
+            # Fall back to finite differences
+            fun = lambda x: self.fitness(x, called_from_grad=True)
+            grad = numerical_differentiation.approx_gradient(
+                fun,
+                x,
+                f0=fun(x),
+                method=self.derivative_method,
+                abs_step=self.derivative_abs_step,  ## TODO make sure it works when design variable takes value 0 * np.abs(x),
+            )
+
+        # Reshape gradient for unconstrained problems
+        grad = np.atleast_2d(grad)
+
+        # Update cache
+        self.cache.update(
+            {
+                "x_jac": x.copy(),
+                "f_jac": grad[0, :],
+                "c_eq_jac": grad[1 : 1 + self.N_eq, :],
+                "c_ineq_jac": grad[1 + self.N_eq :, :],
+                "gradient": grad,
+            }
+        )
+
+        # Update progress report
+        # TODO check that the initial X is exact in cycle optimization
+        self.grad_count += 1
+        if self.update_on == "gradient":
+            self._print_convergence_progress(x)
+
+        return grad
+
+    def _write_header(self):
+        """
+        Print a formatted header for the optimization report.
+
+        This internal method is used to display a consistent header format at the
+        beginning of the optimization process. The header includes columns for function
+        evaluations, gradient evaluations, objective function value, constraint violations,
+        and norm of the steps.
+        """
+
+        # Define header text
+        initial_message = (
+            f" Starting optimization process for {type(self.problem).__name__}\n"
+            f" Optimization algorithm employed: {self.method}"
+        )
+        self.header = f" {'Grad-eval':>13}{'Func-eval':>13}{'Func-value':>16}{'Infeasibility':>18}{'Norm of step':>18} "
+        separator = "-" * len(self.header)
+        lines_to_output = [
+            separator,
+            initial_message,
+            separator,
+            self.header,
+            separator,
+        ]
+
+        # Display to stdout
+        if self.display:
+            for line in lines_to_output:
+                print(line)
+
+        # Write to log
+        if self.logger:
+            for line in lines_to_output:
+                self.logger.info(line)
+
+        # Store text in memory
+        self.solution_report.extend(lines_to_output)
+
+    def _print_convergence_progress(self, x):
+        """
+        Print the current optimization status and update convergence history.
+
+        This method captures and prints the following metrics:
+        - Number of gradient evaluations
+        - Number of function evaluations
+        - Objective function value
+        - Maximum constraint violation
+        - Norm of the update step
+
+        The method also updates the stored convergence history for potential future analysis.
+
+        Parameters
+        ----------
+        x : array-like
+            The current solution (i.e., vector of independent variable values)
+
+        Notes
+        -----
+        The norm of the update step is calculated as the two-norm of the difference
+        between the current and the last independent variables. Constraints violation is
+        computed as the infinity norm of the active constraints.
+        """
+
+        # Ensure fitness is computed at least once before printing
+        if self.cache["fitness"] is None:
+            self.fitness(x)
+
+        # Compute the norm of the last step
+        norm_step = np.linalg.norm(x - self.x_last) if self.x_last is not None else 0
+        self.x_last = x.copy()
+
+        # Compute the maximun constraint violation
+        c_eq = self.cache["c_eq"]
+        c_ineq = self.cache["c_ineq"]
+        violation_all = np.concatenate((c_eq, np.minimum(c_ineq, 0)))
+        violation_max = np.max(np.abs(violation_all)) if len(violation_all) > 0 else 0.0
+
+        # Store convergence status
+        self.convergence_history["grad_count"].append(self.grad_count)
+        self.convergence_history["func_count"].append(self.func_count)
+        self.convergence_history["func_count_total"].append(self.func_count_tot)
+        self.convergence_history["objective_value"].append(self.cache["f"])
+        self.convergence_history["constraint_violation"].append(violation_max)
+        self.convergence_history["norm_step"].append(norm_step)
+
+        # Current convergence message
+        status = f" {self.grad_count:13d}{self.func_count:13d}{self.cache['f']:+16.3e}{violation_max:+18.3e}{norm_step:+18.3e} "
+
+        # Display to stdout
+        if self.display:
+            print(status)
+
+        # Write to log
+        if self.logger:
+            self.logger.info(status)
+
+        # Store text in memory
+        self.solution_report.append(status)
+
+        # Refresh the plot with current values
+        if self.plot:
+            self._plot_callback()
+
+        # Evaluate callback functions
+        if self.callback_functions:
+            self.callback_function_call_count += 1
+            for func in self.callback_functions:
+                func(x, self.callback_function_call_count)
+
+    def _write_footer(self):
+        """
+        Print a formatted footer for the optimization report.
+
+        This method displays the final optimization result, including the
+        exit message, success status, objective function value, and decision variables.
+
+        Notes
+        -----
+        The footer's structure is intended to match the header's style,
+        providing a consistent look to the optimization report.
+        """
+        # Define footer text
+        separator = "-" * len(self.header)
+        exit_message = f"Exit message: {self.message}"
+        success_status = f"Success: {self.success}"
+        time_message = f"Solution time: {self.elapsed_time:.3f} seconds"
+        solution_header = "Solution:"
+        solution_objective = f"   f  = {self.f_final:+6e}"
+        solution_vars = [f"   x{i} = {x:+6e}" for i, x in enumerate(self.x_final)]
+        lines_to_output = [separator, success_status, exit_message, time_message]
+        if self.include_solution_in_footer:
+            lines_to_output += [solution_header]
+            lines_to_output += solution_objective
+            lines_to_output += solution_vars
+        lines_to_output += [separator, ""]
+
+        # Display to stdout
+        if self.display:
+            for line in lines_to_output:
+                print(line)
+
+        # Write to log
+        if self.logger:
+            for line in lines_to_output:
+                self.logger.info(line)
+
+        # Store text in memory
+        self.solution_report.extend(lines_to_output)
+
+    def _plot_callback(self, initialize=False):
+        """
+        Callback function to dynamically update the convergence progress plot.
+
+        This method initializes a matplotlib plot on the first iteration and updates
+        the data for each subsequent iteration. The plot showcases the evolution of
+        the objective function and the constraint violation with respect to the
+        number of iterations.
+
+        The left y-axis depicts the objective function values, while the right y-axis
+        showcases the constraint violation values. The x-axis represents the number
+        of iterations. Both lines are updated and redrawn dynamically as iterations progress.
+
+        Note:
+            This is an internal method, meant to be called within the optimization process.
+        """
+
+        # Initialize figure before first iteration
+        if initialize:
+            self.fig, self.ax_1 = plt.subplots()
+            (self.obj_line_1,) = self.ax_1.plot(
+                [], [], color="#0072BD", marker="o", label="Objective function"
+            )
+            self.ax_1.set_xlabel("Number of iterations")
+            self.ax_1.set_ylabel("Objective function")
+            self.ax_1.set_yscale(self.plot_scale_objective)
+            self.ax_1.xaxis.set_major_locator(
+                MaxNLocator(integer=True)
+            )  # Integer ticks
+            if self.N_eq > 0 or self.N_ineq > 0:
+                self.ax_2 = self.ax_1.twinx()
+                self.ax_2.set_ylabel("Constraint violation")
+                self.ax_2.set_yscale(self.plot_scale_constraints)
+                (self.obj_line_2,) = self.ax_2.plot(
+                    [], [], color="#D95319", marker="o", label="Constraint violation"
+                )
+                lines = [self.obj_line_1, self.obj_line_2]
+                labels = [l.get_label() for l in lines]
+                self.ax_2.legend(lines, labels, loc="upper right")
+            else:
+                self.ax_1.legend(loc="upper right")
+
+            self.fig.tight_layout(pad=1)
+
+        # Update plot data with current values
+        iteration = (
+            self.convergence_history["func_count"]
+            if self.update_on == "function"
+            else self.convergence_history["grad_count"]
+        )
+        objective_function = self.convergence_history["objective_value"]
+        constraint_violation = self.convergence_history["constraint_violation"]
+        self.obj_line_1.set_xdata(iteration)
+        self.obj_line_1.set_ydata(objective_function)
+        if self.N_eq > 0 or self.N_ineq > 0:
+            self.obj_line_2.set_xdata(iteration)
+            self.obj_line_2.set_ydata(constraint_violation)
+
+        # Adjust the plot limits
+        self.ax_1.relim()
+        self.ax_1.autoscale_view()
+        if self.N_eq > 0 or self.N_ineq > 0:
+            self.ax_2.relim()
+            self.ax_2.autoscale_view()
+
+        # Redraw the plot
+        plt.draw()
+        plt.pause(0.01)  # small pause to allow for update
+
+    def print_convergence_history(
+        self, savefile=False, filename=None, output_dir="output"
+    ):
+        """
+        Print the convergence history of the problem.
+
+        The convergence history includes:
+            - Number of function evaluations
+            - Number of gradient evaluations
+            - Objective function value
+            - Maximum constraint violation
+            - Two-norm of the update step
+
+        The method provides a detailed report on:
+            - Exit message
+            - Success status
+            - Execution time
+
+        This method should be called only after the optimization problem has been solved, as it relies on data generated by the solving process.
+
+        Parameters
+        ----------
+        savefile : bool, optional
+            If True, the convergence history will be saved to a file, otherwise printed to standard output. Default is False.
+        filename : str, optional
+            The name of the file to save the convergence history. If not specified, the filename is automatically generated
+            using the problem name and the start datetime. The file extension is not required.
+        output_dir : str, optional
+            The directory where the plot file will be saved if savefile is True. Default is "output".
+
+        Raises
+        ------
+        ValueError
+            If this method is called before the problem has been solved.
+
+        """
+        if self.x_final is not None:
+            if savefile:
+                # Create output directory if it does not exist
+                if not os.path.exists(output_dir):
+                    os.makedirs(output_dir)
+
+                # Give a name to the file if it is not specified
+                if filename is None:
+                    filename = f"convergence_{type(self.problem).__name__}_{self.start_datetime}.txt"
+
+                # Write report to file
+                fullfile = os.path.join(output_dir, filename)
+                with open(fullfile, "w") as f:
+                    f.write("\n".join(self.solution_report))
+
+            else:
+                for line in self.solution_report:
+                    print(line)
+
+        else:
+            raise ValueError(
+                "This method can only be used after invoking the 'solve()' method."
+            )
+
+    def plot_convergence_history(
+        self, savefile=False, filename=None, output_dir="output"
+    ):
+        """
+        Plot the convergence history of the problem.
+
+        This method plots the optimization progress against the number of iterations:
+            - Objective function value (left y-axis)
+            - Maximum constraint violation (right y-axis)
+
+        The constraint violation is only displayed if the problem has nonlinear constraints
+
+        This method should be called only after the optimization problem has been solved, as it relies on data generated by the solving process.
+
+        Parameters
+        ----------
+        savefile : bool, optional
+            If True, the plot is saved to a file instead of being displayed. Default is False.
+        filename : str, optional
+            The name of the file to save the plot to. If not specified, the filename is automatically generated
+            using the problem name and the start datetime. The file extension is not required.
+        output_dir : str, optional
+            The directory where the plot file will be saved if savefile is True. Default is "output".
+
+        Returns
+        -------
+        matplotlib.figure.Figure
+            The Matplotlib figure object for the plot. This can be used for further customization or display.
+
+        Raises
+        ------
+        ValueError
+            If this method is called before the problem has been solved.
+        """
+        if self.x_final is not None:
+            self._plot_callback(initialize=True)
+        else:
+            raise ValueError(
+                "This method can only be used after invoking the 'solve()' method."
+            )
+
+        if savefile:
+            # Create output directory if it does not exist
+            if not os.path.exists(output_dir):
+                os.makedirs(output_dir)
+
+            # Give a name to the file if it is not specified
+            if filename is None:
+                filename = (
+                    f"convergence_{type(self.problem).__name__}_{self.start_datetime}"
+                )
+
+            # Save plots
+            fullfile = os.path.join(output_dir, filename)
+            savefig_in_formats(self.fig, fullfile, formats=[".png", ".svg"])
+
+        return self.fig
+
+
+class OptimizationProblem(ABC):
+    """
+    Abstract base class for optimization problems.
+
+    Derived optimization problem objects must implement the following methods:
+
+    - `fitness`: Evaluate the objective function and constraints for a given set of decision variables.
+    - `get_bounds`: Get the bounds for each decision variable.
+    - `get_neq`: Return the number of equality constraints associated with the problem.
+    - `get_nineq`: Return the number of inequality constraints associated with the problem.
+
+    Additionally, specific problem classes can define the `gradient` method to compute the Jacobians. If this method is not present in the derived class, the solver will revert to using forward finite differences for Jacobian calculations.
+
+    Methods
+    -------
+    fitness(x)
+        Evaluate the objective function and constraints for a given set of decision variables.
+    get_bounds()
+        Get the bounds for each decision variable.
+    get_neq()
+        Return the number of equality constraints associated with the problem.
+    get_nineq()
+        Return the number of inequality constraints associated with the problem.
+
+    """
+
+    @abstractmethod
+    def fitness(self, x):
+        """
+        Evaluate the objective function and constraints for given decision variables.
+
+        Parameters
+        ----------
+        x : array-like
+            Vector of independent variables (i.e., degrees of freedom).
+
+        Returns
+        -------
+        array_like
+            Vector containing the objective function, equality constraints, and inequality constraints.
+        """
+        pass
+
+    @abstractmethod
+    def get_bounds(self):
+        """
+        Get the bounds for each decision variable (Pygmo format)
+
+        Returns
+        -------
+        bounds : tuple of lists
+            A tuple of two items where the first item is the list of lower bounds and the second
+            item of the list of upper bounds for the vector of decision variables. For example,
+            ([-2 -1], [2, 1]) indicates that the first decision variable has bounds between
+            -2 and 2, and the second has bounds between -1 and 1.
+        """
+        pass
+
+    @abstractmethod
+    def get_nec(self):
+        """
+        Return the number of equality constraints associated with the problem.
+
+        Returns
+        -------
+        neq : int
+            Number of equality constraints.
+        """
+        pass
+
+    @abstractmethod
+    def get_nic(self):
+        """
+        Return the number of inequality constraints associated with the problem.
+
+        Returns
+        -------
+        nineq : int
+            Number of inequality constraints.
+        """
+        pass
+
+
+def count_constraints(var):
+    """
+    Retrieve the number of constraints based on the provided input.
+
+    This function returns the count of constraints based on the nature of the
+    input:
+
+    - `None` returns 0
+    - Scalar values return 1
+    - Array-like structures return their length
+
+    Parameters
+    ----------
+    var : None, scalar, or array-like (list, tuple, np.ndarray)
+        The input representing the constraint(s). This can be `None`, a scalar value,
+        or an array-like structure containing multiple constraints.
+
+    Returns
+    -------
+    int
+        The number of constraints:
+
+        - 0 for `None`
+        - 1 for scalar values
+        - Length of the array-like for array-like inputs
+
+    Examples
+    --------
+    >>> count_constraints(None)
+    0
+
+    >>> count_constraints(5.0)
+    1
+
+    >>> count_constraints([1.0, 2.0, 3.0])
+    3
+    """
+    # If constraint is None
+    if var is None:
+        return 0
+    # If constraint is a scalar (assuming it's numeric)
+    elif np.isscalar(var):
+        return 1
+    # If constraint is array-like
+    else:
+        return len(var)
+
+
+def combine_objective_and_constraints(f, c_eq=None, c_ineq=None):
+    """
+    Combine an objective function with its associated equality and inequality constraints.
+
+    This function takes in an objective function value, a set of equality constraints,
+    and a set of inequality constraints. It then returns a combined Numpy array of
+    these values. The constraints can be given as a list, tuple, numpy array, or as
+    individual values.
+
+    Parameters
+    ----------
+    f : float
+        The value of the objective function.
+    c_eq : float, list, tuple, np.ndarray, or None
+        The equality constraint(s). This can be a single value or a collection of values.
+        If `None`, no equality constraints will be added.
+    c_ineq : float, list, tuple, np.ndarray, or None
+        The inequality constraint(s). This can be a single value or a collection of values.
+        If `None`, no inequality constraints will be added.
+
+    Returns
+    -------
+    np.ndarray
+        A numpy array consisting of the objective function value followed by equality and
+        inequality constraints.
+
+    Examples
+    --------
+    >>> combine_objective_and_constraints(1.0, [0.5, 0.6], [0.7, 0.8])
+    array([1. , 0.5, 0.6, 0.7, 0.8])
+
+    >>> combine_objective_and_constraints(1.0, 0.5, 0.7)
+    array([1. , 0.5, 0.7])
+    """
+
+    # Validate objective function value
+    if isinstance(f, (list, tuple, np.ndarray)):
+        if len(f) != 1:
+            raise ValueError(
+                "Objective function value 'f' must be a scalar or single-element array."
+            )
+        f = f[0]  # Unwrap the single element to ensure it's treated as a scalar
+
+    # Add objective function
+    combined_list = [f]
+
+    # Add equality constraints
+    if c_eq is not None:
+        if isinstance(c_eq, (list, tuple, np.ndarray)):
+            combined_list.extend(c_eq)
+        else:
+            combined_list.append(c_eq)
+
+    # Add inequality constraints
+    if c_ineq is not None:
+        if isinstance(c_ineq, (list, tuple, np.ndarray)):
+            combined_list.extend(c_ineq)
+        else:
+            combined_list.append(c_ineq)
+
+    return np.array(combined_list)
+
+
+class _PygmoProblem:
+    """
+    A wrapper class for optimization problems to be compatible with Pygmo's need for deep-copiable problems.
+    This class uses anonymous functions (lambda) to prevent issues with deep copying complex objects,
+    (like Coolprop's AbstractState objects) which are not deep-copiable.
+    """
+
+    def __init__(self, wrapped_problem):
+        # Pygmo requires a flattened Jacobian for gradients, unlike SciPy's two-dimensional array.
+        self.fitness = lambda x: wrapped_problem.fitness(x)
+        self.gradient = lambda x: wrapped_problem.gradient(x).flatten()
+
+        # Directly link bounds and constraint counts from the original problem.
+        self.get_bounds = lambda: wrapped_problem.problem.get_bounds()
+        self.get_nec = lambda: wrapped_problem.problem.get_nec()
+        self.get_nic = lambda: wrapped_problem.problem.get_nic()
+
+        # If the original problem defines Hessians, provide them as well.
+        if hasattr(wrapped_problem.problem, "hessians"):
+            self.hessians = lambda x: wrapped_problem.problem.hessians(x)
+
+        # Define anonymous functions for objective and constraints with their Jacobians.
+        self.f = lambda x: wrapped_problem.fitness(x)[0]
+        self.c_eq = lambda x: wrapped_problem.fitness(x)[1 : 1 + self.get_nec()]
+        self.c_ineq = lambda x: wrapped_problem.fitness(x)[1 + self.get_nec() :]
+
+        self.f_jac = lambda x: wrapped_problem.gradient(x)[0, :]
+        self.c_eq_jac = lambda x: wrapped_problem.gradient(x)[1 : 1 + self.get_nec(), :]
+        self.c_ineq_jac = lambda x: wrapped_problem.gradient(x)[1 + self.get_nec() :, :]
```

### Comparing `turboflow-0.1.2/turboflow/pysolver_view/optimization_problems.py` & `turboflow-0.1.3/turboflow/pysolver_view/optimization_problems.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,273 +1,273 @@
-import numpy as np
-
-from .optimization import OptimizationProblem, combine_objective_and_constraints
-from .numerical_differentiation import approx_derivative, approx_jacobian_hessians
-
-
-class RosenbrockProblem(OptimizationProblem):
-    r"""
-    Implementation of the Rosenbrock problem.
-
-    The Rosenbrock problem, also known as Rosenbrock's valley or banana function,
-    is defined as:
-
-    .. math::
-       
-        \begin{align}
-        \text{minimize} \quad  & f(\mathbf{x}) = \sum_{i=1}^{n-1} \left[ 100(x_{i+1} - x_i^2)^2 + (x_i - 1)^2 \right] \\
-        \end{align}
-
-    Methods
-    -------
-    evaluate_problem(x)
-        Evaluates the Rosenbrock function and its constraints.
-    get_bounds()
-        Returns the bounds for the problem.
-    get_n_eq()
-        Returns the number of equality constraints.
-    get_n_ineq()
-        Returns the number of inequality constraints.
-    """
-
-    def __init__(self, dim):
-        self.dim = dim
-
-    def fitness(self, x):
-        """Rosenbrock function value"""
-        f = np.sum(100.0 * (x[1:] - x[:-1] ** 2) ** 2 + (x[:-1] - 1) ** 2)
-        return combine_objective_and_constraints(f, None, None)
-
-    def gradient(self, x):
-        """Rosenbrock function gradient"""
-        x = np.asarray(x)
-        xm = x[1:-1]
-        xm_m1 = x[:-2]
-        xm_p1 = x[2:]
-        grad = np.zeros_like(x)
-        grad[1:-1] = 200 * (xm - xm_m1**2) - 400 * (xm_p1 - xm**2) * xm - 2 * (1 - xm)
-        grad[0] = -400 * x[0] * (x[1] - x[0] ** 2) - 2 * (1 - x[0])
-        grad[-1] = 200 * (x[-1] - x[-2] ** 2)
-        return grad
-
-    def hessians(self, x, lower_triangular=True):
-        """Rosenbrock function gradient"""
-        x = np.atleast_1d(x)
-        H = np.diag(-400 * x[:-1], 1) - np.diag(400 * x[:-1], -1)
-        diagonal = np.zeros(len(x), dtype=x.dtype)
-        diagonal[0] = 1200 * x[0] ** 2 - 400 * x[1] + 2
-        diagonal[-1] = 200
-        diagonal[1:-1] = 202 + 1200 * x[1:-1] ** 2 - 400 * x[2:]
-        H = H + np.diag(diagonal)
-        if lower_triangular:
-            H = H[np.tril_indices(len(x))]  # Lower triangular
-            H = np.asarray([H])  # Correct Pygmo array shape
-        # return H.squeeze()
-        return H
-
-    # def hessians(self, x, lower_triangular=True):
-    #     return approx_jacobian_hessians(self.fitness, x, lower_triangular=lower_triangular)
-
-    def get_bounds(self):
-        return (-10 * np.ones(self.dim), 10 * np.ones(self.dim))
-
-    def get_nec(self):
-        return 0
-
-    def get_nic(self):
-        return 0
-
-
-class RosenbrockProblemConstrained(OptimizationProblem):
-    r"""
-    Implementation of the Chained Rosenbrock function with trigonometric-exponential constraints.
-
-    This problem is also referred to as Example 5.1 in the report by Luksan and Vlcek. The optimization problem is described as:
-
-    .. math::
-
-        \begin{align}
-        \text{minimize} \quad & \sum_{i=1}^{n-1}\left[100\left(x_i^2-x_{i+1}\right)^2 + \left(x_i-1\right)^2\right] \\
-        \text{s.t.} \quad & 3x_{k+1}^3 + 2x_{k+2} - 5 + \sin(x_{k+1}-x_{k+2})\sin(x_{k+1}+x_{k+2}) + \\
-                            & + 4x_{k+1} - x_k \exp(x_k-x_{k+1}) - 3 = 0, \; \forall k=1,...,n-2 \\
-                            & -5 \le x_i \le 5, \forall i=1,...,n
-        \end{align}
-
-    References
-    ----------
-    - Luksan, L., and Jan Vlcek. Sparse and partially separable test problems for unconstrained and equality constrained optimization. (1999). `doi: link provided <http://hdl.handle.net/11104/0123965>`_.
-
-    Methods
-    -------
-    evaluate_problem(x):
-        Compute objective, equality, and inequality constraint.
-    get_bounds():
-        Return variable bounds.
-    get_n_eq():
-        Get number of equality constraints.
-    get_n_ineq():  
-        Get number of inequality constraints.
-    """
-
-    def __init__(self, dim):
-        self.dim = dim
-
-    def fitness(self, x):
-        # Objective function
-        f = [np.sum(100.0 * (x[1:] - x[:-1] ** 2) ** 2 + (x[:-1] - 1) ** 2)]
-
-        # Equality constraints
-        c_eq = []
-        for k in range(self.dim - 2):
-            val = (
-                3 * x[k + 1] ** 3
-                + 2 * x[k + 2]
-                - 5
-                + np.sin(x[k + 1] - x[k + 2]) * np.sin(x[k + 1] + x[k + 2])
-                + 4 * x[k + 1]
-                - x[k] * np.exp(x[k] - x[k + 1])
-                - 3
-            )
-            c_eq.append(val)
-
-        return combine_objective_and_constraints(f, c_eq, None)
-
-    def gradient(self, x):
-        gradient = approx_derivative(
-            self.fitness,
-            x,
-            method="2-point",
-            abs_step=1e-6 * np.abs(x),
-        )
-        return gradient
-
-    def hessians(self, x):
-        H = approx_jacobian_hessians(
-            self.fitness, x, abs_step=1e-4, lower_triangular=True
-        )
-        return H
-
-    def get_bounds(self):
-        return ([-10] * self.dim, [+10] * self.dim)
-
-    def get_nec(self):
-        return self.dim - 2
-
-    def get_nic(self):
-        return 0
-
-
-class HS71Problem(OptimizationProblem):
-    r"""
-    Implementation of the Hock Schittkowski problem No.71.
-
-    This class implements the following optimization problem:
-
-    .. math::
-
-        \begin{align}
-        \text{minimize} \quad  & f(\mathbf{x}) = x_1x_4(x_1+x_2+x_3) + x_3 \\
-        \text{s.t.} \quad      & x_1^2 + x_2^2 + x_3^2 + x_4^2 = 40 \\
-                                & 25 - x_1 x_2 x_3 x_4 \le 0 \\
-                                & 1 \le x_1, x_2, x_3, x_4 \le 5
-        \end{align}
-
-       
-    References
-    ----------
-    - W. Hock and K. Schittkowski. Test examples for nonlinear programming codes. Lecture Notes in Economics and Mathematical Systems, 187, 1981. `doi: 10.1007/978-3-642-48320-2 <https://doi.org/10.1007/978-3-642-48320-2>`_.
-
-
-    Methods
-    -------
-    evaluate_problem(x)`:
-        Compute objective, equality, and inequality constraint.
-    get_bounds()`:
-        Return variable bounds.
-    get_n_eq()`:
-        Get number of equality constraints.
-    get_n_ineq()`:  
-        Get number of inequality constraints.
-
-    """
-
-    def fitness(self, x):
-        # Objective function
-        f = x[0] * x[3] * (x[0] + x[1] + x[2]) + x[2]
-
-        # Equality constraints
-        c_eq = (x[0] ** 2 + x[1] ** 2 + x[2] ** 2 + x[3] ** 2) - 40.0
-
-        # Inequality constraints
-        c_ineq = 25 - x[0] * x[1] * x[2] * x[3]
-
-        return combine_objective_and_constraints(f, c_eq, c_ineq)
-
-    def get_bounds(self):
-        return (4 * [1], 4 * [5])
-
-    def get_nec(self):
-        return 1
-
-    def get_nic(self):
-        return 1
-
-
-class LorentzEquationsOpt(OptimizationProblem):
-    r"""
-    Implementation of the Lorentz System of Nonlinear Equations as an optimization problem
-
-    This class implements the following system of algebraic nonlinear equations:
-
-    .. math::
-
-        \begin{align}
-        \dot{x} &= \sigma(y - x) = 0\\
-        \dot{y} &= x(\rho - z) - y = 0\\
-        \dot{z} &= xy - \beta z = 0
-        \end{align}
-
-    Where:
-
-    - :math:`\sigma` is related to the Prandtl number
-    - :math:`\rho` is related to the Rayleigh number
-    - :math:`\beta` is a geometric factor
-
-    References
-    ----------
-    - Edward N. Lorenz. "Deterministic Nonperiodic Flow". Journal of the Atmospheric Sciences, 20(2):130-141, 1963.
-    
-    Methods
-    -------
-    evaluate_problem(vars)`:
-        Evaluate the Lorentz system at a given state.
-
-    Attributes
-    ----------
-    sigma : float
-        The Prandtl number.
-    beta : float
-        The geometric factor.
-    rho : float
-        The Rayleigh number.
-    """
-
-    def __init__(self, sigma=1.0, beta=2.0, rho=3.0):
-        self.sigma = sigma
-        self.beta = beta
-        self.rho = rho
-
-    def fitness(self, vars):
-        x, y, z = vars
-        eq1 = self.sigma * (y - x)
-        eq2 = x * (self.rho - z) - y
-        eq3 = x * y - self.beta * z
-        return [0, eq1, eq2, eq3]
-
-    def get_nec(self):
-        return 3
-
-    def get_nic(self):
-        return 0
-
-    def get_bounds(self):
-        return (-10 * np.ones(3), 10 * np.ones(3))
+import numpy as np
+
+from .optimization import OptimizationProblem, combine_objective_and_constraints
+from .numerical_differentiation import approx_derivative, approx_jacobian_hessians
+
+
+class RosenbrockProblem(OptimizationProblem):
+    r"""
+    Implementation of the Rosenbrock problem.
+
+    The Rosenbrock problem, also known as Rosenbrock's valley or banana function,
+    is defined as:
+
+    .. math::
+       
+        \begin{align}
+        \text{minimize} \quad  & f(\mathbf{x}) = \sum_{i=1}^{n-1} \left[ 100(x_{i+1} - x_i^2)^2 + (x_i - 1)^2 \right] \\
+        \end{align}
+
+    Methods
+    -------
+    evaluate_problem(x)
+        Evaluates the Rosenbrock function and its constraints.
+    get_bounds()
+        Returns the bounds for the problem.
+    get_n_eq()
+        Returns the number of equality constraints.
+    get_n_ineq()
+        Returns the number of inequality constraints.
+    """
+
+    def __init__(self, dim):
+        self.dim = dim
+
+    def fitness(self, x):
+        """Rosenbrock function value"""
+        f = np.sum(100.0 * (x[1:] - x[:-1] ** 2) ** 2 + (x[:-1] - 1) ** 2)
+        return combine_objective_and_constraints(f, None, None)
+
+    def gradient(self, x):
+        """Rosenbrock function gradient"""
+        x = np.asarray(x)
+        xm = x[1:-1]
+        xm_m1 = x[:-2]
+        xm_p1 = x[2:]
+        grad = np.zeros_like(x)
+        grad[1:-1] = 200 * (xm - xm_m1**2) - 400 * (xm_p1 - xm**2) * xm - 2 * (1 - xm)
+        grad[0] = -400 * x[0] * (x[1] - x[0] ** 2) - 2 * (1 - x[0])
+        grad[-1] = 200 * (x[-1] - x[-2] ** 2)
+        return grad
+
+    def hessians(self, x, lower_triangular=True):
+        """Rosenbrock function gradient"""
+        x = np.atleast_1d(x)
+        H = np.diag(-400 * x[:-1], 1) - np.diag(400 * x[:-1], -1)
+        diagonal = np.zeros(len(x), dtype=x.dtype)
+        diagonal[0] = 1200 * x[0] ** 2 - 400 * x[1] + 2
+        diagonal[-1] = 200
+        diagonal[1:-1] = 202 + 1200 * x[1:-1] ** 2 - 400 * x[2:]
+        H = H + np.diag(diagonal)
+        if lower_triangular:
+            H = H[np.tril_indices(len(x))]  # Lower triangular
+            H = np.asarray([H])  # Correct Pygmo array shape
+        # return H.squeeze()
+        return H
+
+    # def hessians(self, x, lower_triangular=True):
+    #     return approx_jacobian_hessians(self.fitness, x, lower_triangular=lower_triangular)
+
+    def get_bounds(self):
+        return (-10 * np.ones(self.dim), 10 * np.ones(self.dim))
+
+    def get_nec(self):
+        return 0
+
+    def get_nic(self):
+        return 0
+
+
+class RosenbrockProblemConstrained(OptimizationProblem):
+    r"""
+    Implementation of the Chained Rosenbrock function with trigonometric-exponential constraints.
+
+    This problem is also referred to as Example 5.1 in the report by Luksan and Vlcek. The optimization problem is described as:
+
+    .. math::
+
+        \begin{align}
+        \text{minimize} \quad & \sum_{i=1}^{n-1}\left[100\left(x_i^2-x_{i+1}\right)^2 + \left(x_i-1\right)^2\right] \\
+        \text{s.t.} \quad & 3x_{k+1}^3 + 2x_{k+2} - 5 + \sin(x_{k+1}-x_{k+2})\sin(x_{k+1}+x_{k+2}) + \\
+                            & + 4x_{k+1} - x_k \exp(x_k-x_{k+1}) - 3 = 0, \; \forall k=1,...,n-2 \\
+                            & -5 \le x_i \le 5, \forall i=1,...,n
+        \end{align}
+
+    References
+    ----------
+    - Luksan, L., and Jan Vlcek. Sparse and partially separable test problems for unconstrained and equality constrained optimization. (1999). `doi: link provided <http://hdl.handle.net/11104/0123965>`_.
+
+    Methods
+    -------
+    evaluate_problem(x):
+        Compute objective, equality, and inequality constraint.
+    get_bounds():
+        Return variable bounds.
+    get_n_eq():
+        Get number of equality constraints.
+    get_n_ineq():  
+        Get number of inequality constraints.
+    """
+
+    def __init__(self, dim):
+        self.dim = dim
+
+    def fitness(self, x):
+        # Objective function
+        f = [np.sum(100.0 * (x[1:] - x[:-1] ** 2) ** 2 + (x[:-1] - 1) ** 2)]
+
+        # Equality constraints
+        c_eq = []
+        for k in range(self.dim - 2):
+            val = (
+                3 * x[k + 1] ** 3
+                + 2 * x[k + 2]
+                - 5
+                + np.sin(x[k + 1] - x[k + 2]) * np.sin(x[k + 1] + x[k + 2])
+                + 4 * x[k + 1]
+                - x[k] * np.exp(x[k] - x[k + 1])
+                - 3
+            )
+            c_eq.append(val)
+
+        return combine_objective_and_constraints(f, c_eq, None)
+
+    def gradient(self, x):
+        gradient = approx_derivative(
+            self.fitness,
+            x,
+            method="2-point",
+            abs_step=1e-6 * np.abs(x),
+        )
+        return gradient
+
+    def hessians(self, x):
+        H = approx_jacobian_hessians(
+            self.fitness, x, abs_step=1e-4, lower_triangular=True
+        )
+        return H
+
+    def get_bounds(self):
+        return ([-10] * self.dim, [+10] * self.dim)
+
+    def get_nec(self):
+        return self.dim - 2
+
+    def get_nic(self):
+        return 0
+
+
+class HS71Problem(OptimizationProblem):
+    r"""
+    Implementation of the Hock Schittkowski problem No.71.
+
+    This class implements the following optimization problem:
+
+    .. math::
+
+        \begin{align}
+        \text{minimize} \quad  & f(\mathbf{x}) = x_1x_4(x_1+x_2+x_3) + x_3 \\
+        \text{s.t.} \quad      & x_1^2 + x_2^2 + x_3^2 + x_4^2 = 40 \\
+                                & 25 - x_1 x_2 x_3 x_4 \le 0 \\
+                                & 1 \le x_1, x_2, x_3, x_4 \le 5
+        \end{align}
+
+       
+    References
+    ----------
+    - W. Hock and K. Schittkowski. Test examples for nonlinear programming codes. Lecture Notes in Economics and Mathematical Systems, 187, 1981. `doi: 10.1007/978-3-642-48320-2 <https://doi.org/10.1007/978-3-642-48320-2>`_.
+
+
+    Methods
+    -------
+    evaluate_problem(x)`:
+        Compute objective, equality, and inequality constraint.
+    get_bounds()`:
+        Return variable bounds.
+    get_n_eq()`:
+        Get number of equality constraints.
+    get_n_ineq()`:  
+        Get number of inequality constraints.
+
+    """
+
+    def fitness(self, x):
+        # Objective function
+        f = x[0] * x[3] * (x[0] + x[1] + x[2]) + x[2]
+
+        # Equality constraints
+        c_eq = (x[0] ** 2 + x[1] ** 2 + x[2] ** 2 + x[3] ** 2) - 40.0
+
+        # Inequality constraints
+        c_ineq = 25 - x[0] * x[1] * x[2] * x[3]
+
+        return combine_objective_and_constraints(f, c_eq, c_ineq)
+
+    def get_bounds(self):
+        return (4 * [1], 4 * [5])
+
+    def get_nec(self):
+        return 1
+
+    def get_nic(self):
+        return 1
+
+
+class LorentzEquationsOpt(OptimizationProblem):
+    r"""
+    Implementation of the Lorentz System of Nonlinear Equations as an optimization problem
+
+    This class implements the following system of algebraic nonlinear equations:
+
+    .. math::
+
+        \begin{align}
+        \dot{x} &= \sigma(y - x) = 0\\
+        \dot{y} &= x(\rho - z) - y = 0\\
+        \dot{z} &= xy - \beta z = 0
+        \end{align}
+
+    Where:
+
+    - :math:`\sigma` is related to the Prandtl number
+    - :math:`\rho` is related to the Rayleigh number
+    - :math:`\beta` is a geometric factor
+
+    References
+    ----------
+    - Edward N. Lorenz. "Deterministic Nonperiodic Flow". Journal of the Atmospheric Sciences, 20(2):130-141, 1963.
+    
+    Methods
+    -------
+    evaluate_problem(vars)`:
+        Evaluate the Lorentz system at a given state.
+
+    Attributes
+    ----------
+    sigma : float
+        The Prandtl number.
+    beta : float
+        The geometric factor.
+    rho : float
+        The Rayleigh number.
+    """
+
+    def __init__(self, sigma=1.0, beta=2.0, rho=3.0):
+        self.sigma = sigma
+        self.beta = beta
+        self.rho = rho
+
+    def fitness(self, vars):
+        x, y, z = vars
+        eq1 = self.sigma * (y - x)
+        eq2 = x * (self.rho - z) - y
+        eq3 = x * y - self.beta * z
+        return [0, eq1, eq2, eq3]
+
+    def get_nec(self):
+        return 3
+
+    def get_nic(self):
+        return 0
+
+    def get_bounds(self):
+        return (-10 * np.ones(3), 10 * np.ones(3))
```

### Comparing `turboflow-0.1.2/turboflow/pysolver_view/pysolver_utilities.py` & `turboflow-0.1.3/turboflow/utilities/graphics.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,287 +1,273 @@
-import os
-import logging
-import numpy as np
-import matplotlib as mpl
-import matplotlib.pyplot as plt
-
-from cycler import cycler
-from datetime import datetime
-
-COLORS_PYTHON = [
-    "#1f77b4",
-    "#ff7f0e",
-    "#2ca02c",
-    "#d62728",
-    "#9467bd",
-    "#8c564b",
-    "#e377c2",
-    "#7f7f7f",
-    "#bcbd22",
-    "#17becf",
-]
-
-COLORS_MATLAB = [
-    "#0072BD",
-    "#D95319",
-    "#EDB120",
-    "#7E2F8E",
-    "#77AC30",
-    "#4DBEEE",
-    "#A2142F",
-]
-
-
-def set_plot_options(
-    fontsize=13,
-    grid=False,
-    major_ticks=True,
-    minor_ticks=True,
-    margin=0.05,
-    color_order="matlab",
-):
-    """
-    Set options for creating publication-quality figures using Matplotlib.
-
-    This function updates the internal Matplotlib settings to better align with standards for publication-quality figures.
-    Features include improved font selections, tick marks, grid appearance, and color selections.
-
-    Parameters
-    ----------
-    fontsize : int, optional
-        Font size for text elements in the plot. Default is 13.
-    grid : bool, optional
-        Whether to show grid lines on the plot. Default is True.
-    major_ticks : bool, optional
-        Whether to show major ticks. Default is True.
-    minor_ticks : bool, optional
-        Whether to show minor ticks. Default is True.
-    margin : float, optional
-        Margin size for axes. Default is 0.05.
-    color_order : str, optional
-        Color order to be used for plot lines. Options include "python" and "matlab". Default is "matlab".
-
-    """
-
-    if isinstance(color_order, str):
-        if color_order.lower() == "python":
-            color_order = [
-                "#1f77b4",
-                "#ff7f0e",
-                "#2ca02c",
-                "#d62728",
-                "#9467bd",
-                "#8c564b",
-                "#e377c2",
-                "#7f7f7f",
-                "#bcbd22",
-                "#17becf",
-            ]
-
-        elif color_order.lower() == "matlab":
-            color_order = [
-                "#0072BD",
-                "#D95319",
-                "#EDB120",
-                "#7E2F8E",
-                "#77AC30",
-                "#4DBEEE",
-                "#A2142F",
-            ]
-
-    # Define dictionary of custom settings
-    rcParams = {
-        "text.usetex": False,
-        "font.size": fontsize,
-        "font.style": "normal",
-        "font.family": "serif",  # 'serif', 'sans-serif', 'cursive', 'fantasy', 'monospace'
-        "font.serif": ["Times New Roman"],  # ['times new roman', 'cmr10']
-        "mathtext.fontset": "stix",  # ["stix", 'cm']
-        "axes.edgecolor": "black",
-        "axes.linewidth": 1.25,
-        "axes.titlesize": fontsize,
-        "axes.titleweight": "normal",
-        "axes.titlepad": fontsize * 1.4,
-        "axes.labelsize": fontsize,
-        "axes.labelweight": "normal",
-        "axes.labelpad": fontsize,
-        "axes.xmargin": margin,
-        "axes.ymargin": margin,
-        # "axes.zmargin": margin,
-        "axes.grid": grid,
-        "axes.grid.axis": "both",
-        "axes.grid.which": "major",
-        "axes.prop_cycle": cycler(color=color_order),
-        "grid.alpha": 0.5,
-        "grid.color": "black",
-        "grid.linestyle": "-",
-        "grid.linewidth": 0.5,
-        "legend.borderaxespad": 1,
-        "legend.borderpad": 0.6,
-        "legend.edgecolor": "black",
-        "legend.facecolor": "white",
-        "legend.labelcolor": "black",
-        "legend.labelspacing": 0.3,
-        "legend.fancybox": True,
-        "legend.fontsize": fontsize - 2,
-        "legend.framealpha": 1.00,
-        "legend.handleheight": 0.7,
-        "legend.handlelength": 1.25,
-        "legend.handletextpad": 0.8,
-        "legend.markerscale": 1.0,
-        "legend.numpoints": 1,
-        "lines.linewidth": 1.25,
-        "lines.markersize": 4,
-        "lines.markeredgewidth": 1.25,
-        "lines.markerfacecolor": "white",
-        "xtick.direction": "in",
-        "xtick.labelsize": fontsize - 1,
-        "xtick.bottom": major_ticks,
-        "xtick.top": major_ticks,
-        "xtick.major.size": 6,
-        "xtick.major.width": 1.25,
-        "xtick.minor.size": 3,
-        "xtick.minor.width": 0.75,
-        "xtick.minor.visible": minor_ticks,
-        "ytick.direction": "in",
-        "ytick.labelsize": fontsize - 1,
-        "ytick.left": major_ticks,
-        "ytick.right": major_ticks,
-        "ytick.major.size": 6,
-        "ytick.major.width": 1.25,
-        "ytick.minor.size": 3,
-        "ytick.minor.width": 0.75,
-        "ytick.minor.visible": minor_ticks,
-        "savefig.dpi": 500,
-    }
-
-    # Update the internal Matplotlib settings dictionary
-    mpl.rcParams.update(rcParams)
-
-
-def print_installed_fonts():
-    """
-    Print the list of fonts installed on the system.
-
-    This function identifies and prints all available fonts for use in Matplotlib.
-    """
-    fonts = mpl.font_manager.findSystemFonts(fontpaths=None, fontext="ttf")
-    for font in sorted(fonts):
-        print(font)
-
-
-def print_rc_parameters(filename=None):
-    """
-    Print the current rcParams used by Matplotlib or write to file if provided.
-
-    This function provides a quick overview of the active configuration parameters within Matplotlib.
-    """
-    params = mpl.rcParams
-    for key, value in params.items():
-        print(f"{key}: {value}")
-
-    if filename:
-        with open(filename, "w") as file:
-            for key, value in params.items():
-                file.write(f"{key}: {value}\n")
-
-
-def savefig_in_formats(fig, path_without_extension, formats=[".png", ".svg"], dpi=500):
-    """
-    Save a given Matplotlib figure in multiple file formats.
-
-    Parameters
-    ----------
-    fig : matplotlib.figure.Figure
-        The figure object to be saved.
-    path_without_extension : str
-        The full path to save the figure excluding the file extension.
-    formats : list of str, optional
-        A list of string file extensions to specify which formats the figure should be saved in.
-        Supported formats are ['.png', '.svg', '.pdf', '.eps'].
-        Default is ['.png', '.svg'].
-    dpi : int, optional
-        The resolution in dots per inch with which to save the image. This parameter affects only PNG files.
-        Default is 500.
-
-    Examples
-    --------
-    >>> import matplotlib.pyplot as plt
-    >>> fig, ax = plt.subplots()
-    >>> ax.plot([0, 1], [0, 1])
-    >>> save_fig_in_formats(fig, "/path/to/figure/filename")
-
-    This will save the figure as "filename.png", "filename.svg", and "filename.pdf" in the "/path/to/figure/" directory.
-    """
-
-    # Validate export formats
-    allowed_formats = {".png", ".svg", ".pdf", ".eps"}
-    invalid_formats = set(formats) - allowed_formats
-    if invalid_formats:
-        raise ValueError(
-            f"Unsupported file formats: {', '.join(invalid_formats)}. Allowed formats: {', '.join(allowed_formats)}"
-        )
-
-    for ext in formats:
-        if ext == ".png":
-            # Save PNG files with specified DPI
-            fig.savefig(f"{path_without_extension}{ext}", bbox_inches="tight", dpi=dpi)
-        else:
-            # Save in vector formats without DPI setting
-            fig.savefig(f"{path_without_extension}{ext}", bbox_inches="tight")
-
-
-def create_logger(name, path=None, use_datetime=True):
-    """
-    Creates and configures a logging object for recording logs during program execution.
-
-    Parameters
-    ----------
-    name : str
-        Name of the log file. Allows for differentiation when analyzing logs from different components or runs of a program.
-    path : str, optional
-        Specifies the directory where the log files will be saved. By default, a directory named "logs"
-        will be created in the current working directory (cwd).
-    use_datetime : bool, optional
-        Determines whether the log filename should have a unique datetime identifier appended. Default is True.
-
-    Returns
-    -------
-    logger : object
-        Configured logger object.
-
-    Notes
-    -----
-    - By default, the function sets the log level to `INFO`, which means the logger will handle
-      messages of level `INFO` and above (like `ERROR`, `WARNING`, etc.). The log entries will contain
-      the timestamp, log level, and the actual log message.
-    - When `use_datetime=True`, each log file will have a unique datetime identifier. This ensures
-      traceability, avoids overwriting previous logs, allows chronological ordering of log files, and
-      handles concurrency in multi-instance environments.
-    """
-
-    # Define logs directory if it is not provided
-    if path is None:
-        path = os.path.join(os.getcwd(), "logs")
-
-    # Create logs directory if it does not exist
-    if not os.path.exists(path):
-        os.makedirs(path)
-
-    # Define file name and path
-    if use_datetime:
-        current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
-        log_filename = os.path.join(path, f"{name}_{current_time}.log")
-    else:
-        log_filename = os.path.join(path, f"{name}.log")
-
-    # Set logger configuration
-    logging.basicConfig(
-        filename=log_filename,
-        level=logging.INFO,
-        format="%(asctime)s - %(levelname)s - %(message)s",
-    )
-
-    # Create logger object
-    logger = logging.getLogger()
-
-    return logger
+import os
+import numpy as np
+import matplotlib as mpl
+import matplotlib.pyplot as plt
+
+# Attempt to import imageio for video generatioj
+try:
+    import imageio
+
+    IMEGEIO_AVAILABLE = True
+except ImportError:
+    IMAGEIO_AVAILABLE = False
+
+
+from cycler import cycler
+
+COLORS_PYTHON = [
+    "#1f77b4",
+    "#ff7f0e",
+    "#2ca02c",
+    "#d62728",
+    "#9467bd",
+    "#8c564b",
+    "#e377c2",
+    "#7f7f7f",
+    "#bcbd22",
+    "#17becf",
+]
+
+COLORS_MATLAB = [
+    "#0072BD",
+    "#D95319",
+    "#EDB120",
+    "#7E2F8E",
+    "#77AC30",
+    "#4DBEEE",
+    "#A2142F",
+]
+
+
+def set_plot_options(
+    fontsize=14,
+    grid=True,
+    major_ticks=True,
+    minor_ticks=True,
+    margin=0.05,
+    color_order="matlab",
+):
+    """
+    Set options for creating publication-quality figures using Matplotlib.
+
+    This function updates the internal Matplotlib settings to better align with standards for publication-quality figures.
+    Features include improved font selections, tick marks, grid appearance, and color selections.
+
+    Parameters
+    ----------
+    fontsize : int, optional
+        Font size for text elements in the plot. Default is 13.
+    grid : bool, optional
+        Whether to show grid lines on the plot. Default is True.
+    major_ticks : bool, optional
+        Whether to show major ticks. Default is True.
+    minor_ticks : bool, optional
+        Whether to show minor ticks. Default is True.
+    margin : float, optional
+        Margin size for axes. Default is 0.05.
+    color_order : str, optional
+        Color order to be used for plot lines. Options include "python" and "matlab". Default is "matlab".
+
+    """
+
+    if isinstance(color_order, str):
+        if color_order.lower() == "default":
+            color_order = COLORS_PYTHON
+
+        elif color_order.lower() == "matlab":
+            color_order = COLORS_MATLAB
+
+    # Define dictionary of custom settings
+    rcParams = {
+        "text.usetex": False,
+        "font.size": fontsize,
+        "font.style": "normal",
+        "font.family": "serif",  # 'serif', 'sans-serif', 'cursive', 'fantasy', 'monospace'
+        "font.serif": ["Times New Roman"],  # ['times new roman', 'cmr10']
+        "mathtext.fontset": "stix",  # ["stix", 'cm']
+        "axes.edgecolor": "black",
+        "axes.linewidth": 1.25,
+        "axes.titlesize": fontsize,
+        "axes.titleweight": "normal",
+        "axes.titlepad": fontsize * 1.4,
+        "axes.labelsize": fontsize,
+        "axes.labelweight": "normal",
+        "axes.labelpad": fontsize,
+        "axes.xmargin": margin,
+        "axes.ymargin": margin,
+        "axes.zmargin": margin,
+        "axes.grid": grid,
+        "axes.grid.axis": "both",
+        "axes.grid.which": "major",
+        "axes.prop_cycle": cycler(color=color_order),
+        "grid.alpha": 1,
+        "grid.color": "#808080",  # Grey equivalent to 50% transparency
+        "grid.linestyle": "-",
+        "grid.linewidth": 0.5,
+        "legend.borderaxespad": 1,
+        "legend.borderpad": 0.6,
+        "legend.edgecolor": "black",
+        "legend.facecolor": "white",
+        "legend.labelcolor": "black",
+        "legend.labelspacing": 0.3,
+        "legend.fancybox": True,
+        "legend.fontsize": fontsize - 2,
+        "legend.framealpha": 1.00,
+        "legend.handleheight": 0.7,
+        "legend.handlelength": 1.25,
+        "legend.handletextpad": 0.8,
+        "legend.markerscale": 1.0,
+        "legend.numpoints": 1,
+        "lines.linewidth": 1.25,
+        "lines.markersize": 5,
+        "lines.markeredgewidth": 1.25,
+        "lines.markerfacecolor": "white",
+        "xtick.direction": "in",
+        "xtick.labelsize": fontsize - 1,
+        "xtick.bottom": major_ticks,
+        "xtick.top": major_ticks,
+        "xtick.major.size": 6,
+        "xtick.major.width": 1.25,
+        "xtick.minor.size": 3,
+        "xtick.minor.width": 0.75,
+        "xtick.minor.visible": minor_ticks,
+        "ytick.direction": "in",
+        "ytick.labelsize": fontsize - 1,
+        "ytick.left": major_ticks,
+        "ytick.right": major_ticks,
+        "ytick.major.size": 6,
+        "ytick.major.width": 1.25,
+        "ytick.minor.size": 3,
+        "ytick.minor.width": 0.75,
+        "ytick.minor.visible": minor_ticks,
+        "savefig.dpi": 500,
+    }
+
+    # Update the internal Matplotlib settings dictionary
+    mpl.rcParams.update(rcParams)
+
+
+def print_installed_fonts():
+    """
+    Print the list of fonts installed on the system.
+
+    This function identifies and prints all available fonts for use in Matplotlib.
+    """
+    fonts = mpl.font_manager.findSystemFonts(fontpaths=None, fontext="ttf")
+    for font in sorted(fonts):
+        print(font)
+
+
+def print_rc_parameters(filename=None):
+    """
+    Print the current rcParams used by Matplotlib or write to file if provided.
+
+    This function provides a quick overview of the active configuration parameters within Matplotlib.
+    """
+    params = mpl.rcParams
+    for key, value in params.items():
+        print(f"{key}: {value}")
+
+    if filename:
+        with open(filename, "w") as file:
+            for key, value in params.items():
+                file.write(f"{key}: {value}\n")
+
+
+def savefig_in_formats(fig, path_without_extension, formats=[".png", ".svg"], dpi=500):
+    """
+    Save a given Matplotlib figure in multiple file formats.
+
+    Parameters
+    ----------
+    fig : matplotlib.figure.Figure
+        The figure object to be saved.
+    path_without_extension : str
+        The full path to save the figure excluding the file extension.
+    formats : list of str, optional
+        A list of string file extensions to specify which formats the figure should be saved in.
+        Supported formats are ['.png', '.svg', '.pdf', '.eps'].
+        Default is ['.png', '.svg'].
+    dpi : int, optional
+        The resolution in dots per inch with which to save the image. This parameter affects only PNG files.
+        Default is 500.
+
+    Examples
+    --------
+    >>> import matplotlib.pyplot as plt
+    >>> fig, ax = plt.subplots()
+    >>> ax.plot([0, 1], [0, 1])
+    >>> save_fig_in_formats(fig, "/path/to/figure/filename")
+
+    This will save the figure as "filename.png", "filename.svg", and "filename.pdf" in the "/path/to/figure/" directory.
+    """
+
+    # Validate export formats
+    allowed_formats = {".png", ".svg", ".pdf", ".eps"}
+    invalid_formats = set(formats) - allowed_formats
+    if invalid_formats:
+        raise ValueError(
+            f"Unsupported file formats: {', '.join(invalid_formats)}. Allowed formats: {', '.join(allowed_formats)}"
+        )
+
+    for ext in formats:
+        if ext == ".png":
+            # Save PNG files with specified DPI
+            fig.savefig(f"{path_without_extension}{ext}", bbox_inches="tight", dpi=dpi)
+        else:
+            # Save in vector formats without DPI setting
+            fig.savefig(f"{path_without_extension}{ext}", bbox_inches="tight")
+
+
+def create_gif(image_folder, output_file, duration=0.5):
+    """
+    Create a GIF from a series of images.
+
+    Parameters
+    ----------
+    image_folder : str
+        The path to the folder containing the images.
+    output_file : str
+        The path and filename of the output GIF.
+    duration : float, optional
+        Duration of each frame in the GIF, by default 0.5 seconds.
+    """
+
+    if not IMEGEIO_AVAILABLE:
+        raise ImportError(
+            "imageio is not installed. Please run `pip install imageio-ffmpeg` to run this function."
+        )
+
+    images = []
+    for filename in sorted(os.listdir(image_folder)):
+        if filename.endswith(".png"):
+            file_path = os.path.join(image_folder, filename)
+            images.append(imageio.imread(file_path))
+
+    imageio.mimsave(output_file, images, duration=duration)
+
+
+def create_mp4(image_folder, output_file, fps=10):
+    """
+    Create an MP4 video from a series of images.
+
+    Parameters
+    ----------
+    image_folder : str
+        The path to the folder containing the images.
+    output_file : str
+        The path and filename of the output MP4 video.
+    fps : int, optional
+        Frames per second in the output video, by default 10.
+    """
+
+    if not IMEGEIO_AVAILABLE:
+        raise ImportError(
+            "imageio is not installed. Please run `pip install imageio-ffmpeg` to run this function."
+        )
+
+    with imageio.get_writer(output_file, fps=fps) as writer:
+        for filename in sorted(os.listdir(image_folder)):
+            if filename.endswith(".png"):
+                file_path = os.path.join(image_folder, filename)
+                image = imageio.imread(file_path)
+                writer.append_data(image)
```

### Comparing `turboflow-0.1.2/turboflow/thermodynamic_cycles/brayton_recuperated.py` & `turboflow-0.1.3/turboflow/thermodynamic_cycles/brayton_recuperated.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,328 +1,328 @@
-import copy
-from .. import utilities
-from .. import fluid_properties as props
-
-from .components import compression_process, expansion_process, heat_exchanger
-
-COLORS_MATLAB = utilities.COLORS_MATLAB
-
-
-def evaluate_cycle(
-    variables,
-    parameters,
-    constraints,
-    objective_function,
-):
-    # Create copies to not change the originals
-    variables = copy.deepcopy(variables)
-    parameters = copy.deepcopy(parameters)
-
-    # Initialize fluid objects
-    working_fluid = props.Fluid(
-        **parameters.pop("working_fluid"), identifier="working_fluid"
-    )
-    heating_fluid = props.Fluid(
-        **parameters.pop("heating_fluid"), identifier="heating_fluid"
-    )
-    cooling_fluid = props.Fluid(
-        **parameters.pop("cooling_fluid"), identifier="cooling_fluid"
-    )
-
-    # Extract heat source/sink parameters and give short names
-    T_source_out_max = parameters["heat_source"].pop("exit_temperature_max")
-    T_source_out_min = parameters["heat_source"].pop("exit_temperature_min")
-    T_sink_out_max = parameters["heat_sink"].pop("exit_temperature_max")
-    T_sink_out_min = parameters["heat_sink"].pop("exit_temperature_min")
-    p_source_out = parameters["heat_source"].pop("exit_pressure")
-    p_sink_out = parameters["heat_sink"].pop("exit_pressure")
-
-    # Compute coldest state at the heat source exit
-    source_out_min = heating_fluid.set_state(
-        props.PT_INPUTS, p_source_out, T_source_out_min
-    )
-
-    # Extract pressure drops and give short names
-    dp_heater_h = parameters["heater"].pop("pressure_drop_hot_side")
-    dp_heater_c = parameters["heater"].pop("pressure_drop_cold_side")
-    dp_recup_h = parameters["recuperator"].pop("pressure_drop_hot_side")
-    dp_recup_c = parameters["recuperator"].pop("pressure_drop_cold_side")
-    dp_cooler_h = parameters["cooler"].pop("pressure_drop_hot_side")
-    dp_cooler_c = parameters["cooler"].pop("pressure_drop_cold_side")
-
-    # Extract design variables from dictionary (make sure all are used)
-    turbine_inlet_p = variables.pop("turbine_inlet_pressure")
-    turbine_inlet_h = variables.pop("turbine_inlet_enthalpy")
-    compressor_inlet_p = variables.pop("compressor_inlet_pressure")
-    compressor_inlet_h = variables.pop("compressor_inlet_enthalpy")
-    recuperator_effectiveness = variables.pop("recuperator_effectiveness")
-    heat_source_temperature_out = variables.pop("heat_source_exit_temperature")
-    heat_sink_temperature_out = variables.pop("heat_sink_exit_temperature")
-
-    # Evaluate  compressor
-    dp = (1.0 - dp_heater_c) * (1.0 - dp_recup_c)
-    compressor_outlet_p = turbine_inlet_p / dp
-    compressor_eff = parameters["compressor"].pop("efficiency")
-    compressor_eff_type = parameters["compressor"].pop("efficiency_type")
-    compressor = compression_process(
-        working_fluid,
-        compressor_inlet_h,
-        compressor_inlet_p,
-        compressor_outlet_p,
-        compressor_eff,
-        compressor_eff_type,
-    )
-
-    # Evaluate turbine
-    dp = (1.0 - dp_cooler_h) * (1.0 - dp_recup_h)
-    turbine_outlet_p = compressor_inlet_p / dp
-    turbine_efficiency = parameters["turbine"].pop("efficiency")
-    turbine_efficiency_type = parameters["turbine"].pop("efficiency_type")
-    turbine = expansion_process(
-        working_fluid,
-        turbine_inlet_h,
-        turbine_inlet_p,
-        turbine_outlet_p,
-        turbine_efficiency,
-        turbine_efficiency_type,
-    )
-
-    # Evaluate recuperator
-    eps = recuperator_effectiveness
-    T_in_hot = turbine["state_out"].T
-    p_in_cold = compressor["state_out"].p
-    h_in_cold = compressor["state_out"].h
-    p_out_cold = p_in_cold * (1.0 - dp_recup_c)
-    h_out_cold_ideal = working_fluid.set_state(props.PT_INPUTS, p_out_cold, T_in_hot).h
-    h_out_cold_ideal = working_fluid.set_state(props.PT_INPUTS, p_out_cold, T_in_hot).h
-    h_out_cold_actual = h_in_cold + eps * (h_out_cold_ideal - h_in_cold)
-    p_in_hot = turbine["state_out"].p
-    h_in_hot = turbine["state_out"].h
-    p_out_hot = p_in_hot * (1.0 - dp_recup_h)
-    h_out_hot = h_in_hot - (h_out_cold_actual - h_in_cold)
-    num_elements = parameters["recuperator"].pop("num_elements")
-    recuperator = heat_exchanger(
-        working_fluid,
-        h_in_hot,
-        h_out_hot,
-        p_in_hot,
-        p_out_hot,
-        working_fluid,
-        h_in_cold,
-        h_out_cold_actual,
-        p_in_cold,
-        p_out_cold,
-        counter_current=True,
-        num_steps=num_elements,
-    )
-
-    # Evaluate heater
-    h_in_cold = recuperator["cold_side"]["state_out"].h
-    p_in_cold = recuperator["cold_side"]["state_out"].p
-    h_out_cold = turbine["state_in"].h
-    p_out_cold = turbine["state_in"].p
-    T_in_hot = parameters["heat_source"].pop("inlet_temperature")
-    p_in_hot = parameters["heat_source"].pop("inlet_pressure")
-    h_in_hot = heating_fluid.set_state(props.PT_INPUTS, p_in_hot, T_in_hot).h
-    T_out_hot = heat_source_temperature_out
-    p_out_hot = p_in_hot * (1 - dp_heater_h)
-    h_out_hot = heating_fluid.set_state(props.PT_INPUTS, p_out_hot, T_out_hot).h
-    num_elements = parameters["heater"].pop("num_elements")
-    heater = heat_exchanger(
-        heating_fluid,
-        h_in_hot,
-        h_out_hot,
-        p_in_hot,
-        p_out_hot,
-        working_fluid,
-        h_in_cold,
-        h_out_cold,
-        p_in_cold,
-        p_out_cold,
-        counter_current=True,
-        num_steps=num_elements,
-    )
-
-    # Evaluate heat source pump
-    h_in = heater["hot_side"]["state_out"].h
-    p_in = heater["hot_side"]["state_out"].p
-    p_out = p_source_out
-    efficiency = parameters["heat_source_pump"].pop("efficiency")
-    efficiency_type = parameters["heat_source_pump"].pop("efficiency_type")
-    heat_source_pump = compression_process(
-        heating_fluid,
-        h_in,
-        p_in,
-        p_out,
-        efficiency,
-        efficiency_type,
-    )
-
-    # Evaluate heat sink pump
-    T_in = parameters["heat_sink"].pop("inlet_temperature")
-    p_in = parameters["heat_sink"].pop("inlet_pressure")
-    h_in = cooling_fluid.set_state(props.PT_INPUTS, p_in, T_in).h
-    p_out = p_sink_out / (1 - dp_cooler_c)
-    efficiency = parameters["heat_sink_pump"].pop("efficiency")
-    efficiency_type = parameters["heat_sink_pump"].pop("efficiency_type")
-    heat_sink_pump = compression_process(
-        cooling_fluid,
-        h_in,
-        p_in,
-        p_out,
-        efficiency,
-        efficiency_type,
-    )
-
-    # Evaluate cooler
-    p_in_cold = heat_sink_pump["state_out"].p
-    h_in_cold = heat_sink_pump["state_out"].h
-    p_out_cold = p_in_cold * (1 - dp_cooler_c)
-    T_out_cold = heat_sink_temperature_out
-    h_out_cold = cooling_fluid.set_state(props.PT_INPUTS, p_out_cold, T_out_cold).h
-    h_in_hot = recuperator["hot_side"]["state_out"].h
-    p_in_hot = recuperator["hot_side"]["state_out"].p
-    h_out_hot = compressor["state_in"].h
-    p_out_hot = compressor["state_in"].p
-    num_elements = parameters["cooler"].pop("num_elements")
-    cooler = heat_exchanger(
-        working_fluid,
-        h_in_hot,
-        h_out_hot,
-        p_in_hot,
-        p_out_hot,
-        cooling_fluid,
-        h_in_cold,
-        h_out_cold,
-        p_in_cold,
-        p_out_cold,
-        counter_current=True,
-        num_steps=num_elements,
-    )
-
-    # Compute mass flow rates
-    W_net = parameters.pop("net_power")
-    turbine_work = turbine["specific_work"]
-    compression_work = compressor["specific_work"]
-    m_total = W_net / (turbine_work - compression_work)
-    m_source = m_total * heater["mass_flow_ratio"]
-    m_sink = m_total / cooler["mass_flow_ratio"]
-
-    # Add the mass flow to the components
-    heater["hot_side"]["mass_flow"] = m_source
-    heater["cold_side"]["mass_flow"] = m_total
-    recuperator["hot_side"]["mass_flow"] = m_total
-    recuperator["cold_side"]["mass_flow"] = m_total
-    cooler["hot_side"]["mass_flow"] = m_total
-    cooler["cold_side"]["mass_flow"] = m_sink
-    turbine["mass_flow"] = m_total
-    compressor["mass_flow"] = m_total
-    heat_source_pump["mass_flow"] = m_source
-    heat_sink_pump["mass_flow"] = m_sink
-
-    # Summary of components
-    components = {
-        "turbine": turbine,
-        "compressor": compressor,
-        "recuperator": recuperator,
-        "heater": heater,
-        "cooler": cooler,
-        "heat_source_pump": heat_source_pump,
-        "heat_sink_pump": heat_sink_pump,
-    }
-
-    # Compute energy balances
-    for name, component in components.items():
-        if component["type"] == "heat_exchanger":
-            hot = component["hot_side"]
-            cold = component["cold_side"]
-            Q1 = hot["mass_flow"] * (hot["state_in"].h - hot["state_out"].h)
-            Q2 = cold["mass_flow"] * (cold["state_out"].h - cold["state_in"].h)
-            component["heat_flow_rate"] = Q1
-            component["heat_flow_rate_"] = Q2
-            component["heat_balance"] = Q1 - Q2
-            component["power"] = 0.0
-
-        if component["type"] in ["compressor", "turbine"]:
-            component["power"] = component["mass_flow"] * component["specific_work"]
-            component["heat_flow_rate"] = 0.00
-
-    # First-law analysis
-    Q_in = heater["heat_flow_rate"]
-    Q_out = cooler["heat_flow_rate"]
-    W_out = turbine["power"]
-    W_comp = compressor["power"]
-    W_aux = heat_source_pump["power"] + heat_sink_pump["power"]
-    W_in = W_comp + W_aux
-    Q_in_max = m_source * (heater["hot_side"]["state_in"].h - source_out_min.h)
-    cycle_efficiency = (W_out - W_in) / Q_in
-    system_efficiency = (W_out - W_in) / Q_in_max
-    backwork_ratio = W_comp / W_out
-    energy_balance = (Q_in + W_comp) - (W_out + Q_out)  # Ignore pumps
-
-    # Define dictionary with 1st Law analysis
-    energy_analysis = {
-        "heater_heat_flow": Q_in,
-        "heater_heat_flow_max": Q_in_max,
-        "recuperator_heat_flow": recuperator["heat_flow_rate"],
-        "cooler_heat_flow": Q_out,
-        "turbine_power": W_out,
-        "compressor_power": W_comp,
-        "heat_source_pump_power": heat_source_pump["power"],
-        "heat_sink_pump_power": heat_sink_pump["power"],
-        "net_cycle_power": W_net,
-        "net_system_power": W_out - W_in,
-        "mass_flow_heating_fluid": m_source,
-        "mass_flow_working_fluid": m_total,
-        "mass_flow_cooling_fluid": m_sink,
-        "cycle_efficiency": cycle_efficiency,
-        "system_efficiency": system_efficiency,
-        "backwork_ratio": backwork_ratio,
-        "energy_balance": energy_balance,
-    }
-
-    # Evaluate objective function and constraints
-    output = {"components": components, "energy_analysis": energy_analysis}
-    f = utilities.evaluate_objective_function(output, objective_function)
-    c_eq, c_ineq = utilities.evaluate_constraints(output, constraints)
-    # f = None
-    # c_eq = None
-    # c_ineq = None
-
-    # Set colors for plotting
-    heater["hot_side"]["color"] = COLORS_MATLAB[6]
-    heater["cold_side"]["color"] = COLORS_MATLAB[1]
-    recuperator["hot_side"]["color"] = COLORS_MATLAB[1]
-    recuperator["cold_side"]["color"] = COLORS_MATLAB[1]
-    cooler["hot_side"]["color"] = COLORS_MATLAB[1]
-    cooler["cold_side"]["color"] = COLORS_MATLAB[0]
-    turbine["color"] = COLORS_MATLAB[1]
-    compressor["color"] = COLORS_MATLAB[1]
-
-    # Check if any fixed parameter or design variable was not used
-    utilities.check_for_unused_keys(parameters, "parameters", raise_error=True)
-    utilities.check_for_unused_keys(variables, "variables", raise_error=True)
-
-    # # Summary of components
-    # components = {
-    #     "turbine": turbine,
-    #     "compressor": compressor,
-    #     # "recuperator": recuperator,
-    #     # "heater": heater,
-    #     # "cooler": cooler,
-    #     # "heat_source_pump": heat_source_pump,
-    #     # "heat_sink_pump": heat_sink_pump,
-    # }
-
-    # Cycle performance summary
-    output = {
-        **output,
-        "working_fluid": working_fluid,
-        "heating_fluid": heating_fluid,
-        "cooling_fluid": cooling_fluid,
-        "components": components,
-        "objective_function": f,
-        "equality_constraints": c_eq,
-        "inequality_constraints": c_ineq,
-    }
-
-    return output
+import copy
+from .. import utilities
+from .. import fluid_properties as props
+
+from .components import compression_process, expansion_process, heat_exchanger
+
+COLORS_MATLAB = utilities.COLORS_MATLAB
+
+
+def evaluate_cycle(
+    variables,
+    parameters,
+    constraints,
+    objective_function,
+):
+    # Create copies to not change the originals
+    variables = copy.deepcopy(variables)
+    parameters = copy.deepcopy(parameters)
+
+    # Initialize fluid objects
+    working_fluid = props.Fluid(
+        **parameters.pop("working_fluid"), identifier="working_fluid"
+    )
+    heating_fluid = props.Fluid(
+        **parameters.pop("heating_fluid"), identifier="heating_fluid"
+    )
+    cooling_fluid = props.Fluid(
+        **parameters.pop("cooling_fluid"), identifier="cooling_fluid"
+    )
+
+    # Extract heat source/sink parameters and give short names
+    T_source_out_max = parameters["heat_source"].pop("exit_temperature_max")
+    T_source_out_min = parameters["heat_source"].pop("exit_temperature_min")
+    T_sink_out_max = parameters["heat_sink"].pop("exit_temperature_max")
+    T_sink_out_min = parameters["heat_sink"].pop("exit_temperature_min")
+    p_source_out = parameters["heat_source"].pop("exit_pressure")
+    p_sink_out = parameters["heat_sink"].pop("exit_pressure")
+
+    # Compute coldest state at the heat source exit
+    source_out_min = heating_fluid.set_state(
+        props.PT_INPUTS, p_source_out, T_source_out_min
+    )
+
+    # Extract pressure drops and give short names
+    dp_heater_h = parameters["heater"].pop("pressure_drop_hot_side")
+    dp_heater_c = parameters["heater"].pop("pressure_drop_cold_side")
+    dp_recup_h = parameters["recuperator"].pop("pressure_drop_hot_side")
+    dp_recup_c = parameters["recuperator"].pop("pressure_drop_cold_side")
+    dp_cooler_h = parameters["cooler"].pop("pressure_drop_hot_side")
+    dp_cooler_c = parameters["cooler"].pop("pressure_drop_cold_side")
+
+    # Extract design variables from dictionary (make sure all are used)
+    turbine_inlet_p = variables.pop("turbine_inlet_pressure")
+    turbine_inlet_h = variables.pop("turbine_inlet_enthalpy")
+    compressor_inlet_p = variables.pop("compressor_inlet_pressure")
+    compressor_inlet_h = variables.pop("compressor_inlet_enthalpy")
+    recuperator_effectiveness = variables.pop("recuperator_effectiveness")
+    heat_source_temperature_out = variables.pop("heat_source_exit_temperature")
+    heat_sink_temperature_out = variables.pop("heat_sink_exit_temperature")
+
+    # Evaluate  compressor
+    dp = (1.0 - dp_heater_c) * (1.0 - dp_recup_c)
+    compressor_outlet_p = turbine_inlet_p / dp
+    compressor_eff = parameters["compressor"].pop("efficiency")
+    compressor_eff_type = parameters["compressor"].pop("efficiency_type")
+    compressor = compression_process(
+        working_fluid,
+        compressor_inlet_h,
+        compressor_inlet_p,
+        compressor_outlet_p,
+        compressor_eff,
+        compressor_eff_type,
+    )
+
+    # Evaluate turbine
+    dp = (1.0 - dp_cooler_h) * (1.0 - dp_recup_h)
+    turbine_outlet_p = compressor_inlet_p / dp
+    turbine_efficiency = parameters["turbine"].pop("efficiency")
+    turbine_efficiency_type = parameters["turbine"].pop("efficiency_type")
+    turbine = expansion_process(
+        working_fluid,
+        turbine_inlet_h,
+        turbine_inlet_p,
+        turbine_outlet_p,
+        turbine_efficiency,
+        turbine_efficiency_type,
+    )
+
+    # Evaluate recuperator
+    eps = recuperator_effectiveness
+    T_in_hot = turbine["state_out"].T
+    p_in_cold = compressor["state_out"].p
+    h_in_cold = compressor["state_out"].h
+    p_out_cold = p_in_cold * (1.0 - dp_recup_c)
+    h_out_cold_ideal = working_fluid.set_state(props.PT_INPUTS, p_out_cold, T_in_hot).h
+    h_out_cold_ideal = working_fluid.set_state(props.PT_INPUTS, p_out_cold, T_in_hot).h
+    h_out_cold_actual = h_in_cold + eps * (h_out_cold_ideal - h_in_cold)
+    p_in_hot = turbine["state_out"].p
+    h_in_hot = turbine["state_out"].h
+    p_out_hot = p_in_hot * (1.0 - dp_recup_h)
+    h_out_hot = h_in_hot - (h_out_cold_actual - h_in_cold)
+    num_elements = parameters["recuperator"].pop("num_elements")
+    recuperator = heat_exchanger(
+        working_fluid,
+        h_in_hot,
+        h_out_hot,
+        p_in_hot,
+        p_out_hot,
+        working_fluid,
+        h_in_cold,
+        h_out_cold_actual,
+        p_in_cold,
+        p_out_cold,
+        counter_current=True,
+        num_steps=num_elements,
+    )
+
+    # Evaluate heater
+    h_in_cold = recuperator["cold_side"]["state_out"].h
+    p_in_cold = recuperator["cold_side"]["state_out"].p
+    h_out_cold = turbine["state_in"].h
+    p_out_cold = turbine["state_in"].p
+    T_in_hot = parameters["heat_source"].pop("inlet_temperature")
+    p_in_hot = parameters["heat_source"].pop("inlet_pressure")
+    h_in_hot = heating_fluid.set_state(props.PT_INPUTS, p_in_hot, T_in_hot).h
+    T_out_hot = heat_source_temperature_out
+    p_out_hot = p_in_hot * (1 - dp_heater_h)
+    h_out_hot = heating_fluid.set_state(props.PT_INPUTS, p_out_hot, T_out_hot).h
+    num_elements = parameters["heater"].pop("num_elements")
+    heater = heat_exchanger(
+        heating_fluid,
+        h_in_hot,
+        h_out_hot,
+        p_in_hot,
+        p_out_hot,
+        working_fluid,
+        h_in_cold,
+        h_out_cold,
+        p_in_cold,
+        p_out_cold,
+        counter_current=True,
+        num_steps=num_elements,
+    )
+
+    # Evaluate heat source pump
+    h_in = heater["hot_side"]["state_out"].h
+    p_in = heater["hot_side"]["state_out"].p
+    p_out = p_source_out
+    efficiency = parameters["heat_source_pump"].pop("efficiency")
+    efficiency_type = parameters["heat_source_pump"].pop("efficiency_type")
+    heat_source_pump = compression_process(
+        heating_fluid,
+        h_in,
+        p_in,
+        p_out,
+        efficiency,
+        efficiency_type,
+    )
+
+    # Evaluate heat sink pump
+    T_in = parameters["heat_sink"].pop("inlet_temperature")
+    p_in = parameters["heat_sink"].pop("inlet_pressure")
+    h_in = cooling_fluid.set_state(props.PT_INPUTS, p_in, T_in).h
+    p_out = p_sink_out / (1 - dp_cooler_c)
+    efficiency = parameters["heat_sink_pump"].pop("efficiency")
+    efficiency_type = parameters["heat_sink_pump"].pop("efficiency_type")
+    heat_sink_pump = compression_process(
+        cooling_fluid,
+        h_in,
+        p_in,
+        p_out,
+        efficiency,
+        efficiency_type,
+    )
+
+    # Evaluate cooler
+    p_in_cold = heat_sink_pump["state_out"].p
+    h_in_cold = heat_sink_pump["state_out"].h
+    p_out_cold = p_in_cold * (1 - dp_cooler_c)
+    T_out_cold = heat_sink_temperature_out
+    h_out_cold = cooling_fluid.set_state(props.PT_INPUTS, p_out_cold, T_out_cold).h
+    h_in_hot = recuperator["hot_side"]["state_out"].h
+    p_in_hot = recuperator["hot_side"]["state_out"].p
+    h_out_hot = compressor["state_in"].h
+    p_out_hot = compressor["state_in"].p
+    num_elements = parameters["cooler"].pop("num_elements")
+    cooler = heat_exchanger(
+        working_fluid,
+        h_in_hot,
+        h_out_hot,
+        p_in_hot,
+        p_out_hot,
+        cooling_fluid,
+        h_in_cold,
+        h_out_cold,
+        p_in_cold,
+        p_out_cold,
+        counter_current=True,
+        num_steps=num_elements,
+    )
+
+    # Compute mass flow rates
+    W_net = parameters.pop("net_power")
+    turbine_work = turbine["specific_work"]
+    compression_work = compressor["specific_work"]
+    m_total = W_net / (turbine_work - compression_work)
+    m_source = m_total * heater["mass_flow_ratio"]
+    m_sink = m_total / cooler["mass_flow_ratio"]
+
+    # Add the mass flow to the components
+    heater["hot_side"]["mass_flow"] = m_source
+    heater["cold_side"]["mass_flow"] = m_total
+    recuperator["hot_side"]["mass_flow"] = m_total
+    recuperator["cold_side"]["mass_flow"] = m_total
+    cooler["hot_side"]["mass_flow"] = m_total
+    cooler["cold_side"]["mass_flow"] = m_sink
+    turbine["mass_flow"] = m_total
+    compressor["mass_flow"] = m_total
+    heat_source_pump["mass_flow"] = m_source
+    heat_sink_pump["mass_flow"] = m_sink
+
+    # Summary of components
+    components = {
+        "turbine": turbine,
+        "compressor": compressor,
+        "recuperator": recuperator,
+        "heater": heater,
+        "cooler": cooler,
+        "heat_source_pump": heat_source_pump,
+        "heat_sink_pump": heat_sink_pump,
+    }
+
+    # Compute energy balances
+    for name, component in components.items():
+        if component["type"] == "heat_exchanger":
+            hot = component["hot_side"]
+            cold = component["cold_side"]
+            Q1 = hot["mass_flow"] * (hot["state_in"].h - hot["state_out"].h)
+            Q2 = cold["mass_flow"] * (cold["state_out"].h - cold["state_in"].h)
+            component["heat_flow_rate"] = Q1
+            component["heat_flow_rate_"] = Q2
+            component["heat_balance"] = Q1 - Q2
+            component["power"] = 0.0
+
+        if component["type"] in ["compressor", "turbine"]:
+            component["power"] = component["mass_flow"] * component["specific_work"]
+            component["heat_flow_rate"] = 0.00
+
+    # First-law analysis
+    Q_in = heater["heat_flow_rate"]
+    Q_out = cooler["heat_flow_rate"]
+    W_out = turbine["power"]
+    W_comp = compressor["power"]
+    W_aux = heat_source_pump["power"] + heat_sink_pump["power"]
+    W_in = W_comp + W_aux
+    Q_in_max = m_source * (heater["hot_side"]["state_in"].h - source_out_min.h)
+    cycle_efficiency = (W_out - W_in) / Q_in
+    system_efficiency = (W_out - W_in) / Q_in_max
+    backwork_ratio = W_comp / W_out
+    energy_balance = (Q_in + W_comp) - (W_out + Q_out)  # Ignore pumps
+
+    # Define dictionary with 1st Law analysis
+    energy_analysis = {
+        "heater_heat_flow": Q_in,
+        "heater_heat_flow_max": Q_in_max,
+        "recuperator_heat_flow": recuperator["heat_flow_rate"],
+        "cooler_heat_flow": Q_out,
+        "turbine_power": W_out,
+        "compressor_power": W_comp,
+        "heat_source_pump_power": heat_source_pump["power"],
+        "heat_sink_pump_power": heat_sink_pump["power"],
+        "net_cycle_power": W_net,
+        "net_system_power": W_out - W_in,
+        "mass_flow_heating_fluid": m_source,
+        "mass_flow_working_fluid": m_total,
+        "mass_flow_cooling_fluid": m_sink,
+        "cycle_efficiency": cycle_efficiency,
+        "system_efficiency": system_efficiency,
+        "backwork_ratio": backwork_ratio,
+        "energy_balance": energy_balance,
+    }
+
+    # Evaluate objective function and constraints
+    output = {"components": components, "energy_analysis": energy_analysis}
+    f = utilities.evaluate_objective_function(output, objective_function)
+    c_eq, c_ineq = utilities.evaluate_constraints(output, constraints)
+    # f = None
+    # c_eq = None
+    # c_ineq = None
+
+    # Set colors for plotting
+    heater["hot_side"]["color"] = COLORS_MATLAB[6]
+    heater["cold_side"]["color"] = COLORS_MATLAB[1]
+    recuperator["hot_side"]["color"] = COLORS_MATLAB[1]
+    recuperator["cold_side"]["color"] = COLORS_MATLAB[1]
+    cooler["hot_side"]["color"] = COLORS_MATLAB[1]
+    cooler["cold_side"]["color"] = COLORS_MATLAB[0]
+    turbine["color"] = COLORS_MATLAB[1]
+    compressor["color"] = COLORS_MATLAB[1]
+
+    # Check if any fixed parameter or design variable was not used
+    utilities.check_for_unused_keys(parameters, "parameters", raise_error=True)
+    utilities.check_for_unused_keys(variables, "variables", raise_error=True)
+
+    # # Summary of components
+    # components = {
+    #     "turbine": turbine,
+    #     "compressor": compressor,
+    #     # "recuperator": recuperator,
+    #     # "heater": heater,
+    #     # "cooler": cooler,
+    #     # "heat_source_pump": heat_source_pump,
+    #     # "heat_sink_pump": heat_sink_pump,
+    # }
+
+    # Cycle performance summary
+    output = {
+        **output,
+        "working_fluid": working_fluid,
+        "heating_fluid": heating_fluid,
+        "cooling_fluid": cooling_fluid,
+        "components": components,
+        "objective_function": f,
+        "equality_constraints": c_eq,
+        "inequality_constraints": c_ineq,
+    }
+
+    return output
```

### Comparing `turboflow-0.1.2/turboflow/thermodynamic_cycles/brayton_split_compression.py` & `turboflow-0.1.3/turboflow/thermodynamic_cycles/brayton_split_compression.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,390 +1,390 @@
-import copy
-from .. import utilities
-from .. import fluid_properties as props
-
-from .components import compression_process, expansion_process, heat_exchanger
-
-COLORS_MATLAB = utilities.COLORS_MATLAB
-
-
-def evaluate_cycle(
-    variables,
-    parameters,
-    constraints,
-    objective_function,
-):
-    # Create copies to not change the originals
-    variables = copy.deepcopy(variables)
-    parameters = copy.deepcopy(parameters)
-
-    # Initialize fluid objects
-    working_fluid = props.Fluid(
-        **parameters.pop("working_fluid"), identifier="working_fluid"
-    )
-    heating_fluid = props.Fluid(
-        **parameters.pop("heating_fluid"), identifier="heating_fluid"
-    )
-    cooling_fluid = props.Fluid(
-        **parameters.pop("cooling_fluid"), identifier="cooling_fluid"
-    )
-
-    # Extract heat source/sink parameters and give short names
-    T_source_out_max = parameters["heat_source"].pop("exit_temperature_max")
-    T_source_out_min = parameters["heat_source"].pop("exit_temperature_min")
-    T_sink_out_max = parameters["heat_sink"].pop("exit_temperature_max")
-    T_sink_out_min = parameters["heat_sink"].pop("exit_temperature_min")
-    p_source_out = parameters["heat_source"].pop("exit_pressure")
-    p_sink_out = parameters["heat_sink"].pop("exit_pressure")
-
-    # Compute coldest state at the heat source exit
-    source_out_min = heating_fluid.set_state(
-        props.PT_INPUTS, p_source_out, T_source_out_min
-    )
-
-    # Extract pressure drops and give short names
-    dp_heater_h = parameters["heater"].pop("pressure_drop_hot_side")
-    dp_heater_c = parameters["heater"].pop("pressure_drop_cold_side")
-    dp_recup_lowT_h = parameters["recuperator_lowT"].pop("pressure_drop_hot_side")
-    dp_recup_lowT_c = parameters["recuperator_lowT"].pop("pressure_drop_cold_side")
-    dp_recup_highT_h = parameters["recuperator_highT"].pop("pressure_drop_hot_side")
-    dp_recup_highT_c = parameters["recuperator_highT"].pop("pressure_drop_cold_side")
-    dp_cooler_h = parameters["cooler"].pop("pressure_drop_hot_side")
-    dp_cooler_c = parameters["cooler"].pop("pressure_drop_cold_side")
-
-    # Extract design variables from dictionary (make sure all are used)
-    turbine_inlet_p = variables.pop("turbine_inlet_pressure")
-    turbine_inlet_h = variables.pop("turbine_inlet_enthalpy")
-    main_compressor_inlet_p = variables.pop("main_compressor_inlet_pressure")
-    main_compressor_inlet_h = variables.pop("main_compressor_inlet_enthalpy")
-    split_compressor_inlet_h = variables.pop("split_compressor_inlet_enthalpy")
-    recup_intermediate_h = variables.pop("recuperator_intermediate_enthalpy")
-    mass_split_fraction = variables.pop("mass_split_fraction")
-    heat_source_temperature_out = variables.pop("heat_source_exit_temperature")
-    heat_sink_temperature_out = variables.pop("heat_sink_exit_temperature")
-
-    # Evaluate main compressor
-    dp = (1.0 - dp_heater_c) * (1.0 - dp_recup_highT_c) * (1.0 - dp_recup_lowT_c)
-    main_compressor_outlet_p = turbine_inlet_p / dp
-    main_compressor_eff = parameters["main_compressor"].pop("efficiency")
-    main_compressor_eff_type = parameters["main_compressor"].pop("efficiency_type")
-    main_compressor = compression_process(
-        working_fluid,
-        main_compressor_inlet_h,
-        main_compressor_inlet_p,
-        main_compressor_outlet_p,
-        main_compressor_eff,
-        efficiency_type=main_compressor_eff_type,
-    )
-
-    # Evaluate re-compressor
-    split_compressor_inlet_p = main_compressor_inlet_p / (1 - dp_cooler_h)
-    dp = (1.0 - dp_heater_c) * (1.0 - dp_recup_highT_c)
-    split_compressor_outlet_p = turbine_inlet_p / dp
-    split_compressor_eff = parameters["split_compressor"].pop("efficiency")
-    split_compressor_eff_type = parameters["split_compressor"].pop("efficiency_type")
-    split_compressor = compression_process(
-        working_fluid,
-        split_compressor_inlet_h,
-        split_compressor_inlet_p,
-        split_compressor_outlet_p,
-        split_compressor_eff,
-        split_compressor_eff_type,
-    )
-
-    # Evaluate turbine
-    dp = (1.0 - dp_cooler_h) * (1.0 - dp_recup_lowT_h) * (1.0 - dp_recup_highT_h)
-    turbine_outlet_p = main_compressor_inlet_p / dp
-    turbine_efficiency = parameters["turbine"].pop("efficiency")
-    turbine_efficiency_type = parameters["turbine"].pop("efficiency_type")
-    turbine = expansion_process(
-        working_fluid,
-        turbine_inlet_h,
-        turbine_inlet_p,
-        turbine_outlet_p,
-        turbine_efficiency,
-        turbine_efficiency_type,
-    )
-
-    # Evaluate low temperature recuperator
-    h_in_hot = recup_intermediate_h
-    p_in_hot = turbine_outlet_p * (1 - dp_recup_highT_h)
-    h_out_hot = split_compressor_inlet_h
-    p_out_hot = split_compressor_inlet_p
-    h_in_cold = main_compressor["state_out"].h
-    p_in_cold = main_compressor["state_out"].p
-    h_out_cold = h_in_cold + (1 / (1 - mass_split_fraction)) * (h_in_hot - h_out_hot)
-    p_out_cold = split_compressor_outlet_p
-    num_elements = parameters["recuperator_lowT"].pop("num_elements")
-    recuperator_lowT = heat_exchanger(
-        working_fluid,
-        h_in_hot,
-        h_out_hot,
-        p_in_hot,
-        p_out_hot,
-        working_fluid,
-        h_in_cold,
-        h_out_cold,
-        p_in_cold,
-        p_out_cold,
-        counter_current=True,
-        num_steps=num_elements,
-    )
-
-    # Evaluate high temperature recuperator
-    h_in_cold = (
-        mass_split_fraction * split_compressor["state_out"].h
-        + (1 - mass_split_fraction) * recuperator_lowT["cold_side"]["state_out"].h
-    )
-    h_out_cold = (turbine["state_out"].h - recup_intermediate_h) + h_in_cold
-    p_in_cold = split_compressor["state_out"].p
-    p_out_cold = turbine_inlet_p / (1 - dp_heater_c)
-    h_in_hot = turbine["state_out"].h
-    h_out_hot = recup_intermediate_h
-    p_in_hot = turbine_outlet_p
-    p_out_hot = p_in_hot * (1 - dp_recup_highT_h)
-    num_elements = parameters["recuperator_highT"].pop("num_elements")
-    recuperator_highT = heat_exchanger(
-        working_fluid,
-        h_in_hot,
-        h_out_hot,
-        p_in_hot,
-        p_out_hot,
-        working_fluid,
-        h_in_cold,
-        h_out_cold,
-        p_in_cold,
-        p_out_cold,
-        counter_current=True,
-        num_steps=num_elements,
-    )
-
-    # Evaluate heater
-    h_in_cold = recuperator_highT["cold_side"]["state_out"].h
-    p_in_cold = recuperator_highT["cold_side"]["state_out"].p
-    h_out_cold = turbine["state_in"].h
-    p_out_cold = turbine["state_in"].p
-    T_in_hot = parameters["heat_source"].pop("inlet_temperature")
-    p_in_hot = parameters["heat_source"].pop("inlet_pressure")
-    h_in_hot = heating_fluid.set_state(props.PT_INPUTS, p_in_hot, T_in_hot).h
-    T_out_hot = heat_source_temperature_out
-    p_out_hot = p_in_hot * (1 - dp_heater_h)
-    h_out_hot = heating_fluid.set_state(props.PT_INPUTS, p_out_hot, T_out_hot).h
-    num_elements = parameters["heater"].pop("num_elements")
-    heater = heat_exchanger(
-        heating_fluid,
-        h_in_hot,
-        h_out_hot,
-        p_in_hot,
-        p_out_hot,
-        working_fluid,
-        h_in_cold,
-        h_out_cold,
-        p_in_cold,
-        p_out_cold,
-        counter_current=True,
-        num_steps=num_elements,
-    )
-
-    # Evaluate heat source pump
-    h_in = heater["hot_side"]["state_out"].h
-    p_in = heater["hot_side"]["state_out"].p
-    p_out = p_source_out
-    efficiency = parameters["heat_source_pump"].pop("efficiency")
-    efficiency_type = parameters["heat_source_pump"].pop("efficiency_type")
-    heat_source_pump = compression_process(
-        heating_fluid,
-        h_in,
-        p_in,
-        p_out,
-        efficiency,
-        efficiency_type,
-    )
-
-    # Evaluate heat sink pump
-    T_in = parameters["heat_sink"].pop("inlet_temperature")
-    p_in = parameters["heat_sink"].pop("inlet_pressure")
-    h_in = cooling_fluid.set_state(props.PT_INPUTS, p_in, T_in).h
-    p_out = p_sink_out / (1 - dp_cooler_c)
-    efficiency = parameters["heat_sink_pump"].pop("efficiency")
-    efficiency_type = parameters["heat_sink_pump"].pop("efficiency_type")
-    heat_sink_pump = compression_process(
-        cooling_fluid,
-        h_in,
-        p_in,
-        p_out,
-        efficiency,
-        efficiency_type,
-    )
-
-    # Evaluate cooler
-    p_in_cold = heat_sink_pump["state_out"].p
-    h_in_cold = heat_sink_pump["state_out"].h
-    p_out_cold = p_in_cold * (1 - dp_cooler_c)
-    T_out_cold = heat_sink_temperature_out
-    h_out_cold = cooling_fluid.set_state(props.PT_INPUTS, p_out_cold, T_out_cold).h
-    h_in_hot = recuperator_lowT["hot_side"]["state_out"].h
-    p_in_hot = recuperator_lowT["hot_side"]["state_out"].p
-    h_out_hot = main_compressor["state_in"].h
-    p_out_hot = main_compressor["state_in"].p
-    num_elements = parameters["cooler"].pop("num_elements")
-    cooler = heat_exchanger(
-        working_fluid,
-        h_in_hot,
-        h_out_hot,
-        p_in_hot,
-        p_out_hot,
-        cooling_fluid,
-        h_in_cold,
-        h_out_cold,
-        p_in_cold,
-        p_out_cold,
-        counter_current=True,
-        num_steps=num_elements,
-    )
-
-    # Compute mass flow rates
-    W_net = parameters.pop("net_power")
-    turbine_work = turbine["specific_work"]
-    main_compression_work = main_compressor["specific_work"] * (1 - mass_split_fraction)
-    split_compression_work = split_compressor["specific_work"] * (mass_split_fraction)
-    m_total = W_net / (turbine_work - main_compression_work - split_compression_work)
-    m_main = m_total * (1 - mass_split_fraction)
-    m_split = m_total * mass_split_fraction
-    m_source = m_total * heater["mass_flow_ratio"]
-    m_sink = m_main / cooler["mass_flow_ratio"]
-
-    # Add the mass flow to the components
-    heater["hot_side"]["mass_flow"] = m_source
-    heater["cold_side"]["mass_flow"] = m_total
-    recuperator_lowT["hot_side"]["mass_flow"] = m_total
-    recuperator_lowT["cold_side"]["mass_flow"] = m_main
-    recuperator_highT["hot_side"]["mass_flow"] = m_total
-    recuperator_highT["cold_side"]["mass_flow"] = m_total
-    cooler["hot_side"]["mass_flow"] = m_main
-    cooler["cold_side"]["mass_flow"] = m_sink
-    turbine["mass_flow"] = m_total
-    main_compressor["mass_flow"] = m_main
-    split_compressor["mass_flow"] = m_split
-    heat_source_pump["mass_flow"] = m_source
-    heat_sink_pump["mass_flow"] = m_sink
-
-    # Summary of components
-    components = {
-        "turbine": turbine,
-        "split_compressor": split_compressor,
-        "main_compressor": main_compressor,
-        "recuperator_lowT": recuperator_lowT,
-        "recuperator_highT": recuperator_highT,
-        "heater": heater,
-        "cooler": cooler,
-        "heat_source_pump": heat_source_pump,
-        "heat_sink_pump": heat_sink_pump,
-    }
-
-    # Compute energy balances
-    for name, component in components.items():
-        if component["type"] == "heat_exchanger":
-            hot = component["hot_side"]
-            cold = component["cold_side"]
-            Q1 = hot["mass_flow"] * (hot["state_in"].h - hot["state_out"].h)
-            Q2 = cold["mass_flow"] * (cold["state_out"].h - cold["state_in"].h)
-            component["heat_flow_rate"] = Q1
-            component["heat_flow_rate_"] = Q2
-            component["heat_balance"] = Q1 - Q2
-            component["power"] = 0.0
-
-        if component["type"] in ["compressor", "turbine"]:
-            component["power"] = component["mass_flow"] * component["specific_work"]
-            component["heat_flow_rate"] = 0.00
-
-    # First-law analysis
-    Q_in = heater["heat_flow_rate"]
-    Q_out = cooler["heat_flow_rate"]
-    W_out = turbine["power"]
-    W_comp = main_compressor["power"] + split_compressor["power"]
-    W_aux = heat_source_pump["power"] + heat_sink_pump["power"]
-    W_in = W_comp + W_aux
-    Q_in_max = m_source * (heater["hot_side"]["state_in"].h - source_out_min.h)
-    cycle_efficiency = (W_out - W_in) / Q_in
-    system_efficiency = (W_out - W_in) / Q_in_max
-    backwork_ratio = W_comp / W_out
-    energy_balance = (Q_in + W_in) - (W_out + Q_out)
-
-    # Mixing chamber check
-    # TODO remember to include mixing chamber exergy destruction in Second Law analysis
-    a = m_total * recuperator_highT["cold_side"]["state_in"].h
-    b = m_main * recuperator_lowT["cold_side"]["state_out"].h
-    c = m_split * split_compressor["state_out"].h
-    error = a - b - c
-    if abs(error) > 1e-4:
-        raise ValueError(
-            f"The mixing chamber energy balance does not close (error: {error:0.3}). Check implementation."
-        )
-
-    # Define dictionary with 1st Law analysis
-    energy_analysis = {
-        "heater_heat_flow": heater["heat_flow_rate"],
-        "heater_heat_flow_max": Q_in_max,
-        "cooler_heat_flow": cooler["heat_flow_rate"],
-        "turbine_power": turbine["power"],
-        "main_compressor_power": main_compressor["power"],
-        "split_compressor_power": split_compressor["power"],
-        "heat_source_pump_power": heat_source_pump["power"],
-        "heat_sink_pump_power": heat_sink_pump["power"],
-        "net_cycle_power": W_net,
-        "net_system_power": W_out - W_in,
-        "mass_flow_heat_source": m_source,
-        "mass_flow_heat_sink": m_sink,
-        "mass_flow_total": m_total,
-        "mass_flow_main": m_main,
-        "mass_flow_split": m_split,
-        "split_fraction": mass_split_fraction,
-        "cycle_efficiency": cycle_efficiency,
-        "system_efficiency": system_efficiency,
-        "backwork_ratio": backwork_ratio,
-        "energy_balance": energy_balance,
-    }
-
-    output = {
-        "components": components,
-        "energy_analysis": energy_analysis,
-    }
-
-    # Evaluate objective function and constraints
-    output = {"components": components, "energy_analysis": energy_analysis}
-    f = utilities.evaluate_objective_function(output, objective_function)
-    c_eq, c_ineq = utilities.evaluate_constraints(output, constraints)
-    # f = None
-    # c_eq = None
-    # c_ineq = None
-
-    # Cycle performance summary
-    output = {
-        **output,
-        "working_fluid": working_fluid,
-        "heating_fluid": heating_fluid,
-        "cooling_fluid": cooling_fluid,
-        "components": components,
-        "objective_function": f,
-        "equality_constraints": c_eq,
-        "inequality_constraints": c_ineq,
-    }
-
-    # Set colors for plotting
-    heater["hot_side"]["color"] = COLORS_MATLAB[6]
-    heater["cold_side"]["color"] = COLORS_MATLAB[1]
-    recuperator_lowT["hot_side"]["color"] = COLORS_MATLAB[1]
-    recuperator_lowT["cold_side"]["color"] = COLORS_MATLAB[1]
-    recuperator_highT["hot_side"]["color"] = COLORS_MATLAB[1]
-    recuperator_highT["cold_side"]["color"] = COLORS_MATLAB[1]
-    cooler["hot_side"]["color"] = COLORS_MATLAB[1]
-    cooler["cold_side"]["color"] = COLORS_MATLAB[0]
-    turbine["color"] = COLORS_MATLAB[1]
-    main_compressor["color"] = COLORS_MATLAB[1]
-    split_compressor["color"] = COLORS_MATLAB[1]
-
-    # Check if any fixed parameter or design variable was not used
-    utilities.check_for_unused_keys(parameters, "parameters", raise_error=True)
-    utilities.check_for_unused_keys(variables, "variables", raise_error=True)
-
-    return output
+import copy
+from .. import utilities
+from .. import fluid_properties as props
+
+from .components import compression_process, expansion_process, heat_exchanger
+
+COLORS_MATLAB = utilities.COLORS_MATLAB
+
+
+def evaluate_cycle(
+    variables,
+    parameters,
+    constraints,
+    objective_function,
+):
+    # Create copies to not change the originals
+    variables = copy.deepcopy(variables)
+    parameters = copy.deepcopy(parameters)
+
+    # Initialize fluid objects
+    working_fluid = props.Fluid(
+        **parameters.pop("working_fluid"), identifier="working_fluid"
+    )
+    heating_fluid = props.Fluid(
+        **parameters.pop("heating_fluid"), identifier="heating_fluid"
+    )
+    cooling_fluid = props.Fluid(
+        **parameters.pop("cooling_fluid"), identifier="cooling_fluid"
+    )
+
+    # Extract heat source/sink parameters and give short names
+    T_source_out_max = parameters["heat_source"].pop("exit_temperature_max")
+    T_source_out_min = parameters["heat_source"].pop("exit_temperature_min")
+    T_sink_out_max = parameters["heat_sink"].pop("exit_temperature_max")
+    T_sink_out_min = parameters["heat_sink"].pop("exit_temperature_min")
+    p_source_out = parameters["heat_source"].pop("exit_pressure")
+    p_sink_out = parameters["heat_sink"].pop("exit_pressure")
+
+    # Compute coldest state at the heat source exit
+    source_out_min = heating_fluid.set_state(
+        props.PT_INPUTS, p_source_out, T_source_out_min
+    )
+
+    # Extract pressure drops and give short names
+    dp_heater_h = parameters["heater"].pop("pressure_drop_hot_side")
+    dp_heater_c = parameters["heater"].pop("pressure_drop_cold_side")
+    dp_recup_lowT_h = parameters["recuperator_lowT"].pop("pressure_drop_hot_side")
+    dp_recup_lowT_c = parameters["recuperator_lowT"].pop("pressure_drop_cold_side")
+    dp_recup_highT_h = parameters["recuperator_highT"].pop("pressure_drop_hot_side")
+    dp_recup_highT_c = parameters["recuperator_highT"].pop("pressure_drop_cold_side")
+    dp_cooler_h = parameters["cooler"].pop("pressure_drop_hot_side")
+    dp_cooler_c = parameters["cooler"].pop("pressure_drop_cold_side")
+
+    # Extract design variables from dictionary (make sure all are used)
+    turbine_inlet_p = variables.pop("turbine_inlet_pressure")
+    turbine_inlet_h = variables.pop("turbine_inlet_enthalpy")
+    main_compressor_inlet_p = variables.pop("main_compressor_inlet_pressure")
+    main_compressor_inlet_h = variables.pop("main_compressor_inlet_enthalpy")
+    split_compressor_inlet_h = variables.pop("split_compressor_inlet_enthalpy")
+    recup_intermediate_h = variables.pop("recuperator_intermediate_enthalpy")
+    mass_split_fraction = variables.pop("mass_split_fraction")
+    heat_source_temperature_out = variables.pop("heat_source_exit_temperature")
+    heat_sink_temperature_out = variables.pop("heat_sink_exit_temperature")
+
+    # Evaluate main compressor
+    dp = (1.0 - dp_heater_c) * (1.0 - dp_recup_highT_c) * (1.0 - dp_recup_lowT_c)
+    main_compressor_outlet_p = turbine_inlet_p / dp
+    main_compressor_eff = parameters["main_compressor"].pop("efficiency")
+    main_compressor_eff_type = parameters["main_compressor"].pop("efficiency_type")
+    main_compressor = compression_process(
+        working_fluid,
+        main_compressor_inlet_h,
+        main_compressor_inlet_p,
+        main_compressor_outlet_p,
+        main_compressor_eff,
+        efficiency_type=main_compressor_eff_type,
+    )
+
+    # Evaluate re-compressor
+    split_compressor_inlet_p = main_compressor_inlet_p / (1 - dp_cooler_h)
+    dp = (1.0 - dp_heater_c) * (1.0 - dp_recup_highT_c)
+    split_compressor_outlet_p = turbine_inlet_p / dp
+    split_compressor_eff = parameters["split_compressor"].pop("efficiency")
+    split_compressor_eff_type = parameters["split_compressor"].pop("efficiency_type")
+    split_compressor = compression_process(
+        working_fluid,
+        split_compressor_inlet_h,
+        split_compressor_inlet_p,
+        split_compressor_outlet_p,
+        split_compressor_eff,
+        split_compressor_eff_type,
+    )
+
+    # Evaluate turbine
+    dp = (1.0 - dp_cooler_h) * (1.0 - dp_recup_lowT_h) * (1.0 - dp_recup_highT_h)
+    turbine_outlet_p = main_compressor_inlet_p / dp
+    turbine_efficiency = parameters["turbine"].pop("efficiency")
+    turbine_efficiency_type = parameters["turbine"].pop("efficiency_type")
+    turbine = expansion_process(
+        working_fluid,
+        turbine_inlet_h,
+        turbine_inlet_p,
+        turbine_outlet_p,
+        turbine_efficiency,
+        turbine_efficiency_type,
+    )
+
+    # Evaluate low temperature recuperator
+    h_in_hot = recup_intermediate_h
+    p_in_hot = turbine_outlet_p * (1 - dp_recup_highT_h)
+    h_out_hot = split_compressor_inlet_h
+    p_out_hot = split_compressor_inlet_p
+    h_in_cold = main_compressor["state_out"].h
+    p_in_cold = main_compressor["state_out"].p
+    h_out_cold = h_in_cold + (1 / (1 - mass_split_fraction)) * (h_in_hot - h_out_hot)
+    p_out_cold = split_compressor_outlet_p
+    num_elements = parameters["recuperator_lowT"].pop("num_elements")
+    recuperator_lowT = heat_exchanger(
+        working_fluid,
+        h_in_hot,
+        h_out_hot,
+        p_in_hot,
+        p_out_hot,
+        working_fluid,
+        h_in_cold,
+        h_out_cold,
+        p_in_cold,
+        p_out_cold,
+        counter_current=True,
+        num_steps=num_elements,
+    )
+
+    # Evaluate high temperature recuperator
+    h_in_cold = (
+        mass_split_fraction * split_compressor["state_out"].h
+        + (1 - mass_split_fraction) * recuperator_lowT["cold_side"]["state_out"].h
+    )
+    h_out_cold = (turbine["state_out"].h - recup_intermediate_h) + h_in_cold
+    p_in_cold = split_compressor["state_out"].p
+    p_out_cold = turbine_inlet_p / (1 - dp_heater_c)
+    h_in_hot = turbine["state_out"].h
+    h_out_hot = recup_intermediate_h
+    p_in_hot = turbine_outlet_p
+    p_out_hot = p_in_hot * (1 - dp_recup_highT_h)
+    num_elements = parameters["recuperator_highT"].pop("num_elements")
+    recuperator_highT = heat_exchanger(
+        working_fluid,
+        h_in_hot,
+        h_out_hot,
+        p_in_hot,
+        p_out_hot,
+        working_fluid,
+        h_in_cold,
+        h_out_cold,
+        p_in_cold,
+        p_out_cold,
+        counter_current=True,
+        num_steps=num_elements,
+    )
+
+    # Evaluate heater
+    h_in_cold = recuperator_highT["cold_side"]["state_out"].h
+    p_in_cold = recuperator_highT["cold_side"]["state_out"].p
+    h_out_cold = turbine["state_in"].h
+    p_out_cold = turbine["state_in"].p
+    T_in_hot = parameters["heat_source"].pop("inlet_temperature")
+    p_in_hot = parameters["heat_source"].pop("inlet_pressure")
+    h_in_hot = heating_fluid.set_state(props.PT_INPUTS, p_in_hot, T_in_hot).h
+    T_out_hot = heat_source_temperature_out
+    p_out_hot = p_in_hot * (1 - dp_heater_h)
+    h_out_hot = heating_fluid.set_state(props.PT_INPUTS, p_out_hot, T_out_hot).h
+    num_elements = parameters["heater"].pop("num_elements")
+    heater = heat_exchanger(
+        heating_fluid,
+        h_in_hot,
+        h_out_hot,
+        p_in_hot,
+        p_out_hot,
+        working_fluid,
+        h_in_cold,
+        h_out_cold,
+        p_in_cold,
+        p_out_cold,
+        counter_current=True,
+        num_steps=num_elements,
+    )
+
+    # Evaluate heat source pump
+    h_in = heater["hot_side"]["state_out"].h
+    p_in = heater["hot_side"]["state_out"].p
+    p_out = p_source_out
+    efficiency = parameters["heat_source_pump"].pop("efficiency")
+    efficiency_type = parameters["heat_source_pump"].pop("efficiency_type")
+    heat_source_pump = compression_process(
+        heating_fluid,
+        h_in,
+        p_in,
+        p_out,
+        efficiency,
+        efficiency_type,
+    )
+
+    # Evaluate heat sink pump
+    T_in = parameters["heat_sink"].pop("inlet_temperature")
+    p_in = parameters["heat_sink"].pop("inlet_pressure")
+    h_in = cooling_fluid.set_state(props.PT_INPUTS, p_in, T_in).h
+    p_out = p_sink_out / (1 - dp_cooler_c)
+    efficiency = parameters["heat_sink_pump"].pop("efficiency")
+    efficiency_type = parameters["heat_sink_pump"].pop("efficiency_type")
+    heat_sink_pump = compression_process(
+        cooling_fluid,
+        h_in,
+        p_in,
+        p_out,
+        efficiency,
+        efficiency_type,
+    )
+
+    # Evaluate cooler
+    p_in_cold = heat_sink_pump["state_out"].p
+    h_in_cold = heat_sink_pump["state_out"].h
+    p_out_cold = p_in_cold * (1 - dp_cooler_c)
+    T_out_cold = heat_sink_temperature_out
+    h_out_cold = cooling_fluid.set_state(props.PT_INPUTS, p_out_cold, T_out_cold).h
+    h_in_hot = recuperator_lowT["hot_side"]["state_out"].h
+    p_in_hot = recuperator_lowT["hot_side"]["state_out"].p
+    h_out_hot = main_compressor["state_in"].h
+    p_out_hot = main_compressor["state_in"].p
+    num_elements = parameters["cooler"].pop("num_elements")
+    cooler = heat_exchanger(
+        working_fluid,
+        h_in_hot,
+        h_out_hot,
+        p_in_hot,
+        p_out_hot,
+        cooling_fluid,
+        h_in_cold,
+        h_out_cold,
+        p_in_cold,
+        p_out_cold,
+        counter_current=True,
+        num_steps=num_elements,
+    )
+
+    # Compute mass flow rates
+    W_net = parameters.pop("net_power")
+    turbine_work = turbine["specific_work"]
+    main_compression_work = main_compressor["specific_work"] * (1 - mass_split_fraction)
+    split_compression_work = split_compressor["specific_work"] * (mass_split_fraction)
+    m_total = W_net / (turbine_work - main_compression_work - split_compression_work)
+    m_main = m_total * (1 - mass_split_fraction)
+    m_split = m_total * mass_split_fraction
+    m_source = m_total * heater["mass_flow_ratio"]
+    m_sink = m_main / cooler["mass_flow_ratio"]
+
+    # Add the mass flow to the components
+    heater["hot_side"]["mass_flow"] = m_source
+    heater["cold_side"]["mass_flow"] = m_total
+    recuperator_lowT["hot_side"]["mass_flow"] = m_total
+    recuperator_lowT["cold_side"]["mass_flow"] = m_main
+    recuperator_highT["hot_side"]["mass_flow"] = m_total
+    recuperator_highT["cold_side"]["mass_flow"] = m_total
+    cooler["hot_side"]["mass_flow"] = m_main
+    cooler["cold_side"]["mass_flow"] = m_sink
+    turbine["mass_flow"] = m_total
+    main_compressor["mass_flow"] = m_main
+    split_compressor["mass_flow"] = m_split
+    heat_source_pump["mass_flow"] = m_source
+    heat_sink_pump["mass_flow"] = m_sink
+
+    # Summary of components
+    components = {
+        "turbine": turbine,
+        "split_compressor": split_compressor,
+        "main_compressor": main_compressor,
+        "recuperator_lowT": recuperator_lowT,
+        "recuperator_highT": recuperator_highT,
+        "heater": heater,
+        "cooler": cooler,
+        "heat_source_pump": heat_source_pump,
+        "heat_sink_pump": heat_sink_pump,
+    }
+
+    # Compute energy balances
+    for name, component in components.items():
+        if component["type"] == "heat_exchanger":
+            hot = component["hot_side"]
+            cold = component["cold_side"]
+            Q1 = hot["mass_flow"] * (hot["state_in"].h - hot["state_out"].h)
+            Q2 = cold["mass_flow"] * (cold["state_out"].h - cold["state_in"].h)
+            component["heat_flow_rate"] = Q1
+            component["heat_flow_rate_"] = Q2
+            component["heat_balance"] = Q1 - Q2
+            component["power"] = 0.0
+
+        if component["type"] in ["compressor", "turbine"]:
+            component["power"] = component["mass_flow"] * component["specific_work"]
+            component["heat_flow_rate"] = 0.00
+
+    # First-law analysis
+    Q_in = heater["heat_flow_rate"]
+    Q_out = cooler["heat_flow_rate"]
+    W_out = turbine["power"]
+    W_comp = main_compressor["power"] + split_compressor["power"]
+    W_aux = heat_source_pump["power"] + heat_sink_pump["power"]
+    W_in = W_comp + W_aux
+    Q_in_max = m_source * (heater["hot_side"]["state_in"].h - source_out_min.h)
+    cycle_efficiency = (W_out - W_in) / Q_in
+    system_efficiency = (W_out - W_in) / Q_in_max
+    backwork_ratio = W_comp / W_out
+    energy_balance = (Q_in + W_in) - (W_out + Q_out)
+
+    # Mixing chamber check
+    # TODO remember to include mixing chamber exergy destruction in Second Law analysis
+    a = m_total * recuperator_highT["cold_side"]["state_in"].h
+    b = m_main * recuperator_lowT["cold_side"]["state_out"].h
+    c = m_split * split_compressor["state_out"].h
+    error = a - b - c
+    if abs(error) > 1e-4:
+        raise ValueError(
+            f"The mixing chamber energy balance does not close (error: {error:0.3}). Check implementation."
+        )
+
+    # Define dictionary with 1st Law analysis
+    energy_analysis = {
+        "heater_heat_flow": heater["heat_flow_rate"],
+        "heater_heat_flow_max": Q_in_max,
+        "cooler_heat_flow": cooler["heat_flow_rate"],
+        "turbine_power": turbine["power"],
+        "main_compressor_power": main_compressor["power"],
+        "split_compressor_power": split_compressor["power"],
+        "heat_source_pump_power": heat_source_pump["power"],
+        "heat_sink_pump_power": heat_sink_pump["power"],
+        "net_cycle_power": W_net,
+        "net_system_power": W_out - W_in,
+        "mass_flow_heat_source": m_source,
+        "mass_flow_heat_sink": m_sink,
+        "mass_flow_total": m_total,
+        "mass_flow_main": m_main,
+        "mass_flow_split": m_split,
+        "split_fraction": mass_split_fraction,
+        "cycle_efficiency": cycle_efficiency,
+        "system_efficiency": system_efficiency,
+        "backwork_ratio": backwork_ratio,
+        "energy_balance": energy_balance,
+    }
+
+    output = {
+        "components": components,
+        "energy_analysis": energy_analysis,
+    }
+
+    # Evaluate objective function and constraints
+    output = {"components": components, "energy_analysis": energy_analysis}
+    f = utilities.evaluate_objective_function(output, objective_function)
+    c_eq, c_ineq = utilities.evaluate_constraints(output, constraints)
+    # f = None
+    # c_eq = None
+    # c_ineq = None
+
+    # Cycle performance summary
+    output = {
+        **output,
+        "working_fluid": working_fluid,
+        "heating_fluid": heating_fluid,
+        "cooling_fluid": cooling_fluid,
+        "components": components,
+        "objective_function": f,
+        "equality_constraints": c_eq,
+        "inequality_constraints": c_ineq,
+    }
+
+    # Set colors for plotting
+    heater["hot_side"]["color"] = COLORS_MATLAB[6]
+    heater["cold_side"]["color"] = COLORS_MATLAB[1]
+    recuperator_lowT["hot_side"]["color"] = COLORS_MATLAB[1]
+    recuperator_lowT["cold_side"]["color"] = COLORS_MATLAB[1]
+    recuperator_highT["hot_side"]["color"] = COLORS_MATLAB[1]
+    recuperator_highT["cold_side"]["color"] = COLORS_MATLAB[1]
+    cooler["hot_side"]["color"] = COLORS_MATLAB[1]
+    cooler["cold_side"]["color"] = COLORS_MATLAB[0]
+    turbine["color"] = COLORS_MATLAB[1]
+    main_compressor["color"] = COLORS_MATLAB[1]
+    split_compressor["color"] = COLORS_MATLAB[1]
+
+    # Check if any fixed parameter or design variable was not used
+    utilities.check_for_unused_keys(parameters, "parameters", raise_error=True)
+    utilities.check_for_unused_keys(variables, "variables", raise_error=True)
+
+    return output
```

### Comparing `turboflow-0.1.2/turboflow/thermodynamic_cycles/components.py` & `turboflow-0.1.3/turboflow/thermodynamic_cycles/components.py`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,313 +1,313 @@
-import numpy as np
-
-from scipy.integrate import solve_ivp
-
-# from .. import utilities
-from .. import fluid_properties as props
-
-
-def heat_exchanger(
-    fluid_hot,
-    h_in_hot,
-    h_out_hot,
-    p_in_hot,
-    p_out_hot,
-    fluid_cold,
-    h_in_cold,
-    h_out_cold,
-    p_in_cold,
-    p_out_cold,
-    num_steps=50,
-    counter_current=True,
-):
-    # Evaluate properties on the hot side
-    hot_side = heat_transfer_process(
-        fluid=fluid_hot,
-        h_1=h_in_hot,
-        h_2=h_out_hot,
-        p_1=p_in_hot,
-        p_2=p_out_hot,
-        num_steps=num_steps,
-    )
-
-    # Evaluate properties in the cold side
-    cold_side = heat_transfer_process(
-        fluid=fluid_cold,
-        h_1=h_in_cold,
-        h_2=h_out_cold,
-        p_1=p_in_cold,
-        p_2=p_out_cold,
-        num_steps=num_steps,
-    )
-
-    # Sort values for temperature difference calculation
-    if counter_current:
-        for key, value in hot_side["states"].items():
-            hot_side["states"][key] = np.flip(value)
-
-    # Compute temperature difference
-    dT = hot_side["states"]["T"] - cold_side["states"]["T"]
-
-    # Compute mass flow ratio
-    # Use 1.0 for cases with no heat exchange (i.e., no recuperator)
-    dh_hot = hot_side["state_in"].h - hot_side["state_out"].h
-    dh_cold = cold_side["state_out"].h - cold_side["state_in"].h
-    mass_ratio = 1.00 if (dh_cold == 0 or dh_hot == 0) else dh_cold / dh_hot
-
-    # Create result dictionary
-    result = {
-        "type": "heat_exchanger",
-        "hot_side": hot_side,
-        "cold_side": cold_side,
-        "q_hot_side": dh_hot,
-        "q_cold_side": dh_cold,
-        "temperature_difference": dT,
-        "mass_flow_ratio": mass_ratio,
-    }
-
-    return result
-
-
-def heat_transfer_process(fluid, h_1, p_1, h_2, p_2, num_steps=25):
-    # Generate linearly spaced arrays for pressure and enthalpy
-    p_array = np.linspace(p_1, p_2, num_steps)
-    h_array = np.linspace(h_1, h_2, num_steps)
-
-    # Initialize lists to store states for hot and cold sides
-    states = []
-
-    # Calculate states for hot side
-    for p, h in zip(p_array, h_array):
-        states.append(fluid.set_state(props.HmassP_INPUTS, h, p))
-
-    # Store inlet and outlet states
-    state_in = states[0]
-    state_out = states[-1]
-
-    # Create result dictionary
-    result = {
-        "states": props.states_to_dict(states),
-        "fluid_name": fluid.name,
-        "state_in": state_in,
-        "state_out": state_out,
-        "mass_flow": np.nan,
-        "color": "black",
-    }
-
-    return result
-
-
-def compression_process(
-    fluid, h_in, p_in, p_out, efficiency, efficiency_type="isentropic", num_steps=10
-):
-    """
-    Calculate properties along a compression process defined by a isentropic or polytropic efficiency
-
-    Parameters
-    ----------
-    fluid : Fluid
-        The fluid object used to evaluate thermodynamic properties
-    h_in : float
-        Enthalpy at the start of the compression process.
-    p_in : float
-        Pressure at the start of the compression process.
-    p_out : float
-        Pressure at the end of the compression process.
-    efficiency : float
-        The efficiency of the compression process.
-    efficiency_type : str, optional
-        The type of efficiency to be used in the process ('isentropic' or 'polytropic'). Default is 'isentropic'.
-    num_steps : int, optional
-        The number of steps for the polytropic process calculation. Default is 50.
-
-    Returns
-    -------
-    tuple
-        Tuple containing (state_out, states) where states is a list with all intermediate states
-
-    Raises
-    ------
-    ValueError
-        If an invalid 'efficiency_type' is provided.
-
-    """
-
-    # Compute inlet state
-    state_in = fluid.set_state(props.HmassP_INPUTS, h_in, p_in)
-    state_out_is = fluid.set_state(props.PSmass_INPUTS, p_out, state_in.s)
-
-    # Evaluate compression process
-    if efficiency_type == "isentropic":
-        # Compute outlet state according to the definition of isentropic efficiency
-        h_out = state_in.h + (state_out_is.h - state_in.h) / efficiency
-        state_out = fluid.set_state(props.HmassP_INPUTS, h_out, p_out)
-        states = [state_in, state_out]
-
-    elif efficiency_type == "polytropic":
-        # Differential equation defining the polytropic compression
-        def odefun(p, h):
-            state = fluid.set_state(props.HmassP_INPUTS, h, p)
-            dhdp = 1.0 / (efficiency * state.rho)
-            return dhdp, state
-
-        # Solve polytropic compression differential equation
-        sol = solve_ivp(
-            lambda p, h: odefun(p, h)[0],
-            [state_in.p, p_out],
-            [state_in.h],
-            t_eval=np.linspace(state_in.p, p_out, num_steps),
-            method="RK45",
-        )
-
-        # Evaluate fluid properties at intermediate states
-        states = postprocess_ode(sol.t, sol.y, odefun)
-        state_in, state_out = states[0], states[-1]
-
-    else:
-        raise ValueError("Invalid efficiency_type. Use 'isentropic' or 'polytropic'.")
-
-    # Compute work
-    isentropic_work = state_out_is.h - state_in.h
-    specific_work = state_out.h - state_in.h
-
-    # Compute degree of superheating
-    state_in = props.calculate_superheating(state_in, fluid)
-    state_out = props.calculate_superheating(state_out, fluid)
-
-    # Compute degree of subcooling
-    state_in = props.calculate_subcooling(state_in, fluid)
-    state_out = props.calculate_subcooling(state_out, fluid)
-
-    # Create result dictionary
-    result = {
-        "type": "compressor",
-        "fluid_name": fluid.name,
-        "states": props.states_to_dict(states),
-        "state_in": state_in,
-        "state_out": state_out,
-        "efficiency": efficiency,
-        "efficiency_type": efficiency_type,
-        "specific_work": specific_work,
-        "isentropic_work": isentropic_work,
-        "pressure_ratio": state_out.p / state_in.p,
-        "mass_flow": np.nan,
-        "color": "black",
-    }
-
-    return result
-
-
-def expansion_process(
-    fluid, h_in, p_in, p_out, efficiency, efficiency_type="isentropic", num_steps=50
-):
-    """
-    Calculate properties along a compression process defined by a isentropic or polytropic efficiency
-
-    Parameters
-    ----------
-    fluid : Fluid
-        The fluid object used to evaluate thermodynamic properties
-    h_in : float
-        Enthalpy at the start of the compression process.
-    p_in : float
-        Pressure at the start of the compression process.
-    p_out : float
-        Pressure at the end of the compression process.
-    efficiency : float
-        The efficiency of the compression process.
-    efficiency_type : str, optional
-        The type of efficiency to be used in the process ('isentropic' or 'polytropic'). Default is 'isentropic'.
-    num_steps : int, optional
-        The number of steps for the polytropic process calculation. Default is 50.
-
-    Returns
-    -------
-    tuple
-        Tuple containing (state_out, states) where states is a list with all intermediate states
-
-    Raises
-    ------
-    ValueError
-        If an invalid 'efficiency_type' is provided.
-
-    """
-
-    # Compute inlet state
-    state_in = fluid.set_state(props.HmassP_INPUTS, h_in, p_in)
-    state_out_is = fluid.set_state(props.PSmass_INPUTS, p_out, state_in.s)
-    if efficiency_type == "isentropic":
-        # Compute outlet state according to the definition of isentropic efficiency
-        h_out = state_in.h - efficiency * (state_in.h - state_out_is.h)
-        state_out = fluid.set_state(props.HmassP_INPUTS, h_out, p_out)
-        states = [state_in, state_out]
-
-    elif efficiency_type == "polytropic":
-        # Differential equation defining the polytropic expansion
-        def odefun(p, h):
-            state = fluid.set_state(props.HmassP_INPUTS, h, p)
-            dhdp = efficiency / state.rho
-            return dhdp, state
-
-        # Solve polytropic expansion differential equation
-        sol = solve_ivp(
-            lambda p, h: odefun(p, h)[0],
-            [state_in.p, p_out],
-            [state_in.h],
-            t_eval=np.linspace(state_in.p, p_out, num_steps),
-            method="RK45",
-        )
-
-        # Evaluate fluid properties at intermediate states
-        states = postprocess_ode(sol.t, sol.y, odefun)
-        state_in, state_out = states[0], states[-1]
-
-    else:
-        raise ValueError("Invalid efficiency_type. Use 'isentropic' or 'polytropic'.")
-
-    # Compute work
-    isentropic_work = state_in.h - state_out_is.h
-    specific_work = state_in.h - state_out.h
-
-    # Compute degree of superheating
-    state_in = props.calculate_superheating(state_in, fluid)
-    state_out = props.calculate_superheating(state_out, fluid)
-
-    # Compute degree of subcooling
-    state_in = props.calculate_subcooling(state_in, fluid)
-    state_out = props.calculate_subcooling(state_out, fluid)
-
-    # Create result dictionary
-    result = {
-        "type": "turbine",
-        "fluid_name": fluid.name,
-        "states": props.states_to_dict(states),
-        "state_in": state_in,
-        "state_out": state_out,
-        "efficiency": efficiency,
-        "efficiency_type": efficiency_type,
-        "specific_work": specific_work,
-        "isentropic_work": isentropic_work,
-        "pressure_ratio": state_in.p / state_out.p,
-        "mass_flow": np.nan,
-        "color": "black",
-    }
-
-    return result
-
-
-def isenthalpic_valve(state_in, p_out, fluid, N=50):
-    p_array = np.linspace(state_in.p, p_out, N)
-    h_array = state_in.h * p_array
-    states = []
-    for p, h in zip(p_array, h_array):
-        states.append(fluid.set_state(props.HmassP_INPUTS, h, p))
-    return states
-
-
-def postprocess_ode(t, y, ode_handle):
-    # Collect additional properties for each integration step
-    ode_out = []
-    for t_i, y_i in zip(t, y.T):
-        _, state = ode_handle(t_i, y_i)
-        ode_out.append(state)
-    return ode_out
+import numpy as np
+
+from scipy.integrate import solve_ivp
+
+# from .. import utilities
+from .. import fluid_properties as props
+
+
+def heat_exchanger(
+    fluid_hot,
+    h_in_hot,
+    h_out_hot,
+    p_in_hot,
+    p_out_hot,
+    fluid_cold,
+    h_in_cold,
+    h_out_cold,
+    p_in_cold,
+    p_out_cold,
+    num_steps=50,
+    counter_current=True,
+):
+    # Evaluate properties on the hot side
+    hot_side = heat_transfer_process(
+        fluid=fluid_hot,
+        h_1=h_in_hot,
+        h_2=h_out_hot,
+        p_1=p_in_hot,
+        p_2=p_out_hot,
+        num_steps=num_steps,
+    )
+
+    # Evaluate properties in the cold side
+    cold_side = heat_transfer_process(
+        fluid=fluid_cold,
+        h_1=h_in_cold,
+        h_2=h_out_cold,
+        p_1=p_in_cold,
+        p_2=p_out_cold,
+        num_steps=num_steps,
+    )
+
+    # Sort values for temperature difference calculation
+    if counter_current:
+        for key, value in hot_side["states"].items():
+            hot_side["states"][key] = np.flip(value)
+
+    # Compute temperature difference
+    dT = hot_side["states"]["T"] - cold_side["states"]["T"]
+
+    # Compute mass flow ratio
+    # Use 1.0 for cases with no heat exchange (i.e., no recuperator)
+    dh_hot = hot_side["state_in"].h - hot_side["state_out"].h
+    dh_cold = cold_side["state_out"].h - cold_side["state_in"].h
+    mass_ratio = 1.00 if (dh_cold == 0 or dh_hot == 0) else dh_cold / dh_hot
+
+    # Create result dictionary
+    result = {
+        "type": "heat_exchanger",
+        "hot_side": hot_side,
+        "cold_side": cold_side,
+        "q_hot_side": dh_hot,
+        "q_cold_side": dh_cold,
+        "temperature_difference": dT,
+        "mass_flow_ratio": mass_ratio,
+    }
+
+    return result
+
+
+def heat_transfer_process(fluid, h_1, p_1, h_2, p_2, num_steps=25):
+    # Generate linearly spaced arrays for pressure and enthalpy
+    p_array = np.linspace(p_1, p_2, num_steps)
+    h_array = np.linspace(h_1, h_2, num_steps)
+
+    # Initialize lists to store states for hot and cold sides
+    states = []
+
+    # Calculate states for hot side
+    for p, h in zip(p_array, h_array):
+        states.append(fluid.set_state(props.HmassP_INPUTS, h, p))
+
+    # Store inlet and outlet states
+    state_in = states[0]
+    state_out = states[-1]
+
+    # Create result dictionary
+    result = {
+        "states": props.states_to_dict(states),
+        "fluid_name": fluid.name,
+        "state_in": state_in,
+        "state_out": state_out,
+        "mass_flow": np.nan,
+        "color": "black",
+    }
+
+    return result
+
+
+def compression_process(
+    fluid, h_in, p_in, p_out, efficiency, efficiency_type="isentropic", num_steps=10
+):
+    """
+    Calculate properties along a compression process defined by a isentropic or polytropic efficiency
+
+    Parameters
+    ----------
+    fluid : Fluid
+        The fluid object used to evaluate thermodynamic properties
+    h_in : float
+        Enthalpy at the start of the compression process.
+    p_in : float
+        Pressure at the start of the compression process.
+    p_out : float
+        Pressure at the end of the compression process.
+    efficiency : float
+        The efficiency of the compression process.
+    efficiency_type : str, optional
+        The type of efficiency to be used in the process ('isentropic' or 'polytropic'). Default is 'isentropic'.
+    num_steps : int, optional
+        The number of steps for the polytropic process calculation. Default is 50.
+
+    Returns
+    -------
+    tuple
+        Tuple containing (state_out, states) where states is a list with all intermediate states
+
+    Raises
+    ------
+    ValueError
+        If an invalid 'efficiency_type' is provided.
+
+    """
+
+    # Compute inlet state
+    state_in = fluid.set_state(props.HmassP_INPUTS, h_in, p_in)
+    state_out_is = fluid.set_state(props.PSmass_INPUTS, p_out, state_in.s)
+
+    # Evaluate compression process
+    if efficiency_type == "isentropic":
+        # Compute outlet state according to the definition of isentropic efficiency
+        h_out = state_in.h + (state_out_is.h - state_in.h) / efficiency
+        state_out = fluid.set_state(props.HmassP_INPUTS, h_out, p_out)
+        states = [state_in, state_out]
+
+    elif efficiency_type == "polytropic":
+        # Differential equation defining the polytropic compression
+        def odefun(p, h):
+            state = fluid.set_state(props.HmassP_INPUTS, h, p)
+            dhdp = 1.0 / (efficiency * state.rho)
+            return dhdp, state
+
+        # Solve polytropic compression differential equation
+        sol = solve_ivp(
+            lambda p, h: odefun(p, h)[0],
+            [state_in.p, p_out],
+            [state_in.h],
+            t_eval=np.linspace(state_in.p, p_out, num_steps),
+            method="RK45",
+        )
+
+        # Evaluate fluid properties at intermediate states
+        states = postprocess_ode(sol.t, sol.y, odefun)
+        state_in, state_out = states[0], states[-1]
+
+    else:
+        raise ValueError("Invalid efficiency_type. Use 'isentropic' or 'polytropic'.")
+
+    # Compute work
+    isentropic_work = state_out_is.h - state_in.h
+    specific_work = state_out.h - state_in.h
+
+    # Compute degree of superheating
+    state_in = props.calculate_superheating(state_in, fluid)
+    state_out = props.calculate_superheating(state_out, fluid)
+
+    # Compute degree of subcooling
+    state_in = props.calculate_subcooling(state_in, fluid)
+    state_out = props.calculate_subcooling(state_out, fluid)
+
+    # Create result dictionary
+    result = {
+        "type": "compressor",
+        "fluid_name": fluid.name,
+        "states": props.states_to_dict(states),
+        "state_in": state_in,
+        "state_out": state_out,
+        "efficiency": efficiency,
+        "efficiency_type": efficiency_type,
+        "specific_work": specific_work,
+        "isentropic_work": isentropic_work,
+        "pressure_ratio": state_out.p / state_in.p,
+        "mass_flow": np.nan,
+        "color": "black",
+    }
+
+    return result
+
+
+def expansion_process(
+    fluid, h_in, p_in, p_out, efficiency, efficiency_type="isentropic", num_steps=50
+):
+    """
+    Calculate properties along a compression process defined by a isentropic or polytropic efficiency
+
+    Parameters
+    ----------
+    fluid : Fluid
+        The fluid object used to evaluate thermodynamic properties
+    h_in : float
+        Enthalpy at the start of the compression process.
+    p_in : float
+        Pressure at the start of the compression process.
+    p_out : float
+        Pressure at the end of the compression process.
+    efficiency : float
+        The efficiency of the compression process.
+    efficiency_type : str, optional
+        The type of efficiency to be used in the process ('isentropic' or 'polytropic'). Default is 'isentropic'.
+    num_steps : int, optional
+        The number of steps for the polytropic process calculation. Default is 50.
+
+    Returns
+    -------
+    tuple
+        Tuple containing (state_out, states) where states is a list with all intermediate states
+
+    Raises
+    ------
+    ValueError
+        If an invalid 'efficiency_type' is provided.
+
+    """
+
+    # Compute inlet state
+    state_in = fluid.set_state(props.HmassP_INPUTS, h_in, p_in)
+    state_out_is = fluid.set_state(props.PSmass_INPUTS, p_out, state_in.s)
+    if efficiency_type == "isentropic":
+        # Compute outlet state according to the definition of isentropic efficiency
+        h_out = state_in.h - efficiency * (state_in.h - state_out_is.h)
+        state_out = fluid.set_state(props.HmassP_INPUTS, h_out, p_out)
+        states = [state_in, state_out]
+
+    elif efficiency_type == "polytropic":
+        # Differential equation defining the polytropic expansion
+        def odefun(p, h):
+            state = fluid.set_state(props.HmassP_INPUTS, h, p)
+            dhdp = efficiency / state.rho
+            return dhdp, state
+
+        # Solve polytropic expansion differential equation
+        sol = solve_ivp(
+            lambda p, h: odefun(p, h)[0],
+            [state_in.p, p_out],
+            [state_in.h],
+            t_eval=np.linspace(state_in.p, p_out, num_steps),
+            method="RK45",
+        )
+
+        # Evaluate fluid properties at intermediate states
+        states = postprocess_ode(sol.t, sol.y, odefun)
+        state_in, state_out = states[0], states[-1]
+
+    else:
+        raise ValueError("Invalid efficiency_type. Use 'isentropic' or 'polytropic'.")
+
+    # Compute work
+    isentropic_work = state_in.h - state_out_is.h
+    specific_work = state_in.h - state_out.h
+
+    # Compute degree of superheating
+    state_in = props.calculate_superheating(state_in, fluid)
+    state_out = props.calculate_superheating(state_out, fluid)
+
+    # Compute degree of subcooling
+    state_in = props.calculate_subcooling(state_in, fluid)
+    state_out = props.calculate_subcooling(state_out, fluid)
+
+    # Create result dictionary
+    result = {
+        "type": "turbine",
+        "fluid_name": fluid.name,
+        "states": props.states_to_dict(states),
+        "state_in": state_in,
+        "state_out": state_out,
+        "efficiency": efficiency,
+        "efficiency_type": efficiency_type,
+        "specific_work": specific_work,
+        "isentropic_work": isentropic_work,
+        "pressure_ratio": state_in.p / state_out.p,
+        "mass_flow": np.nan,
+        "color": "black",
+    }
+
+    return result
+
+
+def isenthalpic_valve(state_in, p_out, fluid, N=50):
+    p_array = np.linspace(state_in.p, p_out, N)
+    h_array = state_in.h * p_array
+    states = []
+    for p, h in zip(p_array, h_array):
+        states.append(fluid.set_state(props.HmassP_INPUTS, h, p))
+    return states
+
+
+def postprocess_ode(t, y, ode_handle):
+    # Collect additional properties for each integration step
+    ode_out = []
+    for t_i, y_i in zip(t, y.T):
+        _, state = ode_handle(t_i, y_i)
+        ode_out.append(state)
+    return ode_out
```

### Comparing `turboflow-0.1.2/turboflow/thermodynamic_cycles/cycle_optimization.py` & `turboflow-0.1.3/turboflow/thermodynamic_cycles/cycle_optimization_old.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,965 +1,837 @@
-import os
-import time
-import yaml
-import copy
-import datetime
-import threading
-import numpy as np
-import pandas as pd
-import matplotlib.pyplot as plt
-
-from ..config_validation import read_configuration_file
-from .. import utilities
-from .. import pysolver_view as psv
-from .. import fluid_properties as props
-
-from . import brayton_recuperated
-from . import brayton_split_compression
-
-
-utilities.set_plot_options()
-COLORS_MATLAB = utilities.COLORS_MATLAB
-LABEL_MAPPING = {
-    "s": "Entropy [J/kg/K]",
-    "T": "Temperature [K]",
-    "p": "Pressure [Pa]",
-    "h": "Enthalpy [J/kg]",
-    "d": "Density [kg/m$^3$]",
-    "a": "Speed of sound [m/s]",
-    "Z": "Compressibility factor [-]",
-    "heat": "Heat flow rate [W]",
-}
-
-# Cycle configurations available
-CYCLE_TOPOLOGIES = {
-    "recuperated": brayton_recuperated.evaluate_cycle,
-    "split_compression": brayton_split_compression.evaluate_cycle,
-    "recompression": brayton_split_compression.evaluate_cycle,
-}
-
-GRAPHICS_PLACEHOLDER = {
-    "process_lines": {},
-    "state_points": {},
-    "pinch_point_lines": {},
-}
-
-
-class ThermodynamicCycleOptimization:
-    def __init__(self, config_file, out_dir=None):
-        """
-        Initializes the optimization manager with a configuration file.
-        Parameters:
-            config_file (str): The path to the YAML configuration file.
-        """
-
-        # Define filename with unique date-time identifier
-        if out_dir == None:
-            current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
-            self.out_dir = f"results/case_{current_time}"
-
-        # Create a directory to save simulation results
-        if not os.path.exists(self.out_dir):
-            os.makedirs(self.out_dir)
-
-        self.config = self.load_config(config_file)
-        self.problem = self.setup_problem()
-        self.solver = self.setup_solver()
-
-    def load_config(self, config_file):
-        """
-        Loads configuration from a YAML file.
-        """
-        return read_configuration_file(config_file)
-
-    def setup_problem(self):
-        """
-        Sets up the ThermodynamicCycleProblem based on the loaded configuration.
-        """
-        self.problem = ThermodynamicCycleProblem(self.config["problem_formulation"])
-        self.problem.fitness(self.problem.x0)
-        return self.problem
-
-    def setup_solver(self):
-        """
-        Configures and returns the optimization solver.
-        """
-
-        # Optimize the thermodynamic cycle
-        self.solver = psv.OptimizationSolver(
-            self.problem,
-            **self.config["solver_options"],
-            callback_functions=[
-                self.problem.plot_cycle_callback,
-                self.problem.save_config_callback,
-            ],
-        )
-
-        return self.solver
-
-    def run_optimization(self):
-        """
-        Executes the optimization process.
-        """
-        self.solver.solve(self.problem.x0)
-
-    def save_results(self):
-        """
-        Saves the results of the optimization, including configurations and output files.
-        """
-        filename = os.path.join(self.out_dir, "optimal_solution")
-        self.problem.to_excel(filename=filename + ".xlsx")
-        self.problem.save_current_configuration(filename=filename + ".yaml")
-
-        # Print final solution values
-        print()
-        print("Optimal set of design variables (normalized)")
-        for key, value in self.problem.vars_normalized.items():
-            print(f"{key:40}: {value:0.3f}")
-
-    def generate_output_files(self):
-        """
-        Generates additional output files, such as Excel files or plots.
-        """
-        # Code to handle output generation like creating plots or exporting to Excel
-
-    def create_animation(self):
-        """
-        Creates an animation of the optimization history.
-        """
-        # Code to generate animation from the optimization results
-
-
-class ThermodynamicCycleProblem(psv.OptimizationProblem):
-    """
-    A class to represent a thermodynamic cycle optimization problem.
-
-    This class provides functionalities to load and update the configuration for
-    the thermodynamic cycle, and to perform interactive plotting based on the current
-    configuration.
-
-    Attributes
-    ----------
-    plot_initialized : bool
-        Flag to indicate if the plot has been initialized.
-    constraints : dict
-        Dictionary holding the constraints of the problem.
-    fixed_parameters : dict
-        Dictionary holding the fixed parameters of the problem.
-    design_variables : dict
-        Dictionary holding the current values of the design variables.
-    lower_bounds : dict
-        Dictionary holding the lower bounds of the design variables.
-    upper_bounds : dict
-        Dictionary holding the upper bounds of the design variables.
-    keys : list
-        List of keys (names) of the design variables.
-    x0 : np.ndarray
-        Initial guess for the optimization problem.
-
-    Methods
-    -------
-    update_config(configuration):
-        Update the problem's configuration based on the provided dictionary.
-    load_config_from_file(configuration_file):
-        Load and update the problem's configuration from a specified file.
-    plot_cycle_interactive(configuration_file, update_interval=0.20):
-        Perform interactive plotting, updating the plot based on the configuration file.
-    """
-
-    def __init__(self, configuration, out_dir=None):
-        """
-        Constructs all the necessary attributes for the ThermodynamicCycleProblem object.
-
-        Parameters
-        ----------
-        config : dict
-            Dictionary containing the configuration for the thermodynamic cycle problem.
-        """
-        # Initialize variables
-        self.figure = None
-        self.figure_TQ = None
-        self.configuration = copy.deepcopy(configuration)
-        self.update_configuration(self.configuration)
-        self.graphics = copy.deepcopy(GRAPHICS_PLACEHOLDER)
-
-        # Create fluid object to calculate states used for the scaling of design variables
-        self.fluid = props.Fluid(**self.fixed_parameters["working_fluid"])
-        self._calculate_special_points()
-
-        # Define filename with unique date-time identifier
-        if out_dir == None:
-            current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
-            self.out_dir = f"results/case_{current_time}"
-
-        # Create a directory to save simulation results
-        if not os.path.exists(self.out_dir):
-            os.makedirs(self.out_dir)
-
-        # # Save the initial configuration as YAML file
-        # self.save_current_configuration("initial_configuration.yaml")
-
-    def update_configuration(self, configuration):
-        """
-        Update the problem's configuration based on the provided dictionary.
-
-        Parameters
-        ----------
-        config : dict
-            Dictionary containing the new configuration for the thermodynamic cycle problem.
-        """
-        conf = configuration
-        self.cycle_topology = conf["cycle_topology"]
-        self.plot_settings = conf["plot_settings"]
-        self.constraints = conf["constraints"]
-        self.fixed_parameters = conf["fixed_parameters"]
-        self.objective_function = conf["objective_function"]
-        self.vars_normalized = {
-            k: v["value"] for k, v in conf["design_variables"].items()
-        }
-        self.lower_bounds = {k: v["min"] for k, v in conf["design_variables"].items()}
-        self.upper_bounds = {k: v["max"] for k, v in conf["design_variables"].items()}
-        self.keys = list(self.vars_normalized.keys())
-        self.x0 = np.asarray([var for var in self.vars_normalized.values()])
-
-    def _calculate_special_points(self):
-        """
-        Calculates and stores key thermodynamic states for the fluid used in the cycle.
-
-        This method computes two specific thermodynamic states:
-        1. Saturated liquid state at the heat sink inlet temperature.
-        2. Dilute gas state at the heat source inlet temperature.
-
-        If the heat sink inlet temperature is below the fluid's critical temperature,
-        the saturated liquid state is calculated at this temperature. If it is above
-        the critical temperature, the state is calculated along the pseudocritical line.
-
-        The dilute gas state is calculated at the heat source inlet temperature and at
-        very low pressure (slightly above the triple pressure)
-
-        The calculated states, along with the fluid's critical and triple point properties,
-        are stored in the `fixed_parameters` dictionary under the 'fluid' key.
-
-        These states are intended to define the bounds of the design variables specified
-        in the YAML configuration file, for example:
-
-            compressor_inlet_enthalpy:
-            min: 0.9*$fluid.liquid_sink_temperature.h
-            max: 2.0*$fluid.liquid_sink_temperature.h
-            value: 0.2
-            turbine_inlet_pressure:
-            min: 0.75*$fluid.critical_point.p
-            max: 5.00*$fluid.critical_point.p
-            value: 0.5
-            turbine_inlet_enthalpy:
-            min: 1.10*$fluid.critical_point.h
-            max: 1.00*$fluid.gas_source_temperature.h
-            value: 0.90
-
-        """
-        # Compute saturated liquid state at sink temperature
-        # Use pseudocritical line if heat sink is above critical temperature
-        T_sink = self.fixed_parameters["heat_sink"]["inlet_temperature"]
-        crit = self.fluid.critical_point
-        if T_sink < crit.T:
-            state_sat = self.fluid.set_state(props.QT_INPUTS, 0.0, T_sink)
-        else:
-            state_sat = self.fluid.set_state(props.DmassT_INPUTS, crit.rho, T_sink)
-
-        # Compute dilute gas state at heat source temperature
-        T_source = self.fixed_parameters["heat_source"]["inlet_temperature"]
-        p_triple = 1.01 * self.fluid.triple_point_liquid.p
-        state_dilute = self.fluid.set_state(props.PT_INPUTS, p_triple, T_source)
-
-        # Save states in the fixed parameters dictionary
-        self.fixed_parameters_bis = copy.deepcopy(self.fixed_parameters)
-        self.fixed_parameters_bis["working_fluid"] = {
-            "critical_point": self.fluid.critical_point.to_dict(),
-            "triple_point_liquid": self.fluid.triple_point_liquid.to_dict(),
-            "triple_point_vapor": self.fluid.triple_point_vapor.to_dict(),
-            "liquid_at_heat_sink_temperature": state_sat.to_dict(),
-            "gas_at_heat_source_temperature": state_dilute.to_dict(),
-        }
-
-    def load_configuration_file(self, config_file):
-        """
-        Load and update the problem's configuration from a specified file.
-
-        Useful to plot the cycle according to the latest version of the configuration
-        file in real time (interactive initial guess generation)
-
-        Parameters
-        ----------
-        config_file : str
-            Path to the configuration file.
-        """
-        config = utilities.read_configuration_file(config_file)
-        self.update_configuration(config["problem_formulation"])
-        self._calculate_special_points()
-
-    def save_current_configuration(self, filename):
-        """Save the current configuration to a YAML file."""
-        config_data = {k: v for k, v in self.configuration.items()}
-        config_data = utilities.convert_numpy_to_python(config_data, precision=12)
-        with open(filename, "w") as file:
-            yaml.dump(config_data, file, default_flow_style=False, sort_keys=False)
-
-    def save_config_callback(self, x, iter):
-        """
-        A callback function to save the current configuration during optimization iterations.
-
-        Parameters:
-        - x : The current solution vector from the optimizer.
-        - iter : The current optimization iteration count.
-
-        This function acts as a bridge between the optimizer callback requirements and the
-        existing `save_current_configuration` function.
-        """
-
-        # Create a 'results' directory if it doesn't exist
-        self.optimization_dir = os.path.join(self.out_dir, "optimization")
-        os.makedirs(self.optimization_dir, exist_ok=True)
-
-        # Define the filename using the solver's iteration number
-        filename = os.path.join(self.optimization_dir, f"iteration_{iter:03d}.yaml")
-
-        # Call the existing function to save the configuration
-        self.save_current_configuration(filename)
-
-    def fitness(self, x):
-        """
-        Evaluate optimization problem
-        """
-
-        # print("x", x)
-        # Update configuration with the current values of x
-        self.update_variables(x)
-
-        # Evaluate thermodynamic cycle
-        if self.cycle_topology in CYCLE_TOPOLOGIES.keys():
-            self.cycle_data = CYCLE_TOPOLOGIES[self.cycle_topology](
-                self.vars_physical,
-                self.fixed_parameters,
-                self.constraints,
-                self.objective_function,
-            )
-        else:
-            options = ", ".join(f"'{k}'" for k in CYCLE_TOPOLOGIES.keys())
-            raise ValueError(
-                f"Invalid cycle topology: '{self.cycle_topology}'. Available options: {options}"
-            )
-
-        # Define objective function and constraints
-        self.f = self.cycle_data["objective_function"]
-        self.c_eq = self.cycle_data["equality_constraints"]
-        self.c_ineq = self.cycle_data["inequality_constraints"]
-
-        # Combine objective function and constraints
-        out = psv.combine_objective_and_constraints(self.f, self.c_eq, self.c_ineq)
-
-        return out
-
-    def update_variables(self, x):
-        """Update the problem variables based on the new values provided by the optimizer."""
-        self.vars_normalized = dict(zip(self.keys, x))
-        self.vars_physical = self._scale_normalized_to_physical(self.vars_normalized)
-        for k, v in self.vars_normalized.items():
-            if k in self.configuration["design_variables"]:
-                self.configuration["design_variables"][k]["value"] = v
-            else:
-                # Optionally handle the error or log a warning if the key does not exist
-                print(f"Warning: {k} is not a recognized design variable.")
-
-    def get_bounds(self):
-        dim = len(self.vars_normalized)
-        return ([0.0] * dim, [1.00] * dim)
-
-    def get_nec(self):
-        return psv.count_constraints(self.c_eq)
-
-    def get_nic(self):
-        return psv.count_constraints(self.c_ineq)
-
-    def _scale_normalized_to_physical(self, vars_normalized):
-        # Define helper function
-        def scale_value(min_val, max_val, normalized_val):
-            return min_val + (max_val - min_val) * normalized_val
-
-        # Loop over normalized variables
-        vars_physical = {}
-        for key, value in vars_normalized.items():
-            try:
-                # Fetch the bounds expressions
-                lower_expr = self.lower_bounds[key]
-                upper_expr = self.upper_bounds[key]
-
-                # Try to evaluate bounds as expressions if they are strings
-                if isinstance(lower_expr, str):
-                    lower = utilities.render_and_evaluate(
-                        lower_expr, self.fixed_parameters_bis
-                    )
-                else:
-                    lower = lower_expr
-                if isinstance(upper_expr, str):
-                    upper = utilities.render_and_evaluate(
-                        upper_expr, self.fixed_parameters_bis
-                    )
-                else:
-                    upper = upper_expr
-
-                vars_physical[key] = scale_value(lower, upper, value)
-
-            except (KeyError, ValueError) as e:
-                raise ValueError(f"Error processing bounds for '{key}': {e}")
-
-        return vars_physical
-
-    def to_excel(self, filename="performance.xlsx"):
-        """
-        Exports the cycle performance data to Excel file
-        """
-
-        # Define variable map
-        variable_map = {
-            "fluid_name": {"name": "fluid_name", "unit": "-"},
-            "T": {"name": "temperature", "unit": "K"},
-            "p": {"name": "pressure", "unit": "Pa"},
-            "rho": {"name": "density", "unit": "kg/m3"},
-            "Q": {"name": "quality", "unit": "-"},
-            "Z": {"name": "compressibility_factor", "unit": "-"},
-            "u": {"name": "internal_energy", "unit": "J/kg"},
-            "h": {"name": "enthalpy", "unit": "J/kg"},
-            "s": {"name": "entropy", "unit": "J/kg/K"},
-            "cp": {"name": "isobaric_heat_capacity", "unit": "J/kg/K"},
-            "cv": {"name": "isochoric_heat_capacity", "unit": "J/kg/K"},
-            "gamma": {"name": "heat_capacity_ratio", "unit": "-"},
-            "a": {"name": "speed_of_sound", "unit": "m/s"},
-            "mu": {"name": "dynamic_viscosity", "unit": "Pa*s"},
-            "k": {"name": "thermal_conductivity", "unit": "W/m/K"},
-            "superheating": {"name": "superheating_degree", "unit": "K"},
-            "subcooling": {"name": "subcooling_degree", "unit": "K"},
-        }
-
-        # Initialize a list to hold all rows of the DataFrame
-        data_rows = []
-
-        # Prepare the headers and units rows
-        headers = ["state"]
-        units_row = ["units"]
-
-        for key in variable_map:
-            headers.append(variable_map[key]["name"])
-            units_row.append(variable_map[key]["unit"])
-
-        # Iterate over each component in the dictionary
-        for component_name, component in self.cycle_data["components"].items():
-            if component["type"] == "heat_exchanger":
-                # Handle heat exchanger sides separately
-                for side in ["hot_side", "cold_side"]:
-                    # Append the data for state_in and state_out to the rows list
-                    state_in = component[side]["state_in"].to_dict()
-                    state_out = component[side]["state_out"].to_dict()
-                    data_rows.append(
-                        [f"{component_name}_{side}_in"]
-                        + [state_in.get(key, None) for key in variable_map]
-                    )
-                    data_rows.append(
-                        [f"{component_name}_{side}_out"]
-                        + [state_out.get(key, None) for key in variable_map]
-                    )
-            else:
-                # Handle non-heat exchanger components
-                # Append the data for state_in and state_out to the rows list
-                state_in = component["state_in"].to_dict()
-                state_out = component["state_out"].to_dict()
-                data_rows.append(
-                    [f"{component_name}_in"]
-                    + [state_in.get(key, None) for key in variable_map]
-                )
-                data_rows.append(
-                    [f"{component_name}_out"]
-                    + [state_out.get(key, None) for key in variable_map]
-                )
-
-        # Create a DataFrame with data rows
-        df = pd.DataFrame(data_rows, columns=headers)
-
-        # Insert the units row
-        df.loc[-1] = units_row  # Adding a row
-        df.index = df.index + 1  # Shifting index
-        df = df.sort_index()  # Sorting by index
-
-        # # Export to Excel
-        # df.to_excel(
-        #     os.path.join(self.out_dir, filename),
-        #     index=False,
-        #     header=True,
-        #     sheet_name="cycle_states",
-        # )
-
-        # Prepare energy_analysis data
-        df_2 = pd.DataFrame(
-            list(self.cycle_data["energy_analysis"].items()),
-            columns=["Parameter", "Value"],
-        )
-
-        # Export to Excel
-        # filename = os.path.join(self.out_dir, filename)
-        with pd.ExcelWriter(filename, engine="openpyxl") as writer:
-            df.to_excel(writer, index=False, sheet_name="cycle_states")
-            df_2.to_excel(writer, index=False, sheet_name="energy_analysis")
-
-    def plot_cycle(self):
-        """
-        Plots or updates the thermodynamic cycle diagrams based on current settings,
-        including the option to include a pinch point diagram.
-
-        This function is capable of both creating new cycle diagrams and updating existing ones.
-        It's particularly useful in dynamic scenarios such as during optimization steps,
-        where the plot needs to be refreshed continually with new data.
-        The method also supports real-time updates based on the latest configuration settings.
-
-        The function first determines the number of subplots required based on the cycle diagrams
-        specified in the 'plot_settings' attribute and whether a pinch point diagram is included.
-        It then either initializes a new figure and axes or updates existing ones.
-        Each thermodynamic diagram (phase diagram and cycle components) and the optional pinch point
-        diagram are plotted or updated accordingly.
-
-        The method ensures that the plot reflects the current state of the cycle, including any
-        changes during optimization or adjustments to configuration settings.
-
-        The data of the plots can be updated interactively, but the subplot objects created can
-        only be specified upon class initialization. For example, it is not possible to switch
-        on and off the pinch point diagram or to add new thermodynamic diagrams. The class would
-        have to be re-initialized upon those scenarios. The reason for this choice is that re-creating
-        a figure would be too time consuming and not practical for the real-time updating of the plots
-
-        """
-
-        # Determine the number of subplots
-        include_pinch_diagram = self.plot_settings.get("pinch_point_diagram", False)
-        ncols = len(self.plot_settings["diagrams"]) + int(include_pinch_diagram)
-        nrows = 1
-
-        # Initialize the figure and axes
-        if not (self.figure and plt.fignum_exists(self.figure.number)):
-            # Reset the graphics objects if the figure was closed
-            self.graphics = copy.deepcopy(GRAPHICS_PLACEHOLDER)
-            self.figure, self.axes = plt.subplots(
-                nrows, ncols, figsize=(5.2 * ncols, 4.8)
-            )
-            self.axes = utilities.ensure_iterable(self.axes)
-
-        # Plot or update each thermodynamic_diagram
-        for i, ax in enumerate(self.axes[:-1] if include_pinch_diagram else self.axes):
-            plot_config = self.plot_settings["diagrams"][i]
-            plot_config = self._get_diagram_default_settings(plot_config)
-            self._plot_thermodynamic_diagram(ax, plot_config, i)
-
-        # Plot or update the pinch point diagram
-        if include_pinch_diagram:
-            self._plot_pinch_point_diagram(self.axes[-1], ncols - 1)
-
-        # Adjust layout and refresh plot
-        self.figure.tight_layout(pad=2)
-        plt.draw()
-        plt.pause(0.05)
-
-    def plot_cycle_realtime(self, configuration_file, update_interval=0.1):
-        """
-        Perform interactive plotting, updating the plot based on the configuration file.
-
-        Parameters
-        ----------
-        config_file : str
-            Path to the configuration file.
-        update_interval : float, optional
-            Time interval in seconds between plot updates (default is 0.1 seconds).
-
-        """
-
-        def wait_for_input():
-            input()
-            self.enter_pressed = True
-
-        # Initialize secondary thread to wait for user input
-        self.enter_pressed = False
-        self.input_thread = threading.Thread(target=wait_for_input)
-        self.input_thread.daemon = True
-        self.input_thread.start()
-
-        # Print instructions message
-        print("-" * 80)
-        print(" Creating thermodynamic cycle interactive plot")
-        print("-" * 80)
-        print(f" The current configuration file is: '{configuration_file}'")
-        print(" Modify the configuration file and save it to update the plot")
-        print(" Try to find a good initial guess for the optimization.")
-        print(" Using a feasible initial guess improves optimization convergence.")
-        print(" Press 'enter' to continue.")
-        print("-" * 80)
-
-        # Update the plot until termination signal
-        while not self.enter_pressed:
-            # Read the configuration file
-            self.load_configuration_file(configuration_file)
-            self.fitness(self.x0)
-            self.plot_cycle()
-
-            # Wait for the specified interval before updating again
-            time.sleep(update_interval)
-
-            # Exit interactive plotting when the user closes the figure
-            if not plt.fignum_exists(self.figure.number):
-                break
-
-            # Exit interactive plotting when the user presses enter
-            if self.enter_pressed:
-                break
-
-    def plot_cycle_callback(self, x, iter):
-        """
-        Plot and save the thermodynamic cycle at different optimization iterations
-
-        This function is intended to be passed as callback argument to the optimization solver
-        """
-        # Update and plot the cycle diagram
-        # self.figure.suptitle(f"Optimization iteration: {iter:03d}", fontsize=14, y=0.90)
-        self.plot_cycle()
-
-        # Create a 'results' directory if it doesn't exist
-        self.optimization_dir = os.path.join(self.out_dir, "optimization")
-        os.makedirs(self.optimization_dir, exist_ok=True)
-
-        # Use the solver's iteration number for the filename
-        filename = os.path.join(self.optimization_dir, f"iteration_{iter:03d}.png")
-        self.figure.savefig(filename)
-
-    def _get_diagram_default_settings(self, plot_config):
-        """
-        Merges user-provided plot settings with default settings.
-
-        Parameters
-        ----------
-        user_settings : dict
-            A dictionary of user-defined plot settings.
-
-        Returns
-        -------
-        dict
-            A dictionary containing the merged plot settings.
-        """
-        default_settings = {
-            "x_variable": "s",
-            "y_variable": "T",
-            "x_scale": "linear",
-            "y_scale": "linear",
-            "plot_saturation_line": True,
-            "plot_critical_point": True,
-            "plot_quality_isolines": False,
-            "plot_pseudocritical_line": False,
-            "plot_triple_point_vap": False,
-            "plot_triple_point_liq": False,
-            "plot_spinodal_line": False,
-            # Add other default settings here
-        }
-
-        # Combine the global fluid settings with the "plots" settings
-        fluid_settings = self.plot_settings.get("fluid", {})
-        fluid_settings = {} if fluid_settings is None else fluid_settings
-        plot_config = plot_config | fluid_settings
-
-        # Merge with default values
-        return default_settings | plot_config
-
-    def _plot_thermodynamic_diagram(self, ax, plot_config, ax_index=0):
-        """
-        Plots or updates the thermodynamic diagram on a specified axes.
-
-        This function sets up the axes properties according to the provided plot configuration and
-        plots the phase diagram for the specified fluid. It then iterates over all the components in
-        the cycle data. For heat exchangers, it plots or updates the processes for both the hot and cold sides.
-        For other types of components, it plots or updates the process based on the component's data.
-
-        The function also adjusts the plot limits if it's updating an existing plot,
-        ensuring that the axes are scaled correctly to fit the new data.
-
-        Parameters:
-        ----------
-        ax : matplotlib.axes.Axes
-            The axes on which to plot or update the thermodynamic diagram.
-        plot_config : dict
-            A dictionary containing the plot settings, including variables to be plotted on the x and y axes, and scaling information.
-        ax_index : int
-            The index of the axes in the figure, used for identifying and updating existing plots.
-        """
-        # Set up axes properties
-        ax.set_xlabel(LABEL_MAPPING[plot_config["x_variable"]])
-        ax.set_ylabel(LABEL_MAPPING[plot_config["y_variable"]])
-        ax.set_xscale(plot_config["x_scale"])
-        ax.set_yscale(plot_config["y_scale"])
-
-        # Plot phase diagram
-        self.fluid.plot_phase_diagram(axes=ax, **plot_config)
-
-        # Plot thermodynamic processes
-        for name, component in self.cycle_data["components"].items():
-            # Handle heat exchanger components in a special way
-            if component["type"] == "heat_exchanger":
-                for side in ["hot_side", "cold_side"]:
-                    self._plot_cycle_process(
-                        name + "_" + side, plot_config, ax, ax_index=ax_index
-                    )
-            else:
-                self._plot_cycle_process(name, plot_config, ax, ax_index=ax_index)
-
-        # Adjust plot limits if updating
-        ax.relim(visible_only=True)
-        ax.autoscale_view()
-
-    def _plot_cycle_process(self, name, plot_settings, ax, ax_index=None):
-        """
-        Creates or updates the plot elements for a specific cycle process on a given axes.
-
-        This method checks if the plot elements for the specified cycle process already exist on the given axes.
-        If they exist and an axis index is provided, it updates these elements with new data. Otherwise, it creates
-        new plot elements (lines and points) and stores them for future updates.
-
-        Parameters:
-        ----------
-        name : str
-            The name of the cycle process to plot or update.
-        plot_settings : dict
-            The plot settings dictionary containing settings such as x and y variables.
-        ax : matplotlib.axes.Axes
-            The axes on which to plot or update the cycle process.
-        ax_index : int, optional
-            The index of the axes in the figure, used for updating existing plots. If None, new plots are created.
-        """
-
-        # Retrieve component data
-        x_data, y_data, color = self._get_process_data(
-            name, plot_settings["x_variable"], plot_settings["y_variable"]
-        )
-
-        # Initialize the dictionary for this axis index if it does not exist
-        if ax_index is not None:
-            if ax_index not in self.graphics["process_lines"]:
-                self.graphics["process_lines"][ax_index] = {}
-            if ax_index not in self.graphics["state_points"]:
-                self.graphics["state_points"][ax_index] = {}
-
-        # Handle existing plot elements
-        if ax_index is not None and name in self.graphics["process_lines"][ax_index]:
-            if x_data is None or y_data is None:
-                # Hide existing plot elements if data is None
-                self.graphics["process_lines"][ax_index][name].set_visible(False)
-                self.graphics["state_points"][ax_index][name].set_visible(False)
-            else:
-                # Update existing plot elements with new data
-                self.graphics["process_lines"][ax_index][name].set_data(x_data, y_data)
-                self.graphics["state_points"][ax_index][name].set_data(
-                    [x_data[0], x_data[-1]], [y_data[0], y_data[-1]]
-                )
-                self.graphics["process_lines"][ax_index][name].set_visible(True)
-                self.graphics["state_points"][ax_index][name].set_visible(True)
-        elif x_data is not None and y_data is not None:
-            # Create new plot elements if data is not None
-            (line,) = ax.plot(
-                x_data,
-                y_data,
-                linestyle="-",
-                linewidth=1.25,
-                marker="none",
-                markersize=4.0,
-                markeredgewidth=1.25,
-                markerfacecolor="w",
-                color=color,
-                label=name,
-                zorder=1,
-            )
-            (points,) = ax.plot(
-                [x_data[0], x_data[-1]],
-                [y_data[0], y_data[-1]],
-                linestyle="none",
-                linewidth=1.25,
-                marker="o",
-                markersize=4.0,
-                markeredgewidth=1.25,
-                markerfacecolor="w",
-                color=color,
-                zorder=2,
-            )
-
-            # Store the new plot elements
-            if ax_index is not None:
-                self.graphics["process_lines"][ax_index][name] = line
-                self.graphics["state_points"][ax_index][name] = points
-
-    def _get_process_data(self, name, prop_x, prop_y):
-        """
-        Retrieve thermodynamic data for a specified process of the cycle.
-
-        Parameters:
-        ----------
-        name : str
-            Name of the cycle process.
-        prop_x : str
-            Property name to plot on the x-axis.
-        prop_y : str
-            Property name to plot on the y-axis.
-
-        Returns:
-        -------
-        tuple of (np.ndarray, np.ndarray, str)
-            x_data: Array of data points for the x-axis.
-            y_data: Array of data points for the y-axis.
-            color: Color code for the plot.
-        """
-
-        # Get heat exchanger side data
-        if "_hot_side" in name or "_cold_side" in name:
-            # Extract the component and side from the name
-            if "_hot_side" in name:
-                component_name, side_1 = name.replace("_hot_side", ""), "hot_side"
-                side_2 = "cold_side"
-            else:
-                component_name, side_1 = name.replace("_cold_side", ""), "cold_side"
-                side_2 = "hot_side"
-
-            data = self.cycle_data["components"][component_name][side_1]
-            data_other_side = self.cycle_data["components"][component_name][side_2]
-            is_heat_exchanger = True
-
-        else:  # Handle non-heat exchanger components
-            data = self.cycle_data["components"][name]
-            is_heat_exchanger = False
-
-        # Retrieve data
-        if data["states"]["identifier"][0] == "working_fluid":
-            # Components of the cycle
-            x_data = data["states"][prop_x]
-            y_data = data["states"][prop_y]
-        elif is_heat_exchanger and prop_y == "T" and prop_x in ["h", "s"]:
-            # Special case for heat exchangers
-            x_data = data_other_side["states"][prop_x]
-            y_data = data["states"][prop_y]
-        else:
-            # Other cases
-            x_data = None
-            y_data = None
-
-        color = data["color"]
-
-        return x_data, y_data, color
-
-    def _plot_pinch_point_diagram(self, ax, ax_index):
-        """
-        Plots or updates the pinch point diagram for the thermodynamic cycle's heat exchangers.
-
-        This method visualizes the temperature vs. heat flow rate for each heat exchanger in the cycle.
-        The function is capable of plotting the diagram for the first time or updating it with the latest data
-        if called subsequently. It uses a sorted approach, beginning with the heat exchanger that has the minimum
-        temperature on the cold side and proceeding in ascending order of temperature.
-
-        Parameters:
-        ----------
-        ax : matplotlib.axes.Axes
-            The axes on which to plot or update the pinch point diagram.
-        ax_index : int
-            The index of the axes in the figure, used to identify and access the specific axes
-            for updating the existing plot elements stored in the 'graphics' attribute.
-
-        Notes:
-        -----
-        The method relies on the 'graphics' attribute of the class to store and update plot elements.
-        It handles the creation of new plot elements (lines, endpoints, vertical lines) when first called
-        and updates these elements with new data from 'cycle_data' during subsequent calls.
-        """
-        ax.set_xlabel(LABEL_MAPPING["heat"])
-        ax.set_ylabel(LABEL_MAPPING["T"])
-
-        # Initialize the graphic object dict for this axis index if it does not exist
-        if ax_index not in self.graphics["pinch_point_lines"]:
-            self.graphics["pinch_point_lines"][ax_index] = {}
-
-        # Set the axes labels
-        ax.set_xlabel(LABEL_MAPPING["heat"])
-        ax.set_ylabel(LABEL_MAPPING["T"])
-
-        # Extract heat exchanger names and their minimum cold side temperatures
-        heat_exchangers = [
-            (name, min(component["cold_side"]["states"]["T"]))
-            for name, component in self.cycle_data["components"].items()
-            if component["type"] == "heat_exchanger"
-        ]
-
-        # Sort heat exchangers by minimum temperature on the cold side
-        sorted_heat_exchangers = sorted(heat_exchangers, key=lambda x: x[1])
-
-        # Loop over all heat exchangers
-        Q0 = 0.00
-        for HX_name, _ in sorted_heat_exchangers:
-            component = self.cycle_data["components"][HX_name]
-            # Hot side
-            c_hot = component["hot_side"]["color"]
-            props_hot = component["hot_side"]["states"]
-            mass_flow_hot = component["hot_side"]["mass_flow"]
-            Q_hot = (props_hot["h"] - props_hot["h"][0]) * mass_flow_hot
-            T_hot = props_hot["T"]
-
-            # Cold side
-            c_cold = component["cold_side"]["color"]
-            props_cold = component["cold_side"]["states"]
-            mass_flow_cold = component["cold_side"]["mass_flow"]
-            Q_cold = (props_cold["h"] - props_cold["h"][0]) * mass_flow_cold
-            T_cold = props_cold["T"]
-
-            # Check if the plot elements for this component already exist
-            if HX_name in self.graphics["pinch_point_lines"][ax_index]:
-                # Update existing plot elements
-                plot_elements = self.graphics["pinch_point_lines"][ax_index][HX_name]
-                plot_elements["hot_line"].set_data(Q0 + Q_hot, T_hot)
-                plot_elements["cold_line"].set_data(Q0 + Q_cold, T_cold)
-
-                # Update endpoints
-                plot_elements["hot_start"].set_data(Q0 + Q_hot[0], T_hot[0])
-                plot_elements["hot_end"].set_data(Q0 + Q_hot[-1], T_hot[-1])
-                plot_elements["cold_start"].set_data(Q0 + Q_cold[0], T_cold[0])
-                plot_elements["cold_end"].set_data(Q0 + Q_cold[-1], T_cold[-1])
-
-                # Update vertical lines
-                plot_elements["start_line"].set_xdata([Q0, Q0])
-                plot_elements["end_line"].set_xdata([Q0 + Q_hot[-1], Q0 + Q_hot[-1]])
-            else:
-                # Create new plot elements
-                (hot_line,) = ax.plot(Q0 + Q_hot, T_hot, color=c_hot)
-                (cold_line,) = ax.plot(Q0 + Q_cold, T_cold, color=c_cold)
-
-                # Create endpoints
-                param = {"marker": "o", "markersize": 4.0, "markerfacecolor": "white"}
-                (hot_1,) = ax.plot(Q0 + Q_hot[0], T_hot[0], color=c_hot, **param)
-                (hot_2,) = ax.plot(Q0 + Q_hot[-1], T_hot[-1], color=c_hot, **param)
-                (cold_1,) = ax.plot(Q0 + Q_cold[0], T_cold[0], color=c_cold, **param)
-                (cold_2,) = ax.plot(Q0 + Q_cold[-1], T_cold[-1], color=c_cold, **param)
-
-                # Create vertical lines
-                param = {"color": "black", "linestyle": "-", "linewidth": 0.75}
-                start_line = ax.axvline(x=Q0, zorder=1, **param)
-                end_line = ax.axvline(x=Q0 + Q_hot[-1], zorder=1, **param)
-
-                # Store new plot elements
-                self.graphics["pinch_point_lines"][ax_index][HX_name] = {
-                    "hot_line": hot_line,
-                    "cold_line": cold_line,
-                    "hot_start": hot_1,
-                    "hot_end": hot_2,
-                    "cold_start": cold_1,
-                    "cold_end": cold_2,
-                    "start_line": start_line,
-                    "end_line": end_line,
-                }
-
-            # Update abscissa for the next heat exchanger
-            Q0 += Q_hot[-1]
-
-        ax.set_xlim(left=0 - Q0 / 50, right=Q0 + Q0 / 50)
+# import os
+# import time
+# import yaml
+# import copy
+# import datetime
+# import threading
+# import numpy as np
+# import pandas as pd
+# import matplotlib.pyplot as plt
+
+# from .. import pysolver_view as psv
+# from .. import utilities
+# from .. import fluid_properties as props
+
+# from . import brayton_recuperated
+# from . import brayton_split_compression
+
+# COLORS_MATLAB = utilities.COLORS_MATLAB
+# LABEL_MAPPING = {
+#     "s": "Entropy [J/kg/K]",
+#     "T": "Temperature [K]",
+#     "p": "Pressure [Pa]",
+#     "h": "Enthalpy [J/kg]",
+#     "d": "Density [kg/m$^3$]",
+#     "a": "Speed of sound [m/s]",
+#     "Z": "Compressibility factor [-]",
+#     "heat": "Heat flow rate [W]",
+# }
+
+# # Cycle configurations available
+# CYCLE_TOPOLOGIES = {
+#     "recuperated": brayton_recuperated.evaluate_cycle,
+#     "split_compression": brayton_split_compression.evaluate_cycle,
+#     "recompression": brayton_split_compression.evaluate_cycle,
+# }
+
+# GRAPHICS_PLACEHOLDER = {
+#     "process_lines": {},
+#     "state_points": {},
+#     "pinch_point_lines": {},
+# }
+
+
+# class ThermodynamicCycleProblem(psv.OptimizationProblem):
+#     """
+#     A class to represent a thermodynamic cycle optimization problem.
+
+#     This class provides functionalities to load and update the configuration for
+#     the thermodynamic cycle, and to perform interactive plotting based on the current
+#     configuration.
+
+#     Attributes
+#     ----------
+#     plot_initialized : bool
+#         Flag to indicate if the plot has been initialized.
+#     constraints : dict
+#         Dictionary holding the constraints of the problem.
+#     fixed_parameters : dict
+#         Dictionary holding the fixed parameters of the problem.
+#     design_variables : dict
+#         Dictionary holding the current values of the design variables.
+#     lower_bounds : dict
+#         Dictionary holding the lower bounds of the design variables.
+#     upper_bounds : dict
+#         Dictionary holding the upper bounds of the design variables.
+#     keys : list
+#         List of keys (names) of the design variables.
+#     x0 : np.ndarray
+#         Initial guess for the optimization problem.
+
+#     Methods
+#     -------
+#     update_config(configuration):
+#         Update the problem's configuration based on the provided dictionary.
+#     load_config_from_file(configuration_file):
+#         Load and update the problem's configuration from a specified file.
+#     plot_cycle_interactive(configuration_file, update_interval=0.20):
+#         Perform interactive plotting, updating the plot based on the configuration file.
+#     """
+
+#     def __init__(self, configuration, out_dir=None):
+#         """
+#         Constructs all the necessary attributes for the ThermodynamicCycleProblem object.
+
+#         Parameters
+#         ----------
+#         config : dict
+#             Dictionary containing the configuration for the thermodynamic cycle problem.
+#         """
+#         # Initialize variables
+#         self.figure = None
+#         self.figure_TQ = None
+#         self.configuration = copy.deepcopy(configuration)
+#         self.update_configuration(self.configuration)
+#         self.graphics = copy.deepcopy(GRAPHICS_PLACEHOLDER)
+
+#         # Create fluid object to calculate states used for the scaling of design variables
+#         self.fluid = props.Fluid(**self.fixed_parameters["working_fluid"])
+#         self._calculate_special_points()
+
+#         # Define filename with unique date-time identifier
+#         if out_dir == None:
+#             current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
+#             self.out_dir = f"results/case_{current_time}"
+
+#         # Create a directory to save simulation results
+#         if not os.path.exists(self.out_dir):
+#             os.makedirs(self.out_dir)
+
+#         # Save the initial configuration as YAML file
+#         config_data = {k: v for k, v in self.configuration.items()}
+#         config_data = utilities.convert_numpy_to_python(config_data, precision=12)
+#         config_file = os.path.join(self.out_dir, "initial_configuration.yaml")
+#         with open(config_file, "w") as file:
+#             yaml.dump(config_data, file, default_flow_style=False, sort_keys=False)
+
+#     def update_configuration(self, configuration):
+#         """
+#         Update the problem's configuration based on the provided dictionary.
+
+#         Parameters
+#         ----------
+#         config : dict
+#             Dictionary containing the new configuration for the thermodynamic cycle problem.
+#         """
+#         conf = configuration
+#         self.cycle_topology = conf["cycle_topology"]
+#         self.plot_settings = conf["plot_settings"]
+#         self.constraints = conf["constraints"]
+#         self.fixed_parameters = conf["fixed_parameters"]
+#         self.objective_function = conf["objective_function"]
+#         self.variables = {k: v["value"] for k, v in conf["design_variables"].items()}
+#         self.lower_bounds = {k: v["min"] for k, v in conf["design_variables"].items()}
+#         self.upper_bounds = {k: v["max"] for k, v in conf["design_variables"].items()}
+#         self.keys = list(self.variables.keys())
+#         self.x0 = np.asarray([var for var in self.variables.values()])
+
+#     def _calculate_special_points(self):
+#         """
+#         Calculates and stores key thermodynamic states for the fluid used in the cycle.
+
+#         This method computes two specific thermodynamic states:
+#         1. Saturated liquid state at the heat sink inlet temperature.
+#         2. Dilute gas state at the heat source inlet temperature.
+
+#         If the heat sink inlet temperature is below the fluid's critical temperature,
+#         the saturated liquid state is calculated at this temperature. If it is above
+#         the critical temperature, the state is calculated along the pseudocritical line.
+
+#         The dilute gas state is calculated at the heat source inlet temperature and at
+#         very low pressure (slightly above the triple pressure)
+
+#         The calculated states, along with the fluid's critical and triple point properties,
+#         are stored in the `fixed_parameters` dictionary under the 'fluid' key.
+
+#         These states are intended to define the bounds of the design variables specified
+#         in the YAML configuration file, for example:
+
+#             compressor_inlet_enthalpy:
+#             min: 0.9*$fluid.liquid_sink_temperature.h
+#             max: 2.0*$fluid.liquid_sink_temperature.h
+#             value: 0.2
+#             turbine_inlet_pressure:
+#             min: 0.75*$fluid.critical_point.p
+#             max: 5.00*$fluid.critical_point.p
+#             value: 0.5
+#             turbine_inlet_enthalpy:
+#             min: 1.10*$fluid.critical_point.h
+#             max: 1.00*$fluid.gas_source_temperature.h
+#             value: 0.90
+
+#         """
+#         # Compute saturated liquid state at sink temperature
+#         # Use pseudocritical line if heat sink is above critical temperature
+#         T_sink = self.fixed_parameters["heat_sink"]["inlet_temperature"]
+#         crit = self.fluid.critical_point
+#         if T_sink < crit.T:
+#             state_sat = self.fluid.set_state(props.QT_INPUTS, 0.0, T_sink)
+#         else:
+#             state_sat = self.fluid.set_state(props.DmassT_INPUTS, crit.rho, T_sink)
+
+#         # Compute dilute gas state at heat source temperature
+#         T_source = self.fixed_parameters["heat_source"]["inlet_temperature"]
+#         p_triple = 1.01 * self.fluid.triple_point_liquid.p
+#         state_dilute = self.fluid.set_state(props.PT_INPUTS, p_triple, T_source)
+
+#         # Save states in the fixed parameters dictionary
+#         self.fixed_parameters_bis = copy.deepcopy(self.fixed_parameters)
+#         self.fixed_parameters_bis["cycle_fluid"] = {
+#             "critical_point": self.fluid.critical_point.to_dict(),
+#             "triple_point_liquid": self.fluid.triple_point_liquid.to_dict(),
+#             "triple_point_vapor": self.fluid.triple_point_vapor.to_dict(),
+#             "liquid_at_heat_sink_temperature": state_sat.to_dict(),
+#             "gas_at_heat_source_temperature": state_dilute.to_dict(),
+#         }
+
+#     def load_configuration_file(self, config_file):
+#         """
+#         Load and update the problem's configuration from a specified file.
+
+#         Parameters
+#         ----------
+#         config_file : str
+#             Path to the configuration file.
+#         """
+#         config = utilities.read_configuration_file(config_file)
+#         self.update_configuration(config["problem_formulation"])
+#         self._calculate_special_points()
+
+#     def get_optimization_values(self, x):
+#         """
+#         Evaluate optimization problem
+#         """
+#         # Create dictionary from array of normalized design variables
+#         self.variables = dict(zip(self.keys, x))
+#         vars_physical = self._scale_normalized_to_physical(self.variables)
+
+#         # Evaluate thermodynamic cycle
+#         if self.cycle_topology in CYCLE_TOPOLOGIES.keys():
+#             self.cycle_data = CYCLE_TOPOLOGIES[self.cycle_topology](
+#                 vars_physical,
+#                 self.fixed_parameters,
+#                 self.constraints,
+#                 self.objective_function,
+#             )
+#         else:
+#             options = ", ".join(f"'{k}'" for k in CYCLE_TOPOLOGIES.keys())
+#             raise ValueError(
+#                 f"Invalid cycle topology: '{self.cycle_topology}'. Available options: {options}"
+#             )
+
+#         # Define objective function and constraints
+#         self.f = self.cycle_data["objective_function"]
+#         self.c_eq = self.cycle_data["equality_constraints"]
+#         self.c_ineq = self.cycle_data["inequality_constraints"]
+
+#         # Combine objective function and constraints
+#         out = self.merge_objective_and_constraints(self.f, self.c_eq, self.c_ineq)
+
+#         return out
+
+#     def get_bounds(self):
+#         bounds = []
+#         for _ in self.variables:
+#             bounds.append((0.00, 1.00))
+#         return bounds
+
+#     def get_n_eq(self):
+#         return self.get_number_of_constraints(self.c_eq)
+
+#     def get_n_ineq(self):
+#         return self.get_number_of_constraints(self.c_ineq)
+
+#     def _scale_normalized_to_physical(self, vars_normalized):
+#         # Define helper function
+#         def scale_value(min_val, max_val, normalized_val):
+#             return min_val + (max_val - min_val) * normalized_val
+
+#         # Loop over normalized variables
+#         vars_physical = {}
+#         for key, value in vars_normalized.items():
+#             try:
+#                 # Fetch the bounds expressions
+#                 lower_expr = self.lower_bounds[key]
+#                 upper_expr = self.upper_bounds[key]
+
+#                 # Try to evaluate bounds as expressions if they are strings
+#                 if isinstance(lower_expr, str):
+#                     lower = utilities.render_and_evaluate(
+#                         lower_expr, self.fixed_parameters_bis
+#                     )
+#                 else:
+#                     lower = lower_expr
+#                 if isinstance(upper_expr, str):
+#                     upper = utilities.render_and_evaluate(
+#                         upper_expr, self.fixed_parameters_bis
+#                     )
+#                 else:
+#                     upper = upper_expr
+
+#                 vars_physical[key] = scale_value(lower, upper, value)
+
+#             except (KeyError, ValueError) as e:
+#                 raise ValueError(f"Error processing bounds for '{key}': {e}")
+
+#         return vars_physical
+
+#     def to_excel(self, filename="performance.xlsx"):
+#         """
+#         Exports the cycle performance data to Excel file
+#         """
+
+#         # Define variable map
+#         variable_map = {
+#             "fluid_name": {"name": "fluid_name", "unit": "-"},
+#             "T": {"name": "temperature", "unit": "K"},
+#             "p": {"name": "pressure", "unit": "Pa"},
+#             "rho": {"name": "density", "unit": "kg/m3"},
+#             "Q": {"name": "quality", "unit": "-"},
+#             "Z": {"name": "compressibility_factor", "unit": "-"},
+#             "u": {"name": "internal_energy", "unit": "J/kg"},
+#             "h": {"name": "enthalpy", "unit": "J/kg"},
+#             "s": {"name": "entropy", "unit": "J/kg/K"},
+#             "cp": {"name": "isobaric_heat_capacity", "unit": "J/kg/K"},
+#             "cv": {"name": "isochoric_heat_capacity", "unit": "J/kg/K"},
+#             "gamma": {"name": "heat_capacity_ratio", "unit": "-"},
+#             "a": {"name": "speed_of_sound", "unit": "m/s"},
+#             "mu": {"name": "dynamic_viscosity", "unit": "Pa*s"},
+#             "k": {"name": "thermal_conductivity", "unit": "W/m/K"},
+#             "superheating": {"name": "superheating_degree", "unit": "K"},
+#             "subcooling": {"name": "subcooling_degree", "unit": "K"},
+#         }
+
+#         # Initialize a list to hold all rows of the DataFrame
+#         data_rows = []
+
+#         # Prepare the headers and units rows
+#         headers = ["state"]
+#         units_row = ["units"]
+
+#         for key in variable_map:
+#             headers.append(variable_map[key]["name"])
+#             units_row.append(variable_map[key]["unit"])
+
+#         # Iterate over each component in the dictionary
+#         for component_name, component in self.cycle_data["components"].items():
+#             if component["type"] == "heat_exchanger":
+#                 # Handle heat exchanger sides separately
+#                 for side in ["hot_side", "cold_side"]:
+#                     # Append the data for state_in and state_out to the rows list
+#                     state_in = component[side]["state_in"].to_dict()
+#                     state_out = component[side]["state_out"].to_dict()
+#                     data_rows.append(
+#                         [f"{component_name}_{side}_in"]
+#                         + [state_in.get(key, None) for key in variable_map]
+#                     )
+#                     data_rows.append(
+#                         [f"{component_name}_{side}_out"]
+#                         + [state_out.get(key, None) for key in variable_map]
+#                     )
+#             else:
+#                 # Handle non-heat exchanger components
+#                 # Append the data for state_in and state_out to the rows list
+#                 state_in = component["state_in"].to_dict()
+#                 state_out = component["state_out"].to_dict()
+#                 data_rows.append(
+#                     [f"{component_name}_in"]
+#                     + [state_in.get(key, None) for key in variable_map]
+#                 )
+#                 data_rows.append(
+#                     [f"{component_name}_out"]
+#                     + [state_out.get(key, None) for key in variable_map]
+#                 )
+
+#         # Create a DataFrame with data rows
+#         df = pd.DataFrame(data_rows, columns=headers)
+
+#         # Insert the units row
+#         df.loc[-1] = units_row  # Adding a row
+#         df.index = df.index + 1  # Shifting index
+#         df = df.sort_index()  # Sorting by index
+
+#         # # Export to Excel
+#         # df.to_excel(
+#         #     os.path.join(self.out_dir, filename),
+#         #     index=False,
+#         #     header=True,
+#         #     sheet_name="cycle_states",
+#         # )
+
+#         # Prepare energy_analysis data
+#         df_2 = pd.DataFrame(
+#             list(self.cycle_data["energy_analysis"].items()),
+#             columns=["Parameter", "Value"],
+#         )
+
+#         # Export to Excel
+#         filename = os.path.join(self.out_dir, filename)
+#         with pd.ExcelWriter(filename, engine="openpyxl") as writer:
+#             df.to_excel(writer, index=False, sheet_name="cycle_states")
+#             df_2.to_excel(writer, index=False, sheet_name="energy_analysis")
+
+#     def plot_cycle(self):
+#         """
+#         Plots or updates the thermodynamic cycle diagrams based on current settings,
+#         including the option to include a pinch point diagram.
+
+#         This function is capable of both creating new cycle diagrams and updating existing ones.
+#         It's particularly useful in dynamic scenarios such as during optimization steps,
+#         where the plot needs to be refreshed continually with new data.
+#         The method also supports real-time updates based on the latest configuration settings.
+
+#         The function first determines the number of subplots required based on the cycle diagrams
+#         specified in the 'plot_settings' attribute and whether a pinch point diagram is included.
+#         It then either initializes a new figure and axes or updates existing ones.
+#         Each thermodynamic diagram (phase diagram and cycle components) and the optional pinch point
+#         diagram are plotted or updated accordingly.
+
+#         The method ensures that the plot reflects the current state of the cycle, including any
+#         changes during optimization or adjustments to configuration settings.
+
+#         The data of the plots can be updated interactively, but the subplot objects created can
+#         only be specified upon class initialization. For example, it is not possible to switch
+#         on and off the pinch point diagram or to add new thermodynamic diagrams. The class would
+#         have to be re-initialized upon those scenarios. The reason for this choice is that re-creating
+#         a figure would be too time consuming and not practical for the real-time updating of the plots
+
+#         """
+
+#         # Determine the number of subplots
+#         include_pinch_diagram = self.plot_settings.get("pinch_point_diagram", False)
+#         ncols = len(self.plot_settings["diagrams"]) + int(include_pinch_diagram)
+#         nrows = 1
+
+#         # Initialize the figure and axes
+#         if not (self.figure and plt.fignum_exists(self.figure.number)):
+#             # Reset the graphics objects if the figure was closed
+#             self.graphics = copy.deepcopy(GRAPHICS_PLACEHOLDER)
+#             self.figure, self.axes = plt.subplots(
+#                 nrows, ncols, figsize=(5.2 * ncols, 4.8)
+#             )
+#             self.axes = utilities.ensure_iterable(self.axes)
+
+#         # Plot or update each thermodynamic_diagram
+#         for i, ax in enumerate(self.axes[:-1] if include_pinch_diagram else self.axes):
+#             plot_config = self.plot_settings["diagrams"][i]
+#             plot_config = self._get_diagram_default_settings(plot_config)
+#             self._plot_thermodynamic_diagram(ax, plot_config, i)
+
+#         # Plot or update the pinch point diagram
+#         if include_pinch_diagram:
+#             self._plot_pinch_point_diagram(self.axes[-1], ncols - 1)
+
+#         # Adjust layout and refresh plot
+#         self.figure.tight_layout(pad=2)
+#         plt.draw()
+#         plt.pause(0.05)
+
+#     def plot_cycle_realtime(self, configuration_file, update_interval=0.1):
+#         """
+#         Perform interactive plotting, updating the plot based on the configuration file.
+
+#         Parameters
+#         ----------
+#         config_file : str
+#             Path to the configuration file.
+#         update_interval : float, optional
+#             Time interval in seconds between plot updates (default is 0.1 seconds).
+
+#         """
+
+#         def wait_for_input():
+#             input()
+#             self.enter_pressed = True
+
+#         # Initialize secondary thread to wait for user input
+#         self.enter_pressed = False
+#         self.input_thread = threading.Thread(target=wait_for_input)
+#         self.input_thread.daemon = True
+#         self.input_thread.start()
+
+#         # Print instructions message
+#         print("-" * 80)
+#         print(" Creating thermodynamic cycle interactive plot")
+#         print("-" * 80)
+#         print(f" The current configuration file is: '{configuration_file}'")
+#         print(" Modify the configuration file and save it to update the plot")
+#         print(" Try to find a good initial guess for the optimization.")
+#         print(" Using a feasible initial guess improves optimization convergence.")
+#         print(" Press 'enter' to continue.")
+#         print("-" * 80)
+
+#         # Update the plot until termination signal
+#         while not self.enter_pressed:
+#             # Read the configuration file
+#             self.load_configuration_file(configuration_file)
+#             self.get_optimization_values(self.x0)
+#             self.plot_cycle()
+
+#             # Wait for the specified interval before updating again
+#             time.sleep(update_interval)
+
+#             # Exit interactive plotting when the user closes the figure
+#             if not plt.fignum_exists(self.figure.number):
+#                 break
+
+#             # Exit interactive plotting when the user presses enter
+#             if self.enter_pressed:
+#                 break
+
+#     def plot_cycle_callback(self, x, iter):
+#         """
+#         Plot and save the thermodynamic cycle at different optimization iterations
+
+#         This function is intended to be passed as callback argument to the optimization solver
+#         """
+#         # Update and plot the cycle diagram
+#         self.figure.suptitle(f"Optimization iteration: {iter:03d}", fontsize=14, y=0.90)
+#         self.plot_cycle()
+
+#         # Create a 'results' directory if it doesn't exist
+#         self.optimization_dir = os.path.join(self.out_dir, "optimization")
+#         os.makedirs(self.optimization_dir, exist_ok=True)
+
+#         # Use the solver's iteration number for the filename
+#         filename = os.path.join(self.optimization_dir, f"iteration_{iter:03d}.png")
+#         self.figure.savefig(filename)
+
+#     def _get_diagram_default_settings(self, plot_config):
+#         """
+#         Merges user-provided plot settings with default settings.
+
+#         Parameters
+#         ----------
+#         user_settings : dict
+#             A dictionary of user-defined plot settings.
+
+#         Returns
+#         -------
+#         dict
+#             A dictionary containing the merged plot settings.
+#         """
+#         default_settings = {
+#             "x_variable": "s",
+#             "y_variable": "T",
+#             "x_scale": "linear",
+#             "y_scale": "linear",
+#             "plot_saturation_line": True,
+#             "plot_critical_point": True,
+#             "plot_quality_isolines": False,
+#             "plot_pseudocritical_line": False,
+#             "plot_triple_point_vap": False,
+#             "plot_triple_point_liq": False,
+#             "plot_spinodal_line": False,
+#             # Add other default settings here
+#         }
+
+#         # Combine the global fluid settings with the "plots" settings
+#         fluid_settings = self.plot_settings.get("fluid", {})
+#         fluid_settings = {} if fluid_settings is None else fluid_settings
+#         plot_config = plot_config | fluid_settings
+
+#         # Merge with default values
+#         return default_settings | plot_config
+
+#     def _plot_thermodynamic_diagram(self, ax, plot_config, ax_index):
+#         """
+#         Plots or updates the thermodynamic diagram on a specified axes.
+
+#         This function sets up the axes properties according to the provided plot configuration and
+#         plots the phase diagram for the specified fluid. It then iterates over all the components in
+#         the cycle data. For heat exchangers, it plots or updates the processes for both the hot and cold sides.
+#         For other types of components, it plots or updates the process based on the component's data.
+
+#         The function also adjusts the plot limits if it's updating an existing plot,
+#         ensuring that the axes are scaled correctly to fit the new data.
+
+#         Parameters:
+#         ----------
+#         ax : matplotlib.axes.Axes
+#             The axes on which to plot or update the thermodynamic diagram.
+#         plot_config : dict
+#             A dictionary containing the plot settings, including variables to be plotted on the x and y axes, and scaling information.
+#         ax_index : int
+#             The index of the axes in the figure, used for identifying and updating existing plots.
+#         """
+#         # Set up axes properties
+#         ax.set_xlabel(LABEL_MAPPING[plot_config["x_variable"]])
+#         ax.set_ylabel(LABEL_MAPPING[plot_config["y_variable"]])
+#         ax.set_xscale(plot_config["x_scale"])
+#         ax.set_yscale(plot_config["y_scale"])
+
+#         # Plot phase diagram
+#         self.fluid.plot_phase_diagram(axes=ax, **plot_config)
+
+#         # Plot thermodynamic processes
+#         for name, component in self.cycle_data["components"].items():
+#             # Handle heat exchanger components in a special way
+#             if component["type"] == "heat_exchanger":
+#                 for side in ["hot_side", "cold_side"]:
+#                     self._plot_cycle_process(
+#                         name + "_" + side, plot_config, ax, ax_index=ax_index
+#                     )
+#             else:
+#                 self._plot_cycle_process(name, plot_config, ax, ax_index=ax_index)
+
+#         # Adjust plot limits if updating
+#         ax.relim(visible_only=True)
+#         ax.autoscale_view()
+
+#     def _plot_cycle_process(self, name, plot_settings, ax, ax_index=None):
+#         """
+#         Creates or updates the plot elements for a specific cycle process on a given axes.
+
+#         This method checks if the plot elements for the specified cycle process already exist on the given axes.
+#         If they exist and an axis index is provided, it updates these elements with new data. Otherwise, it creates
+#         new plot elements (lines and points) and stores them for future updates.
+
+#         Parameters:
+#         ----------
+#         name : str
+#             The name of the cycle process to plot or update.
+#         plot_settings : dict
+#             The plot settings dictionary containing settings such as x and y variables.
+#         ax : matplotlib.axes.Axes
+#             The axes on which to plot or update the cycle process.
+#         ax_index : int, optional
+#             The index of the axes in the figure, used for updating existing plots. If None, new plots are created.
+#         """
+
+#         # Retrieve component data
+#         x_data, y_data, color = self._get_process_data(
+#             name, plot_settings["x_variable"], plot_settings["y_variable"]
+#         )
+
+#         # Initialize the dictionary for this axis index if it does not exist
+#         if ax_index is not None:
+#             if ax_index not in self.graphics["process_lines"]:
+#                 self.graphics["process_lines"][ax_index] = {}
+#             if ax_index not in self.graphics["state_points"]:
+#                 self.graphics["state_points"][ax_index] = {}
+
+#         # Handle existing plot elements
+#         if ax_index is not None and name in self.graphics["process_lines"][ax_index]:
+#             if x_data is None or y_data is None:
+#                 # Hide existing plot elements if data is None
+#                 self.graphics["process_lines"][ax_index][name].set_visible(False)
+#                 self.graphics["state_points"][ax_index][name].set_visible(False)
+#             else:
+#                 # Update existing plot elements with new data
+#                 self.graphics["process_lines"][ax_index][name].set_data(x_data, y_data)
+#                 self.graphics["state_points"][ax_index][name].set_data(
+#                     [x_data[0], x_data[-1]], [y_data[0], y_data[-1]]
+#                 )
+#                 self.graphics["process_lines"][ax_index][name].set_visible(True)
+#                 self.graphics["state_points"][ax_index][name].set_visible(True)
+#         elif x_data is not None and y_data is not None:
+#             # Create new plot elements if data is not None
+#             (line,) = ax.plot(
+#                 x_data,
+#                 y_data,
+#                 linestyle="-",
+#                 linewidth=1.25,
+#                 marker="none",
+#                 markersize=4.0,
+#                 markeredgewidth=1.25,
+#                 markerfacecolor="w",
+#                 color=color,
+#                 label=name,
+#                 zorder=1,
+#             )
+#             (points,) = ax.plot(
+#                 [x_data[0], x_data[-1]],
+#                 [y_data[0], y_data[-1]],
+#                 linestyle="none",
+#                 linewidth=1.25,
+#                 marker="o",
+#                 markersize=4.0,
+#                 markeredgewidth=1.25,
+#                 markerfacecolor="w",
+#                 color=color,
+#                 zorder=2,
+#             )
+
+#             # Store the new plot elements
+#             if ax_index is not None:
+#                 self.graphics["process_lines"][ax_index][name] = line
+#                 self.graphics["state_points"][ax_index][name] = points
+
+#     def _get_process_data(self, name, prop_x, prop_y):
+#         """
+#         Retrieve thermodynamic data for a specified process of the cycle.
+
+#         Parameters:
+#         ----------
+#         name : str
+#             Name of the cycle process.
+#         prop_x : str
+#             Property name to plot on the x-axis.
+#         prop_y : str
+#             Property name to plot on the y-axis.
+
+#         Returns:
+#         -------
+#         tuple of (np.ndarray, np.ndarray, str)
+#             x_data: Array of data points for the x-axis.
+#             y_data: Array of data points for the y-axis.
+#             color: Color code for the plot.
+#         """
+
+#         # Get heat exchanger side data
+#         if "_hot_side" in name or "_cold_side" in name:
+#             # Extract the component and side from the name
+#             if "_hot_side" in name:
+#                 component_name, side_1 = name.replace("_hot_side", ""), "hot_side"
+#                 side_2 = "cold_side"
+#             else:
+#                 component_name, side_1 = name.replace("_cold_side", ""), "cold_side"
+#                 side_2 = "hot_side"
+
+#             data = self.cycle_data["components"][component_name][side_1]
+#             data_other_side = self.cycle_data["components"][component_name][side_2]
+#             is_heat_exchanger = True
+
+#         else: # Handle non-heat exchanger components
+#             data = self.cycle_data["components"][name]
+#             is_heat_exchanger = False
+
+#         # Retrieve data
+#         if data["states"]["identifier"][0] == "working_fluid":
+#             # Components of the cycle
+#             x_data = data["states"][prop_x]
+#             y_data = data["states"][prop_y]
+#         elif is_heat_exchanger and prop_y == "T" and prop_x in ["h", "s"]:
+#             # Special case for heat exchangers
+#             x_data = data_other_side["states"][prop_x]
+#             y_data = data["states"][prop_y]
+#         else:
+#             # Other cases
+#             x_data = None
+#             y_data = None
+
+#         color = data["color"]
+
+#         return x_data, y_data, color
+
+#     def _plot_pinch_point_diagram(self, ax, ax_index):
+#         """
+#         Plots or updates the pinch point diagram for the thermodynamic cycle's heat exchangers.
+
+#         This method visualizes the temperature vs. heat flow rate for each heat exchanger in the cycle.
+#         The function is capable of plotting the diagram for the first time or updating it with the latest data
+#         if called subsequently. It uses a sorted approach, beginning with the heat exchanger that has the minimum
+#         temperature on the cold side and proceeding in ascending order of temperature.
+
+#         Parameters:
+#         ----------
+#         ax : matplotlib.axes.Axes
+#             The axes on which to plot or update the pinch point diagram.
+#         ax_index : int
+#             The index of the axes in the figure, used to identify and access the specific axes
+#             for updating the existing plot elements stored in the 'graphics' attribute.
+
+#         Notes:
+#         -----
+#         The method relies on the 'graphics' attribute of the class to store and update plot elements.
+#         It handles the creation of new plot elements (lines, endpoints, vertical lines) when first called
+#         and updates these elements with new data from 'cycle_data' during subsequent calls.
+#         """
+#         ax.set_xlabel(LABEL_MAPPING["heat"])
+#         ax.set_ylabel(LABEL_MAPPING["T"])
+
+#         # Initialize the graphic object dict for this axis index if it does not exist
+#         if ax_index not in self.graphics["pinch_point_lines"]:
+#             self.graphics["pinch_point_lines"][ax_index] = {}
+
+#         # Set the axes labels
+#         ax.set_xlabel(LABEL_MAPPING["heat"])
+#         ax.set_ylabel(LABEL_MAPPING["T"])
+
+#         # Extract heat exchanger names and their minimum cold side temperatures
+#         heat_exchangers = [
+#             (name, min(component["cold_side"]["states"]["T"]))
+#             for name, component in self.cycle_data["components"].items()
+#             if component["type"] == "heat_exchanger"
+#         ]
+
+#         # Sort heat exchangers by minimum temperature on the cold side
+#         sorted_heat_exchangers = sorted(heat_exchangers, key=lambda x: x[1])
+
+#         # Loop over all heat exchangers
+#         Q0 = 0.00
+#         for HX_name, _ in sorted_heat_exchangers:
+#             component = self.cycle_data["components"][HX_name]
+#             # Hot side
+#             c_hot = component["hot_side"]["color"]
+#             props_hot = component["hot_side"]["states"]
+#             mass_flow_hot = component["hot_side"]["mass_flow"]
+#             Q_hot = (props_hot["h"] - props_hot["h"][0]) * mass_flow_hot
+#             T_hot = props_hot["T"]
+
+#             # Cold side
+#             c_cold = component["cold_side"]["color"]
+#             props_cold = component["cold_side"]["states"]
+#             mass_flow_cold = component["cold_side"]["mass_flow"]
+#             Q_cold = (props_cold["h"] - props_cold["h"][0]) * mass_flow_cold
+#             T_cold = props_cold["T"]
+
+#             # Check if the plot elements for this component already exist
+#             if HX_name in self.graphics["pinch_point_lines"][ax_index]:
+#                 # Update existing plot elements
+#                 plot_elements = self.graphics["pinch_point_lines"][ax_index][HX_name]
+#                 plot_elements["hot_line"].set_data(Q0 + Q_hot, T_hot)
+#                 plot_elements["cold_line"].set_data(Q0 + Q_cold, T_cold)
+
+#                 # Update endpoints
+#                 plot_elements["hot_start"].set_data(Q0 + Q_hot[0], T_hot[0])
+#                 plot_elements["hot_end"].set_data(Q0 + Q_hot[-1], T_hot[-1])
+#                 plot_elements["cold_start"].set_data(Q0 + Q_cold[0], T_cold[0])
+#                 plot_elements["cold_end"].set_data(Q0 + Q_cold[-1], T_cold[-1])
+
+#                 # Update vertical lines
+#                 plot_elements["start_line"].set_xdata([Q0, Q0])
+#                 plot_elements["end_line"].set_xdata([Q0 + Q_hot[-1], Q0 + Q_hot[-1]])
+#             else:
+#                 # Create new plot elements
+#                 (hot_line,) = ax.plot(Q0 + Q_hot, T_hot, color=c_hot)
+#                 (cold_line,) = ax.plot(Q0 + Q_cold, T_cold, color=c_cold)
+
+#                 # Create endpoints
+#                 param = {"marker": "o", "markersize": 4.0, "markerfacecolor": "white"}
+#                 (hot_1,) = ax.plot(Q0 + Q_hot[0], T_hot[0], color=c_hot, **param)
+#                 (hot_2,) = ax.plot(Q0 + Q_hot[-1], T_hot[-1], color=c_hot, **param)
+#                 (cold_1,) = ax.plot(Q0 + Q_cold[0], T_cold[0], color=c_cold, **param)
+#                 (cold_2,) = ax.plot(Q0 + Q_cold[-1], T_cold[-1], color=c_cold, **param)
+
+#                 # Create vertical lines
+#                 param = {"color": "black", "linestyle": "-", "linewidth": 0.75}
+#                 start_line = ax.axvline(x=Q0, zorder=1, **param)
+#                 end_line = ax.axvline(x=Q0 + Q_hot[-1], zorder=1, **param)
+
+#                 # Store new plot elements
+#                 self.graphics["pinch_point_lines"][ax_index][HX_name] = {
+#                     "hot_line": hot_line,
+#                     "cold_line": cold_line,
+#                     "hot_start": hot_1,
+#                     "hot_end": hot_2,
+#                     "cold_start": cold_1,
+#                     "cold_end": cold_2,
+#                     "start_line": start_line,
+#                     "end_line": end_line,
+#                 }
+
+#             # Update abscissa for the next heat exchanger
+#             Q0 += Q_hot[-1]
+
+#         ax.set_xlim(left=0 - Q0 / 50, right=Q0 + Q0 / 50)
```

### Comparing `turboflow-0.1.2/turboflow/utilities/file_utils.py` & `turboflow-0.1.3/turboflow/utilities/file_utils.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,422 +1,422 @@
-import os
-import re
-import yaml
-import time
-import logging
-from datetime import datetime
-import numpy as np
-
-
-def convert_numpy_to_python(data, precision=10):
-    """
-    Recursively converts numpy arrays, scalars, and other numpy types to their Python counterparts
-    and rounds numerical values to the specified precision.
-
-    Parameters:
-    - data: The numpy data to convert.
-    - precision: The decimal precision to which float values should be rounded.
-
-    Returns:
-    - The converted data with all numpy types replaced by native Python types and float values rounded.
-    """
-
-    if data is None:
-        return None
-
-    if isinstance(data, dict):
-        return {k: convert_numpy_to_python(v, precision) for k, v in data.items()}
-
-    elif isinstance(data, list):
-        return [convert_numpy_to_python(item, precision) for item in data]
-
-    elif isinstance(data, np.ndarray):
-        # If the numpy array has more than one element, it is iterable.
-        if data.ndim > 0:
-            return [convert_numpy_to_python(item, precision) for item in data.tolist()]
-        else:
-            # This handles the case of a numpy array with a single scalar value.
-            return convert_numpy_to_python(data.item(), precision)
-
-    elif isinstance(
-        data,
-        (np.integer, np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64),
-    ):
-        return int(data.item())
-
-    elif isinstance(data, (np.float_, np.float16, np.float32, np.float64)):
-        return round(float(data.item()), precision)
-
-    elif isinstance(data, np.bool_):
-        return bool(data.item())
-
-    elif isinstance(data, (np.str_, np.unicode_)):
-        return str(data.item())
-
-    # This will handle Python built-in types and other types that are not numpy.
-    elif isinstance(data, (float, int, str, bool)):
-        if isinstance(data, float):
-            return round(data, precision)
-        return data
-
-    else:
-        raise TypeError(f"Unsupported data type: {type(data)}")
-
-
-def compare_contents_or_files(file_or_content_1, file_or_content_2):
-    """
-    Compare the content of two inputs, which can be either file paths or strings.
-
-    This function accepts two arguments. Each argument can be:
-    1. A file path pointing to a file containing text content.
-    2. A string containing text content directly.
-
-    If the argument is a file path that exists, the function reads its content.
-    If the argument is a string, it's directly used for comparison.
-
-    Parameters
-    ----------
-    file_or_content1 : str
-        First input which can be a file path or string content.
-    file_or_content2 : str
-        Second input which can be a file path or string content.
-
-    Returns
-    -------
-    bool
-        True if the contents of the two inputs are identical, False otherwise.
-
-    Examples
-    --------
-    >>> content_same("path/to/file1.txt", "path/to/file2.txt")
-    True
-    >>> content_same("Hello, world!", "path/to/file_with_hello_world_content.txt")
-    True
-    >>> content_same("Hello, world!", "Goodbye, world!")
-    False
-    """
-    # If the first argument is a filepath and it exists, read its content
-    if os.path.exists(file_or_content_1):
-        with open(file_or_content_1, "r") as f1:
-            file_or_content_1 = f1.read()
-
-    # If the second argument is a filepath and it exists, read its content
-    if os.path.exists(file_or_content_2):
-        with open(file_or_content_2, "r") as f2:
-            file_or_content_2 = f2.read()
-
-    return file_or_content_1 == file_or_content_2
-
-
-def create_logger(name, path=None, use_datetime=True):
-    """
-    Creates and configures a logging object for recording logs during program execution.
-
-    Parameters
-    ----------
-    name : str
-        Name of the log file. Allows for differentiation when analyzing logs from different components or runs of a program.
-    path : str, optional
-        Specifies the directory where the log files will be saved. By default, a directory named "logs"
-        will be created in the current working directory (cwd).
-    use_datetime : bool, optional
-        Determines whether the log filename should have a unique datetime identifier appended. Default is True.
-
-    Returns
-    -------
-    logger : object
-        Configured logger object.
-
-    Notes
-    -----
-    - By default, the function sets the log level to `INFO`, which means the logger will handle
-      messages of level `INFO` and above (like `ERROR`, `WARNING`, etc.). The log entries will contain
-      the timestamp, log level, and the actual log message.
-    - When `use_datetime=True`, each log file will have a unique datetime identifier. This ensures
-      traceability, avoids overwriting previous logs, allows chronological ordering of log files, and
-      handles concurrency in multi-instance environments.
-    """
-
-    # Define logs directory if it is not provided
-    if path is None:
-        path = os.path.join(os.getcwd(), "logs")
-
-    # Create logs directory if it does not exist
-    if not os.path.exists(path):
-        os.makedirs(path)
-
-    # Define file name and path
-    if use_datetime:
-        current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
-        log_filename = os.path.join(path, f"{name}_{current_time}.log")
-    else:
-        log_filename = os.path.join(path, f"{name}.log")
-
-    # Set logger configuration
-    logging.basicConfig(
-        filename=log_filename,
-        level=logging.INFO,
-        format="%(asctime)s - %(levelname)s - %(message)s",
-    )
-
-    # Create logger object
-    logger = logging.getLogger()
-
-    return logger
-
-
-def find_latest_results_file(results_path, prefix="performance_analysis_"):
-    """
-    Retrieve all files that match the given prefix and extension .xlsx
-    """
-    files = sorted(
-        [
-            f
-            for f in os.listdir(results_path)
-            if f.startswith(prefix) and f.endswith(".xlsx")
-        ]
-    )
-
-    # Return the first item from the sorted list, which is the latest file
-    if files:
-        return os.path.join(results_path, files[-1])
-    else:
-        raise FileNotFoundError(
-            f"No Excel files found in directory '{results_path}' with prefix '{prefix}'"
-        )
-
-
-def wait_for_file(file_path, timeout=None, poll_interval=0.1):
-    """
-    Wait until the specified file is created.
-
-    This function is used to wait until a file is created.
-
-    Parameters
-    ----------
-    file_path : str
-        Path to the file to wait for.
-    timeout : float, optional
-        Maximum time to wait in seconds. If None, waits indefinitely.
-    poll_interval : int, optional
-        Time interval between checks in seconds.
-
-    Returns
-    -------
-    bool
-        True if the file was found, False otherwise (only if timeout is set).
-    """
-    start_time = time.time()
-    while True:
-        if os.path.exists(file_path):
-            return True
-        if timeout is not None and time.time() - start_time > timeout:
-            raise FileNotFoundError(f"Timeout waiting for file: {file_path}")
-        time.sleep(poll_interval)
-
-
-def add_string_to_keys(input_dict, suffix):
-    """
-    Add a suffix to each key in the input dictionary.
-
-    Parameters
-    ----------
-        input_dict (dict): The input dictionary.
-        suffix (str): The string to add to each key.
-
-    Returns
-    -------
-        dict: A new dictionary with modified keys.
-
-    Examples
-    --------
-        >>> input_dict = {'a': 1, 'b': 2, 'c': 3}
-        >>> add_string_to_keys(input_dict, '_new')
-        {'a_new': 1, 'b_new': 2, 'c_new': 3}
-    """
-    return {f"{key}{suffix}": value for key, value in input_dict.items()}
-
-
-def check_for_unused_keys(dict_in, dict_name, raise_error=False):
-    """
-    Checks for unused parameters in the given dictionaries and sub-dictionaries.
-    If unused items are found, prints them in a tree-like structure and optionally raises an error.
-
-    Parameters
-    ----------
-    dict_in : dict
-        The dictionary of parameters to check.
-    dict_name : str
-        The name of the dictionary variable.
-    raise_error : bool, optional
-        If True, raises an exception when unused items are found, otherwise just prints a warning.
-
-    Returns
-    -------
-    None
-    """
-    if not is_dict_empty(dict_in):
-        dict_str = print_dict(dict_in, indent=1, return_output=True)
-
-        if raise_error:
-            raise ValueError(
-                f"Dictionary '{dict_name}' is not empty. Contains unused items:\n{dict_str}"
-            )
-        else:
-            print(f"Warning: Dictionary '{dict_name}' contains unused items.")
-            print(dict_str)
-
-
-def is_dict_empty(data):
-    """
-    Recursively checks if a dictionary and all its sub-dictionaries are empty.
-
-    Parameters
-    ----------
-    data : dict
-        The dictionary to check.
-
-    Returns
-    -------
-    bool
-        True if the dictionary and all sub-dictionaries are empty, False otherwise.
-    """
-    if isinstance(data, dict):
-        return all(is_dict_empty(v) for v in data.values()) if data else True
-    return False  # Not a dictionary
-
-
-def print_dict(data, indent=0, return_output=False):
-    """
-    Recursively prints nested dictionaries with indentation or returns the formatted string.
-
-    Parameters
-    ----------
-    data : dict
-        The dictionary to print.
-    indent : int, optional
-        The initial level of indentation for the keys of the dictionary, by default 0.
-    return_output : bool, optional
-        If True, returns the formatted string instead of printing it.
-
-    Returns
-    -------
-    str or None
-        The formatted string representation of the dictionary if return_output is True, otherwise None.
-    """
-    lines = []
-    for key, value in data.items():
-        line = "    " * indent + str(key) + ": "
-        if isinstance(value, dict):
-            if value:
-                lines.append(line)
-                lines.append(print_dict(value, indent + 1, True))
-            else:
-                lines.append(line + "{}")
-        else:
-            lines.append(line + str(value))
-
-    output = "\n".join(lines)
-    if return_output:
-        return output
-    else:
-        print(output)
-
-
-def print_object(obj):
-    """
-    Prints all attributes and methods of an object, sorted alphabetically.
-
-    - Methods are identified as callable and printed with the prefix 'Method: '.
-    - Attributes are identified as non-callable and printed with the prefix 'Attribute: '.
-
-    Parameters
-    ----------
-    obj : object
-        The object whose attributes and methods are to be printed.
-
-    Returns
-    -------
-    None
-        This function does not return any value. It prints the attributes and methods of the given object.
-
-    """
-    # Retrieve all attributes and methods
-    attributes = dir(obj)
-
-    # Sort them alphabetically, case-insensitive
-    sorted_attributes = sorted(attributes, key=lambda x: x.lower())
-
-    # Iterate over sorted attributes
-    for attr in sorted_attributes:
-        # Retrieve the attribute or method from the object
-        attribute_or_method = getattr(obj, attr)
-
-        # Check if it is callable (method) or not (attribute)
-        if callable(attribute_or_method):
-            print(f"Method: {attr}")
-        else:
-            print(f"Attribute: {attr}")
-
-
-def validate_keys(checked_dict, required_keys, allowed_keys=None):
-    """
-    Validate the presence of required keys and check for any unexpected keys in a dictionary.
-
-    Give required keys and allowed keys to have complete control
-    Give required keys twice to check that the list of keys is necessary and sufficient
-    Give only required keys to allow all extra additional key
-
-    Parameters
-    ----------
-    checked_dict : dict
-        The dictionary to be checked.
-    required_keys : set
-        A set of keys that are required in the dictionary.
-    allowed_keys : set
-        A set of keys that are allowed in the dictionary.
-
-    Raises
-    ------
-    ConfigurationError
-        If either required keys are missing or unexpected keys are found.
-    """
-
-    # Convert input lists to sets for set operations
-    checked_keys = set(checked_dict.keys())
-    required_keys = set(required_keys)
-
-    # Set allowed_keys to all present keys if not provided
-    if allowed_keys is None:
-        allowed_keys = checked_keys
-    else:
-        allowed_keys = set(allowed_keys)
-
-    # Check for extra and missing keys
-    missing_keys = required_keys - checked_keys
-    extra_keys = checked_keys - allowed_keys
-
-    # Prepare error messages
-    error_messages = []
-    if missing_keys:
-        error_messages.append(f"Missing required keys: {missing_keys}")
-    if extra_keys:
-        error_messages.append(f"Found unexpected keys: {extra_keys}")
-
-    # Raise combined error if there are any issues
-    if error_messages:
-        raise DictionaryValidationError("; ".join(error_messages))
-
-
-class DictionaryValidationError(Exception):
-    """Exception raised for errors in the configuration options."""
-
-    def __init__(self, message, key=None, value=None):
-        self.message = message
-        self.key = key
-        self.value = value
-        super().__init__(self._format_message())
-
-    def _format_message(self):
-        if self.key is not None and self.value is not None:
-            return f"{self.message} Key: '{self.key}', Value: {self.value}"
-        return self.message
+import os
+import re
+import yaml
+import time
+import logging
+from datetime import datetime
+import numpy as np
+
+
+def convert_numpy_to_python(data, precision=10):
+    """
+    Recursively converts numpy arrays, scalars, and other numpy types to their Python counterparts
+    and rounds numerical values to the specified precision.
+
+    Parameters:
+    - data: The numpy data to convert.
+    - precision: The decimal precision to which float values should be rounded.
+
+    Returns:
+    - The converted data with all numpy types replaced by native Python types and float values rounded.
+    """
+
+    if data is None:
+        return None
+
+    if isinstance(data, dict):
+        return {k: convert_numpy_to_python(v, precision) for k, v in data.items()}
+
+    elif isinstance(data, list):
+        return [convert_numpy_to_python(item, precision) for item in data]
+
+    elif isinstance(data, np.ndarray):
+        # If the numpy array has more than one element, it is iterable.
+        if data.ndim > 0:
+            return [convert_numpy_to_python(item, precision) for item in data.tolist()]
+        else:
+            # This handles the case of a numpy array with a single scalar value.
+            return convert_numpy_to_python(data.item(), precision)
+
+    elif isinstance(
+        data,
+        (np.integer, np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64),
+    ):
+        return int(data.item())
+
+    elif isinstance(data, (np.float_, np.float16, np.float32, np.float64)):
+        return round(float(data.item()), precision)
+
+    elif isinstance(data, np.bool_):
+        return bool(data.item())
+
+    elif isinstance(data, (np.str_, np.unicode_)):
+        return str(data.item())
+
+    # This will handle Python built-in types and other types that are not numpy.
+    elif isinstance(data, (float, int, str, bool)):
+        if isinstance(data, float):
+            return round(data, precision)
+        return data
+
+    else:
+        raise TypeError(f"Unsupported data type: {type(data)}")
+
+
+def compare_contents_or_files(file_or_content_1, file_or_content_2):
+    """
+    Compare the content of two inputs, which can be either file paths or strings.
+
+    This function accepts two arguments. Each argument can be:
+    1. A file path pointing to a file containing text content.
+    2. A string containing text content directly.
+
+    If the argument is a file path that exists, the function reads its content.
+    If the argument is a string, it's directly used for comparison.
+
+    Parameters
+    ----------
+    file_or_content1 : str
+        First input which can be a file path or string content.
+    file_or_content2 : str
+        Second input which can be a file path or string content.
+
+    Returns
+    -------
+    bool
+        True if the contents of the two inputs are identical, False otherwise.
+
+    Examples
+    --------
+    >>> content_same("path/to/file1.txt", "path/to/file2.txt")
+    True
+    >>> content_same("Hello, world!", "path/to/file_with_hello_world_content.txt")
+    True
+    >>> content_same("Hello, world!", "Goodbye, world!")
+    False
+    """
+    # If the first argument is a filepath and it exists, read its content
+    if os.path.exists(file_or_content_1):
+        with open(file_or_content_1, "r") as f1:
+            file_or_content_1 = f1.read()
+
+    # If the second argument is a filepath and it exists, read its content
+    if os.path.exists(file_or_content_2):
+        with open(file_or_content_2, "r") as f2:
+            file_or_content_2 = f2.read()
+
+    return file_or_content_1 == file_or_content_2
+
+
+def create_logger(name, path=None, use_datetime=True):
+    """
+    Creates and configures a logging object for recording logs during program execution.
+
+    Parameters
+    ----------
+    name : str
+        Name of the log file. Allows for differentiation when analyzing logs from different components or runs of a program.
+    path : str, optional
+        Specifies the directory where the log files will be saved. By default, a directory named "logs"
+        will be created in the current working directory (cwd).
+    use_datetime : bool, optional
+        Determines whether the log filename should have a unique datetime identifier appended. Default is True.
+
+    Returns
+    -------
+    logger : object
+        Configured logger object.
+
+    Notes
+    -----
+    - By default, the function sets the log level to `INFO`, which means the logger will handle
+      messages of level `INFO` and above (like `ERROR`, `WARNING`, etc.). The log entries will contain
+      the timestamp, log level, and the actual log message.
+    - When `use_datetime=True`, each log file will have a unique datetime identifier. This ensures
+      traceability, avoids overwriting previous logs, allows chronological ordering of log files, and
+      handles concurrency in multi-instance environments.
+    """
+
+    # Define logs directory if it is not provided
+    if path is None:
+        path = os.path.join(os.getcwd(), "logs")
+
+    # Create logs directory if it does not exist
+    if not os.path.exists(path):
+        os.makedirs(path)
+
+    # Define file name and path
+    if use_datetime:
+        current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
+        log_filename = os.path.join(path, f"{name}_{current_time}.log")
+    else:
+        log_filename = os.path.join(path, f"{name}.log")
+
+    # Set logger configuration
+    logging.basicConfig(
+        filename=log_filename,
+        level=logging.INFO,
+        format="%(asctime)s - %(levelname)s - %(message)s",
+    )
+
+    # Create logger object
+    logger = logging.getLogger()
+
+    return logger
+
+
+def find_latest_results_file(results_path, prefix="performance_analysis_"):
+    """
+    Retrieve all files that match the given prefix and extension .xlsx
+    """
+    files = sorted(
+        [
+            f
+            for f in os.listdir(results_path)
+            if f.startswith(prefix) and f.endswith(".xlsx")
+        ]
+    )
+
+    # Return the first item from the sorted list, which is the latest file
+    if files:
+        return os.path.join(results_path, files[-1])
+    else:
+        raise FileNotFoundError(
+            f"No Excel files found in directory '{results_path}' with prefix '{prefix}'"
+        )
+
+
+def wait_for_file(file_path, timeout=None, poll_interval=0.1):
+    """
+    Wait until the specified file is created.
+
+    This function is used to wait until a file is created.
+
+    Parameters
+    ----------
+    file_path : str
+        Path to the file to wait for.
+    timeout : float, optional
+        Maximum time to wait in seconds. If None, waits indefinitely.
+    poll_interval : int, optional
+        Time interval between checks in seconds.
+
+    Returns
+    -------
+    bool
+        True if the file was found, False otherwise (only if timeout is set).
+    """
+    start_time = time.time()
+    while True:
+        if os.path.exists(file_path):
+            return True
+        if timeout is not None and time.time() - start_time > timeout:
+            raise FileNotFoundError(f"Timeout waiting for file: {file_path}")
+        time.sleep(poll_interval)
+
+
+def add_string_to_keys(input_dict, suffix):
+    """
+    Add a suffix to each key in the input dictionary.
+
+    Parameters
+    ----------
+        input_dict (dict): The input dictionary.
+        suffix (str): The string to add to each key.
+
+    Returns
+    -------
+        dict: A new dictionary with modified keys.
+
+    Examples
+    --------
+        >>> input_dict = {'a': 1, 'b': 2, 'c': 3}
+        >>> add_string_to_keys(input_dict, '_new')
+        {'a_new': 1, 'b_new': 2, 'c_new': 3}
+    """
+    return {f"{key}{suffix}": value for key, value in input_dict.items()}
+
+
+def check_for_unused_keys(dict_in, dict_name, raise_error=False):
+    """
+    Checks for unused parameters in the given dictionaries and sub-dictionaries.
+    If unused items are found, prints them in a tree-like structure and optionally raises an error.
+
+    Parameters
+    ----------
+    dict_in : dict
+        The dictionary of parameters to check.
+    dict_name : str
+        The name of the dictionary variable.
+    raise_error : bool, optional
+        If True, raises an exception when unused items are found, otherwise just prints a warning.
+
+    Returns
+    -------
+    None
+    """
+    if not is_dict_empty(dict_in):
+        dict_str = print_dict(dict_in, indent=1, return_output=True)
+
+        if raise_error:
+            raise ValueError(
+                f"Dictionary '{dict_name}' is not empty. Contains unused items:\n{dict_str}"
+            )
+        else:
+            print(f"Warning: Dictionary '{dict_name}' contains unused items.")
+            print(dict_str)
+
+
+def is_dict_empty(data):
+    """
+    Recursively checks if a dictionary and all its sub-dictionaries are empty.
+
+    Parameters
+    ----------
+    data : dict
+        The dictionary to check.
+
+    Returns
+    -------
+    bool
+        True if the dictionary and all sub-dictionaries are empty, False otherwise.
+    """
+    if isinstance(data, dict):
+        return all(is_dict_empty(v) for v in data.values()) if data else True
+    return False  # Not a dictionary
+
+
+def print_dict(data, indent=0, return_output=False):
+    """
+    Recursively prints nested dictionaries with indentation or returns the formatted string.
+
+    Parameters
+    ----------
+    data : dict
+        The dictionary to print.
+    indent : int, optional
+        The initial level of indentation for the keys of the dictionary, by default 0.
+    return_output : bool, optional
+        If True, returns the formatted string instead of printing it.
+
+    Returns
+    -------
+    str or None
+        The formatted string representation of the dictionary if return_output is True, otherwise None.
+    """
+    lines = []
+    for key, value in data.items():
+        line = "    " * indent + str(key) + ": "
+        if isinstance(value, dict):
+            if value:
+                lines.append(line)
+                lines.append(print_dict(value, indent + 1, True))
+            else:
+                lines.append(line + "{}")
+        else:
+            lines.append(line + str(value))
+
+    output = "\n".join(lines)
+    if return_output:
+        return output
+    else:
+        print(output)
+
+
+def print_object(obj):
+    """
+    Prints all attributes and methods of an object, sorted alphabetically.
+
+    - Methods are identified as callable and printed with the prefix 'Method: '.
+    - Attributes are identified as non-callable and printed with the prefix 'Attribute: '.
+
+    Parameters
+    ----------
+    obj : object
+        The object whose attributes and methods are to be printed.
+
+    Returns
+    -------
+    None
+        This function does not return any value. It prints the attributes and methods of the given object.
+
+    """
+    # Retrieve all attributes and methods
+    attributes = dir(obj)
+
+    # Sort them alphabetically, case-insensitive
+    sorted_attributes = sorted(attributes, key=lambda x: x.lower())
+
+    # Iterate over sorted attributes
+    for attr in sorted_attributes:
+        # Retrieve the attribute or method from the object
+        attribute_or_method = getattr(obj, attr)
+
+        # Check if it is callable (method) or not (attribute)
+        if callable(attribute_or_method):
+            print(f"Method: {attr}")
+        else:
+            print(f"Attribute: {attr}")
+
+
+def validate_keys(checked_dict, required_keys, allowed_keys=None):
+    """
+    Validate the presence of required keys and check for any unexpected keys in a dictionary.
+
+    Give required keys and allowed keys to have complete control
+    Give required keys twice to check that the list of keys is necessary and sufficient
+    Give only required keys to allow all extra additional key
+
+    Parameters
+    ----------
+    checked_dict : dict
+        The dictionary to be checked.
+    required_keys : set
+        A set of keys that are required in the dictionary.
+    allowed_keys : set
+        A set of keys that are allowed in the dictionary.
+
+    Raises
+    ------
+    ConfigurationError
+        If either required keys are missing or unexpected keys are found.
+    """
+
+    # Convert input lists to sets for set operations
+    checked_keys = set(checked_dict.keys())
+    required_keys = set(required_keys)
+
+    # Set allowed_keys to all present keys if not provided
+    if allowed_keys is None:
+        allowed_keys = checked_keys
+    else:
+        allowed_keys = set(allowed_keys)
+
+    # Check for extra and missing keys
+    missing_keys = required_keys - checked_keys
+    extra_keys = checked_keys - allowed_keys
+
+    # Prepare error messages
+    error_messages = []
+    if missing_keys:
+        error_messages.append(f"Missing required keys: {missing_keys}")
+    if extra_keys:
+        error_messages.append(f"Found unexpected keys: {extra_keys}")
+
+    # Raise combined error if there are any issues
+    if error_messages:
+        raise DictionaryValidationError("; ".join(error_messages))
+
+
+class DictionaryValidationError(Exception):
+    """Exception raised for errors in the configuration options."""
+
+    def __init__(self, message, key=None, value=None):
+        self.message = message
+        self.key = key
+        self.value = value
+        super().__init__(self._format_message())
+
+    def _format_message(self):
+        if self.key is not None and self.value is not None:
+            return f"{self.message} Key: '{self.key}', Value: {self.value}"
+        return self.message
```

### Comparing `turboflow-0.1.2/turboflow/utilities/numerics.py` & `turboflow-0.1.3/turboflow/utilities/numerics.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,233 +1,233 @@
-import re
-import numbers
-import numpy as np
-import pandas as pd
-
-from collections.abc import Iterable
-
-
-def is_float(element: any) -> bool:
-    """
-    Check if the given element can be converted to a float.
-
-    Parameters
-    ----------
-    element : any
-        The element to be checked.
-
-    Returns
-    -------
-    bool
-        True if the element can be converted to a float, False otherwise.
-    """
-
-    if element is None:
-        return False
-    try:
-        float(element)
-        return True
-    except ValueError:
-        return False
-
-
-def is_numeric(value):
-    """
-    Check if a value is a numeric type, including both Python and NumPy numeric types.
-
-    This function checks if the given value is a numeric type (int, float, complex)
-    in the Python standard library or NumPy, while explicitly excluding boolean types.
-
-    Parameters
-    ----------
-    value : any type
-        The value to be checked for being a numeric type.
-
-    Returns
-    -------
-    bool
-        Returns True if the value is a numeric type (excluding booleans),
-        otherwise False.
-    """
-    if isinstance(value, numbers.Number) and not isinstance(value, bool):
-        return True
-    if isinstance(value, (np.int_, np.float_, np.complex_)):
-        return True
-    if isinstance(value, np.ndarray):
-        return np.issubdtype(value.dtype, np.number) and not np.issubdtype(
-            value.dtype, np.bool_
-        )
-    return False
-
-
-def extract_timestamp(filename):
-    """
-    Extract the timestamp from the filename.
-
-    Parameters
-    ----------
-    filename : str
-        The filename containing the timestamp.
-
-    Returns
-    -------
-    str
-        The extracted timestamp.
-    """
-    # Regular expression to match the timestamp pattern in the filename
-    match = re.search(r"\d{4}-\d{2}-\d{2}_\d{2}-\d{2}-\d{2}", filename)
-    if match:
-        return match.group(0)
-    return ""
-
-
-def isclose_significant_digits(a, b, significant_digits):
-    """
-    Check if two floating-point numbers are close based on a specified number of significant digits.
-
-    Parameters
-    ----------
-    a : float
-        The first number to compare.
-    b : float
-        The second number to compare.
-    sig_digits : int
-        The number of significant digits to use for the comparison.
-
-    Returns
-    -------
-    bool
-        True if numbers are close up to the specified significant digits, False otherwise.
-    """
-    format_spec = f".{significant_digits - 1}e"
-    return format(a, format_spec) == format(b, format_spec)
-
-
-def fill_array_with_increment(n):
-    """
-    Fill an array of length `n` with values that sum to 1,
-    where each value is different but has the same increment
-    between neighboring values.
-
-    Parameters
-    ----------
-    n : int
-        Length of the array.
-
-    Returns
-    -------
-    array : ndarray
-        Array of length 'n' filled with values incrementing
-        by a constant factor, resulting in a sum of 1.
-    """
-
-    if n <= 0:
-        return []
-
-    increment = 1.0 / (n + 1)
-    array = [increment * (i + 1) for i in range(n)]
-    array /= np.sum(array)
-
-    return array
-
-
-def ensure_iterable(obj):
-    """
-    Ensure that an object is iterable. If the object is already an iterable
-    (except for strings, which are not treated as iterables in this context),
-    it will be returned as is. If the object is not an iterable, or if it is
-    a string, it will be placed into a list to make it iterable.
-
-    Parameters
-    ----------
-    obj : any type
-        The object to be checked and possibly converted into an iterable.
-
-    Returns
-    -------
-    Iterable
-        The original object if it is an iterable (and not a string), or a new
-        list containing the object if it was not iterable or was a string.
-
-    Examples
-    --------
-    >>> ensure_iterable([1, 2, 3])
-    [1, 2, 3]
-    >>> ensure_iterable('abc')
-    ['abc']
-    >>> ensure_iterable(42)
-    [42]
-    >>> ensure_iterable(np.array([1, 2, 3]))
-    array([1, 2, 3])
-    """
-    if isinstance(obj, Iterable) and not isinstance(obj, str):
-        return obj
-    else:
-        return [obj]
-
-
-def flatten_dataframe(df: pd.DataFrame) -> pd.DataFrame:
-    """
-    Convert a DataFrame with multiple rows and columns into a single-row DataFrame with each column
-    renamed to include the original row index as a suffix.
-
-    Parameters:
-    - df (pd.DataFrame): The original DataFrame to be flattened.
-
-    Returns:
-    - pd.DataFrame: A single-row DataFrame with (nm) columns.
-
-    Example usage:
-    >>> original_df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})
-    >>> flattened_df = flatten_dataframe(original_df)
-    >>> print(flattened_df)
-
-    Note: Row indices start at 1 for the suffix in the column names.
-    """
-
-    # Stack the DataFrame to create a MultiIndex Series
-    stacked_series = df.stack()
-
-    # Create new column names by combining the original column names with their row index
-    new_index = [f"{var}_{index+1}" for index, var in stacked_series.index]
-
-    # Assign the new index to the stacked Series
-    stacked_series.index = new_index
-
-    # Convert the Series back to a DataFrame and transpose it to get a single row
-    single_row_df = stacked_series.to_frame().T
-
-    return single_row_df
-
-
-def check_lists_match(list1, list2):
-    """
-    Check if two lists contain the exact same elements, regardless of their order.
-
-    Parameters
-    ----------
-    list1 : list
-        The first list for comparison.
-    list2 : list
-        The second list for comparison.
-
-    Returns
-    -------
-    bool
-        Returns True if the lists contain the exact same elements, regardless of their order.
-        Returns False otherwise.
-
-    Examples
-    --------
-    >>> check_lists_match([1, 2, 3], [3, 2, 1])
-    True
-
-    >>> check_lists_match([1, 2, 3], [4, 5, 6])
-    False
-
-    """
-    # Convert the lists to sets to ignore order
-    list1_set = set(list1)
-    list2_set = set(list2)
-
-    # Check if the sets are equal (contain the same elements)
-    return list1_set == list2_set
+import re
+import numbers
+import numpy as np
+import pandas as pd
+
+from collections.abc import Iterable
+
+
+def is_float(element: any) -> bool:
+    """
+    Check if the given element can be converted to a float.
+
+    Parameters
+    ----------
+    element : any
+        The element to be checked.
+
+    Returns
+    -------
+    bool
+        True if the element can be converted to a float, False otherwise.
+    """
+
+    if element is None:
+        return False
+    try:
+        float(element)
+        return True
+    except ValueError:
+        return False
+
+
+def is_numeric(value):
+    """
+    Check if a value is a numeric type, including both Python and NumPy numeric types.
+
+    This function checks if the given value is a numeric type (int, float, complex)
+    in the Python standard library or NumPy, while explicitly excluding boolean types.
+
+    Parameters
+    ----------
+    value : any type
+        The value to be checked for being a numeric type.
+
+    Returns
+    -------
+    bool
+        Returns True if the value is a numeric type (excluding booleans),
+        otherwise False.
+    """
+    if isinstance(value, numbers.Number) and not isinstance(value, bool):
+        return True
+    if isinstance(value, (np.int_, np.float_, np.complex_)):
+        return True
+    if isinstance(value, np.ndarray):
+        return np.issubdtype(value.dtype, np.number) and not np.issubdtype(
+            value.dtype, np.bool_
+        )
+    return False
+
+
+def extract_timestamp(filename):
+    """
+    Extract the timestamp from the filename.
+
+    Parameters
+    ----------
+    filename : str
+        The filename containing the timestamp.
+
+    Returns
+    -------
+    str
+        The extracted timestamp.
+    """
+    # Regular expression to match the timestamp pattern in the filename
+    match = re.search(r"\d{4}-\d{2}-\d{2}_\d{2}-\d{2}-\d{2}", filename)
+    if match:
+        return match.group(0)
+    return ""
+
+
+def isclose_significant_digits(a, b, significant_digits):
+    """
+    Check if two floating-point numbers are close based on a specified number of significant digits.
+
+    Parameters
+    ----------
+    a : float
+        The first number to compare.
+    b : float
+        The second number to compare.
+    sig_digits : int
+        The number of significant digits to use for the comparison.
+
+    Returns
+    -------
+    bool
+        True if numbers are close up to the specified significant digits, False otherwise.
+    """
+    format_spec = f".{significant_digits - 1}e"
+    return format(a, format_spec) == format(b, format_spec)
+
+
+def fill_array_with_increment(n):
+    """
+    Fill an array of length `n` with values that sum to 1,
+    where each value is different but has the same increment
+    between neighboring values.
+
+    Parameters
+    ----------
+    n : int
+        Length of the array.
+
+    Returns
+    -------
+    array : ndarray
+        Array of length 'n' filled with values incrementing
+        by a constant factor, resulting in a sum of 1.
+    """
+
+    if n <= 0:
+        return []
+
+    increment = 1.0 / (n + 1)
+    array = [increment * (i + 1) for i in range(n)]
+    array /= np.sum(array)
+
+    return array
+
+
+def ensure_iterable(obj):
+    """
+    Ensure that an object is iterable. If the object is already an iterable
+    (except for strings, which are not treated as iterables in this context),
+    it will be returned as is. If the object is not an iterable, or if it is
+    a string, it will be placed into a list to make it iterable.
+
+    Parameters
+    ----------
+    obj : any type
+        The object to be checked and possibly converted into an iterable.
+
+    Returns
+    -------
+    Iterable
+        The original object if it is an iterable (and not a string), or a new
+        list containing the object if it was not iterable or was a string.
+
+    Examples
+    --------
+    >>> ensure_iterable([1, 2, 3])
+    [1, 2, 3]
+    >>> ensure_iterable('abc')
+    ['abc']
+    >>> ensure_iterable(42)
+    [42]
+    >>> ensure_iterable(np.array([1, 2, 3]))
+    array([1, 2, 3])
+    """
+    if isinstance(obj, Iterable) and not isinstance(obj, str):
+        return obj
+    else:
+        return [obj]
+
+
+def flatten_dataframe(df: pd.DataFrame) -> pd.DataFrame:
+    """
+    Convert a DataFrame with multiple rows and columns into a single-row DataFrame with each column
+    renamed to include the original row index as a suffix.
+
+    Parameters:
+    - df (pd.DataFrame): The original DataFrame to be flattened.
+
+    Returns:
+    - pd.DataFrame: A single-row DataFrame with (nm) columns.
+
+    Example usage:
+    >>> original_df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})
+    >>> flattened_df = flatten_dataframe(original_df)
+    >>> print(flattened_df)
+
+    Note: Row indices start at 1 for the suffix in the column names.
+    """
+
+    # Stack the DataFrame to create a MultiIndex Series
+    stacked_series = df.stack()
+
+    # Create new column names by combining the original column names with their row index
+    new_index = [f"{var}_{index+1}" for index, var in stacked_series.index]
+
+    # Assign the new index to the stacked Series
+    stacked_series.index = new_index
+
+    # Convert the Series back to a DataFrame and transpose it to get a single row
+    single_row_df = stacked_series.to_frame().T
+
+    return single_row_df
+
+
+def check_lists_match(list1, list2):
+    """
+    Check if two lists contain the exact same elements, regardless of their order.
+
+    Parameters
+    ----------
+    list1 : list
+        The first list for comparison.
+    list2 : list
+        The second list for comparison.
+
+    Returns
+    -------
+    bool
+        Returns True if the lists contain the exact same elements, regardless of their order.
+        Returns False otherwise.
+
+    Examples
+    --------
+    >>> check_lists_match([1, 2, 3], [3, 2, 1])
+    True
+
+    >>> check_lists_match([1, 2, 3], [4, 5, 6])
+    False
+
+    """
+    # Convert the lists to sets to ignore order
+    list1_set = set(list1)
+    list2_set = set(list2)
+
+    # Check if the sets are equal (contain the same elements)
+    return list1_set == list2_set
```

### Comparing `turboflow-0.1.2/turboflow/utilities/optimization_utils.py` & `turboflow-0.1.3/turboflow/utilities/optimization_utils.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,275 +1,275 @@
-import re
-import yaml
-import numbers
-import numpy as np
-
-from . import numerics as num
-
-
-# TODO: @Lasse: We need some of these functions for the cycle optimization, and eventually also for turbomachinery optimization
-# TODO: We have to discuss how to handle "rendered" variables
-
-
-def render_and_evaluate(expression, data):
-    """
-    Render variables prefixed with '$' in an expression and evaluate the resulting expression.
-
-    This function processes an input string `expr`, identifying all occurrences of variables
-    indicated by a leading '$' symbol. Each such variable is resolved to its value from the
-    provided `context` dictionary. The expression with all variables resolved is then evaluated
-    and the result is returned.
-
-    This function is useful to render strings defined in a YAML configuration file to values
-    that are calculated within the code and stored in a dicitonary.
-
-    Parameters
-    ----------
-    expr : str
-        The expression string containing variables to be rendered. Variables in the
-        expression are expected to be prefixed with a '$' symbol.
-    data : dict
-        A dictionary containing variables and their corresponding values. These variables
-        are used to render values in the expression.
-
-    Returns
-    -------
-    The result of evaluating the rendered expression. The type of the result depends on the
-    expression.
-
-    Notes
-    -----
-    - `pattern`: A regular expression pattern used to identify variables within the expression.
-      Variables are expected to be in the format `$variableName`, potentially with dot-separated
-      sub-properties (e.g., `$variable.property`).
-
-    - `replace_with_value`: An inner function that takes a regex match object and returns
-      the value of the variable from `context`. `match.group(1)` returns the first captured
-      group from the matched text, which in this case is the variable name excluding the
-      leading '$' symbol. For example, in `$variableName`, `match.group(1)` would return
-      `variableName`.
-
-    - The function uses Python's `eval` for evaluation, which should be used cautiously as
-      it can execute arbitrary code. Ensure that the context and expressions are from a trusted
-      source.
-    """
-    # Pattern to find $variable expressions
-    pattern = re.compile(r"\$(\w+(\.\w+)*)")
-
-    # Function to replace each match with its resolved value
-    def replace_with_value(match):
-        nested_key = match.group(1)
-        try:
-            value = render_nested_value(nested_key, data)
-            if isinstance(value, np.ndarray):
-                return "np.array(" + repr(value.tolist()) + ")"
-            else:
-                return repr(value)
-        except KeyError:
-            raise KeyError(
-                f"Variable '{nested_key}' not found in the provided data context."
-            )
-
-    try:
-        # Replace all $variable with their actual values
-        resolved_expr = pattern.sub(replace_with_value, expression)
-
-        # Check if any unresolved variables remain
-        if "$" in resolved_expr:
-            raise ValueError(f"Unresolved variable in expression: '{resolved_expr}'")
-
-        # Now evaluate the expression
-        return eval(resolved_expr)
-
-    except SyntaxError as e:
-        raise SyntaxError(f"Syntax error in '{expression}': {e}")
-    except Exception as e:
-        # Enhanced error message
-        raise TypeError(
-            f"Error evaluating expression '{expression}': {e}.\n"
-            "If the expression is meant to use data from the configuration, "
-            "ensure each variable is prefixed with '$'. For example, use '$variable_name' "
-            "instead of 'variable_name'."
-        )
-
-
-def render_nested_value(nested_key, data):
-    """
-    Retrieves a value from a nested structure (dictionaries or objects with attributes) using a dot-separated key.
-
-    This function is designed to navigate through a combination of dictionaries and objects. For an object to be
-    compatible with this function, it must implement a `keys()` method that returns its attribute names.
-
-    This function is intended as a subroutine of the more genera ``render_expression``
-
-    Parameters
-    ----------
-    nested_key : str
-        A dot-separated key string that specifies the path in the structure.
-        For example, 'level1.level2.key' will retrieve data['level1']['level2']['key'] if data is a dictionary,
-        or data.level1.level2.key if data is an object or a combination of dictionaries and objects.
-
-    data : dict or object
-        The starting dictionary or object from which to retrieve the value. This can be a nested structure
-        of dictionaries and objects.
-
-    Returns
-    -------
-    value
-        The value retrieved from the nested structure using the specified key.
-        The type of the value depends on what is stored at the specified key in the structure.
-
-    Raises
-    ------
-    KeyError
-        If the specified nested key is not found in the data structure. The error message includes the part
-        of the path that was successfully traversed and the available keys or attributes at the last valid level.
-    """
-    keys = nested_key.split(".")
-    value = data
-    traversed_path = []
-
-    for key in keys:
-        if isinstance(value, dict):
-            # Handle dictionary-like objects
-            if key in value:
-                traversed_path.append(key)
-                value = value[key]
-            else:
-                valid_keys = ", ".join(value.keys())
-                traversed_path_str = (
-                    ".".join(traversed_path) if traversed_path else "root"
-                )
-                raise KeyError(
-                    f"Nested key '{key}' not found at '{traversed_path_str}'. Available keys: {valid_keys}"
-                )
-        elif hasattr(value, key):
-            # Handle objects with attributes
-            traversed_path.append(key)
-            value = getattr(value, key)
-        else:
-            traversed_path_str = ".".join(traversed_path)
-            available_keys = ", ".join(value.keys())
-            raise KeyError(
-                f"Key '{key}' not found in object at '{traversed_path_str}'. Available keys: {available_keys}"
-            )
-
-    if not num.is_numeric(value):
-        raise ValueError(
-            f"The key '{nested_key}' is not numeric. Key value is: {value}"
-        )
-
-    return value
-
-
-def evaluate_constraints(data, constraints):
-    """
-    Evaluates the constraints based on the provided data and constraint definitions.
-
-    Parameters
-    ----------
-    data : dict
-        A dictionary containing performance data against which the constraints will be evaluated.
-    constraints : list of dicts
-        A list of constraint definitions, where each constraint is defined as a dictionary.
-        Each dictionary must have 'variable' (str), 'type' (str, one of '=', '>', '<'), and 'value' (numeric).
-
-    Returns
-    -------
-    tuple of numpy.ndarray
-        Returns two numpy arrays: the first is an array of equality constraints, and the second is an array of
-        inequality constraints. These arrays are flattened and concatenated from the evaluated constraint values.
-
-    Raises
-    ------
-    ValueError
-        If an unknown constraint type is specified in the constraints list.
-    """
-    # Initialize constraint lists
-    c_eq = []  # Equality constraints
-    c_ineq = []  # Inequality constraints
-
-    # Loop over all constraint from configuration file
-    for constraint in constraints:
-        name = constraint["variable"]
-        constraint_type = constraint["type"]
-        target = constraint["value"]
-        normalize = constraint.get("normalize", False)
-
-        # Get the performance value for the given variable name
-        current = render_and_evaluate(name, data)
-        if isinstance(target, str):  # Try to render variable when not a number
-            target = render_and_evaluate(target, data)
-
-        # Evaluate constraint
-        # mismatch = current - target
-        mismatch = target - current
-
-        # Normalize constraint according to specifications
-        normalize_factor = normalize if num.is_numeric(normalize) else target
-        if normalize is not False:
-            if normalize_factor == 0:
-                raise ValueError(
-                    f"Cannot normalize constraint '{name} {constraint_type} {target}' because the normalization factor is '{normalize_factor}' (division by zero)."
-                )
-            mismatch /= normalize_factor
-
-        # Add constraints to lists
-        if constraint_type == "=":
-            c_eq.append(mismatch)
-        elif constraint_type == ">":
-            c_ineq.append(mismatch)
-        elif constraint_type == "<":
-            # Change sign because optimizer handles c_ineq > 0
-            c_ineq.append(-mismatch)
-        else:
-            raise ValueError(f"Unknown constraint type: {constraint_type}")
-
-    # Flatten and concatenate constraints
-    c_eq = np.hstack([np.atleast_1d(item) for item in c_eq]) if c_eq else np.array([])
-    c_ineq = (
-        np.hstack([np.atleast_1d(item) for item in c_ineq]) if c_ineq else np.array([])
-    )
-
-    return c_eq, c_ineq
-
-
-def evaluate_objective_function(data, objective_function):
-    """
-    Evaluates the objective function based on the provided data and configuration.
-
-    Parameters
-    ----------
-    data : dict
-        A dictionary containing performance data against which the objective function will be evaluated.
-    objective_function : dict
-        A dictionary defining the objective function. It must have 'variable' (str)
-        and 'type' (str, either 'minimize' or 'maximize').
-
-    Returns
-    -------
-    float
-        The value of the objective function, adjusted for optimization. Positive for minimization and
-        negative for maximization.
-
-    Raises
-    ------
-    ValueError
-        If an unknown objective function type is specified in the configuration.
-    """
-
-    # Get the performance value for the given variable name
-    name = objective_function["variable"]
-    type = objective_function["type"]
-    value = render_and_evaluate(name, data)
-
-    if not np.isscalar(value):
-        raise ValueError(
-            f"The objective function '{name}' must be an scalar, but the value is: {value}"
-        )
-
-    if type == "minimize":
-        return value
-    elif type == "maximize":
-        return -value
-    else:
-        raise ValueError(f"Unknown objective function type: {type}")
+import re
+import yaml
+import numbers
+import numpy as np
+
+from . import numerics as num
+
+
+# TODO: @Lasse: We need some of these functions for the cycle optimization, and eventually also for turbomachinery optimization
+# TODO: We have to discuss how to handle "rendered" variables
+
+
+def render_and_evaluate(expression, data):
+    """
+    Render variables prefixed with '$' in an expression and evaluate the resulting expression.
+
+    This function processes an input string `expr`, identifying all occurrences of variables
+    indicated by a leading '$' symbol. Each such variable is resolved to its value from the
+    provided `context` dictionary. The expression with all variables resolved is then evaluated
+    and the result is returned.
+
+    This function is useful to render strings defined in a YAML configuration file to values
+    that are calculated within the code and stored in a dicitonary.
+
+    Parameters
+    ----------
+    expr : str
+        The expression string containing variables to be rendered. Variables in the
+        expression are expected to be prefixed with a '$' symbol.
+    data : dict
+        A dictionary containing variables and their corresponding values. These variables
+        are used to render values in the expression.
+
+    Returns
+    -------
+    The result of evaluating the rendered expression. The type of the result depends on the
+    expression.
+
+    Notes
+    -----
+    - `pattern`: A regular expression pattern used to identify variables within the expression.
+      Variables are expected to be in the format `$variableName`, potentially with dot-separated
+      sub-properties (e.g., `$variable.property`).
+
+    - `replace_with_value`: An inner function that takes a regex match object and returns
+      the value of the variable from `context`. `match.group(1)` returns the first captured
+      group from the matched text, which in this case is the variable name excluding the
+      leading '$' symbol. For example, in `$variableName`, `match.group(1)` would return
+      `variableName`.
+
+    - The function uses Python's `eval` for evaluation, which should be used cautiously as
+      it can execute arbitrary code. Ensure that the context and expressions are from a trusted
+      source.
+    """
+    # Pattern to find $variable expressions
+    pattern = re.compile(r"\$(\w+(\.\w+)*)")
+
+    # Function to replace each match with its resolved value
+    def replace_with_value(match):
+        nested_key = match.group(1)
+        try:
+            value = render_nested_value(nested_key, data)
+            if isinstance(value, np.ndarray):
+                return "np.array(" + repr(value.tolist()) + ")"
+            else:
+                return repr(value)
+        except KeyError:
+            raise KeyError(
+                f"Variable '{nested_key}' not found in the provided data context."
+            )
+
+    try:
+        # Replace all $variable with their actual values
+        resolved_expr = pattern.sub(replace_with_value, expression)
+
+        # Check if any unresolved variables remain
+        if "$" in resolved_expr:
+            raise ValueError(f"Unresolved variable in expression: '{resolved_expr}'")
+
+        # Now evaluate the expression
+        return eval(resolved_expr)
+
+    except SyntaxError as e:
+        raise SyntaxError(f"Syntax error in '{expression}': {e}")
+    except Exception as e:
+        # Enhanced error message
+        raise TypeError(
+            f"Error evaluating expression '{expression}': {e}.\n"
+            "If the expression is meant to use data from the configuration, "
+            "ensure each variable is prefixed with '$'. For example, use '$variable_name' "
+            "instead of 'variable_name'."
+        )
+
+
+def render_nested_value(nested_key, data):
+    """
+    Retrieves a value from a nested structure (dictionaries or objects with attributes) using a dot-separated key.
+
+    This function is designed to navigate through a combination of dictionaries and objects. For an object to be
+    compatible with this function, it must implement a `keys()` method that returns its attribute names.
+
+    This function is intended as a subroutine of the more genera ``render_expression``
+
+    Parameters
+    ----------
+    nested_key : str
+        A dot-separated key string that specifies the path in the structure.
+        For example, 'level1.level2.key' will retrieve data['level1']['level2']['key'] if data is a dictionary,
+        or data.level1.level2.key if data is an object or a combination of dictionaries and objects.
+
+    data : dict or object
+        The starting dictionary or object from which to retrieve the value. This can be a nested structure
+        of dictionaries and objects.
+
+    Returns
+    -------
+    value
+        The value retrieved from the nested structure using the specified key.
+        The type of the value depends on what is stored at the specified key in the structure.
+
+    Raises
+    ------
+    KeyError
+        If the specified nested key is not found in the data structure. The error message includes the part
+        of the path that was successfully traversed and the available keys or attributes at the last valid level.
+    """
+    keys = nested_key.split(".")
+    value = data
+    traversed_path = []
+
+    for key in keys:
+        if isinstance(value, dict):
+            # Handle dictionary-like objects
+            if key in value:
+                traversed_path.append(key)
+                value = value[key]
+            else:
+                valid_keys = ", ".join(value.keys())
+                traversed_path_str = (
+                    ".".join(traversed_path) if traversed_path else "root"
+                )
+                raise KeyError(
+                    f"Nested key '{key}' not found at '{traversed_path_str}'. Available keys: {valid_keys}"
+                )
+        elif hasattr(value, key):
+            # Handle objects with attributes
+            traversed_path.append(key)
+            value = getattr(value, key)
+        else:
+            traversed_path_str = ".".join(traversed_path)
+            available_keys = ", ".join(value.keys())
+            raise KeyError(
+                f"Key '{key}' not found in object at '{traversed_path_str}'. Available keys: {available_keys}"
+            )
+
+    if not num.is_numeric(value):
+        raise ValueError(
+            f"The key '{nested_key}' is not numeric. Key value is: {value}"
+        )
+
+    return value
+
+
+def evaluate_constraints(data, constraints):
+    """
+    Evaluates the constraints based on the provided data and constraint definitions.
+
+    Parameters
+    ----------
+    data : dict
+        A dictionary containing performance data against which the constraints will be evaluated.
+    constraints : list of dicts
+        A list of constraint definitions, where each constraint is defined as a dictionary.
+        Each dictionary must have 'variable' (str), 'type' (str, one of '=', '>', '<'), and 'value' (numeric).
+
+    Returns
+    -------
+    tuple of numpy.ndarray
+        Returns two numpy arrays: the first is an array of equality constraints, and the second is an array of
+        inequality constraints. These arrays are flattened and concatenated from the evaluated constraint values.
+
+    Raises
+    ------
+    ValueError
+        If an unknown constraint type is specified in the constraints list.
+    """
+    # Initialize constraint lists
+    c_eq = []  # Equality constraints
+    c_ineq = []  # Inequality constraints
+
+    # Loop over all constraint from configuration file
+    for constraint in constraints:
+        name = constraint["variable"]
+        constraint_type = constraint["type"]
+        target = constraint["value"]
+        normalize = constraint.get("normalize", False)
+
+        # Get the performance value for the given variable name
+        current = render_and_evaluate(name, data)
+        if isinstance(target, str):  # Try to render variable when not a number
+            target = render_and_evaluate(target, data)
+
+        # Evaluate constraint
+        # mismatch = current - target
+        mismatch = target - current
+
+        # Normalize constraint according to specifications
+        normalize_factor = normalize if num.is_numeric(normalize) else target
+        if normalize is not False:
+            if normalize_factor == 0:
+                raise ValueError(
+                    f"Cannot normalize constraint '{name} {constraint_type} {target}' because the normalization factor is '{normalize_factor}' (division by zero)."
+                )
+            mismatch /= normalize_factor
+
+        # Add constraints to lists
+        if constraint_type == "=":
+            c_eq.append(mismatch)
+        elif constraint_type == ">":
+            c_ineq.append(mismatch)
+        elif constraint_type == "<":
+            # Change sign because optimizer handles c_ineq > 0
+            c_ineq.append(-mismatch)
+        else:
+            raise ValueError(f"Unknown constraint type: {constraint_type}")
+
+    # Flatten and concatenate constraints
+    c_eq = np.hstack([np.atleast_1d(item) for item in c_eq]) if c_eq else np.array([])
+    c_ineq = (
+        np.hstack([np.atleast_1d(item) for item in c_ineq]) if c_ineq else np.array([])
+    )
+
+    return c_eq, c_ineq
+
+
+def evaluate_objective_function(data, objective_function):
+    """
+    Evaluates the objective function based on the provided data and configuration.
+
+    Parameters
+    ----------
+    data : dict
+        A dictionary containing performance data against which the objective function will be evaluated.
+    objective_function : dict
+        A dictionary defining the objective function. It must have 'variable' (str)
+        and 'type' (str, either 'minimize' or 'maximize').
+
+    Returns
+    -------
+    float
+        The value of the objective function, adjusted for optimization. Positive for minimization and
+        negative for maximization.
+
+    Raises
+    ------
+    ValueError
+        If an unknown objective function type is specified in the configuration.
+    """
+
+    # Get the performance value for the given variable name
+    name = objective_function["variable"]
+    type = objective_function["type"]
+    value = render_and_evaluate(name, data)
+
+    if not np.isscalar(value):
+        raise ValueError(
+            f"The objective function '{name}' must be an scalar, but the value is: {value}"
+        )
+
+    if type == "minimize":
+        return value
+    elif type == "maximize":
+        return -value
+    else:
+        raise ValueError(f"Unknown objective function type: {type}")
```

### Comparing `turboflow-0.1.2/PKG-INFO` & `turboflow-0.1.3/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: turboflow
-Version: 0.1.2
+Version: 0.1.3
 Summary: A Python tool for meanline turbomachinery design and analysis.
 License: MIT
 Author: Lasse Borg & Roberto Agromayor
 Requires-Python: >=3.11,<4.0
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.11
@@ -284,7 +284,18 @@
 
 
 Additionally, in order to use snopt you have to define the environmental variables for the license file and the xx.
 
 You can do this in windows following this tutorial, or by addtion these lines to you bashrc file in linux or if using gitbash in windows
 
 instructions for snopt DLLs
+
+
+
+
+
+
+## to create a new version and release to pypi
+
+bumpversion patch  # This increments the version, updates relevant files, commits the changes, and creates a new tag.
+git push origin main --tags  # This pushes the `main` branch and the new tag to the remote repository.
+
```

